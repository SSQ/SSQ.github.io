<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Machine Learning,Coursera," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="For quick searchingCourse can be found hereLecture slides can be found in my Github">
<meta name="keywords" content="Machine Learning,Coursera">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera UW Machine Learning Specialization Notebook">
<meta property="og:url" content="https://ssq.github.io/2017/08/19/Coursera UW Machine Learning Specialization Notebook/index.html">
<meta property="og:site_name" content="SSQ">
<meta property="og:description" content="For quick searchingCourse can be found hereLecture slides can be found in my Github">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/2.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/3.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_1/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_1/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.2.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.2.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/2.1.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.3.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.4.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.5.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.2.2.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/2.1.2.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/3.1.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.1.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.2.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/2.1.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.1.2.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.1.3.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_3/1.1.1.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_3/1.1.2.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_3/1.1.3.PNG?imgeslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.5.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.6.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.7.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.8.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/2.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/2.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/3.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/3.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/4.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/4.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/5.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/5.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week_3_PA/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week_3_PA/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.5.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.6.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.7.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.5.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.5.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.2.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.5.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.2.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.6.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.7.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.2.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/3.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/2.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.1.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.2.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.3.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.4.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.5.PNG?imageslim">
<meta property="og:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/2.1.1.PNG?imageslim">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/ySTNZ-A8EeWIdgqHxZs34w_8a123d76165a120ca663408950b04939_Capture.PNG?expiry=1503878400000&hmac=a0gg3A7dJ9eVtVIudcv8jtCD7OLm4cWyoGoMXFrp5wQ">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Fh-c2uA_EeWOVQ68c1xy2w_112e2746a771957f31fccced680c1482_Capture.PNG?expiry=1503878400000&hmac=Tw3Rm2SZCbEEIwetmoEQPyTdpFAfqnbE6OVk17LYloY">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/s0aljeWUEeWufRJaRfO1AQ_3134541d802ae1aaee26daec7ef39ca3_L_3SbuBaEeW--hK2gi_BIw_fbbbb77fd44af452b3d337434cdb7702_Capture.png?expiry=1503964800000&hmac=s_bRnYQIsuZUJESwIKD6wLwaWasv7xOzJFa2cLV79cQ">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/xUjv0uWUEeWuUgrcWIxPhQ_ac77661c16c2fb40c0c3abfbf076a424_rvPx2eD1EeWOVQ68c1xy2w_ad1639c57963f5eaea6612cb568c4d86_Capture.png?expiry=1503964800000&hmac=GdPLOJl2j_eyVFAn9AaQ0XzTYiEUQ1eYBwZ9tl1Q9fs">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/2f4k6eWUEeWIdgqHxZs34w_1ee2b9e5512622d0ad3a587b44c49922_JcoDWuBbEeWufRJaRfO1AQ_ff849715b543c56709a46f7be7a14c5d_Capture.png?expiry=1503964800000&hmac=_rSGs1Pa3PD47CWmtJF1GEUnQSyIs10UQrphSivVp40">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/5YNc7eWUEeWOVQ68c1xy2w_768b146d738d6e1991ac9d5f678dbef6_9wGVMuBbEeW--hK2gi_BIw_27e6398723f122d80d4caae557763566_Capture.png?expiry=1503964800000&hmac=4o7FiAVcDo20hF_Tlji5buHyFP0hIphb_VECn6vQ-3c">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/A16k-eWVEeW--hK2gi_BIw_0d7afdaf1bd9730fd7b5c51c424f1b95_aJDg6eD2EeWuUgrcWIxPhQ_81584c7620804c5d7093ee0063171269_Capture.png?expiry=1503964800000&hmac=gnxbzQTWLUk1rnv-Tqe2xhEIWMil9FC--JUdkLbdFwk">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/D3JhYeWVEeWufRJaRfO1AQ_0569b1c0c9a8bd6798c91d817f3aef7c_aJbicuBcEeW--hK2gi_BIw_d09a4fde17773e1f5f1a270b3b6d357a_Capture.png?expiry=1503964800000&hmac=Zh5JXL_7gxq_2zW_UBzBZadfGtaw73movErVQufVErM">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Ht-B8uWVEeWufRJaRfO1AQ_bd596c09a8282ec2de28b358ca5f171d_cLTtQeD2EeWufRJaRfO1AQ_4f1f5b7a19b8e305c40e11e7578b1d38_Capture2.png?expiry=1503964800000&hmac=EnaRpFw3Ci_ja1v-FaBZyGOgSIrMKdjpKMIoaTzqF5w">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Mf6t5uWVEeWuUgrcWIxPhQ_2a643ffaf830d9218db5a6314936d52f_V4_Ld-BdEeWIdgqHxZs34w_dfc3556048dfc2bc157ce8bf58d6d74a_Capture.png?expiry=1503964800000&hmac=xM8ZtdtqwNjBDzev1VaowhWpAwdx0YlbhFWwr46Ed_0">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/HswHyeBjEeWufRJaRfO1AQ_b5746ae36ef317b86c126c93db145c34_Capture2.PNG?expiry=1505692800000&hmac=LMnZL5gdQFZMMBxMXXpoPxENUFDAvnbZlfMRhwH-RIU">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/S8wkfeD0EeWIdgqHxZs34w_bee4e1ecb5c161de4cb755e30ebe0576_Capture.PNG?expiry=1505692800000&hmac=wxPmRHwhhwoS0414YzGLdJ_-sXRc5tL9nzeiiBT6YIw">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/3-kqZeD0EeW--hK2gi_BIw_7d5bf2ab0433c80a45f1593efdefc45f_Capture2.PNG?expiry=1505692800000&hmac=Me2wotTAmDMsD-U1EFKwC_BYogs4XOwZtwkVvXBmfJ8">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/6F6K0uD0EeWOVQ68c1xy2w_22bc6b9e118905c85b278d32017fbb9a_Capture3.PNG?expiry=1505692800000&hmac=vJi2NzLNS4WoLAH8R7EHjb0HnhkXGZ9-adKsEX362Zg">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/7_VxOuD0EeWuUgrcWIxPhQ_6fab04ae0606eca1b06362bd07d56aee_Capture4.PNG?expiry=1505692800000&hmac=GomRfwsO8J4iA80zcybuUUAu-UwXIdaocMYpcZXHRz8">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/TDEYl-BmEeWuUgrcWIxPhQ_3c704bfa2b87d7dc0429d29ac709c690_Capture.PNG?expiry=1506902400000&hmac=-1fK0E--IjAPFZXgFOxmbB0UrwVNqzJVhMAzrf0lcG0">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/VyRs8uBtEeWBDQ73-3lhaw_70d1c456f3deba4ffa876e53e041cf7a_Capture.PNG?expiry=1507507200000&hmac=GPeyl-jB1hlWTusMbX5OytaN50LkQY7_shrlHJ6mYM0">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Ym5JpOBtEeWBDQ73-3lhaw_8f2bb6e530dc2b481646181bf6e72a80_Capture2.PNG?expiry=1507507200000&hmac=gZL-m8ELhykkG1o7vHWKLDB97Iz438-YlQberFWebpU">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bS2ZaOBtEeWufRJaRfO1AQ_8d67c76c9a6715f27ebc78fb8df13c0a_Capture3.PNG?expiry=1507507200000&hmac=VxYGJqOhG-WnXEmRD3btsmFP6EXgqAv8AD41kKDXcGU">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/oj-RfOBtEeWOVQ68c1xy2w_6f018b2d10620f6857e5580f40c7eeb5_Capture4.PNG?expiry=1507507200000&hmac=Gj_7ydXkVqT1m-9yoyzMlsxiemUQWwLqdkfHgkUZs4M">
<meta property="og:image" content="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R7CFGk4REeaubA6-qtnryw_1357fe84a2ecd33f4d95bb69f6ebbf89_Capture.PNG?expiry=1508889600000&hmac=O37bVp0hZLEyv9F_xIvf0IXq-qAXPr8qXCDRTWXOC5A">
<meta property="og:updated_time" content="2017-11-18T16:33:04.608Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera UW Machine Learning Specialization Notebook">
<meta name="twitter:description" content="For quick searchingCourse can be found hereLecture slides can be found in my Github">
<meta name="twitter:image" content="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.1.PNG?imageslim">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://ssq.github.io/2017/08/19/Coursera UW Machine Learning Specialization Notebook/"/>





  <title>Coursera UW Machine Learning Specialization Notebook | SSQ</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-91307065-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3c8ad2ecdd2387b44044b1d7cd3536a9";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SSQ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-friends">
          <a href="/friends" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-link"></i> <br />
            
            friends
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://ssq.github.io/2017/08/19/Coursera UW Machine Learning Specialization Notebook/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="SSQ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SSQ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Coursera UW Machine Learning Specialization Notebook</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-19T09:00:49+08:00">
                2017-08-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/19/Coursera UW Machine Learning Specialization Notebook/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/08/19/Coursera UW Machine Learning Specialization Notebook/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/08/19/Coursera UW Machine Learning Specialization Notebook/" class="leancloud_visitors" data-flag-title="Coursera UW Machine Learning Specialization Notebook">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <div class="note primary"><p>For quick searching<br>Course can be found <a href="https://www.coursera.org/specializations/machine-learning" target="_blank" rel="external">here</a><br>Lecture slides can be found in my Github</p>
</div>
<a id="more"></a>
<div class="note primary"><p>This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.</p>
</div>
<h1 id="Machine-Learning-Foundations-A-Case-Study-Approach"><a href="#Machine-Learning-Foundations-A-Case-Study-Approach" class="headerlink" title="Machine Learning Foundations: A Case Study Approach"></a>Machine Learning Foundations: A Case Study Approach</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/ml-foundations" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/tree/master/Lecture%20Slides" target="_blank" rel="external">here</a></p>
<p>About this course: Do you have data and wonder what it can tell you?  Do you need a deeper understanding of the core ways in which machine learning can improve your business?  Do you want to be able to converse with specialists about anything from regression and classification to deep learning and recommender systems?</p>
<p>In this course, you will get hands-on experience with machine learning from a series of practical case-studies.  At the end of the first course you will have studied </p>
<ol>
<li>how to predict house prices based on house-level features, </li>
<li>analyze sentiment from user reviews, </li>
<li>retrieve documents of interest, </li>
<li>recommend products, </li>
<li>and search for images.  </li>
</ol>
<p>Through hands-on practice with these use cases, you will be able to apply machine learning methods in a wide range of domains.</p>
<p>This first course treats the machine learning method as a black box.  Using this abstraction, you will focus on understanding tasks of interest, matching these tasks to machine learning tools, and assessing the quality of the output. In subsequent courses, you will delve into the components of this black box by examining models and algorithms.  Together, these pieces form the machine learning pipeline, which you will use in developing intelligent applications.</p>
<p>Learning Outcomes:  By the end of this course, you will be able to:<br>   -Identify potential applications of machine learning in practice.<br>   -Describe the core differences in analyses enabled by regression, classification, and clustering.<br>   -Select the appropriate machine learning task for a potential application.<br>   -Apply regression, classification, clustering, retrieval, recommender systems, and deep learning.<br>   -Represent your data as features to serve as input to machine learning models.<br>   -Assess the model quality in terms of relevant error metrics for each task.<br>   -Utilize a dataset to fit a model to analyze new data.<br>   -Build an end-to-end application that uses machine learning at its core.<br>   -Implement these techniques in Python.</p>
<p>Welcome to Machine Learning Foundations: A Case Study Approach! By joining this course, you’ve taken a first step in becoming a machine learning expert. You will learn a broad range of machine learning methods for deriving intelligence from data, and by the end of the course you will be able to implement actual intelligent applications. These applications will allow you to perform predictions, personalized recommendations and retrieval, and much more. If you continue with the subsequent courses in the Machine Learning specialization, you will delve deeper into the methods and algorithms, giving you the power to develop and deploy new machine learning services.</p>
<p>To begin, we recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. These assignments—one per Module 2 through 6—will walk you through Python implementations of intelligent applications for:</p>
<ul>
<li>Predicting house prices</li>
<li>Analyzing the sentiment of product reviews</li>
<li>Retrieving Wikipedia articles</li>
<li>Recommending songs</li>
<li>Classifying images with deep learning</li>
</ul>
</div>
<h2 id="Week-1-Welcome"><a href="#Week-1-Welcome" class="headerlink" title="Week 1 Welcome"></a>Week 1 Welcome</h2><div class="note primary"><p>Machine learning is everywhere, but is often operating behind the scenes.</p>
<p>This introduction to the specialization provides you with insights into the power of machine learning, and the multitude of intelligent applications you personally will be able to develop and deploy upon completion.</p>
<p>We also discuss who we are, how we got here, and our view of the future of intelligent applications.</p>
</div>
<h3 id="Why-you-should-learn-machine-learning-with-us"><a href="#Why-you-should-learn-machine-learning-with-us" class="headerlink" title="Why you should learn machine learning with us"></a>Why you should learn machine learning with us</h3><h4 id="Important-Update-regarding-the-Machine-Learning-Specialization10-min"><a href="#Important-Update-regarding-the-Machine-Learning-Specialization10-min" class="headerlink" title="Important Update regarding the Machine Learning Specialization10 min"></a>Important Update regarding the Machine Learning Specialization10 min</h4><h4 id="Slides-presented-in-this-module10-min"><a href="#Slides-presented-in-this-module10-min" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here: <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Foundations-A-Case-Study-Approach/blob/master/Lecture%20Slides/1.0-intro.pdf" target="_blank" rel="external">intro.pdf</a></p>
<h4 id="Welcome-to-this-course-and-specialization41-sec"><a href="#Welcome-to-this-course-and-specialization41-sec" class="headerlink" title="Welcome to this course and specialization41 sec"></a>Welcome to this course and specialization41 sec</h4><h4 id="Who-we-are5-min"><a href="#Who-we-are5-min" class="headerlink" title="Who we are5 min"></a>Who we are5 min</h4><h4 id="Machine-learning-is-changing-the-world3-min"><a href="#Machine-learning-is-changing-the-world3-min" class="headerlink" title="Machine learning is changing the world3 min"></a>Machine learning is changing the world3 min</h4><h4 id="Why-a-case-study-approach-7-min"><a href="#Why-a-case-study-approach-7-min" class="headerlink" title="Why a case study approach?7 min"></a>Why a case study approach?7 min</h4><h4 id="Specialization-overview6-min"><a href="#Specialization-overview6-min" class="headerlink" title="Specialization overview6 min"></a>Specialization overview6 min</h4><h3 id="Who-this-specialization-is-for-and-what-you-will-be-able-to-do"><a href="#Who-this-specialization-is-for-and-what-you-will-be-able-to-do" class="headerlink" title="Who this specialization is for and what you will be able to do"></a>Who this specialization is for and what you will be able to do</h3><h4 id="How-we-got-into-ML3-min"><a href="#How-we-got-into-ML3-min" class="headerlink" title="How we got into ML3 min"></a>How we got into ML3 min</h4><h4 id="Who-is-this-specialization-for-4-min"><a href="#Who-is-this-specialization-for-4-min" class="headerlink" title="Who is this specialization for?4 min"></a>Who is this specialization for?4 min</h4><h4 id="What-you’ll-be-able-to-do57-sec"><a href="#What-you’ll-be-able-to-do57-sec" class="headerlink" title="What you’ll be able to do57 sec"></a>What you’ll be able to do57 sec</h4><h4 id="The-capstone-and-an-example-intelligent-application6-min"><a href="#The-capstone-and-an-example-intelligent-application6-min" class="headerlink" title="The capstone and an example intelligent application6 min"></a>The capstone and an example intelligent application6 min</h4><h4 id="The-future-of-intelligent-applications2-min"><a href="#The-future-of-intelligent-applications2-min" class="headerlink" title="The future of intelligent applications2 min"></a>The future of intelligent applications2 min</h4><h3 id="Getting-started-with-the-tools-for-the-course"><a href="#Getting-started-with-the-tools-for-the-course" class="headerlink" title="Getting started with the tools for the course"></a>Getting started with the tools for the course</h3><h4 id="Reading-Getting-started-with-Python-IPython-Notebook-amp-GraphLab-Create10-min"><a href="#Reading-Getting-started-with-Python-IPython-Notebook-amp-GraphLab-Create10-min" class="headerlink" title="Reading: Getting started with Python, IPython Notebook &amp; GraphLab Create10 min"></a>Reading: Getting started with Python, IPython Notebook &amp; GraphLab Create10 min</h4><h4 id="Reading-where-should-my-files-go-10-min"><a href="#Reading-where-should-my-files-go-10-min" class="headerlink" title="Reading: where should my files go?10 min"></a>Reading: where should my files go?10 min</h4><h3 id="Getting-started-with-Python-and-the-IPython-Notebook"><a href="#Getting-started-with-Python-and-the-IPython-Notebook" class="headerlink" title="Getting started with Python and the IPython Notebook"></a>Getting started with Python and the IPython Notebook</h3><h4 id="Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min"><a href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min" class="headerlink" title="Download the IPython Notebook used in this lesson to follow along10 min"></a>Download the IPython Notebook used in this lesson to follow along10 min</h4><h4 id="Starting-an-IPython-Notebook5-min"><a href="#Starting-an-IPython-Notebook5-min" class="headerlink" title="Starting an IPython Notebook5 min"></a>Starting an IPython Notebook5 min</h4><h4 id="Creating-variables-in-Python7-min"><a href="#Creating-variables-in-Python7-min" class="headerlink" title="Creating variables in Python7 min"></a>Creating variables in Python7 min</h4><h4 id="Conditional-statements-and-loops-in-Python8-min"><a href="#Conditional-statements-and-loops-in-Python8-min" class="headerlink" title="Conditional statements and loops in Python8 min"></a>Conditional statements and loops in Python8 min</h4><h4 id="Creating-functions-and-lambdas-in-Python3-min"><a href="#Creating-functions-and-lambdas-in-Python3-min" class="headerlink" title="Creating functions and lambdas in Python3 min"></a>Creating functions and lambdas in Python3 min</h4><h3 id="Getting-started-with-SFrames-for-data-engineering-and-analysis"><a href="#Getting-started-with-SFrames-for-data-engineering-and-analysis" class="headerlink" title="Getting started with SFrames for data engineering and analysis"></a>Getting started with SFrames for data engineering and analysis</h3><h4 id="Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-1"><a href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-1" class="headerlink" title="Download the IPython Notebook used in this lesson to follow along10 min"></a>Download the IPython Notebook used in this lesson to follow along10 min</h4><h4 id="Starting-GraphLab-Create-amp-loading-an-SFrame4-min"><a href="#Starting-GraphLab-Create-amp-loading-an-SFrame4-min" class="headerlink" title="Starting GraphLab Create &amp; loading an SFrame4 min"></a>Starting GraphLab Create &amp; loading an SFrame4 min</h4><h4 id="Canvas-for-data-visualization4-min"><a href="#Canvas-for-data-visualization4-min" class="headerlink" title="Canvas for data visualization4 min"></a>Canvas for data visualization4 min</h4><h4 id="Interacting-with-columns-of-an-SFrame4-min"><a href="#Interacting-with-columns-of-an-SFrame4-min" class="headerlink" title="Interacting with columns of an SFrame4 min"></a>Interacting with columns of an SFrame4 min</h4><h4 id="Using-apply-for-data-transformation5-min"><a href="#Using-apply-for-data-transformation5-min" class="headerlink" title="Using .apply() for data transformation5 min"></a>Using .apply() for data transformation5 min</h4><h2 id="Week-2-Regression-Predicting-House-Prices"><a href="#Week-2-Regression-Predicting-House-Prices" class="headerlink" title="Week 2 Regression: Predicting House Prices"></a>Week 2 Regression: Predicting House Prices</h2><div class="note primary"><p>This week you will build your first intelligent application that makes predictions from data.</p>
<p>We will explore this idea within the context of our first case study, predicting house prices, where you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,…).</p>
<p>This is just one of the many places where regression can be applied.Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.</p>
<p>You will also examine how to analyze the performance of your predictive model and implement regression in practice using an iPython notebook.</p>
</div>
<h3 id="Linear-regression-modeling"><a href="#Linear-regression-modeling" class="headerlink" title="Linear regression modeling"></a>Linear regression modeling</h3><h4 id="Slides-presented-in-this-module10-min-1"><a href="#Slides-presented-in-this-module10-min-1" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here: <a href="">regression-intro-annotated.pdf</a></p>
<h4 id="Predicting-house-prices-A-case-study-in-regression1-min"><a href="#Predicting-house-prices-A-case-study-in-regression1-min" class="headerlink" title="Predicting house prices: A case study in regression1 min"></a>Predicting house prices: A case study in regression1 min</h4><h4 id="What-is-the-goal-and-how-might-you-naively-address-it-3-min"><a href="#What-is-the-goal-and-how-might-you-naively-address-it-3-min" class="headerlink" title="What is the goal and how might you naively address it?3 min"></a>What is the goal and how might you naively address it?3 min</h4><h4 id="Linear-Regression-A-Model-Based-Approach5-min"><a href="#Linear-Regression-A-Model-Based-Approach5-min" class="headerlink" title="Linear Regression: A Model-Based Approach5 min"></a>Linear Regression: A Model-Based Approach5 min</h4><h4 id="Adding-higher-order-effects4-min"><a href="#Adding-higher-order-effects4-min" class="headerlink" title="Adding higher order effects4 min"></a>Adding higher order effects4 min</h4><h3 id="Evaluating-regression-models"><a href="#Evaluating-regression-models" class="headerlink" title="Evaluating regression models"></a>Evaluating regression models</h3><h4 id="Evaluating-overfitting-via-training-test-split6-min"><a href="#Evaluating-overfitting-via-training-test-split6-min" class="headerlink" title="Evaluating overfitting via training/test split6 min"></a>Evaluating overfitting via training/test split6 min</h4><h4 id="Training-test-curves4-min"><a href="#Training-test-curves4-min" class="headerlink" title="Training/test curves4 min"></a>Training/test curves4 min</h4><h4 id="Adding-other-features2-min"><a href="#Adding-other-features2-min" class="headerlink" title="Adding other features2 min"></a>Adding other features2 min</h4><h4 id="Other-regression-examples3-min"><a href="#Other-regression-examples3-min" class="headerlink" title="Other regression examples3 min"></a>Other regression examples3 min</h4><h3 id="Summary-of-regression"><a href="#Summary-of-regression" class="headerlink" title="Summary of regression"></a>Summary of regression</h3><h4 id="Regression-ML-block-diagram5-min"><a href="#Regression-ML-block-diagram5-min" class="headerlink" title="Regression ML block diagram5 min"></a>Regression ML block diagram5 min</h4><h4 id="Quiz-Regression9-questions"><a href="#Quiz-Regression9-questions" class="headerlink" title="Quiz: Regression9 questions"></a>Quiz: Regression9 questions</h4><h3 id="Predicting-house-prices-IPython-Notebook"><a href="#Predicting-house-prices-IPython-Notebook" class="headerlink" title="Predicting house prices: IPython Notebook"></a>Predicting house prices: IPython Notebook</h3><h4 id="Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-2"><a href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-2" class="headerlink" title="Download the IPython Notebook used in this lesson to follow along10 min"></a>Download the IPython Notebook used in this lesson to follow along10 min</h4><h4 id="Loading-amp-exploring-house-sale-data7-min"><a href="#Loading-amp-exploring-house-sale-data7-min" class="headerlink" title="Loading &amp; exploring house sale data7 min"></a>Loading &amp; exploring house sale data7 min</h4><h4 id="Splitting-the-data-into-training-and-test-sets2-min"><a href="#Splitting-the-data-into-training-and-test-sets2-min" class="headerlink" title="Splitting the data into training and test sets2 min"></a>Splitting the data into training and test sets2 min</h4><h4 id="Learning-a-simple-regression-model-to-predict-house-prices-from-house-size3-min"><a href="#Learning-a-simple-regression-model-to-predict-house-prices-from-house-size3-min" class="headerlink" title="Learning a simple regression model to predict house prices from house size3 min"></a>Learning a simple regression model to predict house prices from house size3 min</h4><h4 id="Evaluating-error-RMSE-of-the-simple-model2-min"><a href="#Evaluating-error-RMSE-of-the-simple-model2-min" class="headerlink" title="Evaluating error (RMSE) of the simple model2 min"></a>Evaluating error (RMSE) of the simple model2 min</h4><h4 id="Visualizing-predictions-of-simple-model-with-Matplotlib4-min"><a href="#Visualizing-predictions-of-simple-model-with-Matplotlib4-min" class="headerlink" title="Visualizing predictions of simple model with Matplotlib4 min"></a>Visualizing predictions of simple model with Matplotlib4 min</h4><h4 id="Inspecting-the-model-coefficients-learned1-min"><a href="#Inspecting-the-model-coefficients-learned1-min" class="headerlink" title="Inspecting the model coefficients learned1 min"></a>Inspecting the model coefficients learned1 min</h4><h4 id="Exploring-other-features-of-the-data6-min"><a href="#Exploring-other-features-of-the-data6-min" class="headerlink" title="Exploring other features of the data6 min"></a>Exploring other features of the data6 min</h4><h4 id="Learning-a-model-to-predict-house-prices-from-more-features3-min"><a href="#Learning-a-model-to-predict-house-prices-from-more-features3-min" class="headerlink" title="Learning a model to predict house prices from more features3 min"></a>Learning a model to predict house prices from more features3 min</h4><h4 id="Applying-learned-models-to-predict-price-of-an-average-house5-min"><a href="#Applying-learned-models-to-predict-price-of-an-average-house5-min" class="headerlink" title="Applying learned models to predict price of an average house5 min"></a>Applying learned models to predict price of an average house5 min</h4><h4 id="Applying-learned-models-to-predict-price-of-two-fancy-houses7-min"><a href="#Applying-learned-models-to-predict-price-of-two-fancy-houses7-min" class="headerlink" title="Applying learned models to predict price of two fancy houses7 min"></a>Applying learned models to predict price of two fancy houses7 min</h4><h3 id="Programming-assignment"><a href="#Programming-assignment" class="headerlink" title="Programming assignment"></a>Programming assignment</h3><h4 id="Reading-Predicting-house-prices-assignment10-min"><a href="#Reading-Predicting-house-prices-assignment10-min" class="headerlink" title="Reading: Predicting house prices assignment10 min"></a>Reading: Predicting house prices assignment10 min</h4><h4 id="Quiz-Predicting-house-prices3-questions"><a href="#Quiz-Predicting-house-prices3-questions" class="headerlink" title="Quiz: Predicting house prices3 questions"></a>Quiz: Predicting house prices3 questions</h4><h2 id="Week-3-Classification-Analyzing-Sentiment"><a href="#Week-3-Classification-Analyzing-Sentiment" class="headerlink" title="Week 3 Classification: Analyzing Sentiment"></a>Week 3 Classification: Analyzing Sentiment</h2><div class="note primary"><p>How do you guess whether a person felt positively or negatively about an experience, just from a short review they wrote?</p>
<p>In our second case study, analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,…).This task is an example of classification, one of the most widely used areas of machine learning, with a broad array of applications, including </p>
<ul>
<li>ad targeting,</li>
<li>spam detection,</li>
<li>medical diagnosis,</li>
<li>image classification.</li>
</ul>
<p>You will analyze the accuracy of your classifier, implement an actual classifier in an iPython notebook, and take a first stab at a core piece of the intelligent application you will build and deploy in your capstone.</p>
</div>
<h3 id="Classification-modeling"><a href="#Classification-modeling" class="headerlink" title="Classification modeling"></a>Classification modeling</h3><h4 id="Slides-presented-in-this-module10-min-2"><a href="#Slides-presented-in-this-module10-min-2" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here: <a href="">classification-annotated.pdf</a></p>
<h4 id="Analyzing-the-sentiment-of-reviews-A-case-study-in-classification38-sec"><a href="#Analyzing-the-sentiment-of-reviews-A-case-study-in-classification38-sec" class="headerlink" title="Analyzing the sentiment of reviews: A case study in classification38 sec"></a>Analyzing the sentiment of reviews: A case study in classification38 sec</h4><h4 id="What-is-an-intelligent-restaurant-review-system-4-min"><a href="#What-is-an-intelligent-restaurant-review-system-4-min" class="headerlink" title="What is an intelligent restaurant review system?4 min"></a>What is an intelligent restaurant review system?4 min</h4><h4 id="Examples-of-classification-tasks4-min"><a href="#Examples-of-classification-tasks4-min" class="headerlink" title="Examples of classification tasks4 min"></a>Examples of classification tasks4 min</h4><h4 id="Linear-classifiers5-min"><a href="#Linear-classifiers5-min" class="headerlink" title="Linear classifiers5 min"></a>Linear classifiers5 min</h4><h4 id="Decision-boundaries3-min"><a href="#Decision-boundaries3-min" class="headerlink" title="Decision boundaries3 min"></a>Decision boundaries3 min</h4><h3 id="Evaluating-classification-models"><a href="#Evaluating-classification-models" class="headerlink" title="Evaluating classification models"></a>Evaluating classification models</h3><h4 id="Training-and-evaluating-a-classifier4-min"><a href="#Training-and-evaluating-a-classifier4-min" class="headerlink" title="Training and evaluating a classifier4 min"></a>Training and evaluating a classifier4 min</h4><h4 id="What’s-a-good-accuracy-3-min"><a href="#What’s-a-good-accuracy-3-min" class="headerlink" title="What’s a good accuracy?3 min"></a>What’s a good accuracy?3 min</h4><h4 id="False-positives-false-negatives-and-confusion-matrices6-min"><a href="#False-positives-false-negatives-and-confusion-matrices6-min" class="headerlink" title="False positives, false negatives, and confusion matrices6 min"></a>False positives, false negatives, and confusion matrices6 min</h4><h4 id="Learning-curves5-min"><a href="#Learning-curves5-min" class="headerlink" title="Learning curves5 min"></a>Learning curves5 min</h4><h4 id="Class-probabilities1-min"><a href="#Class-probabilities1-min" class="headerlink" title="Class probabilities1 min"></a>Class probabilities1 min</h4><h3 id="Summary-of-classification"><a href="#Summary-of-classification" class="headerlink" title="Summary of classification"></a>Summary of classification</h3><h4 id="Classification-ML-block-diagram3-min"><a href="#Classification-ML-block-diagram3-min" class="headerlink" title="Classification ML block diagram3 min"></a>Classification ML block diagram3 min</h4><h4 id="Quiz-Classification7-questions"><a href="#Quiz-Classification7-questions" class="headerlink" title="Quiz: Classification7 questions"></a>Quiz: Classification7 questions</h4><h3 id="Analyzing-sentiment-IPython-Notebook"><a href="#Analyzing-sentiment-IPython-Notebook" class="headerlink" title="Analyzing sentiment: IPython Notebook"></a>Analyzing sentiment: IPython Notebook</h3><h4 id="Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-3"><a href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-3" class="headerlink" title="Download the IPython Notebook used in this lesson to follow along10 min"></a>Download the IPython Notebook used in this lesson to follow along10 min</h4><h4 id="Loading-amp-exploring-product-review-data2-min"><a href="#Loading-amp-exploring-product-review-data2-min" class="headerlink" title="Loading &amp; exploring product review data2 min"></a>Loading &amp; exploring product review data2 min</h4><h4 id="Creating-the-word-count-vector2-min"><a href="#Creating-the-word-count-vector2-min" class="headerlink" title="Creating the word count vector2 min"></a>Creating the word count vector2 min</h4><h4 id="Exploring-the-most-popular-product4-min"><a href="#Exploring-the-most-popular-product4-min" class="headerlink" title="Exploring the most popular product4 min"></a>Exploring the most popular product4 min</h4><h4 id="Defining-which-reviews-have-positive-or-negative-sentiment4-min"><a href="#Defining-which-reviews-have-positive-or-negative-sentiment4-min" class="headerlink" title="Defining which reviews have positive or negative sentiment4 min"></a>Defining which reviews have positive or negative sentiment4 min</h4><h4 id="Training-a-sentiment-classifier3-min"><a href="#Training-a-sentiment-classifier3-min" class="headerlink" title="Training a sentiment classifier3 min"></a>Training a sentiment classifier3 min</h4><h4 id="Evaluating-a-classifier-amp-the-ROC-curve4-min"><a href="#Evaluating-a-classifier-amp-the-ROC-curve4-min" class="headerlink" title="Evaluating a classifier &amp; the ROC curve4 min"></a>Evaluating a classifier &amp; the ROC curve4 min</h4><h4 id="Applying-model-to-find-most-positive-amp-negative-reviews-for-a-product4-min"><a href="#Applying-model-to-find-most-positive-amp-negative-reviews-for-a-product4-min" class="headerlink" title="Applying model to find most positive &amp; negative reviews for a product4 min"></a>Applying model to find most positive &amp; negative reviews for a product4 min</h4><h4 id="Exploring-the-most-positive-amp-negative-aspects-of-a-product4-min"><a href="#Exploring-the-most-positive-amp-negative-aspects-of-a-product4-min" class="headerlink" title="Exploring the most positive &amp; negative aspects of a product4 min"></a>Exploring the most positive &amp; negative aspects of a product4 min</h4><h3 id="Programming-assignment-1"><a href="#Programming-assignment-1" class="headerlink" title="Programming assignment"></a>Programming assignment</h3><h4 id="Reading-Analyzing-product-sentiment-assignment10-min"><a href="#Reading-Analyzing-product-sentiment-assignment10-min" class="headerlink" title="Reading: Analyzing product sentiment assignment10 min"></a>Reading: Analyzing product sentiment assignment10 min</h4><h4 id="Quiz-Analyzing-product-sentiment11-questions"><a href="#Quiz-Analyzing-product-sentiment11-questions" class="headerlink" title="Quiz: Analyzing product sentiment11 questions"></a>Quiz: Analyzing product sentiment11 questions</h4><h2 id="Week-4-Clustering-and-Similarity-Retrieving-Documents"><a href="#Week-4-Clustering-and-Similarity-Retrieving-Documents" class="headerlink" title="Week 4 Clustering and Similarity: Retrieving Documents"></a>Week 4 Clustering and Similarity: Retrieving Documents</h2><div class="note primary"><p>A reader is interested in a specific news article and you want to find a similar articles to recommend. What is the right notion of similarity? How do I automatically search over documents to find the one that is most similar? How do I quantitatively represent the documents in the first place?</p>
<p>In this third case study, retrieving documents, you will examine various document representations and an algorithm to retrieve the most similar subset. You will also consider structured representations of the documents that automatically group articles by similarity (e.g., document topic).</p>
<p>You will actually build an intelligent document retrieval system for Wikipedia entries in an iPython notebook.</p>
</div>
<h3 id="Algorithms-for-retrieval-and-measuring-similarity-of-documents"><a href="#Algorithms-for-retrieval-and-measuring-similarity-of-documents" class="headerlink" title="Algorithms for retrieval and measuring similarity of documents"></a>Algorithms for retrieval and measuring similarity of documents</h3><h4 id="Slides-presented-in-this-module10-min-3"><a href="#Slides-presented-in-this-module10-min-3" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here: <a href="">clustering-intro-annotated.pdf</a></p>
<h4 id="Document-retrieval-A-case-study-in-clustering-and-measuring-similarity35-sec"><a href="#Document-retrieval-A-case-study-in-clustering-and-measuring-similarity35-sec" class="headerlink" title="Document retrieval: A case study in clustering and measuring similarity35 sec"></a>Document retrieval: A case study in clustering and measuring similarity35 sec</h4><h4 id="What-is-the-document-retrieval-task-1-min"><a href="#What-is-the-document-retrieval-task-1-min" class="headerlink" title="What is the document retrieval task?1 min"></a>What is the document retrieval task?1 min</h4><h4 id="Word-count-representation-for-measuring-similarity6-min"><a href="#Word-count-representation-for-measuring-similarity6-min" class="headerlink" title="Word count representation for measuring similarity6 min"></a>Word count representation for measuring similarity6 min</h4><h4 id="Prioritizing-important-words-with-tf-idf3-min"><a href="#Prioritizing-important-words-with-tf-idf3-min" class="headerlink" title="Prioritizing important words with tf-idf3 min"></a>Prioritizing important words with tf-idf3 min</h4><h4 id="Calculating-tf-idf-vectors5-min"><a href="#Calculating-tf-idf-vectors5-min" class="headerlink" title="Calculating tf-idf vectors5 min"></a>Calculating tf-idf vectors5 min</h4><h4 id="Retrieving-similar-documents-using-nearest-neighbor-search2-min"><a href="#Retrieving-similar-documents-using-nearest-neighbor-search2-min" class="headerlink" title="Retrieving similar documents using nearest neighbor search2 min"></a>Retrieving similar documents using nearest neighbor search2 min</h4><h3 id="Clustering-models-and-algorithms"><a href="#Clustering-models-and-algorithms" class="headerlink" title="Clustering models and algorithms"></a>Clustering models and algorithms</h3><h4 id="Clustering-documents-task-overview2-min"><a href="#Clustering-documents-task-overview2-min" class="headerlink" title="Clustering documents task overview2 min"></a>Clustering documents task overview2 min</h4><h4 id="Clustering-documents-An-unsupervised-learning-task4-min"><a href="#Clustering-documents-An-unsupervised-learning-task4-min" class="headerlink" title="Clustering documents: An unsupervised learning task4 min"></a>Clustering documents: An unsupervised learning task4 min</h4><h4 id="k-means-A-clustering-algorithm3-min"><a href="#k-means-A-clustering-algorithm3-min" class="headerlink" title="k-means: A clustering algorithm3 min"></a>k-means: A clustering algorithm3 min</h4><h4 id="Other-examples-of-clustering6-min"><a href="#Other-examples-of-clustering6-min" class="headerlink" title="Other examples of clustering6 min"></a>Other examples of clustering6 min</h4><h3 id="Summary-of-clustering-and-similarity"><a href="#Summary-of-clustering-and-similarity" class="headerlink" title="Summary of clustering and similarity"></a>Summary of clustering and similarity</h3><h4 id="Clustering-and-similarity-ML-block-diagram7-min"><a href="#Clustering-and-similarity-ML-block-diagram7-min" class="headerlink" title="Clustering and similarity ML block diagram7 min"></a>Clustering and similarity ML block diagram7 min</h4><h4 id="Quiz-Clustering-and-Similarity6-questions"><a href="#Quiz-Clustering-and-Similarity6-questions" class="headerlink" title="Quiz: Clustering and Similarity6 questions"></a>Quiz: Clustering and Similarity6 questions</h4><h3 id="Document-retrieval-IPython-Notebook"><a href="#Document-retrieval-IPython-Notebook" class="headerlink" title="Document retrieval: IPython Notebook"></a>Document retrieval: IPython Notebook</h3><h4 id="Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-4"><a href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-4" class="headerlink" title="Download the IPython Notebook used in this lesson to follow along10 min"></a>Download the IPython Notebook used in this lesson to follow along10 min</h4><h4 id="Loading-amp-exploring-Wikipedia-data5-min"><a href="#Loading-amp-exploring-Wikipedia-data5-min" class="headerlink" title="Loading &amp; exploring Wikipedia data5 min"></a>Loading &amp; exploring Wikipedia data5 min</h4><h4 id="Exploring-word-counts5-min"><a href="#Exploring-word-counts5-min" class="headerlink" title="Exploring word counts5 min"></a>Exploring word counts5 min</h4><h4 id="Computing-amp-exploring-TF-IDFs7-min"><a href="#Computing-amp-exploring-TF-IDFs7-min" class="headerlink" title="Computing &amp; exploring TF-IDFs7 min"></a>Computing &amp; exploring TF-IDFs7 min</h4><h4 id="Computing-distances-between-Wikipedia-articles5-min"><a href="#Computing-distances-between-Wikipedia-articles5-min" class="headerlink" title="Computing distances between Wikipedia articles5 min"></a>Computing distances between Wikipedia articles5 min</h4><h4 id="Building-amp-exploring-a-nearest-neighbors-model-for-Wikipedia-articles3-min"><a href="#Building-amp-exploring-a-nearest-neighbors-model-for-Wikipedia-articles3-min" class="headerlink" title="Building &amp; exploring a nearest neighbors model for Wikipedia articles3 min"></a>Building &amp; exploring a nearest neighbors model for Wikipedia articles3 min</h4><h4 id="Examples-of-document-retrieval-in-action4-min"><a href="#Examples-of-document-retrieval-in-action4-min" class="headerlink" title="Examples of document retrieval in action4 min"></a>Examples of document retrieval in action4 min</h4><h3 id="Programming-assignment-2"><a href="#Programming-assignment-2" class="headerlink" title="Programming assignment"></a>Programming assignment</h3><h4 id="Reading-Retrieving-Wikipedia-articles-assignment10-min"><a href="#Reading-Retrieving-Wikipedia-articles-assignment10-min" class="headerlink" title="Reading: Retrieving Wikipedia articles assignment10 min"></a>Reading: Retrieving Wikipedia articles assignment10 min</h4><h4 id="Quiz-Retrieving-Wikipedia-articles9-questions"><a href="#Quiz-Retrieving-Wikipedia-articles9-questions" class="headerlink" title="Quiz: Retrieving Wikipedia articles9 questions"></a>Quiz: Retrieving Wikipedia articles9 questions</h4><h1 id="Machine-Learning-Regression"><a href="#Machine-Learning-Regression" class="headerlink" title="Machine Learning: Regression"></a>Machine Learning: Regression</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/ml-regression" target="_blank" rel="external">here</a></p>
<p>About this course: Case Study - Predicting Housing Prices</p>
<p>In our first case study, predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,…).  This is just one of the many places where regression can be applied.  Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression.</p>
<p>In this course, you will explore regularized linear regression models for the task of prediction and feature selection.  You will be able to handle very large sets of features and select between models of various complexity.  You will also analyze the impact of aspects of your data – such as outliers – on your selected models and predictions.  To fit these models, you will implement optimization algorithms that scale to large datasets.</p>
<p>Learning Outcomes:  By the end of this course, you will be able to:<br>   -Describe the input and output of a regression model.<br>   -Compare and contrast bias and variance when modeling data.<br>   -Estimate model parameters using optimization algorithms.<br>   -Tune parameters with cross validation.<br>   -Analyze the performance of the model.<br>   -Describe the notion of sparsity and how LASSO leads to sparse solutions.<br>   -Deploy methods to select between models.<br>   -Exploit the model to form predictions.<br>   -Build a regression model to predict prices using a housing dataset.<br>   -Implement these techniques in Python.</p>
</div>
<h2 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h2><h3 id="Welcome"><a href="#Welcome" class="headerlink" title="Welcome"></a>Welcome</h3><div class="note primary"><p>Regression is one of the most important and broadly used machine learning and statistics tools out there. It allows you to make predictions from data by learning the relationship between features of your data and some observed, continuous-valued response. Regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networks.</p>
<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.</p>
</div>
<h4 id="What-is-this-course-about"><a href="#What-is-this-course-about" class="headerlink" title="What is this course about?"></a>What is this course about?</h4><h5 id="Slides-presented-in-this-module10-min-4"><a href="#Slides-presented-in-this-module10-min-4" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Lecture%20Slides/1.0-intro.pdf" target="_blank" rel="external">intro.pdf</a></p>
<h5 id="Welcome-1-min"><a href="#Welcome-1-min" class="headerlink" title="Welcome!1 min"></a>Welcome!1 min</h5><h5 id="What-is-the-course-about-3-min"><a href="#What-is-the-course-about-3-min" class="headerlink" title="What is the course about?3 min"></a>What is the course about?3 min</h5><h5 id="Outlining-the-first-half-of-the-course5-min"><a href="#Outlining-the-first-half-of-the-course5-min" class="headerlink" title="Outlining the first half of the course5 min"></a>Outlining the first half of the course5 min</h5><h5 id="Outlining-the-second-half-of-the-course5-min"><a href="#Outlining-the-second-half-of-the-course5-min" class="headerlink" title="Outlining the second half of the course5 min"></a>Outlining the second half of the course5 min</h5><h5 id="Assumed-background4-min"><a href="#Assumed-background4-min" class="headerlink" title="Assumed background4 min"></a>Assumed background4 min</h5><h5 id="Reading-Software-tools-you’ll-need10-min"><a href="#Reading-Software-tools-you’ll-need10-min" class="headerlink" title="Reading: Software tools you’ll need10 min"></a>Reading: Software tools you’ll need10 min</h5><h3 id="Simple-Linear-Regression"><a href="#Simple-Linear-Regression" class="headerlink" title="Simple Linear Regression"></a>Simple Linear Regression</h3><div class="note primary"><p>Our course starts from the most basic regression model: Just fitting a line to data. This simple model for forming predictions from a single, univariate feature of the data is appropriately called “simple linear regression”.</p>
<p>In this module, we describe the high-level regression task and then specialize these concepts to the simple linear regression case. You will learn how to formulate a simple regression model and fit the model to data using both a closed-form solution as well as an iterative optimization algorithm called gradient descent. Based on this fitted function, you will interpret the estimated model parameters and form predictions. You will also analyze the sensitivity of your fit to outlying observations.</p>
<p>You will examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house.</p>
</div>
<h4 id="Regression-fundamentals"><a href="#Regression-fundamentals" class="headerlink" title="Regression fundamentals"></a>Regression fundamentals</h4><h5 id="Slides-presented-in-this-module10-min-5"><a href="#Slides-presented-in-this-module10-min-5" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Lecture%20Slides/1.1-week1_simpleregression-annotated.pdf" target="_blank" rel="external">week1_simpleregression-annotated.pdf</a></p>
<h5 id="A-case-study-in-predicting-house-prices1-min"><a href="#A-case-study-in-predicting-house-prices1-min" class="headerlink" title="A case study in predicting house prices1 min"></a>A case study in predicting house prices1 min</h5><h5 id="Regression-fundamentals-data-amp-model8-min"><a href="#Regression-fundamentals-data-amp-model8-min" class="headerlink" title="Regression fundamentals: data &amp; model8 min"></a>Regression fundamentals: data &amp; model8 min</h5><h5 id="Regression-fundamentals-the-task2-min"><a href="#Regression-fundamentals-the-task2-min" class="headerlink" title="Regression fundamentals: the task2 min"></a>Regression fundamentals: the task2 min</h5><h5 id="Regression-ML-block-diagram4-min"><a href="#Regression-ML-block-diagram4-min" class="headerlink" title="Regression ML block diagram4 min"></a>Regression ML block diagram4 min</h5><h4 id="The-simple-linear-regression-model-its-use-and-interpretation"><a href="#The-simple-linear-regression-model-its-use-and-interpretation" class="headerlink" title="The simple linear regression model, its use, and interpretation"></a>The simple linear regression model, its use, and interpretation</h4><h5 id="The-simple-linear-regression-model2-min"><a href="#The-simple-linear-regression-model2-min" class="headerlink" title="The simple linear regression model2 min"></a>The simple linear regression model2 min</h5><h5 id="The-cost-of-using-a-given-line6-min"><a href="#The-cost-of-using-a-given-line6-min" class="headerlink" title="The cost of using a given line6 min"></a>The cost of using a given line6 min</h5><h5 id="Using-the-fitted-line6-min"><a href="#Using-the-fitted-line6-min" class="headerlink" title="Using the fitted line6 min"></a>Using the fitted line6 min</h5><h5 id="Interpreting-the-fitted-line6-min"><a href="#Interpreting-the-fitted-line6-min" class="headerlink" title="Interpreting the fitted line6 min"></a>Interpreting the fitted line6 min</h5><h4 id="An-aside-on-optimization-one-dimensional-objectives"><a href="#An-aside-on-optimization-one-dimensional-objectives" class="headerlink" title="An aside on optimization: one dimensional objectives"></a>An aside on optimization: one dimensional objectives</h4><h5 id="Defining-our-least-squares-optimization-objective3-min"><a href="#Defining-our-least-squares-optimization-objective3-min" class="headerlink" title="Defining our least squares optimization objective3 min"></a>Defining our least squares optimization objective3 min</h5><h5 id="Finding-maxima-or-minima-analytically7-min"><a href="#Finding-maxima-or-minima-analytically7-min" class="headerlink" title="Finding maxima or minima analytically7 min"></a>Finding maxima or minima analytically7 min</h5><h5 id="Maximizing-a-1d-function-a-worked-example2-min"><a href="#Maximizing-a-1d-function-a-worked-example2-min" class="headerlink" title="Maximizing a 1d function: a worked example2 min"></a>Maximizing a 1d function: a worked example2 min</h5><h5 id="Finding-the-max-via-hill-climbing6-min"><a href="#Finding-the-max-via-hill-climbing6-min" class="headerlink" title="Finding the max via hill climbing6 min"></a>Finding the max via hill climbing6 min</h5><h5 id="Finding-the-min-via-hill-descent3-min"><a href="#Finding-the-min-via-hill-descent3-min" class="headerlink" title="Finding the min via hill descent3 min"></a>Finding the min via hill descent3 min</h5><h5 id="Choosing-stepsize-and-convergence-criteria6-min"><a href="#Choosing-stepsize-and-convergence-criteria6-min" class="headerlink" title="Choosing stepsize and convergence criteria6 min"></a>Choosing stepsize and convergence criteria6 min</h5><h4 id="An-aside-on-optimization-multidimensional-objectives"><a href="#An-aside-on-optimization-multidimensional-objectives" class="headerlink" title="An aside on optimization: multidimensional objectives"></a>An aside on optimization: multidimensional objectives</h4><h5 id="Gradients-derivatives-in-multiple-dimensions5-min"><a href="#Gradients-derivatives-in-multiple-dimensions5-min" class="headerlink" title="Gradients: derivatives in multiple dimensions5 min"></a>Gradients: derivatives in multiple dimensions5 min</h5><h5 id="Gradient-descent-multidimensional-hill-descent6-min"><a href="#Gradient-descent-multidimensional-hill-descent6-min" class="headerlink" title="Gradient descent: multidimensional hill descent6 min"></a>Gradient descent: multidimensional hill descent6 min</h5><h4 id="Finding-the-least-squares-line"><a href="#Finding-the-least-squares-line" class="headerlink" title="Finding the least squares line"></a>Finding the least squares line</h4><h5 id="Computing-the-gradient-of-RSS7-min"><a href="#Computing-the-gradient-of-RSS7-min" class="headerlink" title="Computing the gradient of RSS7 min"></a>Computing the gradient of RSS7 min</h5><h5 id="Approach-1-closed-form-solution5-min"><a href="#Approach-1-closed-form-solution5-min" class="headerlink" title="Approach 1: closed-form solution5 min"></a>Approach 1: closed-form solution5 min</h5><h5 id="Optional-reading-worked-out-example-for-closed-form-solution10-min"><a href="#Optional-reading-worked-out-example-for-closed-form-solution10-min" class="headerlink" title="Optional reading: worked-out example for closed-form solution10 min"></a>Optional reading: worked-out example for closed-form solution10 min</h5><h5 id="Approach-2-gradient-descent7-min"><a href="#Approach-2-gradient-descent7-min" class="headerlink" title="Approach 2: gradient descent7 min"></a>Approach 2: gradient descent7 min</h5><h5 id="Optional-reading-worked-out-example-for-gradient-descent10-min"><a href="#Optional-reading-worked-out-example-for-gradient-descent10-min" class="headerlink" title="Optional reading: worked-out example for gradient descent10 min"></a>Optional reading: worked-out example for gradient descent10 min</h5><h5 id="Comparing-the-approaches1-min"><a href="#Comparing-the-approaches1-min" class="headerlink" title="Comparing the approaches1 min"></a>Comparing the approaches1 min</h5><h4 id="Discussion-and-summary-of-simple-linear-regression"><a href="#Discussion-and-summary-of-simple-linear-regression" class="headerlink" title="Discussion and summary of simple linear regression"></a>Discussion and summary of simple linear regression</h4><h5 id="Download-notebooks-to-follow-along10-min"><a href="#Download-notebooks-to-follow-along10-min" class="headerlink" title="Download notebooks to follow along10 min"></a>Download notebooks to follow along10 min</h5><h5 id="Influence-of-high-leverage-points-exploring-the-data4-min"><a href="#Influence-of-high-leverage-points-exploring-the-data4-min" class="headerlink" title="Influence of high leverage points: exploring the data4 min"></a>Influence of high leverage points: exploring the data4 min</h5><h5 id="Influence-of-high-leverage-points-removing-Center-City7-min"><a href="#Influence-of-high-leverage-points-removing-Center-City7-min" class="headerlink" title="Influence of high leverage points: removing Center City7 min"></a>Influence of high leverage points: removing Center City7 min</h5><h5 id="Influence-of-high-leverage-points-removing-high-end-towns3-min"><a href="#Influence-of-high-leverage-points-removing-high-end-towns3-min" class="headerlink" title="Influence of high leverage points: removing high-end towns3 min"></a>Influence of high leverage points: removing high-end towns3 min</h5><h5 id="Asymmetric-cost-functions3-min"><a href="#Asymmetric-cost-functions3-min" class="headerlink" title="Asymmetric cost functions3 min"></a>Asymmetric cost functions3 min</h5><h5 id="A-brief-recap1-min"><a href="#A-brief-recap1-min" class="headerlink" title="A brief recap1 min"></a>A brief recap1 min</h5><h5 id="Quiz-Simple-Linear-Regression7-questions"><a href="#Quiz-Simple-Linear-Regression7-questions" class="headerlink" title="Quiz: Simple Linear Regression7 questions"></a>Quiz: Simple Linear Regression7 questions</h5><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/2.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_1/3.1.1.PNG?imageslim" alt=""></p>
<h4 id="Programming-assignment-3"><a href="#Programming-assignment-3" class="headerlink" title="Programming assignment"></a>Programming assignment</h4><h5 id="Reading-Fitting-a-simple-linear-regression-model-on-housing-data10-min"><a href="#Reading-Fitting-a-simple-linear-regression-model-on-housing-data10-min" class="headerlink" title="Reading: Fitting a simple linear regression model on housing data10 min"></a>Reading: Fitting a simple linear regression model on housing data10 min</h5><h5 id="Quiz-Fitting-a-simple-linear-regression-model-on-housing-data4-questions"><a href="#Quiz-Fitting-a-simple-linear-regression-model-on-housing-data4-questions" class="headerlink" title="Quiz: Fitting a simple linear regression model on housing data4 questions"></a>Quiz: Fitting a simple linear regression model on housing data4 questions</h5><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_1/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_1/1.1.2.PNG?imageslim" alt=""></p>
<h2 id="Week-2-Multiple-Regression"><a href="#Week-2-Multiple-Regression" class="headerlink" title="Week 2 Multiple Regression"></a>Week 2 Multiple Regression</h2><div class="note primary"><p>The next step in moving beyond simple linear regression is to consider “multiple regression” where multiple features of the data are used to form predictions.</p>
<p>More specifically, in this module, you will learn how to build models of more complex relationship between a single variable (e.g., ‘square feet’) and the observed response (like ‘house sales price’). This includes things like fitting a polynomial to your data, or capturing seasonal changes in the response value. You will also learn how to incorporate multiple input variables (e.g., ‘square feet’, ‘# bedrooms’, ‘# bathrooms’). You will then be able to describe how all of these models can still be cast within the linear regression framework, but now using multiple “features”. Within this multiple regression framework, you will fit models to data, interpret estimated coefficients, and form predictions.</p>
<p>Here, you will also implement a gradient descent algorithm for fitting a multiple regression model.</p>
</div>
<h3 id="Multiple-features-of-one-input"><a href="#Multiple-features-of-one-input" class="headerlink" title="Multiple features of one input"></a>Multiple features of one input</h3><h4 id="Slides-presented-in-this-module10-min-6"><a href="#Slides-presented-in-this-module10-min-6" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="">week2_multipleregression-annotated.pdf</a></p>
<h4 id="Multiple-regression-intro30-sec"><a href="#Multiple-regression-intro30-sec" class="headerlink" title="Multiple regression intro30 sec"></a>Multiple regression intro30 sec</h4><h4 id="Polynomial-regression3-min"><a href="#Polynomial-regression3-min" class="headerlink" title="Polynomial regression3 min"></a>Polynomial regression3 min</h4><h4 id="Modeling-seasonality8-min"><a href="#Modeling-seasonality8-min" class="headerlink" title="Modeling seasonality8 min"></a>Modeling seasonality8 min</h4><h4 id="Where-we-see-seasonality3-min"><a href="#Where-we-see-seasonality3-min" class="headerlink" title="Where we see seasonality3 min"></a>Where we see seasonality3 min</h4><h4 id="Regression-with-general-features-of-1-input2-min"><a href="#Regression-with-general-features-of-1-input2-min" class="headerlink" title="Regression with general features of 1 input2 min"></a>Regression with general features of 1 input2 min</h4><h3 id="Incorporating-multiple-inputs"><a href="#Incorporating-multiple-inputs" class="headerlink" title="Incorporating multiple inputs"></a>Incorporating multiple inputs</h3><h4 id="Motivating-the-use-of-multiple-inputs4-min"><a href="#Motivating-the-use-of-multiple-inputs4-min" class="headerlink" title="Motivating the use of multiple inputs4 min"></a>Motivating the use of multiple inputs4 min</h4><h4 id="Defining-notation3-min"><a href="#Defining-notation3-min" class="headerlink" title="Defining notation3 min"></a>Defining notation3 min</h4><h4 id="Regression-with-features-of-multiple-inputs3-min"><a href="#Regression-with-features-of-multiple-inputs3-min" class="headerlink" title="Regression with features of multiple inputs3 min"></a>Regression with features of multiple inputs3 min</h4><h4 id="Interpreting-the-multiple-regression-fit7-min"><a href="#Interpreting-the-multiple-regression-fit7-min" class="headerlink" title="Interpreting the multiple regression fit7 min"></a>Interpreting the multiple regression fit7 min</h4><h3 id="Setting-the-stage-for-computing-the-least-squares-fit"><a href="#Setting-the-stage-for-computing-the-least-squares-fit" class="headerlink" title="Setting the stage for computing the least squares fit"></a>Setting the stage for computing the least squares fit</h3><h4 id="Optional-reading-review-of-matrix-algebra10-min"><a href="#Optional-reading-review-of-matrix-algebra10-min" class="headerlink" title="Optional reading: review of matrix algebra10 min"></a>Optional reading: review of matrix algebra10 min</h4><p>This section involves some use of matrix algebra. If you’d like to brush up on it, we recommend <a href="http://tutorial.math.lamar.edu/Classes/DE/LA_Matrix.aspx" target="_blank" rel="external">a short tutorial</a>.</p>
<h4 id="Rewriting-the-single-observation-model-in-vector-notation6-min"><a href="#Rewriting-the-single-observation-model-in-vector-notation6-min" class="headerlink" title="Rewriting the single observation model in vector notation6 min"></a>Rewriting the single observation model in vector notation6 min</h4><h4 id="Rewriting-the-model-for-all-observations-in-matrix-notation4-min"><a href="#Rewriting-the-model-for-all-observations-in-matrix-notation4-min" class="headerlink" title="Rewriting the model for all observations in matrix notation4 min"></a>Rewriting the model for all observations in matrix notation4 min</h4><h4 id="Computing-the-cost-of-a-D-dimensional-curve9-min"><a href="#Computing-the-cost-of-a-D-dimensional-curve9-min" class="headerlink" title="Computing the cost of a D-dimensional curve9 min"></a>Computing the cost of a D-dimensional curve9 min</h4><h3 id="Computing-the-least-squares-D-dimensional-curve"><a href="#Computing-the-least-squares-D-dimensional-curve" class="headerlink" title="Computing the least squares D-dimensional curve"></a>Computing the least squares D-dimensional curve</h3><h4 id="Computing-the-gradient-of-RSS3-min"><a href="#Computing-the-gradient-of-RSS3-min" class="headerlink" title="Computing the gradient of RSS3 min"></a>Computing the gradient of RSS3 min</h4><h4 id="Approach-1-closed-form-solution3-min"><a href="#Approach-1-closed-form-solution3-min" class="headerlink" title="Approach 1: closed-form solution3 min"></a>Approach 1: closed-form solution3 min</h4><h4 id="Discussing-the-closed-form-solution4-min"><a href="#Discussing-the-closed-form-solution4-min" class="headerlink" title="Discussing the closed-form solution4 min"></a>Discussing the closed-form solution4 min</h4><h4 id="Approach-2-gradient-descent2-min"><a href="#Approach-2-gradient-descent2-min" class="headerlink" title="Approach 2: gradient descent2 min"></a>Approach 2: gradient descent2 min</h4><h4 id="Feature-by-feature-update9-min"><a href="#Feature-by-feature-update9-min" class="headerlink" title="Feature-by-feature update9 min"></a>Feature-by-feature update9 min</h4><h4 id="Algorithmic-summary-of-gradient-descent-approach4-min"><a href="#Algorithmic-summary-of-gradient-descent-approach4-min" class="headerlink" title="Algorithmic summary of gradient descent approach4 min"></a>Algorithmic summary of gradient descent approach4 min</h4><h3 id="Summarizing-multiple-regression"><a href="#Summarizing-multiple-regression" class="headerlink" title="Summarizing multiple regression"></a>Summarizing multiple regression</h3><h4 id="A-brief-recap1-min-1"><a href="#A-brief-recap1-min-1" class="headerlink" title="A brief recap1 min"></a>A brief recap1 min</h4><h4 id="Quiz-Multiple-Regression9-questions"><a href="#Quiz-Multiple-Regression9-questions" class="headerlink" title="Quiz: Multiple Regression9 questions"></a>Quiz: Multiple Regression9 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.1.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.2.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.2.1.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/2.1.1.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.3.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.4.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.1.5.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/1.2.2.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/2.1.2.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2/3.1.1.PNG?imgeslim" alt=""></p>
<h3 id="Programming-assignment-1"><a href="#Programming-assignment-1" class="headerlink" title="Programming assignment 1"></a>Programming assignment 1</h3><h4 id="Reading-Exploring-different-multiple-regression-models-for-house-price-prediction10-min"><a href="#Reading-Exploring-different-multiple-regression-models-for-house-price-prediction10-min" class="headerlink" title="Reading: Exploring different multiple regression models for house price prediction10 min"></a>Reading: Exploring different multiple regression models for house price prediction10 min</h4><h4 id="Quiz-Exploring-different-multiple-regression-models-for-house-price-prediction8-questions"><a href="#Quiz-Exploring-different-multiple-regression-models-for-house-price-prediction8-questions" class="headerlink" title="Quiz: Exploring different multiple regression models for house price prediction8 questions"></a>Quiz: Exploring different multiple regression models for house price prediction8 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.1.1.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.2.1.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/2.1.1.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.1.2.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_2_real/1.1.3.PNG?imgeslim" alt=""></p>
<h3 id="Programming-assignment-2"><a href="#Programming-assignment-2" class="headerlink" title="Programming assignment 2"></a>Programming assignment 2</h3><h4 id="Numpy-tutorial10-min"><a href="#Numpy-tutorial10-min" class="headerlink" title="Numpy tutorial10 min"></a>Numpy tutorial10 min</h4><p>More information on Numpy, beyond this tutorial, can be found in the <a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">Numpy getting started guide</a>.</p>
<h4 id="Reading-Implementing-gradient-descent-for-multiple-regression10-min"><a href="#Reading-Implementing-gradient-descent-for-multiple-regression10-min" class="headerlink" title="Reading: Implementing gradient descent for multiple regression10 min"></a>Reading: Implementing gradient descent for multiple regression10 min</h4><h4 id="Quiz-Implementing-gradient-descent-for-multiple-regression5-questions"><a href="#Quiz-Implementing-gradient-descent-for-multiple-regression5-questions" class="headerlink" title="Quiz: Implementing gradient descent for multiple regression5 questions"></a>Quiz: Implementing gradient descent for multiple regression5 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_3/1.1.1.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_3/1.1.2.PNG?imgeslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/PA_3/1.1.3.PNG?imgeslim" alt=""></p>
<h2 id="Week-3-Assessing-Performance"><a href="#Week-3-Assessing-Performance" class="headerlink" title="Week 3 Assessing Performance"></a>Week 3 Assessing Performance</h2><div class="note primary"><p>Having learned about linear regression models and algorithms for estimating the parameters of such models, you are now ready to assess how well your considered method should perform in predicting new data. You are also ready to select amongst possible models to choose the best performing.</p>
<p>This module is all about these important topics of model selection and assessment. You will examine both theoretical and practical aspects of such analyses. You will first explore the concept of measuring the “loss” of your predictions, and use this to define training, test, and generalization error. For these measures of error, you will analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance. This leads directly to an important conversation about the bias-variance tradeoff, which is fundamental to machine learning. Finally, you will devise a method to first select amongst models and then assess the performance of the selected model.</p>
<p>The concepts described in this module are key to all machine learning problems, well-beyond the regression setting addressed in this course.</p>
</div>
<h3 id="Defining-how-we-assess-performance"><a href="#Defining-how-we-assess-performance" class="headerlink" title="Defining how we assess performance"></a>Defining how we assess performance</h3><h4 id="Slides-presented-in-this-module10-min-7"><a href="#Slides-presented-in-this-module10-min-7" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Lecture%20Slides/3.0-week3_assessingperformance-annotated.pdf" target="_blank" rel="external">week3_assessingperformance-annotated.pdf</a></p>
<h4 id="Assessing-performance-intro32-sec"><a href="#Assessing-performance-intro32-sec" class="headerlink" title="Assessing performance intro32 sec"></a>Assessing performance intro32 sec</h4><h4 id="What-do-we-mean-by-“loss”-4-min"><a href="#What-do-we-mean-by-“loss”-4-min" class="headerlink" title="What do we mean by “loss”?4 min"></a>What do we mean by “loss”?4 min</h4><h3 id="3-measures-of-loss-and-their-trends-with-model-complexity"><a href="#3-measures-of-loss-and-their-trends-with-model-complexity" class="headerlink" title="3 measures of loss and their trends with model complexity"></a>3 measures of loss and their trends with model complexity</h3><h4 id="Training-error-assessing-loss-on-the-training-set7-min"><a href="#Training-error-assessing-loss-on-the-training-set7-min" class="headerlink" title="Training error: assessing loss on the training set7 min"></a>Training error: assessing loss on the training set7 min</h4><h4 id="Generalization-error-what-we-really-want8-min"><a href="#Generalization-error-what-we-really-want8-min" class="headerlink" title="Generalization error: what we really want8 min"></a>Generalization error: what we really want8 min</h4><h4 id="Test-error-what-we-can-actually-compute4-min"><a href="#Test-error-what-we-can-actually-compute4-min" class="headerlink" title="Test error: what we can actually compute4 min"></a>Test error: what we can actually compute4 min</h4><h4 id="Defining-overfitting2-min"><a href="#Defining-overfitting2-min" class="headerlink" title="Defining overfitting2 min"></a>Defining overfitting2 min</h4><h4 id="Training-test-split1-min"><a href="#Training-test-split1-min" class="headerlink" title="Training/test split1 min"></a>Training/test split1 min</h4><h3 id="3-sources-of-error-and-the-bias-variance-tradeoff"><a href="#3-sources-of-error-and-the-bias-variance-tradeoff" class="headerlink" title="3 sources of error and the bias-variance tradeoff"></a>3 sources of error and the bias-variance tradeoff</h3><h4 id="Irreducible-error-and-bias6-min"><a href="#Irreducible-error-and-bias6-min" class="headerlink" title="Irreducible error and bias6 min"></a>Irreducible error and bias6 min</h4><h4 id="Variance-and-the-bias-variance-tradeoff6-min"><a href="#Variance-and-the-bias-variance-tradeoff6-min" class="headerlink" title="Variance and the bias-variance tradeoff6 min"></a>Variance and the bias-variance tradeoff6 min</h4><h4 id="Error-vs-amount-of-data6-min"><a href="#Error-vs-amount-of-data6-min" class="headerlink" title="Error vs. amount of data6 min"></a>Error vs. amount of data6 min</h4><h3 id="OPTIONAL-ADVANCED-MATERIAL-Formally-defining-and-deriving-the-3-sources-of-error"><a href="#OPTIONAL-ADVANCED-MATERIAL-Formally-defining-and-deriving-the-3-sources-of-error" class="headerlink" title="OPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of error"></a>OPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of error</h3><h4 id="Formally-defining-the-3-sources-of-error14-min"><a href="#Formally-defining-the-3-sources-of-error14-min" class="headerlink" title="Formally defining the 3 sources of error14 min"></a>Formally defining the 3 sources of error14 min</h4><h4 id="Formally-deriving-why-3-sources-of-error20-min"><a href="#Formally-deriving-why-3-sources-of-error20-min" class="headerlink" title="Formally deriving why 3 sources of error20 min"></a>Formally deriving why 3 sources of error20 min</h4><h3 id="Putting-the-pieces-together"><a href="#Putting-the-pieces-together" class="headerlink" title="Putting the pieces together"></a>Putting the pieces together</h3><h4 id="Training-validation-test-split-for-model-selection-fitting-and-assessment7-min"><a href="#Training-validation-test-split-for-model-selection-fitting-and-assessment7-min" class="headerlink" title="Training/validation/test split for model selection, fitting, and assessment7 min"></a>Training/validation/test split for model selection, fitting, and assessment7 min</h4><h4 id="A-brief-recap1-min-2"><a href="#A-brief-recap1-min-2" class="headerlink" title="A brief recap1 min"></a>A brief recap1 min</h4><h4 id="Quiz-Assessing-Performance13-questions"><a href="#Quiz-Assessing-Performance13-questions" class="headerlink" title="Quiz: Assessing Performance13 questions"></a>Quiz: Assessing Performance13 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.5.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.6.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.7.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/1.1.8.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/2.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/2.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/3.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/3.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/4.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/4.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/5.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/quiz_3/5.1.2.PNG?imageslim" alt=""></p>
<h3 id="Programming-assignment-4"><a href="#Programming-assignment-4" class="headerlink" title="Programming assignment"></a>Programming assignment</h3><h4 id="Reading-Exploring-the-bias-variance-tradeoff10-min"><a href="#Reading-Exploring-the-bias-variance-tradeoff10-min" class="headerlink" title="Reading: Exploring the bias-variance tradeoff10 min"></a>Reading: Exploring the bias-variance tradeoff10 min</h4><h4 id="Quiz-Exploring-the-bias-variance-tradeoff4-questions"><a href="#Quiz-Exploring-the-bias-variance-tradeoff4-questions" class="headerlink" title="Quiz: Exploring the bias-variance tradeoff4 questions"></a>Quiz: Exploring the bias-variance tradeoff4 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week_3_PA/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week_3_PA/1.1.2.PNG?imageslim" alt=""></p>
<h2 id="Week-4-Ridge-Regression"><a href="#Week-4-Ridge-Regression" class="headerlink" title="Week 4 Ridge Regression"></a>Week 4 Ridge Regression</h2><div class="note primary"><p>You have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data. In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue. This method is called “ridge regression”. You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions. To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective. You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression. To select the strength of the bias away from overfitting, you will explore a general-purpose method called “cross validation”.</p>
<p>You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant.</p>
</div>
<h3 id="Characteristics-of-overfit-models"><a href="#Characteristics-of-overfit-models" class="headerlink" title="Characteristics of overfit models"></a>Characteristics of overfit models</h3><h4 id="Slides-presented-in-this-module10-min-8"><a href="#Slides-presented-in-this-module10-min-8" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Lecture%20Slides/4.0-week4_ridgeregression-annotated.pdf" target="_blank" rel="external">week4_ridgeregression-annotated.pdf</a></p>
<h4 id="Symptoms-of-overfitting-in-polynomial-regression2-min"><a href="#Symptoms-of-overfitting-in-polynomial-regression2-min" class="headerlink" title="Symptoms of overfitting in polynomial regression2 min"></a>Symptoms of overfitting in polynomial regression2 min</h4><h4 id="Download-the-notebook-and-follow-along10-min"><a href="#Download-the-notebook-and-follow-along10-min" class="headerlink" title="Download the notebook and follow along10 min"></a>Download the notebook and follow along10 min</h4><p>Next, we will see a demo illustrating the concept of overfitting. We recommend you download the IPython Notebook used in the demo to follow along. (The second and third parts of this notebook will be used to demonstrate ridge regression and LASSO; two techniques to address overfitting.)</p>
<p>IPython Notebook:<br><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Programming%20Assignment%205/Overfitting_Demo_Ridge_Lasso.html" target="_blank" rel="external">Overfitting_Demo_Ridge_Lasso.ipynb.zip</a></p>
<h4 id="Overfitting-demo7-min"><a href="#Overfitting-demo7-min" class="headerlink" title="Overfitting demo7 min"></a>Overfitting demo7 min</h4><h4 id="Overfitting-for-more-general-multiple-regression-models3-min"><a href="#Overfitting-for-more-general-multiple-regression-models3-min" class="headerlink" title="Overfitting for more general multiple regression models3 min"></a>Overfitting for more general multiple regression models3 min</h4><h3 id="The-ridge-objective"><a href="#The-ridge-objective" class="headerlink" title="The ridge objective"></a>The ridge objective</h3><h4 id="Balancing-fit-and-magnitude-of-coefficients7-min"><a href="#Balancing-fit-and-magnitude-of-coefficients7-min" class="headerlink" title="Balancing fit and magnitude of coefficients7 min"></a>Balancing fit and magnitude of coefficients7 min</h4><h4 id="The-resulting-ridge-objective-and-its-extreme-solutions5-min"><a href="#The-resulting-ridge-objective-and-its-extreme-solutions5-min" class="headerlink" title="The resulting ridge objective and its extreme solutions5 min"></a>The resulting ridge objective and its extreme solutions5 min</h4><h4 id="How-ridge-regression-balances-bias-and-variance1-min"><a href="#How-ridge-regression-balances-bias-and-variance1-min" class="headerlink" title="How ridge regression balances bias and variance1 min"></a>How ridge regression balances bias and variance1 min</h4><h4 id="Download-the-notebook-and-follow-along10-min-1"><a href="#Download-the-notebook-and-follow-along10-min-1" class="headerlink" title="Download the notebook and follow along10 min"></a>Download the notebook and follow along10 min</h4><h4 id="Ridge-regression-demo9-min"><a href="#Ridge-regression-demo9-min" class="headerlink" title="Ridge regression demo9 min"></a>Ridge regression demo9 min</h4><h4 id="The-ridge-coefficient-path4-min"><a href="#The-ridge-coefficient-path4-min" class="headerlink" title="The ridge coefficient path4 min"></a>The ridge coefficient path4 min</h4><h3 id="Optimizing-the-ridge-objective"><a href="#Optimizing-the-ridge-objective" class="headerlink" title="Optimizing the ridge objective"></a>Optimizing the ridge objective</h3><h4 id="Computing-the-gradient-of-the-ridge-objective5-min"><a href="#Computing-the-gradient-of-the-ridge-objective5-min" class="headerlink" title="Computing the gradient of the ridge objective5 min"></a>Computing the gradient of the ridge objective5 min</h4><h4 id="Approach-1-closed-form-solution6-min"><a href="#Approach-1-closed-form-solution6-min" class="headerlink" title="Approach 1: closed-form solution6 min"></a>Approach 1: closed-form solution6 min</h4><h4 id="Discussing-the-closed-form-solution5-min"><a href="#Discussing-the-closed-form-solution5-min" class="headerlink" title="Discussing the closed-form solution5 min"></a>Discussing the closed-form solution5 min</h4><h4 id="Approach-2-gradient-descent9-min"><a href="#Approach-2-gradient-descent9-min" class="headerlink" title="Approach 2: gradient descent9 min"></a>Approach 2: gradient descent9 min</h4><h3 id="Tying-up-the-loose-ends"><a href="#Tying-up-the-loose-ends" class="headerlink" title="Tying up the loose ends"></a>Tying up the loose ends</h3><h4 id="Selecting-tuning-parameters-via-cross-validation3-min"><a href="#Selecting-tuning-parameters-via-cross-validation3-min" class="headerlink" title="Selecting tuning parameters via cross validation3 min"></a>Selecting tuning parameters via cross validation3 min</h4><h4 id="K-fold-cross-validation5-min"><a href="#K-fold-cross-validation5-min" class="headerlink" title="K-fold cross validation5 min"></a>K-fold cross validation5 min</h4><h4 id="How-to-handle-the-intercept6-min"><a href="#How-to-handle-the-intercept6-min" class="headerlink" title="How to handle the intercept6 min"></a>How to handle the intercept6 min</h4><h4 id="A-brief-recap1-min-3"><a href="#A-brief-recap1-min-3" class="headerlink" title="A brief recap1 min"></a>A brief recap1 min</h4><h4 id="Quiz-Ridge-Regression9-questions"><a href="#Quiz-Ridge-Regression9-questions" class="headerlink" title="Quiz: Ridge Regression9 questions"></a>Quiz: Ridge Regression9 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.5.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.6.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/quiz1.1.7.PNG?imageslim" alt=""></p>
<h3 id="Programming-Assignment-1"><a href="#Programming-Assignment-1" class="headerlink" title="Programming Assignment 1"></a>Programming Assignment 1</h3><h4 id="Reading-Observing-effects-of-L2-penalty-in-polynomial-regression10-min"><a href="#Reading-Observing-effects-of-L2-penalty-in-polynomial-regression10-min" class="headerlink" title="Reading: Observing effects of L2 penalty in polynomial regression10 min"></a>Reading: Observing effects of L2 penalty in polynomial regression10 min</h4><h4 id="Quiz-Observing-effects-of-L2-penalty-in-polynomial-regression7-questions"><a href="#Quiz-Observing-effects-of-L2-penalty-in-polynomial-regression7-questions" class="headerlink" title="Quiz: Observing effects of L2 penalty in polynomial regression7 questions"></a>Quiz: Observing effects of L2 penalty in polynomial regression7 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA5/1.1.5.PNG?imageslim" alt=""></p>
<h3 id="Programming-Assignment-2"><a href="#Programming-Assignment-2" class="headerlink" title="Programming Assignment 2"></a>Programming Assignment 2</h3><h4 id="Reading-Implementing-ridge-regression-via-gradient-descent10-min"><a href="#Reading-Implementing-ridge-regression-via-gradient-descent10-min" class="headerlink" title="Reading: Implementing ridge regression via gradient descent10 min"></a>Reading: Implementing ridge regression via gradient descent10 min</h4><h4 id="Quiz-Implementing-ridge-regression-via-gradient-descent8-questions"><a href="#Quiz-Implementing-ridge-regression-via-gradient-descent8-questions" class="headerlink" title="Quiz: Implementing ridge regression via gradient descent8 questions"></a>Quiz: Implementing ridge regression via gradient descent8 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week4/PA6/1.1.5.PNG?imageslim" alt=""></p>
<h2 id="Week-5-Feature-Selection-amp-Lasso"><a href="#Week-5-Feature-Selection-amp-Lasso" class="headerlink" title="Week 5 Feature Selection &amp; Lasso"></a>Week 5 Feature Selection &amp; Lasso</h2><div class="note primary"><p>A fundamental machine learning task is to select amongst a set of features to include in a model. In this module, you will explore this idea in the context of multiple regression, and describe how such feature selection is important for both interpretability and efficiency of forming predictions.</p>
<p>To start, you will examine methods that search over an enumeration of models including different subsets of features. You will analyze both exhaustive search and greedy algorithms. Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge. This lasso method has had impact in numerous applied domains, and the ideas behind the method have fundamentally changed machine learning and statistics. You will also implement a coordinate descent algorithm for fitting a Lasso model.</p>
<p>Coordinate descent is another, general, optimization technique, which is useful in many areas of machine learning.</p>
</div>
<h3 id="Feature-selection-via-explicit-model-enumeration"><a href="#Feature-selection-via-explicit-model-enumeration" class="headerlink" title="Feature selection via explicit model enumeration"></a>Feature selection via explicit model enumeration</h3><h4 id="Slides-presented-in-this-module10-min-9"><a href="#Slides-presented-in-this-module10-min-9" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Lecture%20Slides/5.0-week5_lassoregression-annotated.pdf" target="_blank" rel="external">week5_lassoregression-annotated.pdf</a></p>
<h4 id="The-feature-selection-task3-min"><a href="#The-feature-selection-task3-min" class="headerlink" title="The feature selection task3 min"></a>The feature selection task3 min</h4><h4 id="All-subsets6-min"><a href="#All-subsets6-min" class="headerlink" title="All subsets6 min"></a>All subsets6 min</h4><h4 id="Complexity-of-all-subsets3-min"><a href="#Complexity-of-all-subsets3-min" class="headerlink" title="Complexity of all subsets3 min"></a>Complexity of all subsets3 min</h4><h4 id="Greedy-algorithms7-min"><a href="#Greedy-algorithms7-min" class="headerlink" title="Greedy algorithms7 min"></a>Greedy algorithms7 min</h4><h4 id="Complexity-of-the-greedy-forward-stepwise-algorithm2-min"><a href="#Complexity-of-the-greedy-forward-stepwise-algorithm2-min" class="headerlink" title="Complexity of the greedy forward stepwise algorithm2 min"></a>Complexity of the greedy forward stepwise algorithm2 min</h4><h3 id="Feature-selection-implicitly-via-regularized-regression"><a href="#Feature-selection-implicitly-via-regularized-regression" class="headerlink" title="Feature selection implicitly via regularized regression"></a>Feature selection implicitly via regularized regression</h3><h4 id="Can-we-use-regularization-for-feature-selection-3-min"><a href="#Can-we-use-regularization-for-feature-selection-3-min" class="headerlink" title="Can we use regularization for feature selection?3 min"></a>Can we use regularization for feature selection?3 min</h4><h4 id="Thresholding-ridge-coefficients-4-min"><a href="#Thresholding-ridge-coefficients-4-min" class="headerlink" title="Thresholding ridge coefficients?4 min"></a>Thresholding ridge coefficients?4 min</h4><h4 id="The-lasso-objective-and-its-coefficient-path7-min"><a href="#The-lasso-objective-and-its-coefficient-path7-min" class="headerlink" title="The lasso objective and its coefficient path7 min"></a>The lasso objective and its coefficient path7 min</h4><h3 id="Geometric-intuition-for-sparsity-of-lasso-solutions"><a href="#Geometric-intuition-for-sparsity-of-lasso-solutions" class="headerlink" title="Geometric intuition for sparsity of lasso solutions"></a>Geometric intuition for sparsity of lasso solutions</h3><h4 id="Visualizing-the-ridge-cost7-min"><a href="#Visualizing-the-ridge-cost7-min" class="headerlink" title="Visualizing the ridge cost7 min"></a>Visualizing the ridge cost7 min</h4><h4 id="Visualizing-the-ridge-solution6-min"><a href="#Visualizing-the-ridge-solution6-min" class="headerlink" title="Visualizing the ridge solution6 min"></a>Visualizing the ridge solution6 min</h4><h4 id="Visualizing-the-lasso-cost-and-solution7-min"><a href="#Visualizing-the-lasso-cost-and-solution7-min" class="headerlink" title="Visualizing the lasso cost and solution7 min"></a>Visualizing the lasso cost and solution7 min</h4><h4 id="Download-the-notebook-and-follow-along10-min-2"><a href="#Download-the-notebook-and-follow-along10-min-2" class="headerlink" title="Download the notebook and follow along10 min"></a>Download the notebook and follow along10 min</h4><h4 id="Lasso-demo5-min"><a href="#Lasso-demo5-min" class="headerlink" title="Lasso demo5 min"></a>Lasso demo5 min</h4><h3 id="Setting-the-stage-for-solving-the-lasso"><a href="#Setting-the-stage-for-solving-the-lasso" class="headerlink" title="Setting the stage for solving the lasso"></a>Setting the stage for solving the lasso</h3><h4 id="What-makes-the-lasso-objective-different3-min"><a href="#What-makes-the-lasso-objective-different3-min" class="headerlink" title="What makes the lasso objective different3 min"></a>What makes the lasso objective different3 min</h4><h4 id="Coordinate-descent5-min"><a href="#Coordinate-descent5-min" class="headerlink" title="Coordinate descent5 min"></a>Coordinate descent5 min</h4><h4 id="Normalizing-features3-min"><a href="#Normalizing-features3-min" class="headerlink" title="Normalizing features3 min"></a>Normalizing features3 min</h4><h4 id="Coordinate-descent-for-least-squares-regression-normalized-features-8-min"><a href="#Coordinate-descent-for-least-squares-regression-normalized-features-8-min" class="headerlink" title="Coordinate descent for least squares regression (normalized features)8 min"></a>Coordinate descent for least squares regression (normalized features)8 min</h4><h3 id="Optimizing-the-lasso-objective"><a href="#Optimizing-the-lasso-objective" class="headerlink" title="Optimizing the lasso objective"></a>Optimizing the lasso objective</h3><h4 id="Coordinate-descent-for-lasso-normalized-features-5-min"><a href="#Coordinate-descent-for-lasso-normalized-features-5-min" class="headerlink" title="Coordinate descent for lasso (normalized features)5 min"></a>Coordinate descent for lasso (normalized features)5 min</h4><h4 id="Assessing-convergence-and-other-lasso-solvers2-min"><a href="#Assessing-convergence-and-other-lasso-solvers2-min" class="headerlink" title="Assessing convergence and other lasso solvers2 min"></a>Assessing convergence and other lasso solvers2 min</h4><h4 id="Coordinate-descent-for-lasso-unnormalized-features-1-min"><a href="#Coordinate-descent-for-lasso-unnormalized-features-1-min" class="headerlink" title="Coordinate descent for lasso (unnormalized features)1 min"></a>Coordinate descent for lasso (unnormalized features)1 min</h4><h3 id="OPTIONAL-ADVANCED-MATERIAL-Deriving-the-lasso-coordinate-descent-update"><a href="#OPTIONAL-ADVANCED-MATERIAL-Deriving-the-lasso-coordinate-descent-update" class="headerlink" title="OPTIONAL ADVANCED MATERIAL: Deriving the lasso coordinate descent update"></a>OPTIONAL ADVANCED MATERIAL: Deriving the lasso coordinate descent update</h3><h4 id="Deriving-the-lasso-coordinate-descent-update19-min"><a href="#Deriving-the-lasso-coordinate-descent-update19-min" class="headerlink" title="Deriving the lasso coordinate descent update19 min"></a>Deriving the lasso coordinate descent update19 min</h4><h3 id="Tying-up-loose-ends"><a href="#Tying-up-loose-ends" class="headerlink" title="Tying up loose ends"></a>Tying up loose ends</h3><h4 id="Choosing-the-penalty-strength-and-other-practical-issues-with-lasso5-min"><a href="#Choosing-the-penalty-strength-and-other-practical-issues-with-lasso5-min" class="headerlink" title="Choosing the penalty strength and other practical issues with lasso5 min"></a>Choosing the penalty strength and other practical issues with lasso5 min</h4><h4 id="A-brief-recap3-min"><a href="#A-brief-recap3-min" class="headerlink" title="A brief recap3 min"></a>A brief recap3 min</h4><h4 id="Quiz-Feature-Selection-and-Lasso7-questions"><a href="#Quiz-Feature-Selection-and-Lasso7-questions" class="headerlink" title="Quiz: Feature Selection and Lasso7 questions"></a>Quiz: Feature Selection and Lasso7 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.2.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.5.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.2.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.6.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.1.7.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/1.2.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/2.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/quiz6/3.1.1.PNG?imageslim" alt=""></p>
<h3 id="Programming-Assignment-1-1"><a href="#Programming-Assignment-1-1" class="headerlink" title="Programming Assignment 1"></a>Programming Assignment 1</h3><h4 id="Reading-Using-LASSO-to-select-features10-min"><a href="#Reading-Using-LASSO-to-select-features10-min" class="headerlink" title="Reading: Using LASSO to select features10 min"></a>Reading: Using LASSO to select features10 min</h4><h4 id="Quiz-Using-LASSO-to-select-features6-questions"><a href="#Quiz-Using-LASSO-to-select-features6-questions" class="headerlink" title="Quiz: Using LASSO to select features6 questions"></a>Quiz: Using LASSO to select features6 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/1.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA7/2.1.1.PNG?imageslim" alt=""></p>
<h3 id="Programming-Assignment-2-1"><a href="#Programming-Assignment-2-1" class="headerlink" title="Programming Assignment 2"></a>Programming Assignment 2</h3><h4 id="Reading-Implementing-LASSO-using-coordinate-descent10-min"><a href="#Reading-Implementing-LASSO-using-coordinate-descent10-min" class="headerlink" title="Reading: Implementing LASSO using coordinate descent10 min"></a>Reading: Implementing LASSO using coordinate descent10 min</h4><h4 id="Quiz-Implementing-LASSO-using-coordinate-descent8-questions"><a href="#Quiz-Implementing-LASSO-using-coordinate-descent8-questions" class="headerlink" title="Quiz: Implementing LASSO using coordinate descent8 questions"></a>Quiz: Implementing LASSO using coordinate descent8 questions</h4><p><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.1.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.2.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.3.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.4.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/1.1.5.PNG?imageslim" alt=""><br><img src="http://oktwqa7br.bkt.clouddn.com/Coursera/UW/2/Week5/PA8/2.1.1.PNG?imageslim" alt=""></p>
<h2 id="Week-6"><a href="#Week-6" class="headerlink" title="Week 6"></a>Week 6</h2><h3 id="Nearest-Neighbors-amp-Kernel-Regression"><a href="#Nearest-Neighbors-amp-Kernel-Regression" class="headerlink" title="Nearest Neighbors &amp; Kernel Regression"></a>Nearest Neighbors &amp; Kernel Regression</h3><div class="note primary"><p>Up to this point, we have focused on methods that fit parametric functions—like polynomials and hyperplanes—to the entire dataset. In this module, we instead turn our attention to a class of “nonparametric” methods. These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations.</p>
<p>We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set. This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors. Building on this idea, we turn to kernel regression. Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point. You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle. You will also implement these techniques and observe their practical behavior.</p>
</div>
<h4 id="Motivating-local-fits"><a href="#Motivating-local-fits" class="headerlink" title="Motivating local fits"></a>Motivating local fits</h4><h5 id="Slides-presented-in-this-module10-min-10"><a href="#Slides-presented-in-this-module10-min-10" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Lecture%20Slides/6.0-week6_NNkernelregression-annotated.pdf" target="_blank" rel="external">week6_NNkernelregression-annotated.pdf</a></p>
<h5 id="Limitations-of-parametric-regression3-min"><a href="#Limitations-of-parametric-regression3-min" class="headerlink" title="Limitations of parametric regression3 min"></a>Limitations of parametric regression3 min</h5><h4 id="Nearest-neighbor-regression"><a href="#Nearest-neighbor-regression" class="headerlink" title="Nearest neighbor regression"></a>Nearest neighbor regression</h4><h5 id="1-Nearest-neighbor-regression-approach8-min"><a href="#1-Nearest-neighbor-regression-approach8-min" class="headerlink" title="1-Nearest neighbor regression approach8 min"></a>1-Nearest neighbor regression approach8 min</h5><h5 id="Distance-metrics4-min"><a href="#Distance-metrics4-min" class="headerlink" title="Distance metrics4 min"></a>Distance metrics4 min</h5><h5 id="1-Nearest-neighbor-algorithm3-min"><a href="#1-Nearest-neighbor-algorithm3-min" class="headerlink" title="1-Nearest neighbor algorithm3 min"></a>1-Nearest neighbor algorithm3 min</h5><h4 id="k-Nearest-neighbors-and-weighted-k-nearest-neighbors"><a href="#k-Nearest-neighbors-and-weighted-k-nearest-neighbors" class="headerlink" title="k-Nearest neighbors and weighted k-nearest neighbors"></a>k-Nearest neighbors and weighted k-nearest neighbors</h4><h5 id="k-Nearest-neighbors-regression7-min"><a href="#k-Nearest-neighbors-regression7-min" class="headerlink" title="k-Nearest neighbors regression7 min"></a>k-Nearest neighbors regression7 min</h5><h5 id="k-Nearest-neighbors-in-practice3-min"><a href="#k-Nearest-neighbors-in-practice3-min" class="headerlink" title="k-Nearest neighbors in practice3 min"></a>k-Nearest neighbors in practice3 min</h5><h5 id="Weighted-k-nearest-neighbors4-min"><a href="#Weighted-k-nearest-neighbors4-min" class="headerlink" title="Weighted k-nearest neighbors4 min"></a>Weighted k-nearest neighbors4 min</h5><h4 id="Kernel-regression"><a href="#Kernel-regression" class="headerlink" title="Kernel regression"></a>Kernel regression</h4><h5 id="From-weighted-k-NN-to-kernel-regression6-min"><a href="#From-weighted-k-NN-to-kernel-regression6-min" class="headerlink" title="From weighted k-NN to kernel regression6 min"></a>From weighted k-NN to kernel regression6 min</h5><h5 id="Global-fits-of-parametric-models-vs-local-fits-of-kernel-regression6-min"><a href="#Global-fits-of-parametric-models-vs-local-fits-of-kernel-regression6-min" class="headerlink" title="Global fits of parametric models vs. local fits of kernel regression6 min"></a>Global fits of parametric models vs. local fits of kernel regression6 min</h5><h4 id="k-NN-and-kernel-regression-wrapup"><a href="#k-NN-and-kernel-regression-wrapup" class="headerlink" title="k-NN and kernel regression wrapup"></a>k-NN and kernel regression wrapup</h4><h5 id="Performance-of-NN-as-amount-of-data-grows7-min"><a href="#Performance-of-NN-as-amount-of-data-grows7-min" class="headerlink" title="Performance of NN as amount of data grows7 min"></a>Performance of NN as amount of data grows7 min</h5><h5 id="Issues-with-high-dimensions-data-scarcity-and-computational-complexity3-min"><a href="#Issues-with-high-dimensions-data-scarcity-and-computational-complexity3-min" class="headerlink" title="Issues with high-dimensions, data scarcity, and computational complexity3 min"></a>Issues with high-dimensions, data scarcity, and computational complexity3 min</h5><h5 id="k-NN-for-classification1-min"><a href="#k-NN-for-classification1-min" class="headerlink" title="k-NN for classification1 min"></a>k-NN for classification1 min</h5><h5 id="A-brief-recap1-min-4"><a href="#A-brief-recap1-min-4" class="headerlink" title="A brief recap1 min"></a>A brief recap1 min</h5><h5 id="Quiz-Nearest-Neighbors-amp-Kernel-Regression7-questions"><a href="#Quiz-Nearest-Neighbors-amp-Kernel-Regression7-questions" class="headerlink" title="Quiz: Nearest Neighbors &amp; Kernel Regression7 questions"></a>Quiz: Nearest Neighbors &amp; Kernel Regression7 questions</h5><h4 id="Programming-Assignment"><a href="#Programming-Assignment" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h4><h5 id="Reading-Predicting-house-prices-using-k-nearest-neighbors-regression10-min"><a href="#Reading-Predicting-house-prices-using-k-nearest-neighbors-regression10-min" class="headerlink" title="Reading: Predicting house prices using k-nearest neighbors regression10 min"></a>Reading: Predicting house prices using k-nearest neighbors regression10 min</h5><h5 id="Quiz-Predicting-house-prices-using-k-nearest-neighbors-regression8-questions"><a href="#Quiz-Predicting-house-prices-using-k-nearest-neighbors-regression8-questions" class="headerlink" title="Quiz: Predicting house prices using k-nearest neighbors regression8 questions"></a>Quiz: Predicting house prices using k-nearest neighbors regression8 questions</h5><h3 id="Closing-Remarks"><a href="#Closing-Remarks" class="headerlink" title="Closing Remarks"></a>Closing Remarks</h3><div class="note primary"><p>In the conclusion of the course, we will recap what we have covered. This represents both techniques specific to regression, as well as foundational machine learning concepts that will appear throughout the specialization. We also briefly discuss some important regression techniques we did not cover in this course.</p>
<p>We conclude with an overview of what’s in store for you in the rest of the specialization.</p>
</div>
<h4 id="What-we’ve-learned"><a href="#What-we’ve-learned" class="headerlink" title="What we’ve learned"></a>What we’ve learned</h4><h5 id="Slides-presented-in-this-module10-min-11"><a href="#Slides-presented-in-this-module10-min-11" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Regression/blob/master/Lecture%20Slides/6.1-closing.pdf" target="_blank" rel="external">closing.pdf</a></p>
<h5 id="Simple-and-multiple-regression4-min"><a href="#Simple-and-multiple-regression4-min" class="headerlink" title="Simple and multiple regression4 min"></a>Simple and multiple regression4 min</h5><h5 id="Assessing-performance-and-ridge-regression7-min"><a href="#Assessing-performance-and-ridge-regression7-min" class="headerlink" title="Assessing performance and ridge regression7 min"></a>Assessing performance and ridge regression7 min</h5><h5 id="Feature-selection-lasso-and-nearest-neighbor-regression4-min"><a href="#Feature-selection-lasso-and-nearest-neighbor-regression4-min" class="headerlink" title="Feature selection, lasso, and nearest neighbor regression4 min"></a>Feature selection, lasso, and nearest neighbor regression4 min</h5><h4 id="Summary-and-what’s-ahead-in-the-specialization"><a href="#Summary-and-what’s-ahead-in-the-specialization" class="headerlink" title="Summary and what’s ahead in the specialization"></a>Summary and what’s ahead in the specialization</h4><h5 id="What-we-covered-and-what-we-didn’t-cover5-min"><a href="#What-we-covered-and-what-we-didn’t-cover5-min" class="headerlink" title="What we covered and what we didn’t cover5 min"></a>What we covered and what we didn’t cover5 min</h5><h5 id="Thank-you-1-min"><a href="#Thank-you-1-min" class="headerlink" title="Thank you!1 min"></a>Thank you!1 min</h5><h1 id="Machine-Learning-Classification"><a href="#Machine-Learning-Classification" class="headerlink" title="Machine Learning: Classification"></a>Machine Learning: Classification</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/ml-classification" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/tree/master/Lecture%20Slides" target="_blank" rel="external">here</a></p>
<p>About this course: Case Studies: Analyzing Sentiment &amp; Loan Default Prediction</p>
<p>In our case study on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,…).  In our second case study for this course, loan default prediction, you will tackle financial data, and predict when a loan is likely to be risky or safe for the bank. These tasks are an examples of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification. </p>
<p>In this course, you will create classifiers that provide state-of-the-art performance on a variety of tasks.  You will become familiar with  the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting.  In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent.  You will implement these technique on real-world, large-scale machine learning tasks.  You will also address significant tasks you will face in real-world applications of ML, including handling missing data and measuring precision and recall to evaluate a classifier.  This course is hands-on, action-packed, and full of visualizations and illustrations of how these techniques will behave on real data.  We’ve also included optional content in every module, covering advanced topics for those who want to go even deeper! </p>
<p>Learning Objectives: By the end of this course, you will be able to:<br>   -Describe the input and output of a classification model.<br>   -Tackle both binary and multiclass classification problems.<br>   -Implement a logistic regression model for large-scale classification.<br>   -Create a non-linear model using decision trees.<br>   -Improve the performance of any model using boosting.<br>   -Scale your methods with stochastic gradient ascent.<br>   -Describe the underlying decision boundaries.<br>   -Build a classification model to predict sentiment in a product review dataset.<br>   -Analyze financial data to predict loan defaults.<br>   -Use techniques for handling missing data.<br>   -Evaluate your models using precision-recall metrics.<br>   -Implement these techniques in Python (or in the language of your choice, though Python is highly recommended).</p>
</div>
<h2 id="Week-1-1"><a href="#Week-1-1" class="headerlink" title="Week 1"></a>Week 1</h2><h3 id="Welcome-1"><a href="#Welcome-1" class="headerlink" title="Welcome !"></a>Welcome !</h3><div class="note primary"><p>Classification is one of the most widely used techniques in machine learning, with a broad array of applications, including sentiment analysis, ad targeting, spam detection, risk assessment, medical diagnosis and image classification. The core goal of classification is to predict a category or class y from some inputs x. Through this course, you will become familiar with the fundamental models and algorithms used in classification, as well as a number of core machine learning concepts. Rather than covering all aspects of classification, you will focus on a few core techniques, which are widely used in the real-world to get state-of-the-art performance. By following our hands-on approach, you will implement your own algorithms on multiple real-world tasks, and deeply grasp the core techniques needed to be successful with these approaches in practice. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.</p>
</div>
<h4 id="Welcome-to-the-course"><a href="#Welcome-to-the-course" class="headerlink" title="Welcome to the course"></a>Welcome to the course</h4><h5 id="Important-Update-regarding-the-Machine-Learning-Specialization-10-min"><a href="#Important-Update-regarding-the-Machine-Learning-Specialization-10-min" class="headerlink" title="Important Update regarding the Machine Learning Specialization 10 min"></a>Important Update regarding the Machine Learning Specialization 10 min</h5><p>Hello Machine Learning learners,</p>
<p>Please know that due to unforeseen circumstances, courses 5 and 6 - Recommender Systems &amp; Dimensionality Reduction and An Intelligent Application with Deep Learning - will not be launching as part of the Machine Learning Specialization. We understand this may come as very disappointing news and we’re deeply sorry for this inconvenience. If you have paid for these courses or have received financial aid from Coursera, you will remain eligible to earn your Specialization Certificate upon successfully completing courses 1-4 of the Specialization. If you paid for courses 5 &amp; 6 via a pre-payment toward the Specialization, Coursera has provided you with free access to two other courses offered by the University of Washington: <strong>Computational Neuroscience</strong> and <strong>Data Manipulation at Scale: Systems and Algorithms</strong>. An email has been sent out with specific instructions on how to enroll in these courses. If you individually paid for either x or y course, you will receive a refund within the next two weeks.</p>
<p>If you have any questions or would like to request a refund, please feel free to contact Coursera’s 24/7 learner support team via the Request a Refund article in the Learner Help Center. The last day to request a refund will be April 30, 2017. We value you as a Coursera learner and want to ensure that your experience with the Machine Learning Specialization remains a positive one.</p>
<p>Regards,</p>
<p>The Coursera Team</p>
<h5 id="Slides-presented-in-this-module-10-min"><a href="#Slides-presented-in-this-module-10-min" class="headerlink" title="Slides presented in this module 10 min"></a>Slides presented in this module 10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/1.0-intro.pdf" target="_blank" rel="external">intro.pdf</a></p>
<h5 id="Welcome-to-the-classification-course-a-part-of-the-Machine-Learning-Specialization-1-min"><a href="#Welcome-to-the-classification-course-a-part-of-the-Machine-Learning-Specialization-1-min" class="headerlink" title="Welcome to the classification course, a part of the Machine Learning Specialization 1 min"></a>Welcome to the classification course, a part of the Machine Learning Specialization 1 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/YMpzf/welcome-to-the-classification-course-a-part-of-the-machine-learning" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/YMpzf/welcome-to-the-classification-course-a-part-of-the-machine-learning</a></p>
<h5 id="What-is-this-course-about-6-min"><a href="#What-is-this-course-about-6-min" class="headerlink" title="What is this course about? 6 min"></a>What is this course about? 6 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/qZhKx/what-is-this-course-about" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/qZhKx/what-is-this-course-about</a></p>
<h5 id="Impact-of-classification-1-min"><a href="#Impact-of-classification-1-min" class="headerlink" title="Impact of classification 1 min"></a>Impact of classification 1 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/OnpWH/impact-of-classification" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/OnpWH/impact-of-classification</a></p>
<h4 id="Course-overview-and-details"><a href="#Course-overview-and-details" class="headerlink" title="Course overview and details"></a>Course overview and details</h4><h5 id="Course-overview-3-min"><a href="#Course-overview-3-min" class="headerlink" title="Course overview 3 min"></a>Course overview 3 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/84fuF/course-overview" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/84fuF/course-overview</a></p>
<h5 id="Outline-of-first-half-of-course-5-min"><a href="#Outline-of-first-half-of-course-5-min" class="headerlink" title="Outline of first half of course 5 min"></a>Outline of first half of course 5 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/LyubT/outline-of-first-half-of-course" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/LyubT/outline-of-first-half-of-course</a></p>
<h5 id="Outline-of-second-half-of-course-5-min"><a href="#Outline-of-second-half-of-course-5-min" class="headerlink" title="Outline of second half of course 5 min"></a>Outline of second half of course 5 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/z1g9k/outline-of-second-half-of-course" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/z1g9k/outline-of-second-half-of-course</a></p>
<h5 id="Assumed-background-3-min"><a href="#Assumed-background-3-min" class="headerlink" title="Assumed background 3 min"></a>Assumed background 3 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/IindM/assumed-background" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/IindM/assumed-background</a></p>
<h5 id="Let’s-get-started-45-sec"><a href="#Let’s-get-started-45-sec" class="headerlink" title="Let’s get started! 45 sec"></a>Let’s get started! 45 sec</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/AktDn/lets-get-started" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/AktDn/lets-get-started</a></p>
<h5 id="Reading-Software-tools-you’ll-need-10-min"><a href="#Reading-Software-tools-you’ll-need-10-min" class="headerlink" title="Reading: Software tools you’ll need 10 min"></a>Reading: Software tools you’ll need 10 min</h5><h6 id="Software-tools-you’ll-need-for-this-course"><a href="#Software-tools-you’ll-need-for-this-course" class="headerlink" title="Software tools you’ll need for this course"></a>Software tools you’ll need for this course</h6><p>How this specialization was designed. The learning approach in this specialization is to start from use cases and then dig into algorithms and methods, what we call a case-studies approach. We are very excited about this approach, since it has worked well in several other courses. The first course, <a href="https://www.coursera.org/learn/ml-foundations" target="_blank" rel="external">Machine Learning: Foundations</a>, was focused on understanding how ML can be used in various cases studies. The second course, <a href="https://www.coursera.org/learn/ml-regression" target="_blank" rel="external">Machine Learning: Regression</a>, was focused on models that predict a continuous value from input features. The follow on courses will dig into more details of algorithms and methods of other ML areas. We expect all learners to have taken the first and second course, before taking this course.</p>
<p>Classification - A Machine Learning Approach. This course focuses classification, one of the most important types of data analysis, with a wide range of applications. After successfully completing this course, you will be able to use classification methods in practice, implement some of the most fundamental algorithms in this area, and choose the right model for your task. You will become familiar with the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting. In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent.</p>
<h6 id="Programming-assignment-format"><a href="#Programming-assignment-format" class="headerlink" title="Programming assignment format"></a>Programming assignment format</h6><p>Almost every module will be associated with one or two programming assignments. The goal of these assignments is to have hands-on experience on the techniques we discuss in lectures. To test your implementations, you will be asked questions in a quiz following the assignment.</p>
<p>You will be implementing core classification techniques or other ML concepts from scratch in most modules. In a few module, you will also explore fundamental ML concepts, such as regularization or precision-recall, using existing implementations of ML algorithms, with the goal of gaining proficiency in the ML concepts.</p>
<h6 id="Why-Python"><a href="#Why-Python" class="headerlink" title="Why Python"></a>Why Python</h6><p>In this course, we are going to use the Python programming language to build several intelligent applications that use machine learning. Python is a simple scripting language that makes it easy to interact with data. Furthermore, Python has a wide range of packages that make it easy to get started and build applications, from the simplest ones to the most complex. Python is widely used in industry, and is becoming the de facto language for data science in industry. (R is another alternative language. However, R tends to be significantly less scalable and has very few deployment tools, thus it is seldomly used for production code in industry. It is possible, but discouraged to use R in this specialization.)</p>
<p>We will also encourage the use the IPython Notebook in our assignments. The IPython Notebook is a simple interactive environment for programming with Python, which makes it really easy to share your results. Think about it as a combination of a Python terminal and a wiki page. Thus, you can combine code, plots and text to explain what you did. (You are not required to use IPython Notebook in the assignments, and should have no problem using straight up Python if you prefer.)</p>
<h6 id="Useful-software-tools"><a href="#Useful-software-tools" class="headerlink" title="Useful software tools"></a>Useful software tools</h6><p>Although you will be implementing algorithms from scratch in various assignments, some software tools will be useful in the process. In particular, there are four types of data tools that would be helpful:</p>
<ul>
<li>Data manipulation: to help you slice-and-dice the data, create new features, and clean the data.</li>
<li>Matrix operations: in the inner loops of your algorithms, you will do various matrix operations, and libraries focus on these will speed-up your code significantly.</li>
<li>Plotting library: so you can visualize data and models.</li>
<li>Pre-implemented ML algorithms: in some assignments where we are focusing on exploring ML classification models, you will use a pre-implemented ML algorithms to help focus your efforts on the fundamentals.</li>
</ul>
<p>1.Tools for data manipulation</p>
<p>For data manipulation, we recommend using <a href="https://github.com/turi-code/SFrame" target="_blank" rel="external">SFrame</a>, an open-source, highly-scalable Python library for data manipulation. An alternative is the <a href="http://pandas.pydata.org/" target="_blank" rel="external">Pandas</a> library. A huge advantage of SFrame over Pandas is that with SFrame, you are not limited to datasets that fit in memory, which allows you to deal with large datasets, even on a laptop. (The SFrame API is very similar to Pandas’ API.<a href="https://turi.com/learn/translator/" target="_blank" rel="external"> Here is a doc showing the relationship between the two of them</a>.)</p>
<p>2.Tools for matrix operation</p>
<p>For matrix operations, we strongly recommend <a href="http://www.numpy.org/" target="_blank" rel="external">Numpy</a>, an open-source Python library that provides fast performance, for data that fits in memory.</p>
<p>3.Tools for plotting</p>
<p>For plotting, we strongly recommend you use <a href="http://matplotlib.org/" target="_blank" rel="external">Matplotlib</a>, an open-source Python library with extensive plotting functionality.</p>
<p>4.Tools with pre-implemented ML algorithms</p>
<p>For the few assignments where you will be using pre-implemented ML algorithms, we recommend you use <a href="https://turi.com/products/create/" target="_blank" rel="external">GraphLab Create</a>, which we used in the first and second course, a package we have been working on for many years now, and has seen an exciting adoption curve, especially in industry with folks building real applications. A popular alternative is to use <a href="http://scikit-learn.org/stable/" target="_blank" rel="external">scikit-learn</a>. GraphLab Create is more scalable than scikit-learn and simpler to use when your data is not numeric vectors. On the other hand, scikit-learn is open-source.</p>
<p>In this course, most of the assignments are about implementing algorithms from scratch, so this choice is more flexible than in the first course. We are happy, however, for you to use any tool(s) of your liking. As you will notice, we are only grading the output of your programs, so the specific software tool is not the focus of the course. More details on using other tools are at the end of this doc.</p>
<p>It’s important to emphasize that this specialization is not about providing training for a specific software package. The goal of the specialization is for your effort to be spent on learning the fundamental concepts and algorithms behind machine learning in a hands-on fashion. These concepts transcend any single package. What you learn here you can use whether you write code from scratch, use any existing ML packages out there, or any that may be developed in the future. We are happy to hear that so many of you are enjoying this approach so far!</p>
<p>5.Licenses for SFrame &amp; GraphLab Create</p>
<p>The SFrame package is available in <a href="https://github.com/turi-code/SFrame" target="_blank" rel="external">open-source under a permissive BSD license</a>. So, you will always be able to use SFrames for free. GraphLab Create is free on a 1-year, renewable license for educational purposes, including Coursera. The reason we suggest you use GraphLab Create for this course is because this software will make it much easier for you see machine learning in action and to help you complete your assignments quickly.</p>
<h6 id="Upgrade-GraphLab-Create"><a href="#Upgrade-GraphLab-Create" class="headerlink" title="Upgrade GraphLab Create"></a>Upgrade GraphLab Create</h6><p>If you are using GraphLab Create and already have it installed, please make sure you upgrade to the latest version! The simplest way to do this is to:</p>
<p>Open the GraphLab Launcher.<br>Click on ‘TERMINAL’.<br>On the terminal window, type:<br><code>pip install --upgrade graphlab-create</code></p>
<h6 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h6><p>These are some good resources you can explore, if you are using the recommended software tools:</p>
<p>In the <a href="https://www.coursera.org/learn/ml-foundations" target="_blank" rel="external">first course of this ML specialization, Machine Learning Foundations</a>, we provided many tutorials and getting started guides. We recommend you go over those before tackling this course.<br>There are many Python resources available online. <a href="https://docs.python.org/2/index.html" target="_blank" rel="external">Here is a good place for documentation</a>.<br>For SFrame &amp; GraphLab Create, there is also a lot of information available online. Here are some starting points: the <a href="https://turi.com/learn/userguide/" target="_blank" rel="external">User Guide</a> and <a href="https://turi.com/products/create/docs/" target="_blank" rel="external">detailed API docs</a>.<br>For Numpy, here is a <a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">getting started guide</a>. We will also provide a tutorial when it’s time to use it.</p>
<h6 id="Installing-the-recommended-software-tools"><a href="#Installing-the-recommended-software-tools" class="headerlink" title="Installing the recommended software tools"></a>Installing the recommended software tools</h6><p>If you choose to use the recommended tools, you have two options: downloading and installing the required software or using a prepackaged version on a free instance on Amazon EC2.</p>
<p>1.Option 1: Downloading and installing all software on your own machine</p>
<p>Download and install Python, IPython Notebook, Numpy, SFrame and GraphLab Create. <a href="https://turi.com/learn/coursera/" target="_blank" rel="external">You can find the instructions here</a>.</p>
<p>2.Option 2: Using a free Amazon EC2 with all the software pre-installed</p>
<p>If you do not have a 64-bit computer, you will not be able to run GraphLab Create. Additionally, some of you may want a simple experience where you don’t have to download the course content and install everything locally. Here, we’ll address these situations!</p>
<p>Amazon EC2 offers free cloud computing hours with what they call micro instances. These instances are all we need to do the work for this course. We have created an image for one such instance that is easy to launch and contains all the course content. This will allow you to run everything you need for this course in the cloud for free, without having to install anything locally. (You do need to create an Amazon EC2 account and have internet access.)</p>
<p>You can find step-by-step instructions here:</p>
<p><a href="https://turi.com/download/install-graphlab-create-aws-coursera.html" target="_blank" rel="external">https://turi.com/download/install-graphlab-create-aws-coursera.html</a></p>
<p>We note that installing all the software on your own local machine may be the right option for most people; especially since you can run locally everything without needing to be online to do the homeworks. But, the option using Amazon EC2 should be a great alternative.</p>
<h6 id="Github-repository-with-starter-code"><a href="#Github-repository-with-starter-code" class="headerlink" title="Github repository with starter code"></a>Github repository with starter code</h6><p>In each module of the course, we have a reading with the assignments for that module as well as some starter code. For those interested, the starter code and demos used in this course are also available in a public Github repository:</p>
<p><a href="https://github.com/learnml/machine-learning-specialization" target="_blank" rel="external">https://github.com/learnml/machine-learning-specialization</a></p>
<h6 id="Using-other-software-packages"><a href="#Using-other-software-packages" class="headerlink" title="Using other software packages"></a>Using other software packages</h6><p>We strongly encourage you to use the recommended software packages for this course, since they will allow you to learn the fundamental concepts more quickly. However, you are welcome to use others. Here are a few notes if you do so.</p>
<p>1.Installing other software tools</p>
<p>In the instructions above, you will be using the GraphLab Launcher, which will automatically install Python, IPython Notebook, Numpy, Matplotlib, SFrame and GraphLab Create. If you don’t use the GraphLab Launcher, you will need to install each of these tools separately, by following the pages linked above. Anaconda is a good tool to help simplify some of this installation.</p>
<p>2.If you are using SFrame, but not GraphLab Create</p>
<p>GraphLab Create uses SFrame under the hood, but you can use just SFrame for most assignments. If you choose to do so, in the starter code for the assignments, you should change the line</p>
<p><code>import graphlab</code></p>
<p>import sframe</p>
<p><code>import sframe</code><br>and everything should work with just some small modifications, e.g., the calls:</p>
<p><code>graphlab.SFrame(...)</code><br>will become</p>
<p><code>sframe.SFrame(...)</code></p>
<p>3.If you are using other software tools out there</p>
<p>You are welcome to use other packages, e.g., <a href="http://scikit-learn.org/stable/" target="_blank" rel="external">scikit-learn</a> instead of GraphLab Create, or Pandas instead of SFrame, or even R instead of Python. If you choose to use all these different packages, we will provide the datasets (in standard CSV format) and the assignment questions will not depend specifically on the recommended tools.</p>
<h3 id="Linear-Classifiers-amp-Logistic-Regression"><a href="#Linear-Classifiers-amp-Logistic-Regression" class="headerlink" title="Linear Classifiers &amp; Logistic Regression"></a>Linear Classifiers &amp; Logistic Regression</h3><div class="note primary"><p>Linear classifiers are amongst the most practical classification methods. For example, in our sentiment analysis case-study, a linear classifier associates a coefficient with the counts of each word in the sentence. In this module, you will become proficient in this type of representation. You will focus on a particularly useful type of linear classifier called logistic regression, which, in addition to allowing you to predict a class, provides a probability associated with the prediction. These probabilities are extremely useful, since they provide a degree of confidence in the predictions. In this module, you will also be able to construct features from categorical inputs, and to tackle classification problems with more than two class (multiclass problems). You will examine the results of these techniques on a real-world product sentiment analysis task.</p>
</div>
<h4 id="Linear-classifiers"><a href="#Linear-classifiers" class="headerlink" title="Linear classifiers"></a>Linear classifiers</h4><h5 id="Slides-presented-in-this-module-10-min-1"><a href="#Slides-presented-in-this-module-10-min-1" class="headerlink" title="Slides presented in this module 10 min"></a>Slides presented in this module 10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/1.1-logistic-regression-model-annotated.rar" target="_blank" rel="external">logistic-regression-model-annotated.pdf</a></p>
<h5 id="Linear-classifiers-A-motivating-example-2-min"><a href="#Linear-classifiers-A-motivating-example-2-min" class="headerlink" title="Linear classifiers: A motivating example 2 min"></a>Linear classifiers: A motivating example 2 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/HNKIj/linear-classifiers-a-motivating-example" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/HNKIj/linear-classifiers-a-motivating-example</a></p>
<h5 id="Intuition-behind-linear-classifiers-3-min"><a href="#Intuition-behind-linear-classifiers-3-min" class="headerlink" title="Intuition behind linear classifiers 3 min"></a>Intuition behind linear classifiers 3 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/lCBwS/intuition-behind-linear-classifiers" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/lCBwS/intuition-behind-linear-classifiers</a></p>
<h5 id="Decision-boundaries-3-min"><a href="#Decision-boundaries-3-min" class="headerlink" title="Decision boundaries 3 min"></a>Decision boundaries 3 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/NIdE0/decision-boundaries" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/NIdE0/decision-boundaries</a></p>
<h5 id="Linear-classifier-model-5-min"><a href="#Linear-classifier-model-5-min" class="headerlink" title="Linear classifier model 5 min"></a>Linear classifier model 5 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/XBc9n/linear-classifier-model" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/XBc9n/linear-classifier-model</a></p>
<h5 id="Effect-of-coefficient-values-on-decision-boundary-2-min"><a href="#Effect-of-coefficient-values-on-decision-boundary-2-min" class="headerlink" title="Effect of coefficient values on decision boundary 2 min"></a>Effect of coefficient values on decision boundary 2 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/Qy2js/effect-of-coefficient-values-on-decision-boundary" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/Qy2js/effect-of-coefficient-values-on-decision-boundary</a></p>
<h5 id="Using-features-of-the-inputs-2-min"><a href="#Using-features-of-the-inputs-2-min" class="headerlink" title="Using features of the inputs 2 min"></a>Using features of the inputs 2 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/WHIMY/using-features-of-the-inputs" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/WHIMY/using-features-of-the-inputs</a></p>
<h4 id="Class-probabilities"><a href="#Class-probabilities" class="headerlink" title="Class probabilities"></a>Class probabilities</h4><h5 id="Predicting-class-probabilities-1-min"><a href="#Predicting-class-probabilities-1-min" class="headerlink" title="Predicting class probabilities 1 min"></a>Predicting class probabilities 1 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/j4Ji0/predicting-class-probabilities" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/j4Ji0/predicting-class-probabilities</a></p>
<h5 id="Review-of-basics-of-probabilities-6-min"><a href="#Review-of-basics-of-probabilities-6-min" class="headerlink" title="Review of basics of probabilities 6 min"></a>Review of basics of probabilities 6 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/p6rtM/review-of-basics-of-probabilities" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/p6rtM/review-of-basics-of-probabilities</a></p>
<h5 id="Review-of-basics-of-conditional-probabilities-8-min"><a href="#Review-of-basics-of-conditional-probabilities-8-min" class="headerlink" title="Review of basics of conditional probabilities 8 min"></a>Review of basics of conditional probabilities 8 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/Cun2N/review-of-basics-of-conditional-probabilities" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/Cun2N/review-of-basics-of-conditional-probabilities</a></p>
<h5 id="Using-probabilities-in-classification-2-min"><a href="#Using-probabilities-in-classification-2-min" class="headerlink" title="Using probabilities in classification 2 min"></a>Using probabilities in classification 2 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/f0nhO/using-probabilities-in-classification" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/f0nhO/using-probabilities-in-classification</a></p>
<h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h4><h5 id="Predicting-class-probabilities-with-generalized-linear-models-5-min"><a href="#Predicting-class-probabilities-with-generalized-linear-models-5-min" class="headerlink" title="Predicting class probabilities with (generalized) linear models 5 min"></a>Predicting class probabilities with (generalized) linear models 5 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/OV5Kt/predicting-class-probabilities-with-generalized-linear-models" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/OV5Kt/predicting-class-probabilities-with-generalized-linear-models</a></p>
<h5 id="The-sigmoid-or-logistic-link-function-4-min"><a href="#The-sigmoid-or-logistic-link-function-4-min" class="headerlink" title="The sigmoid (or logistic) link function 4 min"></a>The sigmoid (or logistic) link function 4 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/KXvGC/the-sigmoid-or-logistic-link-function" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/KXvGC/the-sigmoid-or-logistic-link-function</a></p>
<h5 id="Logistic-regression-model-5-min"><a href="#Logistic-regression-model-5-min" class="headerlink" title="Logistic regression model 5 min"></a>Logistic regression model 5 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/OJQXu/logistic-regression-model" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/OJQXu/logistic-regression-model</a></p>
<h5 id="Effect-of-coefficient-values-on-predicted-probabilities-7-min"><a href="#Effect-of-coefficient-values-on-predicted-probabilities-7-min" class="headerlink" title="Effect of coefficient values on predicted probabilities 7 min"></a>Effect of coefficient values on predicted probabilities 7 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/JkEEH/effect-of-coefficient-values-on-predicted-probabilities" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/JkEEH/effect-of-coefficient-values-on-predicted-probabilities</a></p>
<h5 id="Overview-of-learning-logistic-regression-models-2-min"><a href="#Overview-of-learning-logistic-regression-models-2-min" class="headerlink" title="Overview of learning logistic regression models 2 min"></a>Overview of learning logistic regression models 2 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/GuxAJ/overview-of-learning-logistic-regression-models" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/GuxAJ/overview-of-learning-logistic-regression-models</a></p>
<h4 id="Practical-issues-for-classification"><a href="#Practical-issues-for-classification" class="headerlink" title="Practical issues for classification"></a>Practical issues for classification</h4><h5 id="Encoding-categorical-inputs-4-min"><a href="#Encoding-categorical-inputs-4-min" class="headerlink" title="Encoding categorical inputs 4 min"></a>Encoding categorical inputs 4 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/kCY0D/encoding-categorical-inputs" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/kCY0D/encoding-categorical-inputs</a></p>
<h5 id="Multiclass-classification-with-1-versus-all-7-min"><a href="#Multiclass-classification-with-1-versus-all-7-min" class="headerlink" title="Multiclass classification with 1 versus all 7 min"></a>Multiclass classification with 1 versus all 7 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/N7QA6/multiclass-classification-with-1-versus-all" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/N7QA6/multiclass-classification-with-1-versus-all</a></p>
<h4 id="Summarizing-linear-classifiers-amp-logistic-regression"><a href="#Summarizing-linear-classifiers-amp-logistic-regression" class="headerlink" title="Summarizing linear classifiers &amp; logistic regression"></a>Summarizing linear classifiers &amp; logistic regression</h4><h5 id="Recap-of-logistic-regression-classifier-1-min"><a href="#Recap-of-logistic-regression-classifier-1-min" class="headerlink" title="Recap of logistic regression classifier 1 min"></a>Recap of logistic regression classifier 1 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/laPcB/recap-of-logistic-regression-classifier" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/laPcB/recap-of-logistic-regression-classifier</a></p>
<h5 id="Quiz-Linear-Classifiers-amp-Logistic-Regression-5-questions"><a href="#Quiz-Linear-Classifiers-amp-Logistic-Regression-5-questions" class="headerlink" title="Quiz: Linear Classifiers &amp; Logistic Regression 5 questions"></a>Quiz: Linear Classifiers &amp; Logistic Regression 5 questions</h5><div class="note primary"><p>QUIZ<br>Linear Classifiers &amp; Logistic Regression<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>August 20, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.(True/False) A linear classifier assigns the predicted class based on the sign of Score(x)=wTh(x).</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>2.(True/False) For a conditional probability distribution over y|x, where y takes on two values (+1, -1, i.e. good review, bad review) P(y=+1|x)+P(y=−1|x)=1.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.Which function does logistic regression use to “squeeze” the real line to [0, 1]?</p>
<p><input type="radio" disabled checked><label>Logistic function</label></p>
<p><input type="radio" disabled><label>Absolute value function</label></p>
<p><input type="radio" disabled><label>Zero function</label><br><br><br>1 point<br>4.If Score(x)=wTh(x)&gt;0, which of the following is true about P(y=+1|x)?</p>
<p><input type="radio" disabled><label>P(y = +1 | x) &lt;= 0.5</label></p>
<p><input type="radio" disabled checked><label>P(y = +1 | x) &gt; 0.5</label></p>
<p><input type="radio" disabled><label>Can’t say anything about P(y = +1 | x)</label><br><br><br>1 point<br>5.Consider training a 1 vs. all multiclass classifier for the problem of digit recognition using logistic regression. There are 10 digits, thus there are 10 classes. How many logistic regression classifiers will we have to train?</p>
<input type="”text”" placeholder="10">

<h4 id="Programming-Assignment-1"><a href="#Programming-Assignment-1" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h4><h5 id="Predicting-sentiment-from-product-reviews-10-min"><a href="#Predicting-sentiment-from-product-reviews-10-min" class="headerlink" title="Predicting sentiment from product reviews 10 min"></a>Predicting sentiment from product reviews 10 min</h5><h5 id="Quiz-Predicting-sentiment-from-product-reviews-12-questions"><a href="#Quiz-Predicting-sentiment-from-product-reviews-12-questions" class="headerlink" title="Quiz: Predicting sentiment from product reviews 12 questions"></a>Quiz: Predicting sentiment from product reviews 12 questions</h5><div class="note primary"><p>QUIZ<br>Predicting sentiment from product reviews<br>12 questions<br>To Pass70% or higher<br>Attempts3 every 8 hours<br>Deadline<br>August 20, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.How many weights are greater than or equal to 0?</p>
<p><input type="”text”" placeholder="85928"><br><br><br>1 point<br>2.Of the three data points in sample_test_data, which one has the lowest probability of being classified as a positive review?</p>
<p><input type="radio" disabled><label>First</label></p>
<p><input type="radio" disabled><label>Second</label></p>
<p><input type="radio" disabled checked><label>Third</label><br><br><br>1 point<br>3.Which of the following products are represented in the 20 most positive reviews?</p>
<p><input type="checkbox" disabled><label>Snuza Portable Baby Movement Monitor</label></p>
<p><input type="checkbox" disabled><label>MamaDoo Kids Foldable Play Yard Mattress Topper, Blue</label></p>
<p><input type="checkbox" disabled checked><label>Britax Decathlon Convertible Car Seat, Tiffany</label></p>
<p><input type="checkbox" disabled><label>Safety 1st Exchangeable Tip 3 in 1 Thermometer</label><br><br><br>1 point<br>4.Which of the following products are represented in the 20 most negative reviews?</p>
<p><input type="checkbox" disabled checked><label>The First Years True Choice P400 Premium Digital Monitor, 2 Parent Unit</label></p>
<p><input type="checkbox" disabled><label>JP Lizzy Chocolate Ice Classic Tote Set</label></p>
<p><input type="checkbox" disabled checked><label>Peg-Perego Tatamia High Chair, White Latte</label></p>
<p><input type="checkbox" disabled checked><label>Safety 1st High-Def Digital Monitor</label><br><br><br>1 point<br>5.What is the accuracy of the sentiment_model on the test_data? Round your answer to 2 decimal places (e.g. 0.76).</p>
<p><input type="”text”" placeholder="0.93"><br><br><br>1 point<br>6.Does a higher accuracy value on the training_data always imply that the classifier is better?</p>
<p><input type="radio" disabled><label>Yes, higher accuracy on training data always implies that the classifier is better.</label></p>
<p><input type="radio" disabled checked><label>No, higher accuracy on training data does not necessarily imply that the classifier is better.</label><br><br><br>1 point<br>7.Consider the coefficients of simple_model. There should be 21 of them, an intercept term + one for each word in significant_words.</p>
<p>How many of the 20 coefficients (corresponding to the 20 significant_words and excluding the intercept term) are positive for the simple_model?</p>
<p><input type="”text”" placeholder="10"><br><br><br>1 point<br>8.Are the positive words in the simple_model also positive words in the sentiment_model?</p>
<p><input type="radio" disabled checked><label>Yes</label></p>
<p><input type="radio" disabled><label>No</label><br><br><br>1 point<br>9.Which model (sentiment_model or simple_model) has higher accuracy on the TRAINING set?</p>
<p><input type="radio" disabled checked><label>Sentiment_model</label></p>
<p><input type="radio" disabled><label>Simple_model</label><br><br><br>1 point<br>10.Which model (sentiment_model or simple_model) has higher accuracy on the TEST set?</p>
<p><input type="radio" disabled checked><label>Sentiment_model</label></p>
<p><input type="radio" disabled><label>Simple_model</label><br><br><br>1 point<br>11.Enter the accuracy of the majority class classifier model on the test_data. Round your answer to two decimal places (e.g. 0.76).</p>
<p><input type="”text”" placeholder="0.84"><br><br><br>1 point<br>12.Is the sentiment_model definitely better than the majority class classifier (the baseline)?</p>
<p><input type="radio" disabled checked><label>Yes</label></p>
<p><input type="radio" disabled><label>No</label><br><br></p>
<h2 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h2><h3 id="Learning-Linear-Classifiers"><a href="#Learning-Linear-Classifiers" class="headerlink" title="Learning Linear Classifiers"></a>Learning Linear Classifiers</h3><div class="note primary"><p>Once familiar with linear classifiers and logistic regression, you can now dive in and write your first learning algorithm for classification. In particular, you will use gradient ascent to learn the coefficients of your classifier from data. You first will need to define the quality metric for these tasks using an approach called maximum likelihood estimation (MLE). You will also become familiar with a simple technique for selecting the step size for gradient ascent. An optional, advanced part of this module will cover the derivation of the gradient for logistic regression. You will implement your own learning algorithm for logistic regression from scratch, and use it to learn a sentiment analysis classifier.</p>
</div>
<h4 id="Maximum-likelihood-estimation"><a href="#Maximum-likelihood-estimation" class="headerlink" title="Maximum likelihood estimation"></a>Maximum likelihood estimation</h4><h5 id="Slides-presented-in-this-module10-min-12"><a href="#Slides-presented-in-this-module10-min-12" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/2.0-logistic-regression-learning-annotated.pdf" target="_blank" rel="external">logistic-regression-learning-annotated.pdf</a></p>
<h5 id="Goal-Learning-parameters-of-logistic-regression2-min"><a href="#Goal-Learning-parameters-of-logistic-regression2-min" class="headerlink" title="Goal: Learning parameters of logistic regression2 min"></a>Goal: Learning parameters of logistic regression2 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/uxALW/goal-learning-parameters-of-logistic-regression" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/uxALW/goal-learning-parameters-of-logistic-regression</a></p>
<h5 id="Intuition-behind-maximum-likelihood-estimation4-min"><a href="#Intuition-behind-maximum-likelihood-estimation4-min" class="headerlink" title="Intuition behind maximum likelihood estimation4 min"></a>Intuition behind maximum likelihood estimation4 min</h5><h5 id="Data-likelihood8-min"><a href="#Data-likelihood8-min" class="headerlink" title="Data likelihood8 min"></a>Data likelihood8 min</h5><h5 id="Finding-best-linear-classifier-with-gradient-ascent3-min"><a href="#Finding-best-linear-classifier-with-gradient-ascent3-min" class="headerlink" title="Finding best linear classifier with gradient ascent3 min"></a>Finding best linear classifier with gradient ascent3 min</h5><h4 id="Gradient-ascent-algorithm-for-learning-logistic-regression-classifier"><a href="#Gradient-ascent-algorithm-for-learning-logistic-regression-classifier" class="headerlink" title="Gradient ascent algorithm for learning logistic regression classifier"></a>Gradient ascent algorithm for learning logistic regression classifier</h4><h5 id="Review-of-gradient-ascent6-min"><a href="#Review-of-gradient-ascent6-min" class="headerlink" title="Review of gradient ascent6 min"></a>Review of gradient ascent6 min</h5><h5 id="Learning-algorithm-for-logistic-regression3-min"><a href="#Learning-algorithm-for-logistic-regression3-min" class="headerlink" title="Learning algorithm for logistic regression3 min"></a>Learning algorithm for logistic regression3 min</h5><h5 id="Example-of-computing-derivative-for-logistic-regression5-min"><a href="#Example-of-computing-derivative-for-logistic-regression5-min" class="headerlink" title="Example of computing derivative for logistic regression5 min"></a>Example of computing derivative for logistic regression5 min</h5><h5 id="Interpreting-derivative-for-logistic-regression5-min"><a href="#Interpreting-derivative-for-logistic-regression5-min" class="headerlink" title="Interpreting derivative for logistic regression5 min"></a>Interpreting derivative for logistic regression5 min</h5><h5 id="Summary-of-gradient-ascent-for-logistic-regression2-min"><a href="#Summary-of-gradient-ascent-for-logistic-regression2-min" class="headerlink" title="Summary of gradient ascent for logistic regression2 min"></a>Summary of gradient ascent for logistic regression2 min</h5><h4 id="Choosing-step-size-for-gradient-ascent-descent"><a href="#Choosing-step-size-for-gradient-ascent-descent" class="headerlink" title="Choosing step size for gradient ascent/descent"></a>Choosing step size for gradient ascent/descent</h4><h5 id="Choosing-step-size5-min"><a href="#Choosing-step-size5-min" class="headerlink" title="Choosing step size5 min"></a>Choosing step size5 min</h5><h5 id="Careful-with-step-sizes-that-are-too-large4-min"><a href="#Careful-with-step-sizes-that-are-too-large4-min" class="headerlink" title="Careful with step sizes that are too large4 min"></a>Careful with step sizes that are too large4 min</h5><h5 id="Rule-of-thumb-for-choosing-step-size3-min"><a href="#Rule-of-thumb-for-choosing-step-size3-min" class="headerlink" title="Rule of thumb for choosing step size3 min"></a>Rule of thumb for choosing step size3 min</h5><h4 id="VERY-OPTIONAL-LESSON-Deriving-gradient-of-logistic-regression"><a href="#VERY-OPTIONAL-LESSON-Deriving-gradient-of-logistic-regression" class="headerlink" title="(VERY OPTIONAL LESSON) Deriving gradient of logistic regression"></a>(VERY OPTIONAL LESSON) Deriving gradient of logistic regression</h4><h5 id="VERY-OPTIONAL-Deriving-gradient-of-logistic-regression-Log-trick4-min"><a href="#VERY-OPTIONAL-Deriving-gradient-of-logistic-regression-Log-trick4-min" class="headerlink" title="(VERY OPTIONAL) Deriving gradient of logistic regression: Log trick4 min"></a>(VERY OPTIONAL) Deriving gradient of logistic regression: Log trick4 min</h5><h5 id="VERY-OPTIONAL-Expressing-the-log-likelihood3-min"><a href="#VERY-OPTIONAL-Expressing-the-log-likelihood3-min" class="headerlink" title="(VERY OPTIONAL) Expressing the log-likelihood3 min"></a>(VERY OPTIONAL) Expressing the log-likelihood3 min</h5><h5 id="VERY-OPTIONAL-Deriving-probability-y-1-given-x2-min"><a href="#VERY-OPTIONAL-Deriving-probability-y-1-given-x2-min" class="headerlink" title="(VERY OPTIONAL) Deriving probability y=-1 given x2 min"></a>(VERY OPTIONAL) Deriving probability y=-1 given x2 min</h5><h5 id="VERY-OPTIONAL-Rewriting-the-log-likelihood-into-a-simpler-form8-min"><a href="#VERY-OPTIONAL-Rewriting-the-log-likelihood-into-a-simpler-form8-min" class="headerlink" title="(VERY OPTIONAL) Rewriting the log likelihood into a simpler form8 min"></a>(VERY OPTIONAL) Rewriting the log likelihood into a simpler form8 min</h5><h5 id="VERY-OPTIONAL-Deriving-gradient-of-log-likelihood8-min"><a href="#VERY-OPTIONAL-Deriving-gradient-of-log-likelihood8-min" class="headerlink" title="(VERY OPTIONAL) Deriving gradient of log likelihood8 min"></a>(VERY OPTIONAL) Deriving gradient of log likelihood8 min</h5><h4 id="Summarizing-learning-linear-classifiers"><a href="#Summarizing-learning-linear-classifiers" class="headerlink" title="Summarizing learning linear classifiers"></a>Summarizing learning linear classifiers</h4><h5 id="Recap-of-learning-logistic-regression-classifiers1-min"><a href="#Recap-of-learning-logistic-regression-classifiers1-min" class="headerlink" title="Recap of learning logistic regression classifiers1 min"></a>Recap of learning logistic regression classifiers1 min</h5><h5 id="Quiz-Learning-Linear-Classifiers6-questions"><a href="#Quiz-Learning-Linear-Classifiers6-questions" class="headerlink" title="Quiz: Learning Linear Classifiers6 questions"></a>Quiz: Learning Linear Classifiers6 questions</h5><div class="note primary"><p>QUIZ<br>Learning Linear Classifiers<br>6 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>August 27, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.(True/False) A linear classifier can only learn positive coefficients.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>2.(True/False) In order to train a logistic regression model, we find the weights that maximize the likelihood of the model.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.(True/False) The data likelihood is the product of the probability of the inputs x given the weights w and response y.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>4.Questions 4 and 5 refer to the following scenario.</p>
<p>Consider the setting where our inputs are 1-dimensional. We have data</p>
<table>
<thead>
<tr>
<th>x</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.5</td>
<td>+1</td>
</tr>
<tr>
<td>0.3</td>
<td>-1</td>
</tr>
<tr>
<td>2.8</td>
<td>+1</td>
</tr>
<tr>
<td>0.5</td>
<td>+1</td>
</tr>
</tbody>
</table>
<p>and the current estimates of the weights are w0=0 and w1=1. (w0: the intercept, w1: the weight for x).</p>
<p>Calculate the likelihood of this data. Round your answer to 2 decimal places.</p>
<p><input type="”text”" placeholder="0.23"><br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/ySTNZ-A8EeWIdgqHxZs34w_8a123d76165a120ca663408950b04939_Capture.PNG?expiry=1503878400000&amp;hmac=a0gg3A7dJ9eVtVIudcv8jtCD7OLm4cWyoGoMXFrp5wQ" alt=""><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">x = np.array([2.5,0.3,2.8,0.5])</div><div class="line">y = np.array([1,0,1,1])</div><div class="line">w0 = 0</div><div class="line">w1 = 1</div><div class="line"></div><div class="line">def ypre(x,w0,w1):</div><div class="line">    score = w0 + x * w1</div><div class="line">    return sigmoid(score)</div><div class="line"></div><div class="line">def sigmoid(score):</div><div class="line">    return 1.0/(1+ (np.exp(-score)))</div><div class="line"></div><div class="line">lik = 1</div><div class="line">for i in range(len(x)):</div><div class="line">    if i == 1:</div><div class="line">        lik *= (1 - ypre(x[i], 0, 1))</div><div class="line">    else:</div><div class="line">        lik *= (ypre(x[i], 0, 1))</div><div class="line"></div><div class="line">print lik</div><div class="line"># 0.230765141474</div></pre></td></tr></table></figure></p>
<p><br><br>1 point<br>5.Refer to the scenario given in Question 4 to answer the following:</p>
<p>Calculate the derivative of the log likelihood with respect to w1. Round your answer to 2 decimal places.</p>
<p><input type="”text”" placeholder="0.37"><br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Fh-c2uA_EeWOVQ68c1xy2w_112e2746a771957f31fccced680c1482_Capture.PNG?expiry=1503878400000&amp;hmac=Tw3Rm2SZCbEEIwetmoEQPyTdpFAfqnbE6OVk17LYloY" alt=""><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def der(hx, ytrue, ypre):</div><div class="line">    return hx * (ytrue - ypre)</div><div class="line"></div><div class="line">sum = 0</div><div class="line">for i in range(len(x)):</div><div class="line">    sum += der(x[i],y[i],ypre(x[i],0,1))</div><div class="line">    </div><div class="line">print sum</div><div class="line"># 0.366590721926</div></pre></td></tr></table></figure></p>
<p><br><br>1 point<br>6.Which of the following is true about gradient ascent? Select all that apply.</p>
<p><input type="checkbox" disabled checked><label>It is an iterative algorithm</label></p>
<p><input type="checkbox" disabled><label>It only updates a few of the parameters, not all of them</label></p>
<p><input type="checkbox" disabled checked><label>It finds the maximum by “hill climbing”</label><br><br></p>
<h4 id="Programming-Assignment-2"><a href="#Programming-Assignment-2" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h4><h5 id="Implementing-logistic-regression-from-scratch10-min"><a href="#Implementing-logistic-regression-from-scratch10-min" class="headerlink" title="Implementing logistic regression from scratch10 min"></a>Implementing logistic regression from scratch10 min</h5><h5 id="Quiz-Implementing-logistic-regression-from-scratch8-questions"><a href="#Quiz-Implementing-logistic-regression-from-scratch8-questions" class="headerlink" title="Quiz: Implementing logistic regression from scratch8 questions"></a>Quiz: Implementing logistic regression from scratch8 questions</h5><div class="note primary"><p>QUIZ<br>Implementing logistic regression from scratch<br>8 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>August 27, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.How many reviews in amazon_baby_subset.gl contain the word perfect?</p>
<p><input type="”text”" placeholder="2955"><br><br><br>1 point<br>2.Consider the feature_matrix that was obtained by converting our data to NumPy format.</p>
<p>How many features are there in the feature_matrix?</p>
<p><input type="”text”" placeholder="194"><br><br><br>1 point<br>3.Assuming that the intercept is present, how does the number of features in feature_matrix relate to the number of features in the logistic regression model? Let x = [number of features in feature_matrix] and y = [number of features in logistic regression model].</p>
<p><input type="radio" disabled><label>y = x - 1</label></p>
<p><input type="radio" disabled checked><label>y = x</label></p>
<p><input type="radio" disabled><label>y = x + 1</label></p>
<p><input type="radio" disabled><label>None of the above</label><br><br><br>1 point<br>4.Run your logistic regression solver with provided parameters.</p>
<p>As each iteration of gradient ascent passes, does the log-likelihood increase or decrease?</p>
<p><input type="radio" disabled checked><label>It increases.</label></p>
<p><input type="radio" disabled><label>It decreases.</label></p>
<p><input type="radio" disabled><label>None of the above</label><br><br><br>1 point<br>5.We make predictions using the weights just learned.</p>
<p>How many reviews were predicted to have positive sentiment?</p>
<p><input type="”text”" placeholder="25126"><br><br><br>1 point<br>6.What is the accuracy of the model on predictions made above? (round to 2 digits of accuracy)</p>
<p><input type="”text”" placeholder="0.75"><br><br><br>1 point<br>7.We look at “most positive” words, the words that correspond most strongly with positive reviews.</p>
<p>Which of the following words is not present in the top 10 “most positive” words?</p>
<p><input type="radio" disabled><label>love</label></p>
<p><input type="radio" disabled><label>easy</label></p>
<p><input type="radio" disabled><label>great</label></p>
<p><input type="radio" disabled><label>perfect</label></p>
<p><input type="radio" disabled checked><label>cheap</label><br><br><br>1 point<br>8.Similarly, we look at “most negative” words, the words that correspond most strongly with negative reviews.</p>
<p>Which of the following words is not present in the top 10 “most negative” words?</p>
<p><input type="radio" disabled checked><label>need</label></p>
<p><input type="radio" disabled><label>work</label></p>
<p><input type="radio" disabled><label>disappointed</label></p>
<p><input type="radio" disabled><label>even</label></p>
<p><input type="radio" disabled><label>return</label><br><br></p>
<h3 id="Overfitting-amp-Regularization-in-Logistic-Regression"><a href="#Overfitting-amp-Regularization-in-Logistic-Regression" class="headerlink" title="Overfitting &amp; Regularization in Logistic Regression"></a>Overfitting &amp; Regularization in Logistic Regression</h3><div class="note primary"><p>As we saw in the regression course, overfitting is perhaps the most significant challenge you will face as you apply machine learning approaches in practice. This challenge can be particularly significant for logistic regression, as you will discover in this module, since we not only risk getting an overly complex decision boundary, but your classifier can also become overly confident about the probabilities it predicts. In this module, you will investigate overfitting in classification in significant detail, and obtain broad practical insights from some interesting visualizations of the classifiers’ outputs. You will then add a regularization term to your optimization to mitigate overfitting. You will investigate both L2 regularization to penalize large coefficient values, and L1 regularization to obtain additional sparsity in the coefficients. Finally, you will modify your gradient ascent algorithm to learn regularized logistic regression classifiers. You will implement your own regularized logistic regression classifier from scratch, and investigate the impact of the L2 penalty on real-world sentiment analysis data.</p>
</div>
<h4 id="Overfitting-in-classification"><a href="#Overfitting-in-classification" class="headerlink" title="Overfitting in classification"></a>Overfitting in classification</h4><h5 id="Slides-presented-in-this-module10-min-13"><a href="#Slides-presented-in-this-module10-min-13" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/2.1-logistic-regression-overfitting-annotated.pdf" target="_blank" rel="external">logistic-regression-overfitting-annotated.pdf</a></p>
<h5 id="Evaluating-a-classifier-3-min"><a href="#Evaluating-a-classifier-3-min" class="headerlink" title="Evaluating a classifier 3 min"></a>Evaluating a classifier 3 min</h5><p><a href="https://www.coursera.org/learn/ml-classification/lecture/RzxaQ/evaluating-a-classifier" target="_blank" rel="external">https://www.coursera.org/learn/ml-classification/lecture/RzxaQ/evaluating-a-classifier</a></p>
<h5 id="Review-of-overfitting-in-regression3-min"><a href="#Review-of-overfitting-in-regression3-min" class="headerlink" title="Review of overfitting in regression3 min"></a>Review of overfitting in regression3 min</h5><h5 id="Overfitting-in-classification5-min"><a href="#Overfitting-in-classification5-min" class="headerlink" title="Overfitting in classification5 min"></a>Overfitting in classification5 min</h5><h5 id="Visualizing-overfitting-with-high-degree-polynomial-features3-min"><a href="#Visualizing-overfitting-with-high-degree-polynomial-features3-min" class="headerlink" title="Visualizing overfitting with high-degree polynomial features3 min"></a>Visualizing overfitting with high-degree polynomial features3 min</h5><h4 id="Overconfident-predictions-due-to-overfitting"><a href="#Overconfident-predictions-due-to-overfitting" class="headerlink" title="Overconfident predictions due to overfitting"></a>Overconfident predictions due to overfitting</h4><h5 id="Overfitting-in-classifiers-leads-to-overconfident-predictions5-min"><a href="#Overfitting-in-classifiers-leads-to-overconfident-predictions5-min" class="headerlink" title="Overfitting in classifiers leads to overconfident predictions5 min"></a>Overfitting in classifiers leads to overconfident predictions5 min</h5><h5 id="Visualizing-overconfident-predictions4-min"><a href="#Visualizing-overconfident-predictions4-min" class="headerlink" title="Visualizing overconfident predictions4 min"></a>Visualizing overconfident predictions4 min</h5><h5 id="OPTIONAL-Another-perspecting-on-overfitting-in-logistic-regression8-min"><a href="#OPTIONAL-Another-perspecting-on-overfitting-in-logistic-regression8-min" class="headerlink" title="(OPTIONAL) Another perspecting on overfitting in logistic regression8 min"></a>(OPTIONAL) Another perspecting on overfitting in logistic regression8 min</h5><h4 id="L2-regularized-logistic-regression"><a href="#L2-regularized-logistic-regression" class="headerlink" title="L2 regularized logistic regression"></a>L2 regularized logistic regression</h4><h5 id="Penalizing-large-coefficients-to-mitigate-overfitting5-min"><a href="#Penalizing-large-coefficients-to-mitigate-overfitting5-min" class="headerlink" title="Penalizing large coefficients to mitigate overfitting5 min"></a>Penalizing large coefficients to mitigate overfitting5 min</h5><h5 id="L2-regularized-logistic-regression4-min"><a href="#L2-regularized-logistic-regression4-min" class="headerlink" title="L2 regularized logistic regression4 min"></a>L2 regularized logistic regression4 min</h5><h5 id="Visualizing-effect-of-L2-regularization-in-logistic-regression5-min"><a href="#Visualizing-effect-of-L2-regularization-in-logistic-regression5-min" class="headerlink" title="Visualizing effect of L2 regularization in logistic regression5 min"></a>Visualizing effect of L2 regularization in logistic regression5 min</h5><h5 id="Learning-L2-regularized-logistic-regression-with-gradient-ascent7-min"><a href="#Learning-L2-regularized-logistic-regression-with-gradient-ascent7-min" class="headerlink" title="Learning L2 regularized logistic regression with gradient ascent7 min"></a>Learning L2 regularized logistic regression with gradient ascent7 min</h5><h4 id="Sparse-logistic-regression"><a href="#Sparse-logistic-regression" class="headerlink" title="Sparse logistic regression"></a>Sparse logistic regression</h4><h5 id="Sparse-logistic-regression-with-L1-regularization7-min"><a href="#Sparse-logistic-regression-with-L1-regularization7-min" class="headerlink" title="Sparse logistic regression with L1 regularization7 min"></a>Sparse logistic regression with L1 regularization7 min</h5><h4 id="Summarizing-overfitting-amp-regularization-in-logistic-regression"><a href="#Summarizing-overfitting-amp-regularization-in-logistic-regression" class="headerlink" title="Summarizing overfitting &amp; regularization in logistic regression"></a>Summarizing overfitting &amp; regularization in logistic regression</h4><h5 id="Recap-of-overfitting-amp-regularization-in-logistic-regression58-sec"><a href="#Recap-of-overfitting-amp-regularization-in-logistic-regression58-sec" class="headerlink" title="Recap of overfitting &amp; regularization in logistic regression58 sec"></a>Recap of overfitting &amp; regularization in logistic regression58 sec</h5><h5 id="Quiz-Overfitting-amp-Regularization-in-Logistic-Regression8-questions"><a href="#Quiz-Overfitting-amp-Regularization-in-Logistic-Regression8-questions" class="headerlink" title="Quiz: Overfitting &amp; Regularization in Logistic Regression8 questions"></a>Quiz: Overfitting &amp; Regularization in Logistic Regression8 questions</h5><div class="note primary"><p>QUIZ<br>Overfitting &amp; Regularization in Logistic Regression<br>8 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>August 27, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Consider four classifiers, whose classification performance is given by the following table:</p>
<table>
<thead>
<tr>
<th>Classification error on training set</th>
<th>Classification error on validation set</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classifier 1</td>
<td>0.2 0.6</td>
</tr>
<tr>
<td>Classifier 2</td>
<td>0.8 0.6</td>
</tr>
<tr>
<td>Classifier 3</td>
<td>0.2 0.2</td>
</tr>
<tr>
<td>Classifier 4</td>
<td>0.5 0.4</td>
</tr>
</tbody>
</table>
<p>Which of the four classifiers is most likely overfit?</p>
<p><input type="radio" disabled checked><label>Classifier 1</label></p>
<p><input type="radio" disabled><label>Classifier 2</label></p>
<p><input type="radio" disabled><label>Classifier 3</label></p>
<p><input type="radio" disabled><label>Classifier 4</label><br><br><br>1 point<br>2.Suppose a classifier classifies 23100 examples correctly and 1900 examples incorrectly. Compute error by hand. Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.076"><br><br><br>1 point<br>3.(True/False) Accuracy and error measured on the same dataset always sum to 1.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>4.Which of the following is NOT a correct description of complex models?</p>
<p><input type="radio" disabled><label>Complex models accommodate many features.</label></p>
<p><input type="radio" disabled><label>Complex models tend to produce lower training error than simple models.</label></p>
<p><input type="radio" disabled checked><label>Complex models tend to generalize better than simple models.</label></p>
<p><input type="radio" disabled><label>Complex models tend to exhibit high variance in response to perturbation in the training data.</label></p>
<p><input type="radio" disabled><label>Complex models tend to exhibit low bias, capturing many patterns in the training data that simple models may have missed.</label><br><br><br>1 point<br>5.Which of the following is a symptom of overfitting in the context of logistic regression? Select all that apply.</p>
<p><input type="checkbox" disabled checked><label>Large estimated coefficients</label></p>
<p><input type="checkbox" disabled><label>Good generalization to previously unseen data</label></p>
<p><input type="checkbox" disabled><label>Simple decision boundary</label></p>
<p><input type="checkbox" disabled checked><label>Complex decision boundary</label></p>
<p><input type="checkbox" disabled checked><label>Overconfident predictions of class probabilities</label><br><br><br>1 point<br>6.Suppose we perform L2 regularized logistic regression to fit a sentiment classifier. Which of the following plots does NOT describe a possible coefficient path? Choose all that apply.</p>
<p>Note. Assume that the algorithm runs for a wide range of L2 penalty values and each coefficient plot is zoomed out enough to capture all long-term trends.</p>
<p><input type="checkbox" disabled><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/s0aljeWUEeWufRJaRfO1AQ_3134541d802ae1aaee26daec7ef39ca3_L_3SbuBaEeW--hK2gi_BIw_fbbbb77fd44af452b3d337434cdb7702_Capture.png?expiry=1503964800000&amp;hmac=s_bRnYQIsuZUJESwIKD6wLwaWasv7xOzJFa2cLV79cQ" alt=""></label></p>
<p><input type="checkbox" disabled><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/xUjv0uWUEeWuUgrcWIxPhQ_ac77661c16c2fb40c0c3abfbf076a424_rvPx2eD1EeWOVQ68c1xy2w_ad1639c57963f5eaea6612cb568c4d86_Capture.png?expiry=1503964800000&amp;hmac=GdPLOJl2j_eyVFAn9AaQ0XzTYiEUQ1eYBwZ9tl1Q9fs" alt=""></label></p>
<p><input type="checkbox" disabled checked><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/2f4k6eWUEeWIdgqHxZs34w_1ee2b9e5512622d0ad3a587b44c49922_JcoDWuBbEeWufRJaRfO1AQ_ff849715b543c56709a46f7be7a14c5d_Capture.png?expiry=1503964800000&amp;hmac=_rSGs1Pa3PD47CWmtJF1GEUnQSyIs10UQrphSivVp40" alt=""></label></p>
<p><input type="checkbox" disabled checked><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/5YNc7eWUEeWOVQ68c1xy2w_768b146d738d6e1991ac9d5f678dbef6_9wGVMuBbEeW--hK2gi_BIw_27e6398723f122d80d4caae557763566_Capture.png?expiry=1503964800000&amp;hmac=4o7FiAVcDo20hF_Tlji5buHyFP0hIphb_VECn6vQ-3c" alt=""></label><br><br><br>1 point<br>7.Suppose we perform L1 regularized logistic regression to fit a sentiment classifier. Which of the following plots does NOT describe a possible coefficient path? Choose all that apply.</p>
<p>Note. Assume that the algorithm runs for a wide range of L1 penalty values and each coefficient plot is zoomed out enough to capture all long-term trends.</p>
<p><input type="checkbox" disabled checked><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/A16k-eWVEeW--hK2gi_BIw_0d7afdaf1bd9730fd7b5c51c424f1b95_aJDg6eD2EeWuUgrcWIxPhQ_81584c7620804c5d7093ee0063171269_Capture.png?expiry=1503964800000&amp;hmac=gnxbzQTWLUk1rnv-Tqe2xhEIWMil9FC--JUdkLbdFwk" alt=""></label></p>
<p><input type="checkbox" disabled><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/D3JhYeWVEeWufRJaRfO1AQ_0569b1c0c9a8bd6798c91d817f3aef7c_aJbicuBcEeW--hK2gi_BIw_d09a4fde17773e1f5f1a270b3b6d357a_Capture.png?expiry=1503964800000&amp;hmac=Zh5JXL_7gxq_2zW_UBzBZadfGtaw73movErVQufVErM" alt=""></label></p>
<p><input type="checkbox" disabled><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Ht-B8uWVEeWufRJaRfO1AQ_bd596c09a8282ec2de28b358ca5f171d_cLTtQeD2EeWufRJaRfO1AQ_4f1f5b7a19b8e305c40e11e7578b1d38_Capture2.png?expiry=1503964800000&amp;hmac=EnaRpFw3Ci_ja1v-FaBZyGOgSIrMKdjpKMIoaTzqF5w" alt=""></label></p>
<p><input type="checkbox" disabled checked><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Mf6t5uWVEeWuUgrcWIxPhQ_2a643ffaf830d9218db5a6314936d52f_V4_Ld-BdEeWIdgqHxZs34w_dfc3556048dfc2bc157ce8bf58d6d74a_Capture.png?expiry=1503964800000&amp;hmac=xM8ZtdtqwNjBDzev1VaowhWpAwdx0YlbhFWwr46Ed_0" alt=""></label><br><br><br>1 point<br>8.In the context of L2 regularized logistic regression, which of the following occurs as we increase the L2 penalty λ? Choose all that apply.</p>
<p><input type="checkbox" disabled checked><label>The L2 norm of the set of coefficients gets smaller</label></p>
<p><input type="checkbox" disabled><label><br>Region of uncertainty becomes narrower, i.e., the classifier makes predictions with higher confidence.</label></p>
<p><input type="checkbox" disabled checked><label>Decision boundary becomes less complex</label></p>
<p><input type="checkbox" disabled><label>Training error decreases</label></p>
<p><input type="checkbox" disabled checked><label>The classifier has lower variance</label></p>
<p><input type="checkbox" disabled><label>Some features are excluded from the classifier</label><br><br></p>
<h4 id="Programming-Assignment-3"><a href="#Programming-Assignment-3" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h4><h5 id="Logistic-Regression-with-L2-regularization10-min"><a href="#Logistic-Regression-with-L2-regularization10-min" class="headerlink" title="Logistic Regression with L2 regularization10 min"></a>Logistic Regression with L2 regularization10 min</h5><h5 id="Quiz-Logistic-Regression-with-L2-regularization8-questions"><a href="#Quiz-Logistic-Regression-with-L2-regularization8-questions" class="headerlink" title="Quiz: Logistic Regression with L2 regularization8 questions"></a>Quiz: Logistic Regression with L2 regularization8 questions</h5><div class="note primary"><p>QUIZ<br>Logistic Regression with L2 regularization<br>8 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>August 27, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.In the function feature_derivative_with_L2, was the intercept term regularized?</p>
<p><input type="radio" disabled><label>Yes</label></p>
<p><input type="radio" disabled checked><label>No</label><br><br><br>1 point<br>2.Does the term with L2 regularization increase or decrease the log likelihood ℓℓ(w)?</p>
<p><input type="radio" disabled><label>Increases</label></p>
<p><input type="radio" disabled checked><label>Decreases</label><br><br><br>1 point<br>3.Which of the following words is not listed in either positive_words or negative_words?</p>
<p><input type="radio" disabled><label>love</label></p>
<p><input type="radio" disabled><label>disappointed</label></p>
<p><input type="radio" disabled><label>great</label></p>
<p><input type="radio" disabled><label>money</label></p>
<p><input type="radio" disabled checked><label>quality</label><br><br><br>1 point<br>4.Questions 5 and 6 use the coefficient plot of the words in positive_words and negative_words.</p>
<p>(True/False) All coefficients consistently get smaller in size as the L2 penalty is increased.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>5.Questions 5 and 6 use the coefficient plot of the words in positive_words and negative_words.</p>
<p>(True/False) The relative order of coefficients is preserved as the L2 penalty is increased. (For example, if the coefficient for ‘cat’ was more positive than that for ‘dog’, this remains true as the L2 penalty increases.)</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>6.Questions 7, 8, and 9 ask you about the 6 models trained with different L2 penalties.</p>
<p>Which of the following models has the highest accuracy on the training data?</p>
<p><input type="radio" disabled checked><label>Model trained with L2 penalty = 0</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 4</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 10</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 100</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 1e3</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 1e5</label><br><br><br>1 point<br>7.Questions 7, 8, and 9 ask you about the 6 models trained with different L2 penalties.</p>
<p>Which of the following models has the highest accuracy on the validation data?</p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 0</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 4</label></p>
<p><input type="radio" disabled checked><label>Model trained with L2 penalty = 10</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 100</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 1e3</label></p>
<p><input type="radio" disabled><label>Model trained with L2 penalty = 1e5</label><br><br><br>1 point<br>8.Questions 7, 8, and 9 ask you about the 6 models trained with different L2 penalties.</p>
<p>Does the highest accuracy on the training data imply that the model is the best one?</p>
<p><input type="radio" disabled><label>Yes</label></p>
<p><input type="radio" disabled checked><label>No</label><br><br></p>
<h2 id="Week-3-Decision-Trees"><a href="#Week-3-Decision-Trees" class="headerlink" title="Week 3 Decision Trees"></a>Week 3 Decision Trees</h2><div class="note primary"><p>Along with linear classifiers, decision trees are amongst the most widely used classification techniques in the real world. This method is extremely intuitive, simple to implement and provides interpretable predictions. </p>
<ul>
<li>In this module, you will become familiar with the core decision trees representation. </li>
<li>You will then design a simple, recursive greedy algorithm to learn decision trees from data. </li>
<li>Finally, you will extend this approach to deal with continuous inputs, a fundamental requirement for practical problems. </li>
<li>In this module, you will investigate a brand new case-study in the financial sector: predicting the risk associated with a bank loan. You will implement your own decision tree learning algorithm on real loan data.</li>
</ul>
</div>
<h3 id="Intuition-behind-decision-trees"><a href="#Intuition-behind-decision-trees" class="headerlink" title="Intuition behind decision trees"></a>Intuition behind decision trees</h3><h4 id="Slides-presented-in-this-module10-min-14"><a href="#Slides-presented-in-this-module10-min-14" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p>decision-trees-annotated.pdf</p>
<h4 id="Predicting-loan-defaults-with-decision-trees3-min"><a href="#Predicting-loan-defaults-with-decision-trees3-min" class="headerlink" title="Predicting loan defaults with decision trees3 min"></a>Predicting loan defaults with decision trees3 min</h4><h4 id="Intuition-behind-decision-trees1-min"><a href="#Intuition-behind-decision-trees1-min" class="headerlink" title="Intuition behind decision trees1 min"></a>Intuition behind decision trees1 min</h4><h4 id="Task-of-learning-decision-trees-from-data3-min"><a href="#Task-of-learning-decision-trees-from-data3-min" class="headerlink" title="Task of learning decision trees from data3 min"></a>Task of learning decision trees from data3 min</h4><h3 id="Learning-decision-trees"><a href="#Learning-decision-trees" class="headerlink" title="Learning decision trees"></a>Learning decision trees</h3><h4 id="Recursive-greedy-algorithm4-min"><a href="#Recursive-greedy-algorithm4-min" class="headerlink" title="Recursive greedy algorithm4 min"></a>Recursive greedy algorithm4 min</h4><h4 id="Learning-a-decision-stump3-min"><a href="#Learning-a-decision-stump3-min" class="headerlink" title="Learning a decision stump3 min"></a>Learning a decision stump3 min</h4><h4 id="Selecting-best-feature-to-split-on6-min"><a href="#Selecting-best-feature-to-split-on6-min" class="headerlink" title="Selecting best feature to split on6 min"></a>Selecting best feature to split on6 min</h4><h4 id="When-to-stop-recursing4-min"><a href="#When-to-stop-recursing4-min" class="headerlink" title="When to stop recursing4 min"></a>When to stop recursing4 min</h4><h3 id="Using-the-learned-decision-tree"><a href="#Using-the-learned-decision-tree" class="headerlink" title="Using the learned decision tree"></a>Using the learned decision tree</h3><h4 id="Making-predictions-with-decision-trees1-min"><a href="#Making-predictions-with-decision-trees1-min" class="headerlink" title="Making predictions with decision trees1 min"></a>Making predictions with decision trees1 min</h4><h4 id="Multiclass-classification-with-decision-trees2-min"><a href="#Multiclass-classification-with-decision-trees2-min" class="headerlink" title="Multiclass classification with decision trees2 min"></a>Multiclass classification with decision trees2 min</h4><h3 id="Learning-decision-trees-with-continuous-inputs"><a href="#Learning-decision-trees-with-continuous-inputs" class="headerlink" title="Learning decision trees with continuous inputs"></a>Learning decision trees with continuous inputs</h3><h4 id="Threshold-splits-for-continuous-inputs6-min"><a href="#Threshold-splits-for-continuous-inputs6-min" class="headerlink" title="Threshold splits for continuous inputs6 min"></a>Threshold splits for continuous inputs6 min</h4><h4 id="OPTIONAL-Picking-the-best-threshold-to-split-on3-min"><a href="#OPTIONAL-Picking-the-best-threshold-to-split-on3-min" class="headerlink" title="(OPTIONAL) Picking the best threshold to split on3 min"></a>(OPTIONAL) Picking the best threshold to split on3 min</h4><h4 id="Visualizing-decision-boundaries5-min"><a href="#Visualizing-decision-boundaries5-min" class="headerlink" title="Visualizing decision boundaries5 min"></a>Visualizing decision boundaries5 min</h4><h3 id="Summarizing-decision-trees"><a href="#Summarizing-decision-trees" class="headerlink" title="Summarizing decision trees"></a>Summarizing decision trees</h3><h4 id="Recap-of-decision-trees56-sec"><a href="#Recap-of-decision-trees56-sec" class="headerlink" title="Recap of decision trees56 sec"></a>Recap of decision trees56 sec</h4><h4 id="Quiz-Decision-Trees11-questions"><a href="#Quiz-Decision-Trees11-questions" class="headerlink" title="Quiz: Decision Trees11 questions"></a>Quiz: Decision Trees11 questions</h4><div class="note primary"><p>QUIZ<br>Decision Trees<br>11 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 3, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Questions 1 to 6 refer to the following common scenario:</p>
<p>Consider the following dataset:</p>
<table>
<thead>
<tr>
<th>x1</th>
<th>x2</th>
<th>x3</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>+1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>-1</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>+1</td>
</tr>
</tbody>
</table>
<p>Let us train a decision tree with this data. Let’s call this tree T1. What feature will we split on at the root?</p>
<p><input type="radio" disabled><label>x1</label></p>
<p><input type="radio" disabled><label>x2</label></p>
<p><input type="radio" disabled checked><label>x3</label><br><br><br><div class="note primary"><p>x1: .5<br>x2: .5<br>x3: .25</p>
</div><br>1 point<br>2.Refer to the dataset presented in Question 1 to answer the following.</p>
<p>Fully train T1 (until each leaf has data points of the same output label). What is the depth of T1?</p>
<p><input type="”text”" placeholder="3"><br><br><br>1 point<br>3.Refer to the dataset presented in Question 1 to answer the following.</p>
<p>What is the training error of T1?</p>
<p><input type="”text”" placeholder="0"><br><br><br>1 point<br>4.Refer to the dataset presented in Question 1 to answer the following.</p>
<p>Now consider a tree T2, which splits on x1 at the root, and splits on x2 in the 1st level, and has leaves at the 2nd level. Note: this is the XOR function on features 1 and 2. What is the depth of T2?</p>
<p><input type="”text”" placeholder="2"><br><br><br>1 point<br>5.Refer to the dataset presented in Question 1 to answer the following.</p>
<p>What is the training error of T2?</p>
<p><input type="”text”" placeholder="0"><br><br><br>1 point<br>6.Refer to the dataset presented in Question 1 to answer the following.</p>
<p>Which has smaller depth, T1 or T2?</p>
<p><input type="radio" disabled><label>T1</label></p>
<p><input type="radio" disabled checked><label>T2</label><br><br><br>1 point<br>7.(True/False) When deciding to split a node, we find the best feature to split on that minimizes classification error.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>8.If you are learning a decision tree, and you are at a node in which all of its data has the same y value, you should</p>
<p><input type="radio" disabled><label>find the best feature to split on</label></p>
<p><input type="radio" disabled checked><label>create a leaf that predicts the y value of all the data</label></p>
<p><input type="radio" disabled><label>terminate recursions on all branches and return the current tree</label></p>
<p><input type="radio" disabled><label>go back to the PARENT node and select a DIFFERENT feature to split on so that the y values are not all the same at THIS node</label><br><br><br><div class="note danger"><p>3: False</p>
</div><br>1 point<br>8.Let’s say we have learned a decision tree on dataset D. Consider the split learned at the root of the decision tree. Which of the following is true if one of the data points in D is removed and we re-train the tree?</p>
<p><input type="radio" disabled><label>The split at the root will be different</label></p>
<p><input type="radio" disabled><label>The split at the root will be exactly the same as before</label></p>
<p><input type="radio" disabled checked><label>The split could be the same or could be different</label><br><br><br>1 point<br>9.Consider two datasets D1 and D2, where D2 has the same data points as D1, but has an extra feature for each data point. Let T1 be the decision tree trained with D1, and T2 be the tree trained with D2. Which of the following is true?</p>
<p><input type="radio" disabled><label>T2 has better training error than T1</label></p>
<p><input type="radio" disabled><label>T2 has better test error than T1</label></p>
<p><input type="radio" disabled checked><label>Too little information to guarantee anything</label><br><br><br>1 point<br>10.(True/False) Logistic regression with polynomial degree 1 features will always have equal or lower training error than decision stumps (depth 1 decision trees).</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>11.(True/False) Decision trees (with depth &gt; 1) are always linear classifiers.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>11.(True/False) Decision stumps (depth 1 decision trees) are always linear classifiers.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br></p>
<h3 id="Programming-Assignment-1-2"><a href="#Programming-Assignment-1-2" class="headerlink" title="Programming Assignment 1"></a>Programming Assignment 1</h3><h4 id="Identifying-safe-loans-with-decision-trees10-min"><a href="#Identifying-safe-loans-with-decision-trees10-min" class="headerlink" title="Identifying safe loans with decision trees10 min"></a>Identifying safe loans with decision trees10 min</h4><h4 id="Quiz-Identifying-safe-loans-with-decision-trees7-questions"><a href="#Quiz-Identifying-safe-loans-with-decision-trees7-questions" class="headerlink" title="Quiz: Identifying safe loans with decision trees7 questions"></a>Quiz: Identifying safe loans with decision trees7 questions</h4><div class="note primary"><p>QUIZ<br>Identifying safe loans with decision trees<br>7 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 3, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.What percentage of the predictions on sample_validation_data did decision_tree_model get correct?</p>
<p><input type="radio" disabled><label>25%</label></p>
<p><input type="radio" disabled checked><label>50%</label></p>
<p><input type="radio" disabled><label>75%</label></p>
<p><input type="radio" disabled><label>100%</label><br><br><br>1 point<br>2.Which loan has the highest probability of being classified as a safe loan?</p>
<p><input type="radio" disabled><label>First</label></p>
<p><input type="radio" disabled><label>Second</label></p>
<p><input type="radio" disabled><label>Third</label></p>
<p><input type="radio" disabled checked><label>Fourth</label><br><br><br>1 point<br>3.Notice that the probability preditions are the exact same for the 2nd and 3rd loans. Why would this happen?</p>
<p><input type="radio" disabled checked><label>During tree traversal both examples fall into the same leaf node.</label></p>
<p><input type="radio" disabled><label>This can only happen with sheer coincidence.</label><br><br><br>1 point<br>4.Based on the visualized tree, what prediction would you make for this data point?</p>
<p><input type="radio" disabled><label>+1</label></p>
<p><input type="radio" disabled checked><label>-1</label><br><br><br>1 point<br>5.What is the accuracy of decision_tree_model on the validation set, rounded to the nearest .01 (e.g. 0.76)?</p>
<p><input type="”text”" placeholder="0.64"><br><br><br>1 point<br>6.How does the performance of big_model on the validation set compare to decision_tree_model on the validation set? Is this a sign of overfitting?</p>
<p><input type="radio" disabled><label>big_model has higher accuracy on the validation set than decision_tree_model. This is overfitting.</label></p>
<p><input type="radio" disabled><label>big_model has higher accuracy on the validation set than decision_tree_model. This is not overfitting.</label></p>
<p><input type="radio" disabled checked><label>big_model has lower accuracy on the validation set than decision_tree_model. This is overfitting.</label></p>
<p><input type="radio" disabled><label>big_model has lower accuracy on the validation set than decision_tree_model. This is not overfitting.</label><br><br><br>1 point<br>7.Let us assume that each mistake costs money:</p>
<p>Assume a cost of $10,000 per false negative.<br>Assume a cost of $20,000 per false positive.<br>What is the total cost of mistakes made by decision_tree_model on validation_data? Please enter your answer as a plain integer, without the dollar sign or the comma separator, e.g. 3002000.</p>
<p><input type="”text”" placeholder="50390000"><br><br></p>
<h3 id="Programming-Assignment-2-2"><a href="#Programming-Assignment-2-2" class="headerlink" title="Programming Assignment 2"></a>Programming Assignment 2</h3><h4 id="Implementing-binary-decision-trees10-min"><a href="#Implementing-binary-decision-trees10-min" class="headerlink" title="Implementing binary decision trees10 min"></a>Implementing binary decision trees10 min</h4><h4 id="Quiz-Implementing-binary-decision-trees7-questions"><a href="#Quiz-Implementing-binary-decision-trees7-questions" class="headerlink" title="Quiz: Implementing binary decision trees7 questions"></a>Quiz: Implementing binary decision trees7 questions</h4><div class="note primary"><p>QUIZ<br>Implementing binary decision trees<br>7 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 3, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.What was the feature that my_decision_tree first split on while making the prediction for test_data[0]?</p>
<p><input type="radio" disabled><label>emp_length.4 years</label></p>
<p><input type="radio" disabled><label>grade.A</label></p>
<p><input type="radio" disabled checked><label>term. 36 months</label></p>
<p><input type="radio" disabled><label>home_ownership.MORTGAGE</label><br><br><br>1 point<br>2.What was the first feature that lead to a right split of test_data[0]?</p>
<p><input type="radio" disabled><label>emp_length.&lt; 1 year</label></p>
<p><input type="radio" disabled><label>emp_length.10+ years</label></p>
<p><input type="radio" disabled><label>grade.B</label></p>
<p><input type="radio" disabled checked><label>grade.D</label><br><br><br>1 point<br>3.What was the last feature split on before reaching a leaf node for test_data[0]?</p>
<p><input type="radio" disabled checked><label>grade.D</label></p>
<p><input type="radio" disabled><label>grade.B</label></p>
<p><input type="radio" disabled><label>term. 36 months</label></p>
<p><input type="radio" disabled><label>grade.A</label><br><br><br>1 point<br>4.Rounded to 2nd decimal point (e.g. 0.76), what is the classification error of my_decision_tree on the test_data?</p>
<p><input type="”text”" placeholder="0.38"><br><br><br>1 point<br>5.What is the feature that is used for the split at the root node?</p>
<p><input type="radio" disabled><label>grade.A</label></p>
<p><input type="radio" disabled checked><label>term. 36 months</label></p>
<p><input type="radio" disabled><label>term. 60 months</label></p>
<p><input type="radio" disabled><label>home_ownership.OWN</label><br><br><br>1 point<br>6.What is the path of the first 3 feature splits considered along the left-most branch of my_decision_tree?</p>
<p><input type="radio" disabled checked><label>term. 36 months, grade.A, grade.B</label></p>
<p><input type="radio" disabled><label>term. 36 months, grade.A, emp_length.4 years</label></p>
<p><input type="radio" disabled><label>term. 36 months, grade.A, no third feature because second split resulted in leaf</label><br><br><br>1 point<br>7.What is the path of the first 3 feature splits considered along the right-most branch of my_decision_tree?</p>
<p><input type="radio" disabled><label>term. 36 months, grade.D, grade.B</label></p>
<p><input type="radio" disabled><label>term. 36 months, grade.D, home_ownership.OWN</label></p>
<p><input type="radio" disabled checked><label>term. 36 months, grade.D, no third feature because second split resulted in leaf</label><br><br></p>
<h2 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h2><h3 id="Preventing-Overfitting-in-Decision-Trees"><a href="#Preventing-Overfitting-in-Decision-Trees" class="headerlink" title="Preventing Overfitting in Decision Trees"></a>Preventing Overfitting in Decision Trees</h3><div class="note primary"><p>Out of all machine learning techniques, decision trees are amongst the most prone to overfitting. No practical implementation is possible without including approaches that mitigate this challenge. In this module, through various visualizations and investigations, you will investigate why decision trees suffer from significant overfitting problems. Using the principle of Occam’s razor, you will mitigate overfitting by learning simpler trees. At first, you will design algorithms that stop the learning process before the decision trees become overly complex. In an optional segment, you will design a very practical approach that learns an overly-complex tree, and then simplifies it with pruning. Your implementation will investigate the effect of these techniques on mitigating overfitting on our real-world loan data set.</p>
</div>
<h4 id="Overfitting-in-decision-trees"><a href="#Overfitting-in-decision-trees" class="headerlink" title="Overfitting in decision trees"></a>Overfitting in decision trees</h4><h5 id="Slides-presented-in-this-module10-min-15"><a href="#Slides-presented-in-this-module10-min-15" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/4.0-decision-trees-overfitting-annotated.pdf" target="_blank" rel="external">decision-trees-overfitting-annotated.pdf</a></p>
<h5 id="A-review-of-overfitting2-min"><a href="#A-review-of-overfitting2-min" class="headerlink" title="A review of overfitting2 min"></a>A review of overfitting2 min</h5><h5 id="Overfitting-in-decision-trees5-min"><a href="#Overfitting-in-decision-trees5-min" class="headerlink" title="Overfitting in decision trees5 min"></a>Overfitting in decision trees5 min</h5><h4 id="Early-stopping-to-avoid-overfitting"><a href="#Early-stopping-to-avoid-overfitting" class="headerlink" title="Early stopping to avoid overfitting"></a>Early stopping to avoid overfitting</h4><h5 id="Principle-of-Occam’s-razor-Learning-simpler-decision-trees5-min"><a href="#Principle-of-Occam’s-razor-Learning-simpler-decision-trees5-min" class="headerlink" title="Principle of Occam’s razor: Learning simpler decision trees5 min"></a>Principle of Occam’s razor: Learning simpler decision trees5 min</h5><h5 id="Early-stopping-in-learning-decision-trees6-min"><a href="#Early-stopping-in-learning-decision-trees6-min" class="headerlink" title="Early stopping in learning decision trees6 min"></a>Early stopping in learning decision trees6 min</h5><h4 id="OPTIONAL-LESSON-Pruning-decision-trees"><a href="#OPTIONAL-LESSON-Pruning-decision-trees" class="headerlink" title="(OPTIONAL LESSON) Pruning decision trees"></a>(OPTIONAL LESSON) Pruning decision trees</h4><h5 id="OPTIONAL-Motivating-pruning8-min"><a href="#OPTIONAL-Motivating-pruning8-min" class="headerlink" title="(OPTIONAL) Motivating pruning8 min"></a>(OPTIONAL) Motivating pruning8 min</h5><h5 id="OPTIONAL-Pruning-decision-trees-to-avoid-overfitting6-min"><a href="#OPTIONAL-Pruning-decision-trees-to-avoid-overfitting6-min" class="headerlink" title="(OPTIONAL) Pruning decision trees to avoid overfitting6 min"></a>(OPTIONAL) Pruning decision trees to avoid overfitting6 min</h5><h5 id="OPTIONAL-Tree-pruning-algorithm3-min"><a href="#OPTIONAL-Tree-pruning-algorithm3-min" class="headerlink" title="(OPTIONAL) Tree pruning algorithm3 min"></a>(OPTIONAL) Tree pruning algorithm3 min</h5><h4 id="Summarizing-preventing-overfitting-in-decision-trees"><a href="#Summarizing-preventing-overfitting-in-decision-trees" class="headerlink" title="Summarizing preventing overfitting in decision trees"></a>Summarizing preventing overfitting in decision trees</h4><h5 id="Recap-of-overfitting-and-regularization-in-decision-trees1-min"><a href="#Recap-of-overfitting-and-regularization-in-decision-trees1-min" class="headerlink" title="Recap of overfitting and regularization in decision trees1 min"></a>Recap of overfitting and regularization in decision trees1 min</h5><h5 id="Quiz-Preventing-Overfitting-in-Decision-Trees11-questions"><a href="#Quiz-Preventing-Overfitting-in-Decision-Trees11-questions" class="headerlink" title="Quiz: Preventing Overfitting in Decision Trees11 questions"></a>Quiz: Preventing Overfitting in Decision Trees11 questions</h5><div class="note primary"><p>QUIZ<br>Preventing Overfitting in Decision Trees<br>11 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 10, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.(True/False) When learning decision trees, smaller depth USUALLY translates to lower training error.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>2.(True/False) If no two data points have the same input values, we can always learn a decision tree that achieves 0 training error.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.(True/False) If decision tree T1 has lower training error than decision tree T2, then T1 will always have better test error than T2.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>4.Which of the following is true for decision trees?</p>
<p><input type="radio" disabled><label>Model complexity increases with size of the data.</label></p>
<p><input type="radio" disabled checked><label>Model complexity increases with depth.</label></p>
<p><input type="radio" disabled><label>None of the above</label><br><br><br>1 point<br>5.Pruning and early stopping in decision trees is used to</p>
<p><input type="radio" disabled checked><label>combat overfitting</label></p>
<p><input type="radio" disabled><label>improve training error</label></p>
<p><input type="radio" disabled><label>None of the above</label><br><br><br>1 point<br>6.Which of the following is NOT an early stopping method?</p>
<p><input type="radio" disabled><label>Stop when the tree hits a certain depth</label></p>
<p><input type="radio" disabled><label>Stop when node has too few data points (minimum node “size”)</label></p>
<p><input type="radio" disabled checked><label>Stop when every possible split results in the same amount of error reduction</label></p>
<p><input type="radio" disabled><label>Stop when best split results in too small of an error reduction</label><br><br><br>1 point<br>7.Consider decision tree T1 learned with minimum node size parameter = 1000. Now consider decision tree T2 trained on the same dataset and parameters, except that the minimum node size parameter is now 100. Which of the following is always true?</p>
<p><input type="checkbox" disabled checked><label>The depth of T2 &gt;= the depth of T1</label></p>
<p><input type="checkbox" disabled checked><label>The number of nodes in T2 &gt;= the number of nodes in T1</label></p>
<p><input type="checkbox" disabled><label>The test error of T2 &lt;= the test error of T1</label></p>
<p><input type="checkbox" disabled checked><label>The training error of T2 &lt;= the training error of T1</label><br><br><br>1 point<br>8.Questions 8 to 11 refer to the following common scenario:</p>
<p>Imagine we are training a decision tree, and we are at a node. Each data point is (x1, x2, y), where x1,x2 are features, and y is the label. The data at this node is:</p>
<table>
<thead>
<tr>
<th>x1</th>
<th>x2</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1</td>
<td>+1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>+1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>+1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>-1</td>
</tr>
</tbody>
</table>
<p>What is the classification error at this node (assuming a majority class classifier)?</p>
<p><input type="”text”" placeholder="0.25"><br>1 point<br>9.Refer to the scenario presented in Question 8.</p>
<p>If we split on x1, what is the classification error?</p>
<p><input type="”text”" placeholder="0.25"><br>1<br>point</p>
<ol>
<li>Refer to the scenario presented in Question 8.</li>
</ol>
<p>If we split on x2, what is the classification error?</p>
<p><input type="”text”" placeholder="0.25"><br>1 point<br>11.Refer to the scenario presented in Question 8.</p>
<p>If our parameter for minimum gain in error reduction is 0.1, do we split or stop early?</p>
<p><input type="radio" disabled><label>Split</label></p>
<p><input type="radio" disabled checked><label>Stop early</label><br><br></p>
<h4 id="Programming-Assignment-4"><a href="#Programming-Assignment-4" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h4><h5 id="Decision-Trees-in-Practice10-min"><a href="#Decision-Trees-in-Practice10-min" class="headerlink" title="Decision Trees in Practice10 min"></a>Decision Trees in Practice10 min</h5><h5 id="Quiz-Decision-Trees-in-Practice14-questions"><a href="#Quiz-Decision-Trees-in-Practice14-questions" class="headerlink" title="Quiz: Decision Trees in Practice14 questions"></a>Quiz: Decision Trees in Practice14 questions</h5><div class="note primary"><p>QUIZ<br>Decision Trees in Practice<br>14 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 10, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Given an intermediate node with 6 safe loans and 3 risky loans, if the min_node_size parameter is 10, what should the tree learning algorithm do next?</p>
<p><input type="radio" disabled checked><label>Create a leaf and return it</label></p>
<p><input type="radio" disabled><label>Continue building the tree by finding the best splitting feature</label><br><br><br>1 point<br>2.Assume an intermediate node has 6 safe loans and 3 risky loans. For each of 4 possible features to split on, the error reduction is 0.0, 0.05, 0.1, and 0.14, respectively. If the minimum gain in error reduction parameter is set to 0.2, what should the tree learning algorithm do next?</p>
<p><input type="radio" disabled checked><label>Create a leaf and return it</label></p>
<p><input type="radio" disabled><label>Continue building the tree by using the splitting feature that gives 0.14 error reduction</label><br><br><br>1 point<br>3.Consider the prediction path validation_set[0] with my_decision_tree_old and my_decision_tree_new. For my_decision_tree_new trained with<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">max_depth = 6, min_node_size = 100, min_error_reduction=0.0</div></pre></td></tr></table></figure></p>
<p>is the prediction path shorter, longer, or the same as the prediction path using my_decision_tree_old that ignored the early stopping conditions 2 and 3?</p>
<p><input type="radio" disabled checked><label>Shorter</label></p>
<p><input type="radio" disabled><label>Longer</label></p>
<p><input type="radio" disabled><label>The same</label><br><br><br>1 point<br>4.Consider the prediction path for ANY new data point. For my_decision_tree_new trained with<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">max_depth = 6, min_node_size = 100, min_error_reduction=0.0</div></pre></td></tr></table></figure></p>
<p>is the prediction path for a data point always shorter, always longer, always the same, shorter or the same, or longer or the same as for my_decision_tree_old that ignored the early stopping conditions 2 and 3?</p>
<p><input type="radio" disabled><label>Always shorter</label></p>
<p><input type="radio" disabled><label>Always longer</label></p>
<p><input type="radio" disabled><label>Always the same</label></p>
<p><input type="radio" disabled checked><label>Shorter or the same</label></p>
<p><input type="radio" disabled><label>Longer or the same</label><br><br><br>1 point<br>5.For a tree trained on any dataset using parameters<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">max_depth = 6, min_node_size = 100, min_error_reduction=0.0</div></pre></td></tr></table></figure></p>
<p>what is the maximum possible number of splits encountered while making a single prediction?</p>
<p><input type="”text”" placeholder="6"><br><br><br>1 point<br>6.Is the validation error of the new decision tree (using early stopping conditions 2 and 3) lower than, higher than, or the same as that of the old decision tree from the previous assigment?</p>
<p><input type="radio" disabled><label>Higher than</label></p>
<p><input type="radio" disabled checked><label>Lower than</label></p>
<p><input type="radio" disabled><label>The same</label><br><br><br>1 point<br>7.Which tree has the smallest error on the validation data?</p>
<p><input type="radio" disabled><label>model_1</label></p>
<p><input type="radio" disabled><label>model_2</label></p>
<p><input type="radio" disabled checked><label>model_3</label><br><br><br>1 point<br>8.Does the tree with the smallest error in the training data also have the smallest error in the validation data?</p>
<p><input type="radio" disabled checked><label>Yes</label></p>
<p><input type="radio" disabled><label>No</label><br><br><br>1 point<br>9.Is it always true that the tree with the lowest classification error on the training set will result in the lowest classification error in the validation set?</p>
<p><input type="radio" disabled><label>Yes, this is ALWAYS true.</label></p>
<p><input type="radio" disabled checked><label>No, this is NOT ALWAYS true.</label><br><br><br>1 point<br>10.Which tree has the largest complexity?</p>
<p><input type="radio" disabled><label>model_1</label></p>
<p><input type="radio" disabled><label>model_2</label></p>
<p><input type="radio" disabled checked><label>model_3</label><br><br><br>1 point<br>11.Is it always true that the most complex tree will result in the lowest classification error in the validation_set?</p>
<p><input type="radio" disabled><label>Yes, this is ALWAYS true.</label></p>
<p><input type="radio" disabled checked><label>No, this is NOT ALWAYS true.</label><br><br><br>1 point<br>12.Using the complexity definition, which model (model_4, model_5, or model_6) has the largest complexity?</p>
<p><input type="radio" disabled checked><label>model_4</label></p>
<p><input type="radio" disabled><label>model_5</label></p>
<p><input type="radio" disabled><label>model_6</label><br><br><br>1 point<br>13.model_4 and model_5 have similar classification error on the validation set but model_5 has lower complexity. Should you pick model_5 over model_4?</p>
<p><input type="radio" disabled checked><label>Pick model_5 over model_4</label></p>
<p><input type="radio" disabled><label>Pick model_4 over model_5</label><br><br><br>1 point<br>14.Using the results obtained in this section, which model (model_7, model_8, or model_9) would you choose to use?</p>
<p><input type="radio" disabled><label>model_7</label></p>
<p><input type="radio" disabled checked><label>model_8</label></p>
<p><input type="radio" disabled><label>model_9</label><br><br></p>
<h3 id="Handling-Missing-Data"><a href="#Handling-Missing-Data" class="headerlink" title="Handling Missing Data"></a>Handling Missing Data</h3><div class="note primary"><p>Real-world machine learning problems are fraught with missing data. That is, very often, some of the inputs are not observed for all data points. This challenge is very significant, happens in most cases, and needs to be addressed carefully to obtain great performance. And, this issue is rarely discussed in machine learning courses. In this module, you will tackle the missing data challenge head on. You will start with the two most basic techniques to convert a dataset with missing data into a clean dataset, namely skipping missing values and inputing missing values. In an advanced section, you will also design a modification of the decision tree learning algorithm that builds decisions about missing data right into the model. You will also explore these techniques in your real-data implementation.</p>
</div>
<h4 id="Basic-strategies-for-handling-missing-data"><a href="#Basic-strategies-for-handling-missing-data" class="headerlink" title="Basic strategies for handling missing data"></a>Basic strategies for handling missing data</h4><h5 id="Slides-presented-in-this-module10-min-16"><a href="#Slides-presented-in-this-module10-min-16" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h5><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/4.1-decision-trees-missing-values-annotated.pdf" target="_blank" rel="external">decision-trees-missing-values-annotated.pdf</a></p>
<h5 id="Challenge-of-missing-data3-min"><a href="#Challenge-of-missing-data3-min" class="headerlink" title="Challenge of missing data3 min"></a>Challenge of missing data3 min</h5><h5 id="Strategy-1-Purification-by-skipping-missing-data4-min"><a href="#Strategy-1-Purification-by-skipping-missing-data4-min" class="headerlink" title="Strategy 1: Purification by skipping missing data4 min"></a>Strategy 1: Purification by skipping missing data4 min</h5><h5 id="Strategy-2-Purification-by-imputing-missing-data4-min"><a href="#Strategy-2-Purification-by-imputing-missing-data4-min" class="headerlink" title="Strategy 2: Purification by imputing missing data4 min"></a>Strategy 2: Purification by imputing missing data4 min</h5><h4 id="Strategy-3-Modify-learning-algorithm-to-explicitly-handle-missing-data"><a href="#Strategy-3-Modify-learning-algorithm-to-explicitly-handle-missing-data" class="headerlink" title="Strategy 3: Modify learning algorithm to explicitly handle missing data"></a>Strategy 3: Modify learning algorithm to explicitly handle missing data</h4><h5 id="Modifying-decision-trees-to-handle-missing-data4-min"><a href="#Modifying-decision-trees-to-handle-missing-data4-min" class="headerlink" title="Modifying decision trees to handle missing data4 min"></a>Modifying decision trees to handle missing data4 min</h5><h5 id="Feature-split-selection-with-missing-data5-min"><a href="#Feature-split-selection-with-missing-data5-min" class="headerlink" title="Feature split selection with missing data5 min"></a>Feature split selection with missing data5 min</h5><h4 id="Summarizing-handling-missing-data"><a href="#Summarizing-handling-missing-data" class="headerlink" title="Summarizing handling missing data"></a>Summarizing handling missing data</h4><h5 id="Recap-of-handling-missing-data1-min"><a href="#Recap-of-handling-missing-data1-min" class="headerlink" title="Recap of handling missing data1 min"></a>Recap of handling missing data1 min</h5><h5 id="Quiz-Handling-Missing-Data7-questions"><a href="#Quiz-Handling-Missing-Data7-questions" class="headerlink" title="Quiz: Handling Missing Data7 questions"></a>Quiz: Handling Missing Data7 questions</h5><div class="note primary"><p>QUIZ<br>Handling Missing Data<br>7 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 10, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.(True/False) Skipping data points (i.e., skipping rows of the data) that have missing features only works when the learning algorithm we are using is decision tree learning.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>2.What are potential downsides of skipping features with missing values (i.e., skipping columns of the data) to handle missing data?</p>
<p><input type="checkbox" disabled checked><label>So many features are skipped that accuracy can degrade</label></p>
<p><input type="checkbox" disabled><label>The learning algorithm will have to be modified</label></p>
<p><input type="checkbox" disabled><label>You will have fewer data points (i.e., rows) in the dataset</label></p>
<p><input type="checkbox" disabled checked><label>If an input at prediction time has a feature missing that was always present during training, this approach is not applicable.</label><br><br><br>1 point<br>3.(True/False) It’s always better to remove missing data points (i.e., rows) as opposed to removing missing features (i.e., columns).</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>4.Consider a dataset with N training points. After imputing missing values, the number of data points in the data set is</p>
<p><input type="radio" disabled><label>2 * N</label></p>
<p><input type="radio" disabled checked><label>N</label></p>
<p><input type="radio" disabled><label>5 * N</label><br><br><br>1 point<br>5.Consider a dataset with D features. After imputing missing values, the number of features in the data set is</p>
<p><input type="radio" disabled><label>2 * D</label></p>
<p><input type="radio" disabled checked><label>D</label></p>
<p><input type="radio" disabled><label>0.5 * D</label><br><br><br>1 point<br>6.Which of the following are always true when imputing missing data? Select all that apply.</p>
<p><input type="checkbox" disabled checked><label>Imputed values can be used in any classification algorithm</label></p>
<p><input type="checkbox" disabled checked><label>Imputed values can be used when there is missing data at prediction time</label></p>
<p><input type="checkbox" disabled><label>Using imputed values results in higher accuracies than skipping data points or skipping features</label><br><br><br>1 point<br>7.Consider data that has binary features (i.e. the feature values are 0 or 1) with some feature values of some data points missing. When learning the best feature split at a node, how would we best modify the decision tree learning algorithm to handle data points with missing values for a feature?</p>
<p><input type="radio" disabled checked><label>We choose to assign missing values to the branch of the tree (either the one with feature value equal to 0 or with feature value equal to 1) that minimizes classification error.</label></p>
<p><input type="radio" disabled><label>We assume missing data always has value 0.</label></p>
<p><input type="radio" disabled><label>We ignore all data points with missing values.</label><br><br></p>
<h2 id="Week-5-Boosting"><a href="#Week-5-Boosting" class="headerlink" title="Week 5 Boosting"></a>Week 5 Boosting</h2><div class="note primary"><p>One of the most exciting theoretical questions that have been asked about machine learning is whether simple classifiers can be combined into a highly accurate ensemble. This question lead to the developing of boosting, one of the most important and practical techniques in machine learning today. This simple approach can boost the accuracy of any classifier, and is widely used in practice, e.g., it’s used by more than half of the teams who win the Kaggle machine learning competitions. In this module, you will first define the ensemble classifier, where multiple models vote on the best prediction. You will then explore a boosting algorithm called AdaBoost, which provides a great approach for boosting classifiers. Through visualizations, you will become familiar with many of the practical aspects of this techniques. You will create your very own implementation of AdaBoost, from scratch, and use it to boost the performance of your loan risk predictor on real data.</p>
</div>
<h3 id="The-amazing-idea-of-boosting-a-classifier"><a href="#The-amazing-idea-of-boosting-a-classifier" class="headerlink" title="The amazing idea of boosting a classifier"></a>The amazing idea of boosting a classifier</h3><h4 id="Slides-presented-in-this-module10-min-17"><a href="#Slides-presented-in-this-module10-min-17" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/5.0-boosting-annotated.pdf" target="_blank" rel="external">boosting-annotated.pdf</a></p>
<h4 id="The-boosting-question3-min"><a href="#The-boosting-question3-min" class="headerlink" title="The boosting question3 min"></a>The boosting question3 min</h4><h4 id="Ensemble-classifiers5-min"><a href="#Ensemble-classifiers5-min" class="headerlink" title="Ensemble classifiers5 min"></a>Ensemble classifiers5 min</h4><h4 id="Boosting5-min"><a href="#Boosting5-min" class="headerlink" title="Boosting5 min"></a>Boosting5 min</h4><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><h4 id="AdaBoost-overview3-min"><a href="#AdaBoost-overview3-min" class="headerlink" title="AdaBoost overview3 min"></a>AdaBoost overview3 min</h4><h4 id="Weighted-error4-min"><a href="#Weighted-error4-min" class="headerlink" title="Weighted error4 min"></a>Weighted error4 min</h4><h4 id="Computing-coefficient-of-each-ensemble-component4-min"><a href="#Computing-coefficient-of-each-ensemble-component4-min" class="headerlink" title="Computing coefficient of each ensemble component4 min"></a>Computing coefficient of each ensemble component4 min</h4><h4 id="Reweighing-data-to-focus-on-mistakes4-min"><a href="#Reweighing-data-to-focus-on-mistakes4-min" class="headerlink" title="Reweighing data to focus on mistakes4 min"></a>Reweighing data to focus on mistakes4 min</h4><h4 id="Normalizing-weights2-min"><a href="#Normalizing-weights2-min" class="headerlink" title="Normalizing weights2 min"></a>Normalizing weights2 min</h4><h3 id="Applying-AdaBoost"><a href="#Applying-AdaBoost" class="headerlink" title="Applying AdaBoost"></a>Applying AdaBoost</h3><h4 id="Example-of-AdaBoost-in-action5-min"><a href="#Example-of-AdaBoost-in-action5-min" class="headerlink" title="Example of AdaBoost in action5 min"></a>Example of AdaBoost in action5 min</h4><h4 id="Learning-boosted-decision-stumps-with-AdaBoost4-min"><a href="#Learning-boosted-decision-stumps-with-AdaBoost4-min" class="headerlink" title="Learning boosted decision stumps with AdaBoost4 min"></a>Learning boosted decision stumps with AdaBoost4 min</h4><h3 id="Programming-Assignment-1-3"><a href="#Programming-Assignment-1-3" class="headerlink" title="Programming Assignment 1"></a>Programming Assignment 1</h3><h4 id="Exploring-Ensemble-Methods10-min"><a href="#Exploring-Ensemble-Methods10-min" class="headerlink" title="Exploring Ensemble Methods10 min"></a>Exploring Ensemble Methods10 min</h4><h4 id="Quiz-Exploring-Ensemble-Methods9-questions"><a href="#Quiz-Exploring-Ensemble-Methods9-questions" class="headerlink" title="Quiz: Exploring Ensemble Methods9 questions"></a>Quiz: Exploring Ensemble Methods9 questions</h4><div class="note primary"><p>QUIZ<br>Exploring Ensemble Methods<br>9 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 17, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.What percentage of the predictions on sample_validation_data did model_5 get correct?</p>
<p><input type="radio" disabled><label>25%</label></p>
<p><input type="radio" disabled><label>50%</label></p>
<p><input type="radio" disabled checked><label>75%</label></p>
<p><input type="radio" disabled><label>100%</label><br><br><br>1 point<br>2.According to model_5, which loan is the least likely to be a safe loan?</p>
<p><input type="radio" disabled><label>First</label></p>
<p><input type="radio" disabled><label>Second</label></p>
<p><input type="radio" disabled checked><label>Third</label></p>
<p><input type="radio" disabled><label>Fourth</label><br><br><br>1 point<br>3.What is the number of false positives on the validation data?</p>
<p><input type="”text”" placeholder="1653"><br><br><br>1 point<br>4.Using the same costs of the false positives and false negatives, what is the cost of the mistakes made by the boosted tree model (model_5) as evaluated on the validation_set?</p>
<p><input type="”text”" placeholder="47970000"><br><br><br>1 point<br>5.What grades are the top 5 loans?</p>
<p><input type="radio" disabled checked><label>A</label></p>
<p><input type="radio" disabled><label>B</label></p>
<p><input type="radio" disabled><label>C</label></p>
<p><input type="radio" disabled><label>D</label></p>
<p><input type="radio" disabled><label>E</label><br><br><br>1 point<br>6.Which model has the best accuracy on the validation_data?</p>
<p><input type="radio" disabled><label>model_10</label></p>
<p><input type="radio" disabled><label>model_50</label></p>
<p><input type="radio" disabled checked><label>model_100</label></p>
<p><input type="radio" disabled><label>model_200</label></p>
<p><input type="radio" disabled><label>model_500</label><br><br><br>1 point<br>7.Is it always true that the model with the most trees will perform best on the test/validation set?</p>
<p><input type="radio" disabled><label>Yes, a model with more trees will ALWAYS perform better on the test/validation set.</label></p>
<p><input type="radio" disabled checked><label>No, a model with more trees does not always perform better on the test/validation set.</label><br><br><br>1 point<br>8.Does the training error reduce as the number of trees increases?</p>
<p><input type="radio" disabled checked><label>Yes</label></p>
<p><input type="radio" disabled><label>No</label><br><br><br>1 point<br>9.Is it always true that the test/validation error will reduce as the number of trees increases?</p>
<p><input type="radio" disabled><label>Yes, it is ALWAYS true that the test/validation error will reduce as the number of trees increases.</label></p>
<p><input type="radio" disabled checked><label>No, the test/validation error will not necessarily always reduce as the number of trees increases.</label><br><br></p>
<h3 id="Convergence-and-overfitting-in-boosting"><a href="#Convergence-and-overfitting-in-boosting" class="headerlink" title="Convergence and overfitting in boosting"></a>Convergence and overfitting in boosting</h3><h4 id="The-Boosting-Theorem3-min"><a href="#The-Boosting-Theorem3-min" class="headerlink" title="The Boosting Theorem3 min"></a>The Boosting Theorem3 min</h4><h4 id="Overfitting-in-boosting5-min"><a href="#Overfitting-in-boosting5-min" class="headerlink" title="Overfitting in boosting5 min"></a>Overfitting in boosting5 min</h4><h3 id="Summarizing-boosting"><a href="#Summarizing-boosting" class="headerlink" title="Summarizing boosting"></a>Summarizing boosting</h3><h4 id="Ensemble-methods-impact-of-boosting-amp-quick-recap4-min"><a href="#Ensemble-methods-impact-of-boosting-amp-quick-recap4-min" class="headerlink" title="Ensemble methods, impact of boosting &amp; quick recap4 min"></a>Ensemble methods, impact of boosting &amp; quick recap4 min</h4><h4 id="Quiz-Boosting11-questions"><a href="#Quiz-Boosting11-questions" class="headerlink" title="Quiz:Boosting11 questions"></a>Quiz:Boosting11 questions</h4><div class="note primary"><p>QUIZ<br>Boosting<br>11 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 17, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Which of the following is NOT an ensemble method?</p>
<p><input type="radio" disabled><label>Gradient boosted trees</label></p>
<p><input type="radio" disabled><label>AdaBoost</label></p>
<p><input type="radio" disabled><label>Random forests</label></p>
<p><input type="radio" disabled checked><label>Single decision trees</label><br><br><br>1 point<br>2.Each binary classifier in an ensemble makes predictions on an input x as listed in the table below. Based on the ensemble coefficients also listed in the table, what is the final ensemble model’s prediction for x?</p>
<table>
<thead>
<tr>
<th></th>
<th>Classifier coefficient wt</th>
<th>Prediction for x</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classifier 1</td>
<td>0.61</td>
<td>+1</td>
</tr>
<tr>
<td>Classifier 2</td>
<td>0.53</td>
<td>-1</td>
</tr>
<tr>
<td>Classifier 3</td>
<td>0.88</td>
<td>-1</td>
</tr>
<tr>
<td>Classifier 4</td>
<td>0.34</td>
<td>+1</td>
</tr>
</tbody>
</table>
<p><input type="radio" disabled><label>+1</label></p>
<p><input type="radio" disabled checked><label>-1</label><br><br><br>1 point<br>3.(True/False) Boosted decision stumps is a linear classifier.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>4.(True/False) For AdaBoost, test error is an appropriate criterion for choosing the optimal number of iterations.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>5.In an iteration in AdaBoost, recall that learning the coefficient w_t for learned weak learner f_t is calculated by</p>
<p>$$\displaystyle{\frac{1}{2}\ln{\left( \frac{1-\mathtt{weighted_error}(f_t)}{\mathtt{weighted_error}(f_t)} \right)}}$$<br>If the weighted error of f_t is equal to .25, what is the value of w_t? Round your answer to 2 decimal places.</p>
<p><input type="”text”" placeholder="0.55"><br><br><br>1 point<br>6.Which of the following classifiers is most accurate as computed on a weighted dataset? A classifier with:</p>
<p><input type="radio" disabled checked><label>weighted error = 0.1</label></p>
<p><input type="radio" disabled><label>weighted error = 0.3</label></p>
<p><input type="radio" disabled><label>weighted error = 0.5</label></p>
<p><input type="radio" disabled><label>weighted error = 0.7</label></p>
<p><input type="radio" disabled><label>weighted error = 0.99</label><br><br><br>1 point<br>7.Imagine we are training a decision stump in an iteration of AdaBoost, and we are at a node. Each data point is (x1, x2, y), where x1,x2 are features, and y is the label. Also included are the weights of the data. The data at this node is:</p>
<table>
<thead>
<tr>
<th>Weight</th>
<th>x1</th>
<th>x2</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.3</td>
<td>0</td>
<td>1</td>
<td>+1</td>
</tr>
<tr>
<td>0.35</td>
<td>1</td>
<td>0</td>
<td>-1</td>
</tr>
<tr>
<td>0.1</td>
<td>0</td>
<td>1</td>
<td>+1</td>
</tr>
<tr>
<td>0.25</td>
<td>1</td>
<td>1</td>
<td>+1</td>
</tr>
</tbody>
</table>
<p>Suppose we assign the same class label to all data in this node. (Pick the class label with the greater total weight.) What is the weighted error at the node? Round your answer to 2 decimal places.</p>
<p><input type="”text”" placeholder="0.35"><br><br><br>1 point<br>8.After each iteration of AdaBoost, the weights on the data points are typically normalized to sum to 1. This is used because</p>
<p><input type="radio" disabled checked><label>of issues with numerical instability (underflow/overflow)</label></p>
<p><input type="radio" disabled><label>the weak learners can only learn with normalized weights</label></p>
<p><input type="radio" disabled><label>none of the above</label><br><br><br>1 point<br>9.Consider the following 2D dataset with binary labels.</p>
<p>We train a series of weak binary classifiers using AdaBoost. In one iteration, the weak binary classifier produces the decision boundary as follows:<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/HswHyeBjEeWufRJaRfO1AQ_b5746ae36ef317b86c126c93db145c34_Capture2.PNG?expiry=1505692800000&amp;hmac=LMnZL5gdQFZMMBxMXXpoPxENUFDAvnbZlfMRhwH-RIU" alt=""></p>
<p>Which of the five points (indicated in the second figure) will receive higher weight in the following iteration? Choose all that apply.</p>
<p><input type="checkbox" disabled><label>(1)</label></p>
<p><input type="checkbox" disabled checked><label>(2)</label></p>
<p><input type="checkbox" disabled checked><label>(3)</label></p>
<p><input type="checkbox" disabled><label>(4)</label></p>
<p><input type="checkbox" disabled><label>(5)</label><br><br><br>1 point<br>10.Suppose we are running AdaBoost using decision tree stumps. At a particular iteration, the data points have weights according the figure. (Large points indicate heavy weights.)<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/S8wkfeD0EeWIdgqHxZs34w_bee4e1ecb5c161de4cb755e30ebe0576_Capture.PNG?expiry=1505692800000&amp;hmac=wxPmRHwhhwoS0414YzGLdJ_-sXRc5tL9nzeiiBT6YIw" alt=""><br>Which of the following decision tree stumps is most likely to be fit in the next iteration?</p>
<p><input type="radio" disabled checked><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/3-kqZeD0EeW--hK2gi_BIw_7d5bf2ab0433c80a45f1593efdefc45f_Capture2.PNG?expiry=1505692800000&amp;hmac=Me2wotTAmDMsD-U1EFKwC_BYogs4XOwZtwkVvXBmfJ8" alt=""></label></p>
<p><input type="radio" disabled><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/6F6K0uD0EeWOVQ68c1xy2w_22bc6b9e118905c85b278d32017fbb9a_Capture3.PNG?expiry=1505692800000&amp;hmac=vJi2NzLNS4WoLAH8R7EHjb0HnhkXGZ9-adKsEX362Zg" alt=""></label></p>
<p><input type="radio" disabled><label><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/7_VxOuD0EeWuUgrcWIxPhQ_6fab04ae0606eca1b06362bd07d56aee_Capture4.PNG?expiry=1505692800000&amp;hmac=GomRfwsO8J4iA80zcybuUUAu-UwXIdaocMYpcZXHRz8" alt=""></label><br><br><br>1 point<br>11.(True/False) AdaBoost achieves zero training error after a sufficient number of iterations, as long as we can find weak learners that perform better than random chance at each iteration of AdaBoost (i.e., on weighted data).</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br></p>
<h3 id="Programming-Assignment-2-3"><a href="#Programming-Assignment-2-3" class="headerlink" title="Programming Assignment 2"></a>Programming Assignment 2</h3><h4 id="Boosting-a-decision-stump10-min"><a href="#Boosting-a-decision-stump10-min" class="headerlink" title="Boosting a decision stump10 min"></a>Boosting a decision stump10 min</h4><h4 id="Quiz-Boosting-a-decision-stump5-questions"><a href="#Quiz-Boosting-a-decision-stump5-questions" class="headerlink" title="Quiz:Boosting a decision stump5 questions"></a>Quiz:Boosting a decision stump5 questions</h4><div class="note primary"><p>QUIZ<br>Boosting a decision stump<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>September 17, 11:59 PM PDT<br>You can still pass this quiz before the course ends.</p>
</div>
<p>1 point<br>1.Recall that the classification error for unweighted data is defined as follows:</p>
<p>classification error=# mistakes# all data points<br>Meanwhile, the weight of mistakes for weighted data is given by</p>
<p>$$WM(α,y^)=∑i=1nαi×1[yi≠y^i].$$<br>If we set the weights α=1 for all data points, how is the weight of mistakes WM(α,ŷ) related to the classification error?</p>
<p><input type="radio" disabled><label>WM(α,ŷ) = [classification error]</label></p>
<p><input type="radio" disabled><label>WM(α,ŷ) = [classification error] * [weight of correctly classified data points]</label></p>
<p><input type="radio" disabled checked><label>WM(α,ŷ) = N * [classification error]</label></p>
<p><input type="radio" disabled><label>WM(α,ŷ) = 1 - [classification error]</label><br><br><br>1 point<br>2.Refer to section Example: Training a weighted decision tree.</p>
<p>Will you get the same model as small_data_decision_tree_subset_20 if you trained a decision tree with only 20 data points from the set of points in subset_20?</p>
<p><input type="radio" disabled checked><label>Yes</label></p>
<p><input type="radio" disabled><label>No</label><br><br><br>1 point<br>3.Refer to the 10-component ensemble of tree stumps trained with Adaboost.</p>
<p>As each component is trained sequentially, are the component weights monotonically decreasing, monotonically increasing, or neither?</p>
<p><input type="radio" disabled><label>Monotonically decreasing</label></p>
<p><input type="radio" disabled><label>Monotonically increasing</label></p>
<p><input type="radio" disabled checked><label>Neither</label><br><br><br>1 point<br>4.Which of the following best describes a general trend in accuracy as we add more and more components? Answer based on the 30 components learned so far.</p>
<p><input type="radio" disabled><label>Training error goes down monotonically, i.e. the training error reduces with each iteration but never increases.</label></p>
<p><input type="radio" disabled checked><label>Training error goes down in general, with some ups and downs in the middle.</label></p>
<p><input type="radio" disabled><label>Training error goes up in general, with some ups and downs in the middle.</label></p>
<p><input type="radio" disabled><label>Training error goes down in the beginning, achieves the best error, and then goes up sharply.</label></p>
<p><input type="radio" disabled><label>None of the above</label><br><br><br>1 point<br>5.From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?</p>
<p><input type="radio" disabled><label>Yes</label></p>
<p><input type="radio" disabled checked><label>No</label><br><br></p>
<h2 id="Week-6-Precision-Recall"><a href="#Week-6-Precision-Recall" class="headerlink" title="Week 6 Precision-Recall"></a>Week 6 Precision-Recall</h2><div class="note primary"><p>In many real-world settings, accuracy or error are not the best quality metrics for classification. You will explore a case-study that significantly highlights this issue: using sentiment analysis to display positive reviews on a restaurant website. Instead of accuracy, you will define two metrics: precision and recall, which are widely used in real-world applications to measure the quality of classifiers. You will explore how the probabilities output by your classifier can be used to trade-off precision with recall, and dive into this spectrum, using precision-recall curves. In your hands-on implementation, you will compute these metrics with your learned classifier on real-world sentiment analysis data.</p>
</div>
<h3 id="Why-use-precision-amp-recall-as-quality-metrics"><a href="#Why-use-precision-amp-recall-as-quality-metrics" class="headerlink" title="Why use precision &amp; recall as quality metrics"></a>Why use precision &amp; recall as quality metrics</h3><h4 id="Slides-presented-in-this-module10-min-18"><a href="#Slides-presented-in-this-module10-min-18" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Classification/blob/master/Lecture%20Slides/6.0-precision-recall.pdf" target="_blank" rel="external">precision-recall.pdf</a></p>
<h4 id="Case-study-where-accuracy-is-not-best-metric-for-classification3-min"><a href="#Case-study-where-accuracy-is-not-best-metric-for-classification3-min" class="headerlink" title="Case-study where accuracy is not best metric for classification3 min"></a>Case-study where accuracy is not best metric for classification3 min</h4><h4 id="What-is-good-performance-for-a-classifier-3-min"><a href="#What-is-good-performance-for-a-classifier-3-min" class="headerlink" title="What is good performance for a classifier?3 min"></a>What is good performance for a classifier?3 min</h4><h3 id="Precision-amp-recall-explained"><a href="#Precision-amp-recall-explained" class="headerlink" title="Precision &amp; recall explained"></a>Precision &amp; recall explained</h3><h4 id="Precision-Fraction-of-positive-predictions-that-are-actually-positive5-min"><a href="#Precision-Fraction-of-positive-predictions-that-are-actually-positive5-min" class="headerlink" title="Precision: Fraction of positive predictions that are actually positive5 min"></a>Precision: Fraction of positive predictions that are actually positive5 min</h4><h4 id="Recall-Fraction-of-positive-data-predicted-to-be-positive3-min"><a href="#Recall-Fraction-of-positive-data-predicted-to-be-positive3-min" class="headerlink" title="Recall: Fraction of positive data predicted to be positive3 min"></a>Recall: Fraction of positive data predicted to be positive3 min</h4><h3 id="The-precision-recall-tradeoff"><a href="#The-precision-recall-tradeoff" class="headerlink" title="The precision-recall tradeoff"></a>The precision-recall tradeoff</h3><h4 id="Precision-recall-extremes2-min"><a href="#Precision-recall-extremes2-min" class="headerlink" title="Precision-recall extremes2 min"></a>Precision-recall extremes2 min</h4><h4 id="Trading-off-precision-and-recall4-min"><a href="#Trading-off-precision-and-recall4-min" class="headerlink" title="Trading off precision and recall4 min"></a>Trading off precision and recall4 min</h4><h4 id="Precision-recall-curve5-min"><a href="#Precision-recall-curve5-min" class="headerlink" title="Precision-recall curve5 min"></a>Precision-recall curve5 min</h4><h3 id="Summarizing-precision-recall"><a href="#Summarizing-precision-recall" class="headerlink" title="Summarizing precision-recall"></a>Summarizing precision-recall</h3><h4 id="Recap-of-precision-recall1-min"><a href="#Recap-of-precision-recall1-min" class="headerlink" title="Recap of precision-recall1 min"></a>Recap of precision-recall1 min</h4><h4 id="Quiz-Precision-Recall9-questions"><a href="#Quiz-Precision-Recall9-questions" class="headerlink" title="Quiz: Precision-Recall9 questions"></a>Quiz: Precision-Recall9 questions</h4><div class="note primary"><p>QUIZ<br>Precision-Recall<br>9 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 1, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Questions 1 to 5 refer to the following scenario:</p>
<p>Suppose a binary classifier produced the following confusion matrix.</p>
<table>
<thead>
<tr>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actual Positive    5600</td>
<td>40</td>
</tr>
<tr>
<td>Actual Negative    1900</td>
<td>2460</td>
</tr>
</tbody>
</table>
<p>What is the recall of this classifier? Round your answer to 2 decimal places.</p>
<p><input type="”text”" placeholder="0.99"><br><br><br>1 point<br>2.Refer to the scenario presented in Question 1 to answer the following:</p>
<p>(True/False) This classifier is better than random guessing.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.Refer to the scenario presented in Question 1 to answer the following:</p>
<p>(True/False) This classifier is better than the majority class classifier.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>4.Refer to the scenario presented in Question 1 to answer the following:</p>
<p>Which of the following points in the precision-recall space corresponds to this classifier?<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/TDEYl-BmEeWuUgrcWIxPhQ_3c704bfa2b87d7dc0429d29ac709c690_Capture.PNG?expiry=1506902400000&amp;hmac=-1fK0E--IjAPFZXgFOxmbB0UrwVNqzJVhMAzrf0lcG0" alt=""></p>
<p><input type="radio" disabled><label></label><br>(1)</p>
<p><input type="radio" disabled><label></label><br>(2)</p>
<p><input type="radio" disabled checked><label></label><br>(3)</p>
<p><input type="radio" disabled><label></label><br>(4)</p>
<p><input type="radio" disabled><label></label><br>(5)<br><br><br>1 point<br>5.Refer to the scenario presented in Question 1 to answer the following:</p>
<p>Which of the following best describes this classifier?</p>
<p><input type="radio" disabled checked><label></label><br>It is optimistic</p>
<p><input type="radio" disabled><label></label><br>It is pessimistic</p>
<p><input type="radio" disabled><label></label><br>None of the<br><br><br>1 point<br>6.Suppose we are fitting a logistic regression model on a dataset where the vast majority of the data points are labeled as positive. To compensate for overfitting to the dominant class, we should</p>
<p><input type="radio" disabled checked><label></label><br>Require higher confidence level for positive predictions</p>
<p><input type="radio" disabled><label></label><br>Require lower confidence level for positive predictions<br><br><br>1 point<br>7.It is often the case that false positives and false negatives incur different costs. In situations where false negatives cost much more than false positives, we should</p>
<p><input type="radio" disabled><label></label><br>Require higher confidence level for positive predictions</p>
<p><input type="radio" disabled checked><label></label><br>Require lower confidence level for positive predictions<br><br><br>1 point<br>8.We are interested in reducing the number of false negatives. Which of the following metrics should we primarily look at?</p>
<p><input type="radio" disabled><label></label><br>Accuracy</p>
<p><input type="radio" disabled><label></label><br>Precision</p>
<p><input type="radio" disabled checked><label></label><br>Recall<br><br><br>1 point<br>9.Suppose we set the threshold for positive predictions at 0.9. What is the lowest score that is classified as positive? Round your answer to 2 decimal places.</p>
<p><input type="”text”" placeholder="2.20"><br><br></p>
<h3 id="Programming-Assignment-5"><a href="#Programming-Assignment-5" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h3><h4 id="Exploring-precision-and-recall10-min"><a href="#Exploring-precision-and-recall10-min" class="headerlink" title="Exploring precision and recall10 min"></a>Exploring precision and recall10 min</h4><h4 id="Quiz-Exploring-precision-and-recall13-questions"><a href="#Quiz-Exploring-precision-and-recall13-questions" class="headerlink" title="Quiz: Exploring precision and recall13 questions"></a>Quiz: Exploring precision and recall13 questions</h4><div class="note primary"><p>QUIZ<br>Exploring precision and recall<br>13 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 1, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Consider the logistic regression model trained on amazon_baby.gl using GraphLab Create.</p>
<p>Using accuracy as the evaluation metric, was our logistic regression model better than the majority class classifier?</p>
<p><input type="radio" disabled checked><label>Yes</label></p>
<p><input type="radio" disabled><label>No</label><br><br><br>1 point<br>2.How many predicted values in the test set are false positives?</p>
<p><input type="”text”" placeholder="1451"><br><br><br>1 point<br>3.Consider the scenario where each false positive costs $100 and each false negative $1.</p>
<p>Given the stipulation, what is the cost associated with the logistic regression classifier’s performance on the test set?</p>
<p><input type="radio" disabled><label></label><br>Between $0 and $100,000</p>
<p><input type="radio" disabled checked><label></label><br>Between $100,000 and $200,000</p>
<p><input type="radio" disabled><label></label><br>Between $200,000 and $300,000</p>
<p><input type="radio" disabled><label></label><br>Above $300,000<br><br><br>1 point<br>4.Out of all reviews in the test set that are predicted to be positive, what fraction of them are false positives? (Round to the second decimal place e.g. 0.25)</p>
<p><input type="”text”" placeholder="0.05"><br><br><br>1 point<br>5.Based on what we learned in lecture, if we wanted to reduce this fraction of false positives to be below 3.5%, we would:</p>
<p><input type="radio" disabled><label></label><br>Discard a sufficient number of positive predictions</p>
<p><input type="radio" disabled><label></label><br>Discard a sufficient number of negative predictions</p>
<p><input type="radio" disabled checked><label></label><br>Increase threshold for predicting the positive class (y^=+1)</p>
<p><input type="radio" disabled><label></label><br>Decrease threshold for predicting the positive class (y^=+1)<br><br><br>1 point<br>6.What fraction of the positive reviews in the test_set were correctly predicted as positive by the classifier? Round your answer to 2 decimal places.</p>
<p><input type="”text”" placeholder="0.97"><br><br><br>1 point<br>7.What is the recall value for a classifier that predicts +1 for all data points in the test_data?</p>
<p><input type="”text”" placeholder="1"><br><br><br>1 point<br>8.What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9?</p>
<p><input type="radio" disabled><label></label><br>More reviews are predicted to be positive.</p>
<p><input type="radio" disabled checked><label></label><br>Fewer reviews are predicted to be positive.<br><br><br>1 point<br>9.Consider the metrics obtained from setting the threshold to 0.5 and to 0.9.</p>
<p>Does the recall increase with a higher threshold?</p>
<p><input type="radio" disabled><label>Yes</label></p>
<p><input type="radio" disabled checked><label>No</label><br><br><br>1 point<br>10.Among all the threshold values tried, what is the smallest threshold value that achieves a precision of 96.5% or better? Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.707"><br><br><br>1 point<br>11.Using threshold = 0.98, how many false negatives do we get on the test_data? (Hint: You may use the graphlab.evaluation.confusion_matrix function implemented in GraphLab Create.)</p>
<p><input type="”text”" placeholder="8239"><br><br><br>1 point<br>12.Questions 13 and 14 are concerned with the reviews that contain the word baby.</p>
<p>Among all the threshold values tried, what is the smallest threshold value that achieves a precision of 96.5% or better for the reviews of data in baby_reviews? Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.732"><br><br><br>1 point<br>13.Questions 13 and 14 are concerned with the reviews that contain the word baby.</p>
<p>Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 96.5%?</p>
<p><input type="radio" disabled checked><label></label><br>Larger</p>
<p><input type="radio" disabled><label></label><br>Smaller<br><br></p>
<h2 id="Week-7-Scaling-to-Huge-Datasets-amp-Online-Learning"><a href="#Week-7-Scaling-to-Huge-Datasets-amp-Online-Learning" class="headerlink" title="Week 7 Scaling to Huge Datasets &amp; Online Learning"></a>Week 7 Scaling to Huge Datasets &amp; Online Learning</h2><div class="note primary"><p>With the advent of the internet, the growth of social media, and the embedding of sensors in the world, the magnitudes of data that our machine learning algorithms must handle have grown tremendously over the last decade. This effect is sometimes called “Big Data”. Thus, our learning algorithms must scale to bigger and bigger datasets. In this module, you will develop a small modification of gradient ascent called stochastic gradient, which provides significant speedups in the running time of our algorithms. This simple change can drastically improve scaling, but makes the algorithm less stable and harder to use in practice. In this module, you will investigate the practical techniques needed to make stochastic gradient viable, and to thus to obtain learning algorithms that scale to huge datasets. You will also address a new kind of machine learning problem, online learning, where the data streams in over time, and we must learn the coefficients as the data arrives. This task can also be solved with stochastic gradient. You will implement your very own stochastic gradient ascent algorithm for logistic regression from scratch, and evaluate it on sentiment analysis data.</p>
</div>
<h3 id="Scaling-ML-to-huge-datasets"><a href="#Scaling-ML-to-huge-datasets" class="headerlink" title="Scaling ML to huge datasets"></a>Scaling ML to huge datasets</h3><h4 id="Slides-presented-in-this-module10-min-19"><a href="#Slides-presented-in-this-module10-min-19" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p>online-learning-annotated.pdf</p>
<h4 id="Gradient-ascent-won’t-scale-to-today’s-huge-datasets3-min"><a href="#Gradient-ascent-won’t-scale-to-today’s-huge-datasets3-min" class="headerlink" title="Gradient ascent won’t scale to today’s huge datasets3 min"></a>Gradient ascent won’t scale to today’s huge datasets3 min</h4><h4 id="Timeline-of-scalable-machine-learning-amp-stochastic-gradient4-min"><a href="#Timeline-of-scalable-machine-learning-amp-stochastic-gradient4-min" class="headerlink" title="Timeline of scalable machine learning &amp; stochastic gradient4 min"></a>Timeline of scalable machine learning &amp; stochastic gradient4 min</h4><h3 id="Scaling-ML-with-stochastic-gradient"><a href="#Scaling-ML-with-stochastic-gradient" class="headerlink" title="Scaling ML with stochastic gradient"></a>Scaling ML with stochastic gradient</h3><h4 id="Why-gradient-ascent-won’t-scale3-min"><a href="#Why-gradient-ascent-won’t-scale3-min" class="headerlink" title="Why gradient ascent won’t scale3 min"></a>Why gradient ascent won’t scale3 min</h4><h4 id="Stochastic-gradient-Learning-one-data-point-at-a-time3-min"><a href="#Stochastic-gradient-Learning-one-data-point-at-a-time3-min" class="headerlink" title="Stochastic gradient: Learning one data point at a time3 min"></a>Stochastic gradient: Learning one data point at a time3 min</h4><h4 id="Comparing-gradient-to-stochastic-gradient3-min"><a href="#Comparing-gradient-to-stochastic-gradient3-min" class="headerlink" title="Comparing gradient to stochastic gradient3 min"></a>Comparing gradient to stochastic gradient3 min</h4><h3 id="Understanding-why-stochastic-gradient-works"><a href="#Understanding-why-stochastic-gradient-works" class="headerlink" title="Understanding why stochastic gradient works"></a>Understanding why stochastic gradient works</h3><h4 id="Why-would-stochastic-gradient-ever-work-4-min"><a href="#Why-would-stochastic-gradient-ever-work-4-min" class="headerlink" title="Why would stochastic gradient ever work?4 min"></a>Why would stochastic gradient ever work?4 min</h4><h4 id="Convergence-paths2-min"><a href="#Convergence-paths2-min" class="headerlink" title="Convergence paths2 min"></a>Convergence paths2 min</h4><h3 id="Stochastic-gradient-Practical-tricks"><a href="#Stochastic-gradient-Practical-tricks" class="headerlink" title="Stochastic gradient: Practical tricks"></a>Stochastic gradient: Practical tricks</h3><h4 id="Shuffle-data-before-running-stochastic-gradient2-min"><a href="#Shuffle-data-before-running-stochastic-gradient2-min" class="headerlink" title="Shuffle data before running stochastic gradient2 min"></a>Shuffle data before running stochastic gradient2 min</h4><h4 id="Choosing-step-size3-min"><a href="#Choosing-step-size3-min" class="headerlink" title="Choosing step size3 min"></a>Choosing step size3 min</h4><h4 id="Don’t-trust-last-coefficients1-min"><a href="#Don’t-trust-last-coefficients1-min" class="headerlink" title="Don’t trust last coefficients1 min"></a>Don’t trust last coefficients1 min</h4><h4 id="OPTIONAL-Learning-from-batches-of-data3-min"><a href="#OPTIONAL-Learning-from-batches-of-data3-min" class="headerlink" title="(OPTIONAL) Learning from batches of data3 min"></a>(OPTIONAL) Learning from batches of data3 min</h4><h4 id="OPTIONAL-Measuring-convergence4-min"><a href="#OPTIONAL-Measuring-convergence4-min" class="headerlink" title="(OPTIONAL) Measuring convergence4 min"></a>(OPTIONAL) Measuring convergence4 min</h4><h4 id="OPTIONAL-Adding-regularization3-min"><a href="#OPTIONAL-Adding-regularization3-min" class="headerlink" title="(OPTIONAL) Adding regularization3 min"></a>(OPTIONAL) Adding regularization3 min</h4><h3 id="Online-learning-Fitting-models-from-streaming-data"><a href="#Online-learning-Fitting-models-from-streaming-data" class="headerlink" title="Online learning: Fitting models from streaming data"></a>Online learning: Fitting models from streaming data</h3><h4 id="The-online-learning-task3-min"><a href="#The-online-learning-task3-min" class="headerlink" title="The online learning task3 min"></a>The online learning task3 min</h4><h4 id="Using-stochastic-gradient-for-online-learning3-min"><a href="#Using-stochastic-gradient-for-online-learning3-min" class="headerlink" title="Using stochastic gradient for online learning3 min"></a>Using stochastic gradient for online learning3 min</h4><h3 id="Summarizing-scaling-to-huge-datasets-amp-online-learning"><a href="#Summarizing-scaling-to-huge-datasets-amp-online-learning" class="headerlink" title="Summarizing scaling to huge datasets &amp; online learning"></a>Summarizing scaling to huge datasets &amp; online learning</h3><h4 id="Scaling-to-huge-datasets-through-parallelization-amp-module-recap1-min"><a href="#Scaling-to-huge-datasets-through-parallelization-amp-module-recap1-min" class="headerlink" title="Scaling to huge datasets through parallelization &amp; module recap1 min"></a>Scaling to huge datasets through parallelization &amp; module recap1 min</h4><h4 id="Quiz-Scaling-to-Huge-Datasets-amp-Online-Learning10-questions"><a href="#Quiz-Scaling-to-Huge-Datasets-amp-Online-Learning10-questions" class="headerlink" title="Quiz: Scaling to Huge Datasets &amp; Online Learning10 questions"></a>Quiz: Scaling to Huge Datasets &amp; Online Learning10 questions</h4><div class="note primary"><p>QUIZ<br>Scaling to Huge Datasets &amp; Online Learning<br>10 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 8, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.(True/False) Stochastic gradient ascent often requires fewer passes over the dataset than batch gradient ascent to achieve a similar log likelihood.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>2.(True/False) Choosing a large batch size results in less noisy gradients</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.(True/False) The set of coefficients obtained at the last iteration represents the best coefficients found so far.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>4.Suppose you obtained the plot of log likelihood below after running stochastic gradient ascent.<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/VyRs8uBtEeWBDQ73-3lhaw_70d1c456f3deba4ffa876e53e041cf7a_Capture.PNG?expiry=1507507200000&amp;hmac=GPeyl-jB1hlWTusMbX5OytaN50LkQY7_shrlHJ6mYM0" alt=""></p>
<p>Which of the following actions would help the most to improve the rate of convergence?</p>
<p><input type="radio" disabled><label></label><br>Increase step size</p>
<p><input type="radio" disabled checked><label></label><br>Decrease step size</p>
<p><input type="radio" disabled><label></label><br>Decrease batch size<br><br><br>1 point<br>5.Suppose you obtained the plot of log likelihood below after running stochastic gradient ascent.<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Ym5JpOBtEeWBDQ73-3lhaw_8f2bb6e530dc2b481646181bf6e72a80_Capture2.PNG?expiry=1507507200000&amp;hmac=gZL-m8ELhykkG1o7vHWKLDB97Iz438-YlQberFWebpU" alt=""></p>
<p>Which of the following actions would help to improve the rate of convergence?</p>
<p><input type="radio" disabled><label></label><br>Increase batch size</p>
<p><input type="radio" disabled checked><label></label><br>Increase step size</p>
<p><input type="radio" disabled><label></label><br>Decrease step size<br><br><br>1 point<br>6.Suppose it takes about 1 milliseconds to compute a gradient for a single example. You run an online advertising company and would like to do online learning via mini-batch stochastic gradient ascent. If you aim to update the coefficients once every 5 minutes, how many examples can you cover in each update? Overhead and other operations take up 2 minutes, so you only have 3 minutes for the coefficient update.</p>
<p><input type="”text”" placeholder="180000"><br><br><br>1 point<br>7.In search for an optimal step size, you experiment with multiple step sizes and obtain the following convergence plot.<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bS2ZaOBtEeWufRJaRfO1AQ_8d67c76c9a6715f27ebc78fb8df13c0a_Capture3.PNG?expiry=1507507200000&amp;hmac=VxYGJqOhG-WnXEmRD3btsmFP6EXgqAv8AD41kKDXcGU" alt=""></p>
<p>Which line corresponds to the best step size?</p>
<p><input type="radio" disabled checked><label></label><br>(1)</p>
<p><input type="radio" disabled><label></label><br>(2)</p>
<p><input type="radio" disabled><label></label><br>(3)</p>
<p><input type="radio" disabled><label></label><br>(4)</p>
<p><input type="radio" disabled><label></label><br>(5)<br><br><br>1 point<br>8.Suppose you run stochastic gradient ascent with two different batch sizes. Which of the two lines below corresponds to the smaller batch size (assuming both use the same step size)?<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/oj-RfOBtEeWOVQ68c1xy2w_6f018b2d10620f6857e5580f40c7eeb5_Capture4.PNG?expiry=1507507200000&amp;hmac=Gj_7ydXkVqT1m-9yoyzMlsxiemUQWwLqdkfHgkUZs4M" alt=""></p>
<p><input type="radio" disabled checked><label></label><br>(1)</p>
<p><input type="radio" disabled><label></label><br>(2)<br><br><br>1 point<br>9.Which of the following is NOT a benefit of stochastic gradient ascent over batch gradient ascent? Choose all that apply.</p>
<p><input type="checkbox" disabled><label></label><br>Each coefficient step is very fast.</p>
<p><input type="checkbox" disabled checked><label></label><br>Log likelihood of data improves monotonically.</p>
<p><input type="checkbox" disabled><label></label><br>Stochastic gradient ascent can be used for online learning.</p>
<p><input type="checkbox" disabled><label></label><br>Stochastic gradient ascent can achieve higher likelihood than batch gradient ascent for the same amount of running time.</p>
<p><input type="checkbox" disabled checked><label></label><br>Stochastic gradient ascent is highly robust with respect to parameter choices.<br><br><br>1 point<br>10.Suppose we run the stochastic gradient ascent algorithm described in the lecture with batch size of 100. To make 10 passes over a dataset consisting of 15400 examples, how many iterations does it need to run?</p>
<p><input type="”text”" placeholder="1540"><br><br></p>
<h3 id="Programming-Assignment-6"><a href="#Programming-Assignment-6" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h3><h4 id="Training-Logistic-Regression-via-Stochastic-Gradient-Ascent10-min"><a href="#Training-Logistic-Regression-via-Stochastic-Gradient-Ascent10-min" class="headerlink" title="Training Logistic Regression via Stochastic Gradient Ascent10 min"></a>Training Logistic Regression via Stochastic Gradient Ascent10 min</h4><h4 id="Quiz-Training-Logistic-Regression-via-Stochastic-Gradient-Ascent12-questions"><a href="#Quiz-Training-Logistic-Regression-via-Stochastic-Gradient-Ascent12-questions" class="headerlink" title="Quiz: Training Logistic Regression via Stochastic Gradient Ascent12 questions"></a>Quiz: Training Logistic Regression via Stochastic Gradient Ascent12 questions</h4><div class="note primary"><p>QUIZ<br>Training Logistic Regression via Stochastic Gradient Ascent<br>12 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 8, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.In Module 3 assignment, there were 194 features (an intercept + one feature for each of the 193 important words). In this assignment, we will use stochastic gradient ascent to train the classifier using logistic regression. How does the changing the solver to stochastic gradient ascent affect the number of features?</p>
<p><input type="radio" disabled><label></label><br>Increases</p>
<p><input type="radio" disabled><label></label><br>Decreases</p>
<p><input type="radio" disabled checked><label></label><br>Stays the same<br><br><br>1 point<br>2.Recall from the lecture and the earlier assignment, the log likelihood (without the averaging term) is given by</p>
<p>$$ℓℓ(w)=∑i=1N((1[yi=+1]−1)wTh(xi)−ln(1+exp(−wTh(xi))))$$<br>whereas the average log likelihood is given by</p>
<p>$$ℓℓA(w)=1/N∑i=1N((1[yi=+1]−1)wTh(xi)−ln(1+exp(−wTh(xi))))$$<br>How are the functions ℓℓ(w) and ℓℓA(w) related?</p>
<p><input type="radio" disabled><label></label><br>ℓℓA(w)=ℓℓ(w)</p>
<p><input type="radio" disabled checked><label></label><br>ℓℓA(w)=(1/N)⋅ℓℓ(w)</p>
<p><input type="radio" disabled><label></label><br>ℓℓA(w)=N⋅ℓℓ(w)</p>
<p><input type="radio" disabled><label></label><br>ℓℓA(w)=ℓℓ(w)−∥w∥<br><br><br>1 point<br>3.Refer to the sub-section Computing the gradient for a single data point.</p>
<p>The code block above computed</p>
<p>∂ℓi(w)∂wj<br>for j = 1 and i = 10. Is this quantity a scalar or a 194-dimensional vector?</p>
<p><input type="radio" disabled checked><label></label><br>A scalar</p>
<p><input type="radio" disabled><label></label><br>A 194-dimensional vector<br><br><br>1 point<br>4.Refer to the sub-section Modifying the derivative for using a batch of data points.</p>
<p>The code block computed</p>
<p>∑s=ii+B∂ℓs(w)∂wj<br>for j = 10, i = 10, and B = 10. Is this a scalar or a 194-dimensional vector?</p>
<p><input type="radio" disabled checked><label></label><br>A scalar</p>
<p><input type="radio" disabled><label></label><br>A 194-dimensional vector<br><br><br>1 point<br>5.For what value of B is the term</p>
<p>∑s=1B∂ℓs(w)∂wj<br>the same as the full gradient</p>
<p>∂ℓ(w)∂wj<br>? A numeric answer is expected for this question. Hint: consider the training set we are using now.</p>
<p><input type="”text”" placeholder="47780"><br><br><br>1 point<br>6.For what value of batch size B above is the stochastic gradient ascent function logistic_regression_SG act as a standard gradient ascent algorithm? A numeric answer is expected for this question. Hint: consider the training set we are using now.</p>
<p><input type="”text”" placeholder="47780"><br><br><br>1 point<br>7.When you set batch_size = 1, as each iteration passes, how does the average log likelihood in the batch change?</p>
<p><input type="radio" disabled><label></label><br>Increases</p>
<p><input type="radio" disabled><label></label><br>Decreases</p>
<p><input type="radio" disabled checked><label></label><br>Fluctuates<br><br><br>1 point<br>8.When you set batch_size = len(feature_matrix_train), as each iteration passes, how does the average log likelihood in the batch change?</p>
<p><input type="radio" disabled checked><label></label><br>Increases</p>
<p><input type="radio" disabled><label></label><br>Decreases</p>
<p><input type="radio" disabled><label></label><br>Fluctuates<br><br><br>1 point<br>9.Suppose that we run stochastic gradient ascent with a batch size of 100. How many gradient updates are performed at the end of two passes over a dataset consisting of 50000 data points?</p>
<p><input type="”text”" placeholder="1000"><br><br><br>1 point<br>10.Refer to the section Stochastic gradient ascent vs gradient ascent.</p>
<p>In the first figure, how many passes does batch gradient ascent need to achieve a similar log likelihood as stochastic gradient ascent?</p>
<p><input type="radio" disabled><label></label><br>It’s always better</p>
<p><input type="radio" disabled><label></label><br>10 passes</p>
<p><input type="radio" disabled><label></label><br>20 passes</p>
<p><input type="radio" disabled checked><label></label><br>150 passes or more<br><br><br>1 point<br>11.Questions 11 and 12 refer to the section Plotting the log likelihood as a function of passes for each step size.</p>
<p>Which of the following is the worst step size? Pick the step size that results in the lowest log likelihood in the end.</p>
<p><input type="radio" disabled><label></label><br>1e-2</p>
<p><input type="radio" disabled><label></label><br>1e-1</p>
<p><input type="radio" disabled><label></label><br>1e0</p>
<p><input type="radio" disabled><label></label><br>1e1</p>
<p><input type="radio" disabled checked><label></label><br>1e2<br><br><br>1 point<br>12.Questions 11 and 12 refer to the section Plotting the log likelihood as a function of passes for each step size.</p>
<p>Which of the following is the best step size? Pick the step size that results in the highest log likelihood in the end.</p>
<p><input type="radio" disabled><label></label><br>1e-4</p>
<p><input type="radio" disabled><label></label><br>1e-2</p>
<p><input type="radio" disabled checked><label></label><br>1e0</p>
<p><input type="radio" disabled><label></label><br>1e1</p>
<p><input type="radio" disabled><label></label><br>1e2<br><br></p>
<h1 id="Machine-Learning-Clustering-amp-Retrieval"><a href="#Machine-Learning-Clustering-amp-Retrieval" class="headerlink" title="Machine Learning: Clustering &amp; Retrieval"></a>Machine Learning: Clustering &amp; Retrieval</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/ml-clustering-and-retrieval" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/tree/master/Lecture%20Slides" target="_blank" rel="external">here</a></p>
<p><strong>About this course:</strong> Case Studies: Finding Similar Documents</p>
<p>A reader is interested in a specific news article and you want to find similar articles to recommend.  What is the right notion of similarity?  Moreover, what if there are millions of other documents?  Each time you want to a retrieve a new document, do you need to search through all other documents?  How do you group similar documents together?  How do you discover new, emerging topics that the documents cover?   </p>
<p>In this third case study, finding similar documents, you will examine similarity-based algorithms for retrieval.  In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA).  You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce.</p>
<p>Learning Outcomes:  By the end of this course, you will be able to:<br>   -Create a document retrieval system using k-nearest neighbors.<br>   -Identify various similarity metrics for text data.<br>   -Reduce computations in k-nearest neighbor search by using KD-trees.<br>   -Produce approximate nearest neighbors using locality sensitive hashing.<br>   -Compare and contrast supervised and unsupervised learning tasks.<br>   -Cluster documents by topic using k-means.<br>   -Describe how to parallelize k-means using MapReduce.<br>   -Examine probabilistic clustering approaches using mixtures models.<br>   -Fit a mixture of Gaussian model using expectation maximization (EM).<br>   -Perform mixed membership modeling using latent Dirichlet allocation (LDA).<br>   -Describe the steps of a Gibbs sampler and how to use its output to draw inferences.<br>   -Compare and contrast initialization techniques for non-convex optimization objectives.<br>   -Implement these techniques in Python.</p>
</div>
<h2 id="Week-1-Welcome-1"><a href="#Week-1-Welcome-1" class="headerlink" title="Week 1 Welcome"></a>Week 1 Welcome</h2><div class="note primary"><p>Clustering and retrieval are some of the most high-impact machine learning tools out there. Retrieval is used in almost every applications and device we interact with, like in providing a set of products related to one a shopper is currently considering, or a list of people you might want to connect with on a social media platform. Clustering can be used to aid retrieval, but is a more broadly useful tool for automatically discovering structure in data, like uncovering groups of similar patients.</p>
<p>This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have.</p>
</div>
<h3 id="What-is-this-course-about-1"><a href="#What-is-this-course-about-1" class="headerlink" title="What is this course about?"></a>What is this course about?</h3><h4 id="Slides-presented-in-this-module10-min-20"><a href="#Slides-presented-in-this-module10-min-20" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="">intro.pdf</a></p>
<h4 id="Welcome-and-introduction-to-clustering-and-retrieval-tasks6-min"><a href="#Welcome-and-introduction-to-clustering-and-retrieval-tasks6-min" class="headerlink" title="Welcome and introduction to clustering and retrieval tasks6 min"></a>Welcome and introduction to clustering and retrieval tasks6 min</h4><h4 id="Course-overview3-min"><a href="#Course-overview3-min" class="headerlink" title="Course overview3 min"></a>Course overview3 min</h4><h4 id="Module-by-module-topics-covered8-min"><a href="#Module-by-module-topics-covered8-min" class="headerlink" title="Module-by-module topics covered8 min"></a>Module-by-module topics covered8 min</h4><h4 id="Assumed-background6-min"><a href="#Assumed-background6-min" class="headerlink" title="Assumed background6 min"></a>Assumed background6 min</h4><h4 id="Software-tools-you’ll-need-for-this-course10-min"><a href="#Software-tools-you’ll-need-for-this-course10-min" class="headerlink" title="Software tools you’ll need for this course10 min"></a>Software tools you’ll need for this course10 min</h4><p>Github repository with starter code</p>
<p>In each module of the course, we have a reading with the assignments for that module as well as some starter code. For those interested, the starter code and demos used in this course are also available in a public Github repository:</p>
<p><a href="https://github.com/learnml/machine-learning-specialization" target="_blank" rel="external">https://github.com/learnml/machine-learning-specialization</a></p>
<h4 id="A-big-week-ahead-10-min"><a href="#A-big-week-ahead-10-min" class="headerlink" title="A big week ahead!10 min"></a>A big week ahead!10 min</h4><h2 id="Week-2-Nearest-Neighbor-Search"><a href="#Week-2-Nearest-Neighbor-Search" class="headerlink" title="Week 2 Nearest Neighbor Search"></a>Week 2 Nearest Neighbor Search</h2><div class="note primary"><p>We start the course by considering a retrieval task of fetching a document similar to one someone is currently reading. We cast this problem as one of nearest neighbor search, which is a concept we have seen in the Foundations and Regression courses. However, here, you will take a deep dive into two critical components of the algorithms: the data representation and metric for measuring similarity between pairs of datapoints. You will examine the computational burden of the naive nearest neighbor search algorithm, and instead implement scalable alternatives using KD-trees for handling large datasets and locality sensitive hashing (LSH) for providing approximate nearest neighbors, even in high-dimensional spaces. You will explore all of these ideas on a Wikipedia dataset, comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced.</p>
</div>
<h3 id="Introduction-to-nearest-neighbor-search-and-algorithms"><a href="#Introduction-to-nearest-neighbor-search-and-algorithms" class="headerlink" title="Introduction to nearest neighbor search and algorithms"></a>Introduction to nearest neighbor search and algorithms</h3><h4 id="Slides-presented-in-this-module10-min-21"><a href="#Slides-presented-in-this-module10-min-21" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="">retrieval-intro-annotated.pdf</a></p>
<h4 id="Retrieval-as-k-nearest-neighbor-search2-min"><a href="#Retrieval-as-k-nearest-neighbor-search2-min" class="headerlink" title="Retrieval as k-nearest neighbor search2 min"></a>Retrieval as k-nearest neighbor search2 min</h4><h4 id="1-NN-algorithm2-min"><a href="#1-NN-algorithm2-min" class="headerlink" title="1-NN algorithm2 min"></a>1-NN algorithm2 min</h4><h4 id="k-NN-algorithm6-min"><a href="#k-NN-algorithm6-min" class="headerlink" title="k-NN algorithm6 min"></a>k-NN algorithm6 min</h4><h3 id="The-importance-of-data-representations-and-distance-metrics"><a href="#The-importance-of-data-representations-and-distance-metrics" class="headerlink" title="The importance of data representations and distance metrics"></a>The importance of data representations and distance metrics</h3><h4 id="Document-representation5-min"><a href="#Document-representation5-min" class="headerlink" title="Document representation5 min"></a>Document representation5 min</h4><h4 id="Distance-metrics-Euclidean-and-scaled-Euclidean6-min"><a href="#Distance-metrics-Euclidean-and-scaled-Euclidean6-min" class="headerlink" title="Distance metrics: Euclidean and scaled Euclidean6 min"></a>Distance metrics: Euclidean and scaled Euclidean6 min</h4><h4 id="Writing-scaled-Euclidean-distance-using-weighted-inner-products4-min"><a href="#Writing-scaled-Euclidean-distance-using-weighted-inner-products4-min" class="headerlink" title="Writing (scaled) Euclidean distance using (weighted) inner products4 min"></a>Writing (scaled) Euclidean distance using (weighted) inner products4 min</h4><h4 id="Distance-metrics-Cosine-similarity9-min"><a href="#Distance-metrics-Cosine-similarity9-min" class="headerlink" title="Distance metrics: Cosine similarity9 min"></a>Distance metrics: Cosine similarity9 min</h4><h4 id="To-normalize-or-not-and-other-distance-considerations6-min"><a href="#To-normalize-or-not-and-other-distance-considerations6-min" class="headerlink" title="To normalize or not and other distance considerations6 min"></a>To normalize or not and other distance considerations6 min</h4><h4 id="Quiz-Representations-and-metrics6-questions"><a href="#Quiz-Representations-and-metrics6-questions" class="headerlink" title="Quiz: Representations and metrics6 questions"></a>Quiz: Representations and metrics6 questions</h4><div class="note primary"><p>QUIZ<br>Representations and metrics<br>6 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 22, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Consider three data points with two features as follows:</p>
<p>Among the three points, which two are closest to each other in terms of having the ​smallest Euclidean distance?</p>
<p><input type="radio" disabled><label></label><br>A and B</p>
<p><input type="radio" disabled><label></label><br>A and C</p>
<p><input type="radio" disabled checked><label></label><br>B and C<br><br><br>1 point<br>2.Consider three data points with two features as follows:</p>
<p>Among the three points, which two are closest to each other in terms of having the ​largest cosine similarity (or equivalently, ​smallest cosine distance)?</p>
<p><input type="radio" disabled checked><label></label><br>A and B</p>
<p><input type="radio" disabled><label></label><br>A and C</p>
<p><input type="radio" disabled><label></label><br>B and C<br><br><br>1 point<br>3.Consider the following two sentences.</p>
<p>Sentence 1: The quick brown fox jumps over the lazy dog.<br>Sentence 2: A quick brown dog outpaces a quick fox.<br>Compute the Euclidean distance using word counts. To compute word counts, turn all words into lower case and strip all punctuation, so that “The” and “the” are counted as the same token. That is, document 1 would be represented as</p>
<p>x=[# the,# a,# quick,# brown,# fox,# jumps,# over,# lazy,# dog,# outpaces]<br>where # word is the count of that word in the document.</p>
<p>Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="3.606"><br><br><br><div class="note primary"><p>sum = 13</p>
</div><br>1 point<br>4.Consider the following two sentences.</p>
<p>Sentence 1: The quick brown fox jumps over the lazy dog.<br>Sentence 2: A quick brown dog outpaces a quick fox.<br>Recall that</p>
<p>cosine distance = 1 - cosine similarity = 1−xTy||x||||y||<br>Compute the cosine distance between sentence 1 and sentence 2 using word counts. To compute word counts, turn all words into lower case and strip all punctuation, so that “The” and “the” are counted as the same token. That is, document 1 would be represented as</p>
<p>x=[# the,# a,# quick,# brown,# fox,# jumps,# over,# lazy,# dog,# outpaces]<br>where # word is the count of that word in the document.</p>
<p>Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.565"><br><br><br>1 point<br>5.(True/False) For positive features, cosine similarity is always between 0 and 1.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>6.Which of the following does not describe the word count document representation? (Note: this is different from TF-IDF document representation.)</p>
<p><input type="radio" disabled><label></label><br>Ignores the order of the words</p>
<p><input type="radio" disabled><label></label><br>Assigns a high score to a frequently occurring word</p>
<p><input type="radio" disabled checked><label></label><br>Penalizes words that appear in every document<br><br></p>
<h3 id="Programming-Assignment-1-4"><a href="#Programming-Assignment-1-4" class="headerlink" title="Programming Assignment 1"></a>Programming Assignment 1</h3><h4 id="Choosing-features-and-metrics-for-nearest-neighbor-search10-min"><a href="#Choosing-features-and-metrics-for-nearest-neighbor-search10-min" class="headerlink" title="Choosing features and metrics for nearest neighbor search10 min"></a>Choosing features and metrics for nearest neighbor search10 min</h4><h4 id="Quiz-Choosing-features-and-metrics-for-nearest-neighbor-search5-questions"><a href="#Quiz-Choosing-features-and-metrics-for-nearest-neighbor-search5-questions" class="headerlink" title="Quiz: Choosing features and metrics for nearest neighbor search5 questions"></a>Quiz: Choosing features and metrics for nearest neighbor search5 questions</h4><div class="note primary"><p>QUIZ<br>Choosing features and metrics for nearest neighbor search<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 22, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Among the words that appear in both Barack Obama and Francisco Barrio, take the 5 that appear most frequently in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words?</p>
<p><input type="”text”" placeholder="56066"><br><br><br>1 point<br>2.Measure the pairwise distance between the Wikipedia pages of Barack Obama, George W. Bush, and Joe Biden. Which of the three pairs has the smallest distance?</p>
<p><input type="radio" disabled><label></label><br>Between Obama and Biden</p>
<p><input type="radio" disabled><label></label><br>Between Obama and Bush</p>
<p><input type="radio" disabled checked><label></label><br>Between Biden and Bush<br><br><br>1 point<br>3.Collect all words that appear both in Barack Obama and George W. Bush pages. Out of those words, find the 10 words that show up most often in Obama’s page. Which of the following is NOT one of the 10 words?</p>
<p><input type="radio" disabled><label></label><br>the</p>
<p><input type="radio" disabled checked><label></label><br>presidential</p>
<p><input type="radio" disabled><label></label><br>in</p>
<p><input type="radio" disabled><label></label><br>act</p>
<p><input type="radio" disabled><label></label><br>his<br><br><br>1 point<br>4.Among the words that appear in both Barack Obama and Phil Schiliro, take the 5 that have largest weights in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words?</p>
<p><input type="”text”" placeholder="14"><br><br><br>1<br>point</p>
<ol>
<li>Compute the Euclidean distance between TF-IDF features of Obama and Biden. Round your answer to 3 decimal places. Use American-style decimals (e.g. 110.921).<br><input type="”text”" placeholder="123.297"><br><br></li>
</ol>
<h3 id="Scaling-up-k-NN-search-using-KD-trees"><a href="#Scaling-up-k-NN-search-using-KD-trees" class="headerlink" title="Scaling up k-NN search using KD-trees"></a>Scaling up k-NN search using KD-trees</h3><h4 id="Complexity-of-brute-force-search1-min"><a href="#Complexity-of-brute-force-search1-min" class="headerlink" title="Complexity of brute force search1 min"></a>Complexity of brute force search1 min</h4><h4 id="KD-tree-representation9-min"><a href="#KD-tree-representation9-min" class="headerlink" title="KD-tree representation9 min"></a>KD-tree representation9 min</h4><h4 id="NN-search-with-KD-trees7-min"><a href="#NN-search-with-KD-trees7-min" class="headerlink" title="NN search with KD-trees7 min"></a>NN search with KD-trees7 min</h4><h4 id="Complexity-of-NN-search-with-KD-trees5-min"><a href="#Complexity-of-NN-search-with-KD-trees5-min" class="headerlink" title="Complexity of NN search with KD-trees5 min"></a>Complexity of NN search with KD-trees5 min</h4><h4 id="Visualizing-scaling-behavior-of-KD-trees4-min"><a href="#Visualizing-scaling-behavior-of-KD-trees4-min" class="headerlink" title="Visualizing scaling behavior of KD-trees4 min"></a>Visualizing scaling behavior of KD-trees4 min</h4><h4 id="Approximate-k-NN-search-using-KD-trees7-min"><a href="#Approximate-k-NN-search-using-KD-trees7-min" class="headerlink" title="Approximate k-NN search using KD-trees7 min"></a>Approximate k-NN search using KD-trees7 min</h4><h4 id="OPTIONAL-A-worked-out-example-for-KD-trees10-min"><a href="#OPTIONAL-A-worked-out-example-for-KD-trees10-min" class="headerlink" title="(OPTIONAL) A worked-out example for KD-trees10 min"></a>(OPTIONAL) A worked-out example for KD-trees10 min</h4><h4 id="Quiz-KD-trees5-questions"><a href="#Quiz-KD-trees5-questions" class="headerlink" title="Quiz: KD-trees5 questions"></a>Quiz: KD-trees5 questions</h4><div class="note primary"><p>QUIZ<br>KD-trees<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 22, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Which of the following is not true about KD-trees?</p>
<p><input type="radio" disabled><label></label><br>It divides the feature space into nested axis-aligned boxes.</p>
<p><input type="radio" disabled checked><label></label><br>It can be used only for approximate nearest neighbor search but not for exact nearest neighbor search.</p>
<p><input type="radio" disabled><label></label><br>It prunes parts of the feature space away from consideration by inspecting smallest possible distances that can be achieved.</p>
<p><input type="radio" disabled><label></label><br>The query time scales sublinearly with the number of data points and exponentially with the number of dimensions.</p>
<p><input type="radio" disabled><label></label><br>It works best in low to medium dimension settings.<br><br><br>1 point<br>2.Questions 2, 3, 4, and 5 involves training a KD-tree on the following dataset:</p>
<table>
<thead>
<tr>
<th>–</th>
<th>X1</th>
<th>X2</th>
<th>–</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data point 1</td>
<td>-1.58</td>
<td>-2.01</td>
</tr>
<tr>
<td>Data point 2</td>
<td>0.91</td>
<td>3.98</td>
</tr>
<tr>
<td>Data point 3</td>
<td>-0.73</td>
<td>4.00</td>
</tr>
<tr>
<td>Data point 4</td>
<td>-4.22</td>
<td>1.16</td>
</tr>
<tr>
<td>Data point 5</td>
<td>4.19</td>
<td>-2.02</td>
</tr>
<tr>
<td>Data point 6</td>
<td>-0.33</td>
<td>2.15</td>
</tr>
</tbody>
</table>
<p>Train a KD-tree by hand as follows:</p>
<ul>
<li>First split using X1 and then using X2. Alternate between X1 and X2 in order.</li>
<li>Use “middle-of-the-range” heuristic for each split. Take the maximum and minimum of the coordinates of the member points.</li>
<li>Keep subdividing until every leaf node contains two or fewer data points.</li>
</ul>
<p>What is the split value used for the first split? Enter the exact value, as you are expected to obtain a finite number of decimals. Use American-style decimals (e.g. 0.026).</p>
<p><input type="”text”" placeholder="-0.015"><br><br><br>1 point<br>3.Refer to Question 2 for context.</p>
<p>What is the split value used for the second split? Enter the exact value, as you are expected to obtain a finite number of decimals. Use American-style decimals (e.g. 0.026).</p>
<p><input type="”text”" placeholder="0.995"><br><br><br>1 point<br>4.Refer to Question 2 for context.</p>
<p>Given a query point (-3, 1.5), which of the data points belong to the same leaf node as the query point? Choose all that apply.</p>
<p><input type="checkbox" disabled><label></label><br>Data point 1</p>
<p><input type="checkbox" disabled><label></label><br>Data point 2</p>
<p><input type="checkbox" disabled><label></label><br>Data point 3</p>
<p><input type="checkbox" disabled checked><label></label><br>Data point 4</p>
<p><input type="checkbox" disabled><label></label><br>Data point 5</p>
<p><input type="checkbox" disabled><label></label><br>Data point 6<br><br><br>1 point<br>5.Refer to Question 2 for context.</p>
<p>Perform backtracking with the query point (-3, 1.5) to perform exact nearest neighbor search. Which of the data points would be pruned from the search? Choose all that apply.</p>
<p>Hint: Assume that each node in the KD-tree remembers the tight bound on the coordinates of its member points, as follows:<br><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R7CFGk4REeaubA6-qtnryw_1357fe84a2ecd33f4d95bb69f6ebbf89_Capture.PNG?expiry=1508889600000&amp;hmac=O37bVp0hZLEyv9F_xIvf0IXq-qAXPr8qXCDRTWXOC5A" alt=""></p>
<p><input type="checkbox" disabled checked><label></label><br>Data point 1</p>
<p><input type="checkbox" disabled checked><label></label><br>Data point 2</p>
<p><input type="checkbox" disabled checked><label></label><br>Data point 3</p>
<p><input type="checkbox" disabled><label></label><br>Data point 4</p>
<p><input type="checkbox" disabled checked><label></label><br>Data point 5</p>
<p><input type="checkbox" disabled checked><label></label><br>Data point 6<br><br></p>
<h3 id="Locality-sensitive-hashing-for-approximate-NN-search"><a href="#Locality-sensitive-hashing-for-approximate-NN-search" class="headerlink" title="Locality sensitive hashing for approximate NN search"></a>Locality sensitive hashing for approximate NN search</h3><h4 id="Limitations-of-KD-trees3-min"><a href="#Limitations-of-KD-trees3-min" class="headerlink" title="Limitations of KD-trees3 min"></a>Limitations of KD-trees3 min</h4><h4 id="LSH-as-an-alternative-to-KD-trees4-min"><a href="#LSH-as-an-alternative-to-KD-trees4-min" class="headerlink" title="LSH as an alternative to KD-trees4 min"></a>LSH as an alternative to KD-trees4 min</h4><h4 id="Using-random-lines-to-partition-points5-min"><a href="#Using-random-lines-to-partition-points5-min" class="headerlink" title="Using random lines to partition points5 min"></a>Using random lines to partition points5 min</h4><h4 id="Defining-more-bins3-min"><a href="#Defining-more-bins3-min" class="headerlink" title="Defining more bins3 min"></a>Defining more bins3 min</h4><h4 id="Searching-neighboring-bins8-min"><a href="#Searching-neighboring-bins8-min" class="headerlink" title="Searching neighboring bins8 min"></a>Searching neighboring bins8 min</h4><h4 id="LSH-in-higher-dimensions4-min"><a href="#LSH-in-higher-dimensions4-min" class="headerlink" title="LSH in higher dimensions4 min"></a>LSH in higher dimensions4 min</h4><h4 id="OPTIONAL-Improving-efficiency-through-multiple-tables22-min"><a href="#OPTIONAL-Improving-efficiency-through-multiple-tables22-min" class="headerlink" title="(OPTIONAL) Improving efficiency through multiple tables22 min"></a>(OPTIONAL) Improving efficiency through multiple tables22 min</h4><h4 id="Quiz-Locality-Sensitive-Hashing5-questions"><a href="#Quiz-Locality-Sensitive-Hashing5-questions" class="headerlink" title="Quiz: Locality Sensitive Hashing5 questions"></a>Quiz: Locality Sensitive Hashing5 questions</h4><div class="note primary"><p>QUIZ<br>Locality Sensitive Hashing<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 22, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.(True/False) Like KD-trees, Locality Sensitive Hashing lets us compute exact nearest neighbors while inspecting only a fraction of the data points in the training set.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>2.(True/False) Given two data points with high cosine similarity, the probability that a randomly drawn line would separate the two points is small.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.(True/False) The true nearest neighbor of the query is guaranteed to fall into the same bin as the query.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>4.(True/False) Locality Sensitive Hashing is more efficient than KD-trees in high dimensional setting.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>5.Suppose you trained an LSH model and performed a lookup using the bin index of the query. You notice that the list of candidates returned are not at all similar to the query item. Which of the following changes would not produce a more relevant list of candidates?</p>
<p><input type="radio" disabled><label></label><br>Use multiple tables.</p>
<p><input type="radio" disabled checked><label></label><br>Increase the number of random lines/hyperplanes.</p>
<p><input type="radio" disabled><label></label><br>Inspect more neighboring bins to the bin containing the query.</p>
<p><input type="radio" disabled><label></label><br>Decrease the number of random lines/hyperplanes.<br><br></p>
<h3 id="Programming-Assignment-2-4"><a href="#Programming-Assignment-2-4" class="headerlink" title="Programming Assignment 2"></a>Programming Assignment 2</h3><h4 id="Implementing-Locality-Sensitive-Hashing-from-scratch10-min"><a href="#Implementing-Locality-Sensitive-Hashing-from-scratch10-min" class="headerlink" title="Implementing Locality Sensitive Hashing from scratch10 min"></a>Implementing Locality Sensitive Hashing from scratch10 min</h4><h4 id="Quiz-Implementing-Locality-Sensitive-Hashing-from-scratch5-questions"><a href="#Quiz-Implementing-Locality-Sensitive-Hashing-from-scratch5-questions" class="headerlink" title="Quiz: Implementing Locality Sensitive Hashing from scratch5 questions"></a>Quiz: Implementing Locality Sensitive Hashing from scratch5 questions</h4><div class="note primary"><p>QUIZ<br>Implementing Locality Sensitive Hashing from scratch<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 22, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.What is the document ID of Barack Obama’s article?</p>
<p><input type="”text”" placeholder="35817"><br><br><br>1 point<br>2.Which bin contains Barack Obama’s article? Enter its integer index.</p>
<p><input type="”text”" placeholder="50194"><br><br><br>1 point<br>3.Examine the bit representations of the bins containing Barack Obama and Joe Biden. In how many places do they agree?</p>
<p><input type="radio" disabled><label></label><br>16 out of 16 places (Barack Obama and Joe Biden fall into the same bin)</p>
<p><input type="radio" disabled checked><label></label><br>14 out of 16 places</p>
<p><input type="radio" disabled><label></label><br>12 out of 16 places</p>
<p><input type="radio" disabled><label></label><br>10 out of 16 places</p>
<p><input type="radio" disabled><label></label><br>8 out of 16 places<br><br><br>1 point<br>4.Refer to the section “Effect of nearby bin search”. What was the smallest search radius that yielded the correct nearest neighbor for Obama, namely Joe Biden?</p>
<p><input type="”text”" placeholder="2"><br><br><br>1 point<br>5.Suppose our goal was to produce 10 approximate nearest neighbors whose average distance from the query document is within 0.01 of the average for the true 10 nearest neighbors. For Barack Obama, the true 10 nearest neighbors are on average about 0.77. What was the smallest search radius for Barack Obama that produced an average distance of 0.78 or better?</p>
<p><input type="”text”" placeholder="7"><br><br></p>
<h3 id="Summarizing-nearest-neighbor-search"><a href="#Summarizing-nearest-neighbor-search" class="headerlink" title="Summarizing nearest neighbor search"></a>Summarizing nearest neighbor search</h3><h4 id="A-brief-recap2-min"><a href="#A-brief-recap2-min" class="headerlink" title="A brief recap2 min"></a>A brief recap2 min</h4><h2 id="Week-3-Clustering-with-k-means"><a href="#Week-3-Clustering-with-k-means" class="headerlink" title="Week 3 Clustering with k-means"></a>Week 3 Clustering with k-means</h2><div class="note primary"><p>In clustering, our goal is to group the datapoints in our dataset into disjoint sets. Motivated by our document analysis case study, you will use clustering to discover thematic groups of articles by “topic”. These topics are not provided in this unsupervised learning task; rather, the idea is to output such cluster labels that can be post-facto associated with known topics like “Science”, “World News”, etc. Even without such post-facto labels, you will examine how the clustering output can provide insights into the relationships between datapoints in the dataset. The first clustering algorithm you will implement is k-means, which is the most widely used clustering algorithm out there. To scale up k-means, you will learn about the general MapReduce framework for parallelizing and distributing computations, and then how the iterates of k-means can utilize this framework. You will show that k-means can provide an interpretable grouping of Wikipedia articles when appropriately tuned.</p>
</div>
<h3 id="Introduction-to-clustering"><a href="#Introduction-to-clustering" class="headerlink" title="Introduction to clustering"></a>Introduction to clustering</h3><h4 id="Slides-presented-in-this-module10-min-22"><a href="#Slides-presented-in-this-module10-min-22" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><h4 id="The-goal-of-clustering3-min"><a href="#The-goal-of-clustering3-min" class="headerlink" title="The goal of clustering3 min"></a>The goal of clustering3 min</h4><h4 id="An-unsupervised-task6-min"><a href="#An-unsupervised-task6-min" class="headerlink" title="An unsupervised task6 min"></a>An unsupervised task6 min</h4><h4 id="Hope-for-unsupervised-learning-and-some-challenge-cases4-min"><a href="#Hope-for-unsupervised-learning-and-some-challenge-cases4-min" class="headerlink" title="Hope for unsupervised learning, and some challenge cases4 min"></a>Hope for unsupervised learning, and some challenge cases4 min</h4><h3 id="Clustering-via-k-means"><a href="#Clustering-via-k-means" class="headerlink" title="Clustering via k-means"></a>Clustering via k-means</h3><h4 id="The-k-means-algorithm7-min"><a href="#The-k-means-algorithm7-min" class="headerlink" title="The k-means algorithm7 min"></a>The k-means algorithm7 min</h4><h4 id="k-means-as-coordinate-descent6-min"><a href="#k-means-as-coordinate-descent6-min" class="headerlink" title="k-means as coordinate descent6 min"></a>k-means as coordinate descent6 min</h4><h4 id="Smart-initialization-via-k-means-4-min"><a href="#Smart-initialization-via-k-means-4-min" class="headerlink" title="Smart initialization via k-means++4 min"></a>Smart initialization via k-means++4 min</h4><h4 id="Assessing-the-quality-and-choosing-the-number-of-clusters9-min"><a href="#Assessing-the-quality-and-choosing-the-number-of-clusters9-min" class="headerlink" title="Assessing the quality and choosing the number of clusters9 min"></a>Assessing the quality and choosing the number of clusters9 min</h4><h4 id="Quiz-k-means9-questions"><a href="#Quiz-k-means9-questions" class="headerlink" title="Quiz: k-means9 questions"></a>Quiz: k-means9 questions</h4><div class="note primary"><p>QUIZ<br>k-means<br>9 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 29, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.(True/False) k-means always converges to a local optimum.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>2.(True/False) The clustering objective is non-increasing throughout a run of k-means.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.(True/False) Running k-means with a larger value of k always enables a lower possible final objective value than running k-means with smaller k.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>4.(True/False) Any initialization of the centroids in k-means is just as good as any other.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>5.(True/False) Initializing centroids using k-means++ guarantees convergence to a global optimum.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>6.(True/False) Initializing centroids using k-means++ costs more than random initialization in the beginning, but can pay off eventually by speeding up convergence.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>7.(True/False) Using k-means++ can only influence the number of iterations to convergence, not the quality of the final assignments (i.e., objective value at convergence).</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>4 points<br>8.Consider the following dataset:</p>
<table>
<thead>
<tr>
<th>–</th>
<th>X1</th>
<th>X2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data point 1</td>
<td>-1.88</td>
<td>2.05</td>
</tr>
<tr>
<td>Data point 2</td>
<td>-0.71</td>
<td>0.42</td>
</tr>
<tr>
<td>Data point 3</td>
<td>2.41</td>
<td>-0.67</td>
</tr>
<tr>
<td>Data point 4</td>
<td>1.85</td>
<td>-3.80</td>
</tr>
<tr>
<td>Data point 5</td>
<td>-3.69</td>
<td>-1.33</td>
</tr>
</tbody>
</table>
<p>Perform k-means with k=2 until the cluster assignment does not change between successive iterations. Use the following initialization for the centroids:</p>
<table>
<thead>
<tr>
<th>–</th>
<th>X1</th>
<th>X2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cluster 1</td>
<td>2.00</td>
<td>2.00</td>
</tr>
<tr>
<td>Cluster 2</td>
<td>-2.00</td>
<td>-2.00</td>
</tr>
</tbody>
</table>
<p>Which of the five data points changed its cluster assignment most often during the k-means run?</p>
<p><input type="radio" disabled><label></label><br>Data point 1</p>
<p><input type="radio" disabled checked><label></label><br>Data point 2</p>
<p><input type="radio" disabled><label></label><br>Data point 3</p>
<p><input type="radio" disabled><label></label><br>Data point 4</p>
<p><input type="radio" disabled><label></label><br>Data point 5<br><br><br>1 point<br>9.Suppose we initialize k-means with the following centroids</p>
<p>Which of the following best describes the cluster assignment in the first iteration of k-means?</p>
<h3 id="Programming-Assignment-7"><a href="#Programming-Assignment-7" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h3><h4 id="Clustering-text-data-with-k-means10-min"><a href="#Clustering-text-data-with-k-means10-min" class="headerlink" title="Clustering text data with k-means10 min"></a>Clustering text data with k-means10 min</h4><h4 id="Quiz-Clustering-text-data-with-K-means8-questions"><a href="#Quiz-Clustering-text-data-with-K-means8-questions" class="headerlink" title="Quiz: Clustering text data with K-means8 questions"></a>Quiz: Clustering text data with K-means8 questions</h4><div class="note primary"><p>QUIZ<br>Clustering text data with K-means<br>8 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 29, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Make sure you have the latest versions of the notebook and the file kmeans-arrays.npz Read this post if</p>
<p>… you downloaded the files before September 10<br>… you created an Amazon EC2 instance before October 1</p>
<p><input type="radio" disabled><label></label><br>I acknowledge.<br>1 point<br>2.(True/False) The clustering objective (heterogeneity) is non-increasing for this example.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.Let’s step back from this particular example. If the clustering objective (heterogeneity) would ever increase when running K-means, that would indicate: (choose one)</p>
<p><input type="radio" disabled><label></label><br>K-means algorithm got stuck in a bad local minimum</p>
<p><input type="radio" disabled checked><label></label><br>There is a bug in the K-means code</p>
<p><input type="radio" disabled><label></label><br>All data points consist of exact duplicates</p>
<p><input type="radio" disabled><label></label><br>Nothing is wrong. The objective should generally go down sooner or later.<br>1 point<br>4.Refer to the output of K-means for K=3 and seed=0. Which of the three clusters contains the greatest number of data points in the end?</p>
<p><input type="radio" disabled><label></label><br>Cluster #0</p>
<p><input type="radio" disabled><label></label><br>Cluster #1</p>
<p><input type="radio" disabled checked><label></label><br>Cluster #2<br>1 point</p>
<ol>
<li>Another way to capture the effect of changing initialization is to look at the distribution of cluster assignments. Compute the size (# of member data points) of clusters for each of the multiple runs of K-means.</li>
</ol>
<p>Look at the size of the largest cluster (most # of member data points) across multiple runs, with seeds 0, 20000, …, 120000. What is the maximum value this quantity takes?</p>
<p><input type="”text”" placeholder="18132"><br><br><br>1 point<br>6.Refer to the section “Visualize clusters of documents”. Which of the 10 clusters above contains the greatest number of articles?</p>
<p><input type="radio" disabled checked><label></label><br>Cluster 0: artists, actors, film directors, playwrights</p>
<p><input type="radio" disabled><label></label><br>Cluster 4: professors, researchers, scholars</p>
<p><input type="radio" disabled><label></label><br>Cluster 5: Australian rules football players, American football players</p>
<p><input type="radio" disabled><label></label><br>Cluster 7: composers, songwriters, singers, music producers</p>
<p><input type="radio" disabled><label></label><br>Cluster 9: politicians<br><br><br>1 point<br>7.Refer to the section “Visualize clusters of documents”. Which of the 10 clusters above contains the least number of articles?</p>
<p><input type="radio" disabled><label></label><br>Cluster 1: soccer (association football) players, rugby players</p>
<p><input type="radio" disabled><label></label><br>Cluster 3: baseball players</p>
<p><input type="radio" disabled><label></label><br>Cluster 6: female figures from various fields</p>
<p><input type="radio" disabled><label></label><br>Cluster 7: composers, songwriters, singers, music producers</p>
<p><input type="radio" disabled checked><label></label><br>Cluster 8: ice hockey players<br><b><br>1 point</b></p>
<ol>
<li>Another sign of too large K is having lots of small clusters. Look at the distribution of cluster sizes (by number of member data points). How many of the 100 clusters have fewer than 236 articles, i.e. 0.4% of the dataset?<br><input type="”text”" placeholder="29"><br><br></li>
</ol>
<h3 id="MapReduce-for-scaling-k-means"><a href="#MapReduce-for-scaling-k-means" class="headerlink" title="MapReduce for scaling k-means"></a>MapReduce for scaling k-means</h3><h4 id="Motivating-MapReduce8-min"><a href="#Motivating-MapReduce8-min" class="headerlink" title="Motivating MapReduce8 min"></a>Motivating MapReduce8 min</h4><h4 id="The-general-MapReduce-abstraction5-min"><a href="#The-general-MapReduce-abstraction5-min" class="headerlink" title="The general MapReduce abstraction5 min"></a>The general MapReduce abstraction5 min</h4><h4 id="MapReduce-execution-overview-and-combiners6-min"><a href="#MapReduce-execution-overview-and-combiners6-min" class="headerlink" title="MapReduce execution overview and combiners6 min"></a>MapReduce execution overview and combiners6 min</h4><h4 id="MapReduce-for-k-means7-min"><a href="#MapReduce-for-k-means7-min" class="headerlink" title="MapReduce for k-means7 min"></a>MapReduce for k-means7 min</h4><h4 id="Quiz-MapReduce-for-k-means5-questions"><a href="#Quiz-MapReduce-for-k-means5-questions" class="headerlink" title="Quiz: MapReduce for k-means5 questions"></a>Quiz: MapReduce for k-means5 questions</h4><div class="note primary"><p>QUIZ<br>MapReduce for k-means<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>October 29, 11:59 PM PDT</p>
</div>
<p>1 point<br>1.Suppose we are operating on a 1D vector. Which of the following operation is not data parallel over the vector elements?</p>
<p><input type="radio" disabled><label></label><br>Add a constant to every element.</p>
<p><input type="radio" disabled><label></label><br>Multiply the vector by a constant.</p>
<p><input type="radio" disabled><label></label><br>Increment the vector by another vector of the same dimension.</p>
<p><input type="radio" disabled checked><label></label><br>Compute the average of the elements.</p>
<p><input type="radio" disabled><label></label><br>Compute the sign of each element.<br><br><br>1 point<br>2.(True/False) A single mapper call can emit multiple (key,value) pairs.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.(True/False) More than one reducer can emit (key,value) pairs with the same key simultaneously.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>4.(True/False) Suppose we are running k-means using MapReduce. Some mappers may be launched for a new k-means iteration even if some reducers from the previous iteration are still running.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>5.Consider the following list of binary operations. Which can be used for the reduce step of MapReduce? Choose all that apply.</p>
<p>Hints: The reduce step requires a binary operator that satisfied both of the following conditions.</p>
<p>Commutative: OP(x1,x2)=OP(x2,x1)<br>Associative: OP(OP(x1,x2),x3)=OP(x1,OP(x2,x3))</p>
<p><input type="checkbox" disabled checked><label></label><br>OP1(x1,x2)=max(x1,x2)</p>
<p><input type="checkbox" disabled checked><label></label><br>OP2(x1,x2)=x1+x2−2</p>
<p><input type="checkbox" disabled><label></label><br>OP3(x1,x2)=3x1+2x2</p>
<p><input type="checkbox" disabled><label></label><br>OP4(x1,x2)=x21+x2</p>
<p><input type="checkbox" disabled><label></label><br>OP5(x1,x2)=(x1+x2)/2<br><br></p>
<h3 id="Summarizing-clustering-with-k-means"><a href="#Summarizing-clustering-with-k-means" class="headerlink" title="Summarizing clustering with k-means"></a>Summarizing clustering with k-means</h3><h4 id="Other-applications-of-clustering7-min"><a href="#Other-applications-of-clustering7-min" class="headerlink" title="Other applications of clustering7 min"></a>Other applications of clustering7 min</h4><h2 id="Week-4-Mixture-Models"><a href="#Week-4-Mixture-Models" class="headerlink" title="Week 4 Mixture Models"></a>Week 4 Mixture Models</h2><div class="note primary"><p>In k-means, observations are each hard-assigned to a single cluster, and these assignments are based just on the cluster centers, rather than also incorporating shape information. In our second module on clustering, you will perform probabilistic model-based clustering that provides (1) a more descriptive notion of a “cluster” and (2) accounts for uncertainty in assignments of datapoints to clusters via “soft assignments”. You will explore and implement a broadly useful algorithm called expectation maximization (EM) for inferring these soft assignments, as well as the model parameters. To gain intuition, you will first consider a visually appealing image clustering task. You will then cluster Wikipedia articles, handling the high-dimensionality of the tf-idf document representation considered.</p>
</div>
<h3 id="Motivating-and-setting-the-foundation-for-mixture-models"><a href="#Motivating-and-setting-the-foundation-for-mixture-models" class="headerlink" title="Motivating and setting the foundation for mixture models"></a>Motivating and setting the foundation for mixture models</h3><h4 id="Slides-presented-in-this-module10-min-23"><a href="#Slides-presented-in-this-module10-min-23" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="">mixmodel-EM-annotated.pdf</a></p>
<h4 id="Motiving-probabilistic-clustering-models8-min"><a href="#Motiving-probabilistic-clustering-models8-min" class="headerlink" title="Motiving probabilistic clustering models8 min"></a>Motiving probabilistic clustering models8 min</h4><h4 id="Aggregating-over-unknown-classes-in-an-image-dataset6-min"><a href="#Aggregating-over-unknown-classes-in-an-image-dataset6-min" class="headerlink" title="Aggregating over unknown classes in an image dataset6 min"></a>Aggregating over unknown classes in an image dataset6 min</h4><h4 id="Univariate-Gaussian-distributions2-min"><a href="#Univariate-Gaussian-distributions2-min" class="headerlink" title="Univariate Gaussian distributions2 min"></a>Univariate Gaussian distributions2 min</h4><h4 id="Bivariate-and-multivariate-Gaussians7-min"><a href="#Bivariate-and-multivariate-Gaussians7-min" class="headerlink" title="Bivariate and multivariate Gaussians7 min"></a>Bivariate and multivariate Gaussians7 min</h4><h3 id="Mixtures-of-Gaussians-for-clustering"><a href="#Mixtures-of-Gaussians-for-clustering" class="headerlink" title="Mixtures of Gaussians for clustering"></a>Mixtures of Gaussians for clustering</h3><h4 id="Mixture-of-Gaussians6-min"><a href="#Mixture-of-Gaussians6-min" class="headerlink" title="Mixture of Gaussians6 min"></a>Mixture of Gaussians6 min</h4><h4 id="Interpreting-the-mixture-of-Gaussian-terms5-min"><a href="#Interpreting-the-mixture-of-Gaussian-terms5-min" class="headerlink" title="Interpreting the mixture of Gaussian terms5 min"></a>Interpreting the mixture of Gaussian terms5 min</h4><h4 id="Scaling-mixtures-of-Gaussians-for-document-clustering5-min"><a href="#Scaling-mixtures-of-Gaussians-for-document-clustering5-min" class="headerlink" title="Scaling mixtures of Gaussians for document clustering5 min"></a>Scaling mixtures of Gaussians for document clustering5 min</h4><h3 id="Expectation-Maximization-EM-building-blocks"><a href="#Expectation-Maximization-EM-building-blocks" class="headerlink" title="Expectation Maximization (EM) building blocks"></a>Expectation Maximization (EM) building blocks</h3><h4 id="Computing-soft-assignments-from-known-cluster-parameters7-min"><a href="#Computing-soft-assignments-from-known-cluster-parameters7-min" class="headerlink" title="Computing soft assignments from known cluster parameters7 min"></a>Computing soft assignments from known cluster parameters7 min</h4><h4 id="OPTIONAL-Responsibilities-as-Bayes’-rule5-min"><a href="#OPTIONAL-Responsibilities-as-Bayes’-rule5-min" class="headerlink" title="(OPTIONAL) Responsibilities as Bayes’ rule5 min"></a>(OPTIONAL) Responsibilities as Bayes’ rule5 min</h4><h4 id="Estimating-cluster-parameters-from-known-cluster-assignments6-min"><a href="#Estimating-cluster-parameters-from-known-cluster-assignments6-min" class="headerlink" title="Estimating cluster parameters from known cluster assignments6 min"></a>Estimating cluster parameters from known cluster assignments6 min</h4><h4 id="Estimating-cluster-parameters-from-soft-assignments8-min"><a href="#Estimating-cluster-parameters-from-soft-assignments8-min" class="headerlink" title="Estimating cluster parameters from soft assignments8 min"></a>Estimating cluster parameters from soft assignments8 min</h4><h3 id="The-EM-algorithm"><a href="#The-EM-algorithm" class="headerlink" title="The EM algorithm"></a>The EM algorithm</h3><h4 id="EM-iterates-in-equations-and-pictures6-min"><a href="#EM-iterates-in-equations-and-pictures6-min" class="headerlink" title="EM iterates in equations and pictures6 min"></a>EM iterates in equations and pictures6 min</h4><h4 id="Convergence-initialization-and-overfitting-of-EM9-min"><a href="#Convergence-initialization-and-overfitting-of-EM9-min" class="headerlink" title="Convergence, initialization, and overfitting of EM9 min"></a>Convergence, initialization, and overfitting of EM9 min</h4><h4 id="Relationship-to-k-means3-min"><a href="#Relationship-to-k-means3-min" class="headerlink" title="Relationship to k-means3 min"></a>Relationship to k-means3 min</h4><h4 id="OPTIONAL-A-worked-out-example-for-EM10-min"><a href="#OPTIONAL-A-worked-out-example-for-EM10-min" class="headerlink" title="(OPTIONAL) A worked-out example for EM10 min"></a>(OPTIONAL) A worked-out example for EM10 min</h4><h4 id="Quiz-EM-for-Gaussian-mixtures9-questions"><a href="#Quiz-EM-for-Gaussian-mixtures9-questions" class="headerlink" title="Quiz: EM for Gaussian mixtures9 questions"></a>Quiz: EM for Gaussian mixtures9 questions</h4><div class="note primary"><p>QUIZ<br>EM for Gaussian mixtures<br>9 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 5, 11:59 PM PST</p>
</div>
<p>1 point<br>1.(True/False) While the EM algorithm maintains uncertainty about the cluster assignment for each observation via soft assignments, the model assumes that every observation comes from only one cluster.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>2.(True/False) In high dimensions, the EM algorithm runs the risk of setting cluster variances to zero.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>3.In the EM algorithm, what do the E step and M step represent, respectively?</p>
<p><input type="radio" disabled checked><label></label><br>Estimate cluster responsibilities, Maximize likelihood over parameters</p>
<p><input type="radio" disabled><label></label><br>Estimate likelihood over parameters, Maximize cluster responsibilities</p>
<p><input type="radio" disabled><label></label><br>Estimate number of parameters, Maximize likelihood over parameters</p>
<p><input type="radio" disabled><label></label><br>Estimate likelihood over parameters, Maximize number of parameters<br><br><br>1 point<br>4.Suppose we have data that come from a mixture of 6 Gaussians (i.e., that is the true data structure). Which model would we expect to have the highest log-likelihood after fitting via the EM algorithm?</p>
<p><input type="radio" disabled><label></label><br>A mixture of Gaussians with 2 component clusters</p>
<p><input type="radio" disabled><label></label><br>A mixture of Gaussians with 4 component clusters</p>
<p><input type="radio" disabled><label></label><br>A mixture of Gaussians with 6 component clusters</p>
<p><input type="radio" disabled><label></label><br>A mixture of Gaussians with 7 component clusters</p>
<p><input type="radio" disabled checked><label></label><br>A mixture of Gaussians with 10 component clusters<br>6<br><br><br>1 point<br>5.Which of the following correctly describes the differences between EM for mixtures of Gaussians and k-means? Choose all that apply.</p>
<p><input type="checkbox" disabled><label></label><br>k-means often gets stuck in a local minimum, while EM tends not to</p>
<p><input type="checkbox" disabled checked><label></label><br>EM is better at capturing clusters of different sizes and orientations</p>
<p><input type="checkbox" disabled checked><label></label><br>EM is better at capturing clusters with overlaps</p>
<p><input type="checkbox" disabled><label></label><br>EM is less prone to overfitting than k-means</p>
<p><input type="checkbox" disabled checked><label></label><br>k-means is equivalent to running EM with infinitesimally small diagonal covariances.<br><br><br>1 point<br>6.Suppose we are running the EM algorithm. After an E-step, we obtain the following responsibility matrix:</p>
<table>
<thead>
<tr>
<th>Cluster responsibilities</th>
<th>Cluster A</th>
<th>Cluster B</th>
<th>Cluster C</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data point 1</td>
<td>0.20</td>
<td>0.40</td>
<td>0.40</td>
</tr>
<tr>
<td>Data point 2</td>
<td>0.50</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>Data point 3</td>
<td>0.70</td>
<td>0.20</td>
<td>0.10</td>
</tr>
</tbody>
</table>
<p>Which is the least probable cluster for data point 1?</p>
<p><input type="radio" disabled checked><label></label><br>Cluster A</p>
<p><input type="radio" disabled><label></label><br>Cluster B</p>
<p><input type="radio" disabled><label></label><br>Cluster C<br><br><br>1 point<br>7.Suppose we are running the EM algorithm. After an E-step, we obtain the following responsibility matrix:</p>
<table>
<thead>
<tr>
<th>Cluster responsibilities</th>
<th>Cluster A</th>
<th>Cluster B</th>
<th>Cluster C</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data point 1</td>
<td>0.20</td>
<td>0.40</td>
<td>0.40</td>
</tr>
<tr>
<td>Data point 2</td>
<td>0.50</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>Data point 3</td>
<td>0.70</td>
<td>0.20</td>
<td>0.10</td>
</tr>
</tbody>
</table>
<p>Suppose also that the data points are as follows:</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>X</th>
<th>Y</th>
<th>Z</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data point 1</td>
<td>3</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Data point 2</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>Data point 3</td>
<td>1</td>
<td>3</td>
<td>7</td>
</tr>
</tbody>
</table>
<p>Let us compute the new mean for Cluster A. What is the Z coordinate of the new mean? Round your answer to 3 decimal places.<br>(2*0.2 +3*0.5+7*0.7)/(.2+.5+.7)=</p>
<p><input type="”text”" placeholder="4.857"><br><br><br>1 point<br>8.Which of the following contour plots describes a Gaussian distribution with diagonal covariance? Choose all that apply.</p>
<p><input type="radio" disabled><label>True</label><br>(1)</p>
<p><input type="radio" disabled><label>True</label><br>(2)</p>
<p><input type="radio" disabled checked><label>True</label><br>(3)</p>
<p><input type="radio" disabled checked><label>True</label><br>(4)</p>
<p><input type="radio" disabled><label>True</label><br>(5)<br><br><br>2 points<br>9.Suppose we initialize EM for mixtures of Gaussians (using full covariance matrices) with the following clusters:</p>
<p>Which of the following best describes the updated clusters after the first iteration of EM?</p>
<p><br></p>
<h3 id="Summarizing-mixture-models"><a href="#Summarizing-mixture-models" class="headerlink" title="Summarizing mixture models"></a>Summarizing mixture models</h3><h4 id="A-brief-recap1-min-5"><a href="#A-brief-recap1-min-5" class="headerlink" title="A brief recap1 min"></a>A brief recap1 min</h4><h3 id="Programming-Assignment-1-5"><a href="#Programming-Assignment-1-5" class="headerlink" title="Programming Assignment 1"></a>Programming Assignment 1</h3><h4 id="Implementing-EM-for-Gaussian-mixtures10-min"><a href="#Implementing-EM-for-Gaussian-mixtures10-min" class="headerlink" title="Implementing EM for Gaussian mixtures10 min"></a>Implementing EM for Gaussian mixtures10 min</h4><h4 id="Quiz-Implementing-EM-for-Gaussian-mixtures6-questions"><a href="#Quiz-Implementing-EM-for-Gaussian-mixtures6-questions" class="headerlink" title="Quiz: Implementing EM for Gaussian mixtures6 questions"></a>Quiz: Implementing EM for Gaussian mixtures6 questions</h4><div class="note primary"><p>QUIZ<br>Implementing EM for Gaussian mixtures<br>6 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 5, 11:59 PM PST</p>
</div>
<p>1 point<br>1.What is the weight that EM assigns to the first component after running the above codeblock? Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.301"><br><br><br>1 point<br>2.Using the same set of results, obtain the mean that EM assigns the second component. What is the mean in the first dimension? Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="4.942"><br><br><br>1 point<br>3.Using the same set of results, obtain the covariance that EM assigns the third component. What is the variance in the first dimension? Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.671"><br><br><br>1 point<br>4.Is the loglikelihood plot monotonically increasing, monotonically decreasing, or neither?</p>
<p><input type="radio" disabled checked><label></label><br>Monotonically increasing</p>
<p><input type="radio" disabled><label></label><br>Monotonically decreasing</p>
<p><input type="radio" disabled><label></label><br>Neither<br><br><br>1 point<br>5.Calculate the likelihood (score) of the first image in our data set (img[0]) under each Gaussian component through a call to <code>multivariate_normal.pdf</code>. Given these values, what cluster assignment should we make for this image?</p>
<p><input type="radio" disabled><label></label><br>Cluster 0</p>
<p><input type="radio" disabled><label></label><br>Cluster 1</p>
<p><input type="radio" disabled><label></label><br>Cluster 2</p>
<p><input type="radio" disabled checked><label></label><br>Cluster 3<br><br><br>1 point<br>6.Four of the following images are not in the list of top 5 images in the first cluster. Choose these four.</p>
<p><input type="checkbox" disabled checked><label></label><br>Image 1</p>
<p><input type="checkbox" disabled checked><label></label><br>Image 2</p>
<p><input type="checkbox" disabled><label></label><br>Image 3</p>
<p><input type="checkbox" disabled><label></label><br>Image 4</p>
<p><input type="checkbox" disabled><label></label><br>Image 5</p>
<p><input type="checkbox" disabled checked><label></label><br>Image 6</p>
<p><input type="checkbox" disabled checked><label></label><br>Image 7<br><br></p>
<h3 id="Programming-Assignment-2-5"><a href="#Programming-Assignment-2-5" class="headerlink" title="Programming Assignment 2"></a>Programming Assignment 2</h3><h4 id="Clustering-text-data-with-Gaussian-mixtures10-min"><a href="#Clustering-text-data-with-Gaussian-mixtures10-min" class="headerlink" title="Clustering text data with Gaussian mixtures10 min"></a>Clustering text data with Gaussian mixtures10 min</h4><h4 id="Quiz-Clustering-text-data-with-Gaussian-mixtures4-questions"><a href="#Quiz-Clustering-text-data-with-Gaussian-mixtures4-questions" class="headerlink" title="Quiz: Clustering text data with Gaussian mixtures4 questions"></a>Quiz: Clustering text data with Gaussian mixtures4 questions</h4><div class="note primary"><p>QUIZ<br>Clustering text data with Gaussian mixtures<br>4 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 5, 11:59 PM PST</p>
</div>
<p>1 point<br>1.Select all the topics that have a cluster in the model created above.</p>
<p><input type="checkbox" disabled checked><label></label><br>Baseball</p>
<p><input type="checkbox" disabled checked><label></label><br>Basketball</p>
<p><input type="checkbox" disabled checked><label></label><br>Soccer/football</p>
<p><input type="checkbox" disabled checked><label></label><br>Music</p>
<p><input type="checkbox" disabled><label></label><br>Politics</p>
<p><input type="checkbox" disabled checked><label></label><br>Law</p>
<p><input type="checkbox" disabled><label></label><br>Finance<br><br><br>1 point<br>2.Try fitting EM with the random initial parameters you created above. What is the final loglikelihood that the algorithm converges to? Choose the range that contains this value.</p>
<p><input type="radio" disabled><label></label><br>Less than 2.2e9</p>
<p><input type="radio" disabled><label></label><br>Between 2.2e9 and 2.3e9</p>
<p><input type="radio" disabled checked><label></label><br>Between 2.3e9 and 2.4e9</p>
<p><input type="radio" disabled><label></label><br>Between 2.4e9 and 2.5e9</p>
<p><input type="radio" disabled><label></label><br>Greater than 2.5e9<br><br><br>1 point<br>3.Is the final loglikelihood larger or smaller than the final loglikelihood we obtained above when initializing EM with the results from running k-means?</p>
<p><input type="radio" disabled checked><label></label><br>Initializing EM with k-means led to a larger final loglikelihood</p>
<p><input type="radio" disabled><label></label><br>Initializing EM with k-means led to a smaller final loglikelihood<br><br><br>1 point<br>4.For the above model, <code>out_random_init</code>, use the <code>visualize_EM_clusters</code> method you created above. Are the clusters more or less interpretable than the ones found after initializing using k-means?</p>
<p><input type="radio" disabled><label></label><br>More interpretable</p>
<p><input type="radio" disabled checked><label></label><br>Less interpretable<br><br></p>
<h2 id="Week-5-Mixed-Membership-Modeling-via-Latent-Dirichlet-Allocation"><a href="#Week-5-Mixed-Membership-Modeling-via-Latent-Dirichlet-Allocation" class="headerlink" title="Week 5 Mixed Membership Modeling via Latent Dirichlet Allocation"></a>Week 5 Mixed Membership Modeling via Latent Dirichlet Allocation</h2><div class="note primary"><p>The clustering model inherently assumes that data divide into disjoint sets, e.g., documents by topic. But, often our data objects are better described via memberships in a collection of sets, e.g., multiple topics. In our fourth module, you will explore latent Dirichlet allocation (LDA) as an example of such a mixed membership model particularly useful in document analysis. You will interpret the output of LDA, and various ways the output can be utilized, like as a set of learned document features. The mixed membership modeling ideas you learn about through LDA for document analysis carry over to many other interesting models and applications, like social network models where people have multiple affiliations.</p>
<p>Throughout this module, we introduce aspects of Bayesian modeling and a Bayesian inference algorithm called Gibbs sampling. You will be able to implement a Gibbs sampler for LDA by the end of the module.</p>
</div>
<h2 id="Introduction-to-latent-Dirichlet-allocation"><a href="#Introduction-to-latent-Dirichlet-allocation" class="headerlink" title="Introduction to latent Dirichlet allocation"></a>Introduction to latent Dirichlet allocation</h2><h3 id="Slides-presented-in-this-module10-min-24"><a href="#Slides-presented-in-this-module10-min-24" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h3><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="">LDA-annotated.pdf</a></p>
<h3 id="Mixed-membership-models-for-documents3-min"><a href="#Mixed-membership-models-for-documents3-min" class="headerlink" title="Mixed membership models for documents3 min"></a>Mixed membership models for documents3 min</h3><h3 id="An-alternative-document-clustering-model4-min"><a href="#An-alternative-document-clustering-model4-min" class="headerlink" title="An alternative document clustering model4 min"></a>An alternative document clustering model4 min</h3><h3 id="Components-of-latent-Dirichlet-allocation-model2-min"><a href="#Components-of-latent-Dirichlet-allocation-model2-min" class="headerlink" title="Components of latent Dirichlet allocation model2 min"></a>Components of latent Dirichlet allocation model2 min</h3><h3 id="Goal-of-LDA-inference5-min"><a href="#Goal-of-LDA-inference5-min" class="headerlink" title="Goal of LDA inference5 min"></a>Goal of LDA inference5 min</h3><h3 id="Quiz-Latent-Dirichlet-Allocation5-questions"><a href="#Quiz-Latent-Dirichlet-Allocation5-questions" class="headerlink" title="Quiz: Latent Dirichlet Allocation5 questions"></a>Quiz: Latent Dirichlet Allocation5 questions</h3><div class="note primary"><p>QUIZ<br>Latent Dirichlet Allocation<br>5 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 12, 11:59 PM PST</p>
</div>
<p>1 point<br>1.(True/False) According to the assumptions of LDA, each document in the corpus contains words about a single topic.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>2.(True/False) Using LDA to analyze a set of documents is an example of a supervised learning task.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>3.(True/False) When training an LDA model, changing the ordering of words in a document does not affect the overall joint probability.</p>
<p><input type="radio" disabled checked><label>True</label></p>
<p><input type="radio" disabled><label>False</label><br><br><br>1 point<br>4.(True/False) Suppose in a trained LDA model two documents have no topics in common (i.e., one document has 0 weight on any topic with non-zero weight in the other document). As a result, a single word in the vocabulary cannot have high probability of occurring in both documents.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>5.(True/False) Topic models are guaranteed to produce weights on words that are coherent and easily interpretable by humans.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br></p>
<h2 id="Bayesian-inference-via-Gibbs-sampling"><a href="#Bayesian-inference-via-Gibbs-sampling" class="headerlink" title="Bayesian inference via Gibbs sampling"></a>Bayesian inference via Gibbs sampling</h2><h3 id="The-need-for-Bayesian-inference4-min"><a href="#The-need-for-Bayesian-inference4-min" class="headerlink" title="The need for Bayesian inference4 min"></a>The need for Bayesian inference4 min</h3><h3 id="Gibbs-sampling-from-10-000-feet5-min"><a href="#Gibbs-sampling-from-10-000-feet5-min" class="headerlink" title="Gibbs sampling from 10,000 feet5 min"></a>Gibbs sampling from 10,000 feet5 min</h3><h3 id="A-standard-Gibbs-sampler-for-LDA9-min"><a href="#A-standard-Gibbs-sampler-for-LDA9-min" class="headerlink" title="A standard Gibbs sampler for LDA9 min"></a>A standard Gibbs sampler for LDA9 min</h3><h2 id="Collapsed-Gibbs-sampling-for-LDA"><a href="#Collapsed-Gibbs-sampling-for-LDA" class="headerlink" title="Collapsed Gibbs sampling for LDA"></a>Collapsed Gibbs sampling for LDA</h2><h3 id="What-is-collapsed-Gibbs-sampling-3-min"><a href="#What-is-collapsed-Gibbs-sampling-3-min" class="headerlink" title="What is collapsed Gibbs sampling?3 min"></a>What is collapsed Gibbs sampling?3 min</h3><h3 id="A-worked-example-for-LDA-Initial-setup4-min"><a href="#A-worked-example-for-LDA-Initial-setup4-min" class="headerlink" title="A worked example for LDA: Initial setup4 min"></a>A worked example for LDA: Initial setup4 min</h3><h3 id="A-worked-example-for-LDA-Deriving-the-resampling-distribution7-min"><a href="#A-worked-example-for-LDA-Deriving-the-resampling-distribution7-min" class="headerlink" title="A worked example for LDA: Deriving the resampling distribution7 min"></a>A worked example for LDA: Deriving the resampling distribution7 min</h3><h3 id="Using-the-output-of-collapsed-Gibbs-sampling4-min"><a href="#Using-the-output-of-collapsed-Gibbs-sampling4-min" class="headerlink" title="Using the output of collapsed Gibbs sampling4 min"></a>Using the output of collapsed Gibbs sampling4 min</h3><h2 id="Summarizing-latent-Dirichlet-allocation"><a href="#Summarizing-latent-Dirichlet-allocation" class="headerlink" title="Summarizing latent Dirichlet allocation"></a>Summarizing latent Dirichlet allocation</h2><h3 id="A-brief-recap1-min-6"><a href="#A-brief-recap1-min-6" class="headerlink" title="A brief recap1 min"></a>A brief recap1 min</h3><h3 id="Quiz-Learning-LDA-model-via-Gibbs-sampling10-questions"><a href="#Quiz-Learning-LDA-model-via-Gibbs-sampling10-questions" class="headerlink" title="Quiz: Learning LDA model via Gibbs sampling10 questions"></a>Quiz: Learning LDA model via Gibbs sampling10 questions</h3><div class="note primary"><p>QUIZ<br>Learning LDA model via Gibbs sampling<br>10 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 12, 11:59 PM PST</p>
</div>
<p>1 point<br>1.(True/False) Each iteration of Gibbs sampling for Bayesian inference in topic models is guaranteed to yield a higher joint model probability than the previous sample.</p>
<p><input type="radio" disabled><label>True</label></p>
<p><input type="radio" disabled checked><label>False</label><br><br><br>1 point<br>2.(Check all that are true) Bayesian methods such as Gibbs sampling can be advantageous because they</p>
<p><input type="checkbox" disabled checked><label></label><br>Account for uncertainty over parameters when making predictions</p>
<p><input type="checkbox" disabled><label></label><br>Are faster than methods such as EM</p>
<p><input type="checkbox" disabled><label></label><br>Maximize the log probability of the data under the model</p>
<p><input type="checkbox" disabled checked><label></label><br>Regularize parameter estimates to avoid extreme values<br><br><br>1 point<br>3.For the standard LDA model discussed in the lectures, how many parameters are required to represent the distributions defining the topics?</p>
<p><input type="radio" disabled><label></label><br>[# unique words]</p>
<p><input type="radio" disabled checked><label></label><br>[# unique words] * [# topics]</p>
<p><input type="radio" disabled><label></label><br>[# documents] * [# unique words]</p>
<p><input type="radio" disabled><label></label><br>[# documents] * [# topics]<br><br><br>2 points<br>4.Suppose we have a collection of documents, and we are focusing our analysis to the use of the following 10 words. We ran several iterations of collapsed Gibbs sampling for an LDA model with K=2 topics and alpha=10.0 and gamma=0.1 (with notation as in the collapsed Gibbs sampling lecture). The corpus-wide assignments at our most recent collapsed Gibbs iteration are summarized in the following table of counts:</p>
<table>
<thead>
<tr>
<th>Word</th>
<th>Count in topic 1</th>
<th>Count in topic 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>baseball</td>
<td>52</td>
<td>0</td>
</tr>
<tr>
<td>homerun</td>
<td>15</td>
<td>0</td>
</tr>
<tr>
<td>ticket</td>
<td>9</td>
<td>2</td>
</tr>
<tr>
<td>price</td>
<td>9</td>
<td>25</td>
</tr>
<tr>
<td>manager</td>
<td>20</td>
<td>37</td>
</tr>
<tr>
<td>owner</td>
<td>17</td>
<td>32</td>
</tr>
<tr>
<td>company</td>
<td>1</td>
<td>23</td>
</tr>
<tr>
<td>stock</td>
<td>0</td>
<td>75</td>
</tr>
<tr>
<td>bankrupt</td>
<td>0</td>
<td>19</td>
</tr>
<tr>
<td>taxes</td>
<td>0</td>
<td>29</td>
</tr>
</tbody>
</table>
<p>We also have a single document i with the following topic assignments for each word:</p>
<table>
<thead>
<tr>
<th>topic</th>
<th>1</th>
<th>2</th>
<th>1</th>
<th>2</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>word</td>
<td>baseball</td>
<td>manager</td>
<td>ticket</td>
<td>price</td>
<td>owner</td>
</tr>
</tbody>
</table>
<p>Suppose we want to re-compute the topic assignment for the word “manager”. To sample a new topic, we need to compute several terms to determine how much the document likes each topic, and how much each topic likes the word “manager”. The following questions will all relate to this situation.</p>
<p>First, using the notation in the slides, what is the value of mmanager,1 (i.e., the number of times the word “manager” has been assigned to topic 1)?</p>
<p><input type="”text”" placeholder="20"><br><br><br>1 point<br>5.Consider the situation described in Question 4.</p>
<p>What is the value of ∑wmw,1, where the sum is taken over all words in the vocabulary?</p>
<p><input type="”text”" placeholder="123"><br><br><br>1 point<br>6.Consider the situation described in Question 4.</p>
<p>Following the notation in the slides, what is the value of ni,1 for this document i (i.e., the number of words in document i assigned to topic 1)?</p>
<p><input type="”text”" placeholder="3"><br><br><br>1 point<br>7.In the situation described in Question 4, “manager” was assigned to topic 2. When we remove that assignment prior to sampling, we need to decrement the associated counts.</p>
<p>After decrementing, what is the value of ni,2?</p>
<p><input type="”text”" placeholder="1"><br><br><br>1 point<br>8.In the situation described in Question 4, “manager” was assigned to topic 2. When we remove that assignment prior to sampling, we need to decrement the associated counts.</p>
<p>After decrementing, what is the value of mmanager,2?</p>
<p><input type="”text”" placeholder="36"><br><br><br>1 point<br>9.In the situation described in Question 4, “manager” was assigned to topic 2. When we remove that assignment prior to sampling, we need to decrement the associated counts.</p>
<p>After decrementing, what is the value of ∑wmw,2?</p>
<p><input type="”text”" placeholder="241"><br><br><br>2 points<br>10.Consider the situation described in Question 4.</p>
<p>As discussed in the slides, the unnormalized probability of assigning to topic 1 is</p>
<p>p1=ni,1+αNi−1+Kαmmanager,1+γ∑wmw,1+Vγ<br>where V is the total size of the vocabulary.</p>
<p>Similarly the unnormalized probability of assigning to topic 2 is</p>
<p>p2=ni,2+αNi−1+Kαmmanager,2+γ∑wmw,2+Vγ<br>Using the above equations and the results computed in previous questions, compute the probability of assigning the word “manager” to topic 1.</p>
<p>(Reminder: Normalize across the two topic options so that the probabilities of all possible assignments—topic 1 and topic 2—sum to 1.)</p>
<p>Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.562"><br><br><br><div class="note primary"><p>p1 = (3+10)/(4+2<em>10)</em>(20+0.1)/(123+10<em>0.1)<br>p2 = (1+10)/(4+2</em>10)<em>(36+0.1)/(241+10</em>0.1)</p>
</div></p>
<h2 id="Programming-Assignment-8"><a href="#Programming-Assignment-8" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h2><h3 id="Modeling-text-topics-with-Latent-Dirichlet-Allocation10-min"><a href="#Modeling-text-topics-with-Latent-Dirichlet-Allocation10-min" class="headerlink" title="Modeling text topics with Latent Dirichlet Allocation10 min"></a>Modeling text topics with Latent Dirichlet Allocation10 min</h3><h3 id="Quiz-Modeling-text-topics-with-Latent-Dirichlet-Allocation12-questions"><a href="#Quiz-Modeling-text-topics-with-Latent-Dirichlet-Allocation12-questions" class="headerlink" title="Quiz: Modeling text topics with Latent Dirichlet Allocation12 questions"></a>Quiz: Modeling text topics with Latent Dirichlet Allocation12 questions</h3><div class="note primary"><p>QUIZ<br>Modeling text topics with Latent Dirichlet Allocation<br>12 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 12, 11:59 PM PST</p>
</div>
<p>1 point<br>1.Identify the top 3 most probable words for the first topic.</p>
<p><input type="checkbox" disabled><label></label><br>institute</p>
<p><input type="checkbox" disabled checked><label></label><br>university</p>
<p><input type="checkbox" disabled checked><label></label><br>professor</p>
<p><input type="checkbox" disabled checked><label></label><br>research</p>
<p><input type="checkbox" disabled><label></label><br>studies</p>
<p><input type="checkbox" disabled><label></label><br>game</p>
<p><input type="checkbox" disabled><label></label><br>coach<br><br><br>1 point<br>2.What is the sum of the probabilities assigned to the top 50 words in the 3rd topic? Round your answer to 3 decimal places.</p>
<p><input type="”text”" placeholder="0.210"><br><br><br>1 point<br>3.What is the topic most closely associated with the article about former US President George W. Bush? Use the average results from 100 topic predictions.</p>
<p><input type="”text”" placeholder="general politics"><br><br><br>1 point<br>4.What are the top 3 topics corresponding to the article about English football (soccer) player Steven Gerrard? Use the average results from 100 topic predictions.</p>
<p><input type="checkbox" disabled><label></label><br>science and research</p>
<p><input type="checkbox" disabled checked><label></label><br>team sports</p>
<p><input type="checkbox" disabled><label></label><br>music, TV, and film</p>
<p><input type="checkbox" disabled checked><label></label><br>international athletics</p>
<p><input type="checkbox" disabled checked><label></label><br>Great Britain and Australia<br><br><br>1 point<br>5.Using the LDA representation, compute the 5000 nearest neighbors for American baseball player Alex Rodriguez. For what value of k is Mariano Rivera the k-th nearest neighbor to Alex Rodriguez?</p>
<p><input type="”text”" placeholder="722"><br><br><br>1 point<br>6.Using the TF-IDF representation, compute the 5000 nearest neighbors for American baseball player Alex Rodriguez. For what value of k is Mariano Rivera the k-th nearest neighbor to Alex Rodriguez?</p>
<p><input type="”text”" placeholder="53"><br><br><br>1 point<br>7.What was the value of alpha used to fit our original topic model?</p>
<p><input type="”text”" placeholder="5.0"><br><br><br>1 point<br>8.What was the value of gamma used to fit our original topic model? Remember that GraphLab Create uses “beta” instead of “gamma” to refer to the hyperparameter that influences topic distributions over words.</p>
<p><input type="”text”" placeholder="0.1"><br><br><br>1 point<br>9.How many topics are assigned a weight greater than 0.3 or less than 0.05 for the article on Paul Krugman in the low alpha model? Use the average results from 100 topic predictions.</p>
<p><input type="”text”" placeholder="8"><br><br><br>1 point<br>10.How many topics are assigned a weight greater than 0.3 or less than 0.05 for the article on Paul Krugman in the high alpha model? Use the average results from 100 topic predictions.</p>
<p><input type="”text”" placeholder="2"><br><br><br>1 point<br>11.For each topic of the low gamma model, compute the number of words required to make a list with total probability 0.5. What is the average number of words required across all topics? (HINT: use the get_topics() function from GraphLab Create with the cdf_cutoff argument.)</p>
<p><input type="”text”" placeholder="5"><br><br><br>1 point<br>12.For each topic of the high gamma model, compute the number of words required to make a list with total probability 0.5. What is the average number of words required across all topics? (HINT: use the get_topics() function from GraphLab Create with the cdf_cutoff argument).</p>
<p><input type="”text”" placeholder="5"><br><br></p>
<h2 id="Week-6-Hierarchical-Clustering-amp-Closing-Remarks"><a href="#Week-6-Hierarchical-Clustering-amp-Closing-Remarks" class="headerlink" title="Week 6 Hierarchical Clustering &amp; Closing Remarks"></a>Week 6 Hierarchical Clustering &amp; Closing Remarks</h2><div class="note primary"><p>In the conclusion of the course, we will recap what we have covered. This represents both techniques specific to clustering and retrieval, as well as foundational machine learning concepts that are more broadly useful.</p>
<p>We provide a quick tour into an alternative clustering approach called hierarchical clustering, which you will experiment with on the Wikipedia dataset. Following this exploration, we discuss how clustering-type ideas can be applied in other areas like segmenting time series. We then briefly outline some important clustering and retrieval ideas that we did not cover in this course.</p>
<p>We conclude with an overview of what’s in store for you in the rest of the specialization.</p>
</div>
<h3 id="What-we’ve-learned-1"><a href="#What-we’ve-learned-1" class="headerlink" title="What we’ve learned"></a>What we’ve learned</h3><h4 id="Slides-presented-in-this-module10-min-25"><a href="#Slides-presented-in-this-module10-min-25" class="headerlink" title="Slides presented in this module10 min"></a>Slides presented in this module10 min</h4><p>For those interested, the slides presented in the videos for this module can be downloaded here:</p>
<p><a href="https://github.com/SSQ/Coursera-UW-Machine-Learning-Clustering-Retrieval/blob/master/Lecture%20Slides/6.0-closing-annotated.rar" target="_blank" rel="external">closing-annotated.pdf</a></p>
<h4 id="Module-1-recap10-min"><a href="#Module-1-recap10-min" class="headerlink" title="Module 1 recap10 min"></a>Module 1 recap10 min</h4><h4 id="Module-2-recap3-min"><a href="#Module-2-recap3-min" class="headerlink" title="Module 2 recap3 min"></a>Module 2 recap3 min</h4><h4 id="Module-3-recap6-min"><a href="#Module-3-recap6-min" class="headerlink" title="Module 3 recap6 min"></a>Module 3 recap6 min</h4><h4 id="Module-4-recap7-min"><a href="#Module-4-recap7-min" class="headerlink" title="Module 4 recap7 min"></a>Module 4 recap7 min</h4><h3 id="Hierarchical-clustering-and-clustering-for-time-series-segmentation"><a href="#Hierarchical-clustering-and-clustering-for-time-series-segmentation" class="headerlink" title="Hierarchical clustering and clustering for time series segmentation"></a>Hierarchical clustering and clustering for time series segmentation</h3><h4 id="Why-hierarchical-clustering-2-min"><a href="#Why-hierarchical-clustering-2-min" class="headerlink" title="Why hierarchical clustering?2 min"></a>Why hierarchical clustering?2 min</h4><h4 id="Divisive-clustering4-min"><a href="#Divisive-clustering4-min" class="headerlink" title="Divisive clustering4 min"></a>Divisive clustering4 min</h4><h4 id="Agglomerative-clustering2-min"><a href="#Agglomerative-clustering2-min" class="headerlink" title="Agglomerative clustering2 min"></a>Agglomerative clustering2 min</h4><h4 id="The-dendrogram4-min"><a href="#The-dendrogram4-min" class="headerlink" title="The dendrogram4 min"></a>The dendrogram4 min</h4><h4 id="Agglomerative-clustering-details7-min"><a href="#Agglomerative-clustering-details7-min" class="headerlink" title="Agglomerative clustering details7 min"></a>Agglomerative clustering details7 min</h4><h4 id="Hidden-Markov-models9-min"><a href="#Hidden-Markov-models9-min" class="headerlink" title="Hidden Markov models9 min"></a>Hidden Markov models9 min</h4><h3 id="Programming-Assignment-9"><a href="#Programming-Assignment-9" class="headerlink" title="Programming Assignment"></a>Programming Assignment</h3><h4 id="Modeling-text-data-with-a-hierarchy-of-clusters10-min"><a href="#Modeling-text-data-with-a-hierarchy-of-clusters10-min" class="headerlink" title="Modeling text data with a hierarchy of clusters10 min"></a>Modeling text data with a hierarchy of clusters10 min</h4><h4 id="Quiz-Modeling-text-data-with-a-hierarchy-of-clusters3-questions"><a href="#Quiz-Modeling-text-data-with-a-hierarchy-of-clusters3-questions" class="headerlink" title="Quiz: Modeling text data with a hierarchy of clusters3 questions"></a>Quiz: Modeling text data with a hierarchy of clusters3 questions</h4><div class="note primary"><p>QUIZ<br>Modeling text data with a hierarchy of clusters<br>3 questions<br>To Pass33% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 19, 11:59 PM PST</p>
</div>
<p>1 point<br>1.Make sure you have the latest versions of the notebook. Read this post if</p>
<p>… you downloaded the notebook before September 10<br>… you created an Amazon EC2 instance before October 1</p>
<p>I acknowledge.</p>
<p>1 point<br>2.Which diagram best describes the hierarchy right after splitting the ice_hockey_football cluster?<br>football golf</p>
<p>1 point<br>3.Let us bipartition the clusters male_non_athletes and female_non_athletes. Which diagram best describes the resulting hierarchy of clusters for the non-athletes?</p>
<p>Note. The clusters for the athletes are not shown to save space.</p>
<h3 id="Summary-and-what’s-ahead-in-the-specialization-1"><a href="#Summary-and-what’s-ahead-in-the-specialization-1" class="headerlink" title="Summary and what’s ahead in the specialization"></a>Summary and what’s ahead in the specialization</h3><h4 id="What-we-didn’t-cover2-min"><a href="#What-we-didn’t-cover2-min" class="headerlink" title="What we didn’t cover2 min"></a>What we didn’t cover2 min</h4><h4 id="Thank-you-1-min-1"><a href="#Thank-you-1-min-1" class="headerlink" title="Thank you!1 min"></a>Thank you!1 min</h4>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Coursera/" rel="tag"># Coursera</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/14/Udacity DLND Notebook/" rel="next" title="Udacity DLND Notebook">
                <i class="fa fa-chevron-left"></i> Udacity DLND Notebook
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/26/2018校招笔试题解答/" rel="prev" title="2018校招算法工程师">
                2018校招算法工程师 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  
    <div class="sidebar-toggle">
      <div class="sidebar-toggle-line-wrap">
        <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
        <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
        <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
      </div>
    </div>
  

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          
            
          
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="SSQ" />
          <p class="site-author-name" itemprop="name">SSQ</p>
           
              <p class="site-description motion-element" itemprop="description">Notebook for quick search</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">36</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">33</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/SSQ" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning-Foundations-A-Case-Study-Approach"><span class="nav-number">1.</span> <span class="nav-text">Machine Learning Foundations: A Case Study Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-1-Welcome"><span class="nav-number">1.1.</span> <span class="nav-text">Week 1 Welcome</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-you-should-learn-machine-learning-with-us"><span class="nav-number">1.1.1.</span> <span class="nav-text">Why you should learn machine learning with us</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Important-Update-regarding-the-Machine-Learning-Specialization10-min"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Important Update regarding the Machine Learning Specialization10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Welcome-to-this-course-and-specialization41-sec"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">Welcome to this course and specialization41 sec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Who-we-are5-min"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">Who we are5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Machine-learning-is-changing-the-world3-min"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">Machine learning is changing the world3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Why-a-case-study-approach-7-min"><span class="nav-number">1.1.1.6.</span> <span class="nav-text">Why a case study approach?7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Specialization-overview6-min"><span class="nav-number">1.1.1.7.</span> <span class="nav-text">Specialization overview6 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Who-this-specialization-is-for-and-what-you-will-be-able-to-do"><span class="nav-number">1.1.2.</span> <span class="nav-text">Who this specialization is for and what you will be able to do</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#How-we-got-into-ML3-min"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">How we got into ML3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Who-is-this-specialization-for-4-min"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Who is this specialization for?4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What-you’ll-be-able-to-do57-sec"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">What you’ll be able to do57 sec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-capstone-and-an-example-intelligent-application6-min"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">The capstone and an example intelligent application6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-future-of-intelligent-applications2-min"><span class="nav-number">1.1.2.5.</span> <span class="nav-text">The future of intelligent applications2 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-started-with-the-tools-for-the-course"><span class="nav-number">1.1.3.</span> <span class="nav-text">Getting started with the tools for the course</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Getting-started-with-Python-IPython-Notebook-amp-GraphLab-Create10-min"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Reading: Getting started with Python, IPython Notebook & GraphLab Create10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-where-should-my-files-go-10-min"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">Reading: where should my files go?10 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-started-with-Python-and-the-IPython-Notebook"><span class="nav-number">1.1.4.</span> <span class="nav-text">Getting started with Python and the IPython Notebook</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">Download the IPython Notebook used in this lesson to follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Starting-an-IPython-Notebook5-min"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">Starting an IPython Notebook5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Creating-variables-in-Python7-min"><span class="nav-number">1.1.4.3.</span> <span class="nav-text">Creating variables in Python7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Conditional-statements-and-loops-in-Python8-min"><span class="nav-number">1.1.4.4.</span> <span class="nav-text">Conditional statements and loops in Python8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Creating-functions-and-lambdas-in-Python3-min"><span class="nav-number">1.1.4.5.</span> <span class="nav-text">Creating functions and lambdas in Python3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-started-with-SFrames-for-data-engineering-and-analysis"><span class="nav-number">1.1.5.</span> <span class="nav-text">Getting started with SFrames for data engineering and analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-1"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">Download the IPython Notebook used in this lesson to follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Starting-GraphLab-Create-amp-loading-an-SFrame4-min"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">Starting GraphLab Create & loading an SFrame4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Canvas-for-data-visualization4-min"><span class="nav-number">1.1.5.3.</span> <span class="nav-text">Canvas for data visualization4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interacting-with-columns-of-an-SFrame4-min"><span class="nav-number">1.1.5.4.</span> <span class="nav-text">Interacting with columns of an SFrame4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Using-apply-for-data-transformation5-min"><span class="nav-number">1.1.5.5.</span> <span class="nav-text">Using .apply() for data transformation5 min</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-2-Regression-Predicting-House-Prices"><span class="nav-number">1.2.</span> <span class="nav-text">Week 2 Regression: Predicting House Prices</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-regression-modeling"><span class="nav-number">1.2.1.</span> <span class="nav-text">Linear regression modeling</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-1"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Predicting-house-prices-A-case-study-in-regression1-min"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Predicting house prices: A case study in regression1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What-is-the-goal-and-how-might-you-naively-address-it-3-min"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">What is the goal and how might you naively address it?3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Regression-A-Model-Based-Approach5-min"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">Linear Regression: A Model-Based Approach5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adding-higher-order-effects4-min"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">Adding higher order effects4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-regression-models"><span class="nav-number">1.2.2.</span> <span class="nav-text">Evaluating regression models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating-overfitting-via-training-test-split6-min"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Evaluating overfitting via training/test split6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-test-curves4-min"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">Training/test curves4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adding-other-features2-min"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">Adding other features2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Other-regression-examples3-min"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">Other regression examples3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-of-regression"><span class="nav-number">1.2.3.</span> <span class="nav-text">Summary of regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Regression-ML-block-diagram5-min"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">Regression ML block diagram5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Regression9-questions"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">Quiz: Regression9 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Predicting-house-prices-IPython-Notebook"><span class="nav-number">1.2.4.</span> <span class="nav-text">Predicting house prices: IPython Notebook</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-2"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">Download the IPython Notebook used in this lesson to follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loading-amp-exploring-house-sale-data7-min"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">Loading & exploring house sale data7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Splitting-the-data-into-training-and-test-sets2-min"><span class="nav-number">1.2.4.3.</span> <span class="nav-text">Splitting the data into training and test sets2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-a-simple-regression-model-to-predict-house-prices-from-house-size3-min"><span class="nav-number">1.2.4.4.</span> <span class="nav-text">Learning a simple regression model to predict house prices from house size3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating-error-RMSE-of-the-simple-model2-min"><span class="nav-number">1.2.4.5.</span> <span class="nav-text">Evaluating error (RMSE) of the simple model2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Visualizing-predictions-of-simple-model-with-Matplotlib4-min"><span class="nav-number">1.2.4.6.</span> <span class="nav-text">Visualizing predictions of simple model with Matplotlib4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inspecting-the-model-coefficients-learned1-min"><span class="nav-number">1.2.4.7.</span> <span class="nav-text">Inspecting the model coefficients learned1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploring-other-features-of-the-data6-min"><span class="nav-number">1.2.4.8.</span> <span class="nav-text">Exploring other features of the data6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-a-model-to-predict-house-prices-from-more-features3-min"><span class="nav-number">1.2.4.9.</span> <span class="nav-text">Learning a model to predict house prices from more features3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Applying-learned-models-to-predict-price-of-an-average-house5-min"><span class="nav-number">1.2.4.10.</span> <span class="nav-text">Applying learned models to predict price of an average house5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Applying-learned-models-to-predict-price-of-two-fancy-houses7-min"><span class="nav-number">1.2.4.11.</span> <span class="nav-text">Applying learned models to predict price of two fancy houses7 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-assignment"><span class="nav-number">1.2.5.</span> <span class="nav-text">Programming assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Predicting-house-prices-assignment10-min"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">Reading: Predicting house prices assignment10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Predicting-house-prices3-questions"><span class="nav-number">1.2.5.2.</span> <span class="nav-text">Quiz: Predicting house prices3 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-3-Classification-Analyzing-Sentiment"><span class="nav-number">1.3.</span> <span class="nav-text">Week 3 Classification: Analyzing Sentiment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification-modeling"><span class="nav-number">1.3.1.</span> <span class="nav-text">Classification modeling</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-2"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Analyzing-the-sentiment-of-reviews-A-case-study-in-classification38-sec"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">Analyzing the sentiment of reviews: A case study in classification38 sec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What-is-an-intelligent-restaurant-review-system-4-min"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">What is an intelligent restaurant review system?4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Examples-of-classification-tasks4-min"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">Examples of classification tasks4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-classifiers5-min"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">Linear classifiers5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decision-boundaries3-min"><span class="nav-number">1.3.1.6.</span> <span class="nav-text">Decision boundaries3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-classification-models"><span class="nav-number">1.3.2.</span> <span class="nav-text">Evaluating classification models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-and-evaluating-a-classifier4-min"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">Training and evaluating a classifier4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What’s-a-good-accuracy-3-min"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">What’s a good accuracy?3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#False-positives-false-negatives-and-confusion-matrices6-min"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">False positives, false negatives, and confusion matrices6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-curves5-min"><span class="nav-number">1.3.2.4.</span> <span class="nav-text">Learning curves5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Class-probabilities1-min"><span class="nav-number">1.3.2.5.</span> <span class="nav-text">Class probabilities1 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-of-classification"><span class="nav-number">1.3.3.</span> <span class="nav-text">Summary of classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Classification-ML-block-diagram3-min"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">Classification ML block diagram3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Classification7-questions"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">Quiz: Classification7 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Analyzing-sentiment-IPython-Notebook"><span class="nav-number">1.3.4.</span> <span class="nav-text">Analyzing sentiment: IPython Notebook</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-3"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">Download the IPython Notebook used in this lesson to follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loading-amp-exploring-product-review-data2-min"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">Loading & exploring product review data2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Creating-the-word-count-vector2-min"><span class="nav-number">1.3.4.3.</span> <span class="nav-text">Creating the word count vector2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploring-the-most-popular-product4-min"><span class="nav-number">1.3.4.4.</span> <span class="nav-text">Exploring the most popular product4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Defining-which-reviews-have-positive-or-negative-sentiment4-min"><span class="nav-number">1.3.4.5.</span> <span class="nav-text">Defining which reviews have positive or negative sentiment4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-a-sentiment-classifier3-min"><span class="nav-number">1.3.4.6.</span> <span class="nav-text">Training a sentiment classifier3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating-a-classifier-amp-the-ROC-curve4-min"><span class="nav-number">1.3.4.7.</span> <span class="nav-text">Evaluating a classifier & the ROC curve4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Applying-model-to-find-most-positive-amp-negative-reviews-for-a-product4-min"><span class="nav-number">1.3.4.8.</span> <span class="nav-text">Applying model to find most positive & negative reviews for a product4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploring-the-most-positive-amp-negative-aspects-of-a-product4-min"><span class="nav-number">1.3.4.9.</span> <span class="nav-text">Exploring the most positive & negative aspects of a product4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-assignment-1"><span class="nav-number">1.3.5.</span> <span class="nav-text">Programming assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Analyzing-product-sentiment-assignment10-min"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">Reading: Analyzing product sentiment assignment10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Analyzing-product-sentiment11-questions"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">Quiz: Analyzing product sentiment11 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-4-Clustering-and-Similarity-Retrieving-Documents"><span class="nav-number">1.4.</span> <span class="nav-text">Week 4 Clustering and Similarity: Retrieving Documents</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Algorithms-for-retrieval-and-measuring-similarity-of-documents"><span class="nav-number">1.4.1.</span> <span class="nav-text">Algorithms for retrieval and measuring similarity of documents</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-3"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Document-retrieval-A-case-study-in-clustering-and-measuring-similarity35-sec"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">Document retrieval: A case study in clustering and measuring similarity35 sec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What-is-the-document-retrieval-task-1-min"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">What is the document retrieval task?1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Word-count-representation-for-measuring-similarity6-min"><span class="nav-number">1.4.1.4.</span> <span class="nav-text">Word count representation for measuring similarity6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Prioritizing-important-words-with-tf-idf3-min"><span class="nav-number">1.4.1.5.</span> <span class="nav-text">Prioritizing important words with tf-idf3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Calculating-tf-idf-vectors5-min"><span class="nav-number">1.4.1.6.</span> <span class="nav-text">Calculating tf-idf vectors5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Retrieving-similar-documents-using-nearest-neighbor-search2-min"><span class="nav-number">1.4.1.7.</span> <span class="nav-text">Retrieving similar documents using nearest neighbor search2 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Clustering-models-and-algorithms"><span class="nav-number">1.4.2.</span> <span class="nav-text">Clustering models and algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Clustering-documents-task-overview2-min"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">Clustering documents task overview2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Clustering-documents-An-unsupervised-learning-task4-min"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">Clustering documents: An unsupervised learning task4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-means-A-clustering-algorithm3-min"><span class="nav-number">1.4.2.3.</span> <span class="nav-text">k-means: A clustering algorithm3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Other-examples-of-clustering6-min"><span class="nav-number">1.4.2.4.</span> <span class="nav-text">Other examples of clustering6 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-of-clustering-and-similarity"><span class="nav-number">1.4.3.</span> <span class="nav-text">Summary of clustering and similarity</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Clustering-and-similarity-ML-block-diagram7-min"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">Clustering and similarity ML block diagram7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Clustering-and-Similarity6-questions"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">Quiz: Clustering and Similarity6 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Document-retrieval-IPython-Notebook"><span class="nav-number">1.4.4.</span> <span class="nav-text">Document retrieval: IPython Notebook</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-IPython-Notebook-used-in-this-lesson-to-follow-along10-min-4"><span class="nav-number">1.4.4.1.</span> <span class="nav-text">Download the IPython Notebook used in this lesson to follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loading-amp-exploring-Wikipedia-data5-min"><span class="nav-number">1.4.4.2.</span> <span class="nav-text">Loading & exploring Wikipedia data5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploring-word-counts5-min"><span class="nav-number">1.4.4.3.</span> <span class="nav-text">Exploring word counts5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-amp-exploring-TF-IDFs7-min"><span class="nav-number">1.4.4.4.</span> <span class="nav-text">Computing & exploring TF-IDFs7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-distances-between-Wikipedia-articles5-min"><span class="nav-number">1.4.4.5.</span> <span class="nav-text">Computing distances between Wikipedia articles5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Building-amp-exploring-a-nearest-neighbors-model-for-Wikipedia-articles3-min"><span class="nav-number">1.4.4.6.</span> <span class="nav-text">Building & exploring a nearest neighbors model for Wikipedia articles3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Examples-of-document-retrieval-in-action4-min"><span class="nav-number">1.4.4.7.</span> <span class="nav-text">Examples of document retrieval in action4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-assignment-2"><span class="nav-number">1.4.5.</span> <span class="nav-text">Programming assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Retrieving-Wikipedia-articles-assignment10-min"><span class="nav-number">1.4.5.1.</span> <span class="nav-text">Reading: Retrieving Wikipedia articles assignment10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Retrieving-Wikipedia-articles9-questions"><span class="nav-number">1.4.5.2.</span> <span class="nav-text">Quiz: Retrieving Wikipedia articles9 questions</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning-Regression"><span class="nav-number">2.</span> <span class="nav-text">Machine Learning: Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-1"><span class="nav-number">2.1.</span> <span class="nav-text">Week 1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Welcome"><span class="nav-number">2.1.1.</span> <span class="nav-text">Welcome</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-is-this-course-about"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">What is this course about?</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-4"><span class="nav-number">2.1.1.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Welcome-1-min"><span class="nav-number">2.1.1.1.2.</span> <span class="nav-text">Welcome!1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#What-is-the-course-about-3-min"><span class="nav-number">2.1.1.1.3.</span> <span class="nav-text">What is the course about?3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Outlining-the-first-half-of-the-course5-min"><span class="nav-number">2.1.1.1.4.</span> <span class="nav-text">Outlining the first half of the course5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Outlining-the-second-half-of-the-course5-min"><span class="nav-number">2.1.1.1.5.</span> <span class="nav-text">Outlining the second half of the course5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Assumed-background4-min"><span class="nav-number">2.1.1.1.6.</span> <span class="nav-text">Assumed background4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reading-Software-tools-you’ll-need10-min"><span class="nav-number">2.1.1.1.7.</span> <span class="nav-text">Reading: Software tools you’ll need10 min</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Simple-Linear-Regression"><span class="nav-number">2.1.2.</span> <span class="nav-text">Simple Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Regression-fundamentals"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">Regression fundamentals</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-5"><span class="nav-number">2.1.2.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A-case-study-in-predicting-house-prices1-min"><span class="nav-number">2.1.2.1.2.</span> <span class="nav-text">A case study in predicting house prices1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Regression-fundamentals-data-amp-model8-min"><span class="nav-number">2.1.2.1.3.</span> <span class="nav-text">Regression fundamentals: data & model8 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Regression-fundamentals-the-task2-min"><span class="nav-number">2.1.2.1.4.</span> <span class="nav-text">Regression fundamentals: the task2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Regression-ML-block-diagram4-min"><span class="nav-number">2.1.2.1.5.</span> <span class="nav-text">Regression ML block diagram4 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-simple-linear-regression-model-its-use-and-interpretation"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">The simple linear regression model, its use, and interpretation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#The-simple-linear-regression-model2-min"><span class="nav-number">2.1.2.2.1.</span> <span class="nav-text">The simple linear regression model2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#The-cost-of-using-a-given-line6-min"><span class="nav-number">2.1.2.2.2.</span> <span class="nav-text">The cost of using a given line6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Using-the-fitted-line6-min"><span class="nav-number">2.1.2.2.3.</span> <span class="nav-text">Using the fitted line6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Interpreting-the-fitted-line6-min"><span class="nav-number">2.1.2.2.4.</span> <span class="nav-text">Interpreting the fitted line6 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#An-aside-on-optimization-one-dimensional-objectives"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">An aside on optimization: one dimensional objectives</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Defining-our-least-squares-optimization-objective3-min"><span class="nav-number">2.1.2.3.1.</span> <span class="nav-text">Defining our least squares optimization objective3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Finding-maxima-or-minima-analytically7-min"><span class="nav-number">2.1.2.3.2.</span> <span class="nav-text">Finding maxima or minima analytically7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Maximizing-a-1d-function-a-worked-example2-min"><span class="nav-number">2.1.2.3.3.</span> <span class="nav-text">Maximizing a 1d function: a worked example2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Finding-the-max-via-hill-climbing6-min"><span class="nav-number">2.1.2.3.4.</span> <span class="nav-text">Finding the max via hill climbing6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Finding-the-min-via-hill-descent3-min"><span class="nav-number">2.1.2.3.5.</span> <span class="nav-text">Finding the min via hill descent3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Choosing-stepsize-and-convergence-criteria6-min"><span class="nav-number">2.1.2.3.6.</span> <span class="nav-text">Choosing stepsize and convergence criteria6 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#An-aside-on-optimization-multidimensional-objectives"><span class="nav-number">2.1.2.4.</span> <span class="nav-text">An aside on optimization: multidimensional objectives</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Gradients-derivatives-in-multiple-dimensions5-min"><span class="nav-number">2.1.2.4.1.</span> <span class="nav-text">Gradients: derivatives in multiple dimensions5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Gradient-descent-multidimensional-hill-descent6-min"><span class="nav-number">2.1.2.4.2.</span> <span class="nav-text">Gradient descent: multidimensional hill descent6 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Finding-the-least-squares-line"><span class="nav-number">2.1.2.5.</span> <span class="nav-text">Finding the least squares line</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Computing-the-gradient-of-RSS7-min"><span class="nav-number">2.1.2.5.1.</span> <span class="nav-text">Computing the gradient of RSS7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Approach-1-closed-form-solution5-min"><span class="nav-number">2.1.2.5.2.</span> <span class="nav-text">Approach 1: closed-form solution5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Optional-reading-worked-out-example-for-closed-form-solution10-min"><span class="nav-number">2.1.2.5.3.</span> <span class="nav-text">Optional reading: worked-out example for closed-form solution10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Approach-2-gradient-descent7-min"><span class="nav-number">2.1.2.5.4.</span> <span class="nav-text">Approach 2: gradient descent7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Optional-reading-worked-out-example-for-gradient-descent10-min"><span class="nav-number">2.1.2.5.5.</span> <span class="nav-text">Optional reading: worked-out example for gradient descent10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Comparing-the-approaches1-min"><span class="nav-number">2.1.2.5.6.</span> <span class="nav-text">Comparing the approaches1 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discussion-and-summary-of-simple-linear-regression"><span class="nav-number">2.1.2.6.</span> <span class="nav-text">Discussion and summary of simple linear regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Download-notebooks-to-follow-along10-min"><span class="nav-number">2.1.2.6.1.</span> <span class="nav-text">Download notebooks to follow along10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Influence-of-high-leverage-points-exploring-the-data4-min"><span class="nav-number">2.1.2.6.2.</span> <span class="nav-text">Influence of high leverage points: exploring the data4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Influence-of-high-leverage-points-removing-Center-City7-min"><span class="nav-number">2.1.2.6.3.</span> <span class="nav-text">Influence of high leverage points: removing Center City7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Influence-of-high-leverage-points-removing-high-end-towns3-min"><span class="nav-number">2.1.2.6.4.</span> <span class="nav-text">Influence of high leverage points: removing high-end towns3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Asymmetric-cost-functions3-min"><span class="nav-number">2.1.2.6.5.</span> <span class="nav-text">Asymmetric cost functions3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A-brief-recap1-min"><span class="nav-number">2.1.2.6.6.</span> <span class="nav-text">A brief recap1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Simple-Linear-Regression7-questions"><span class="nav-number">2.1.2.6.7.</span> <span class="nav-text">Quiz: Simple Linear Regression7 questions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-assignment-3"><span class="nav-number">2.1.2.7.</span> <span class="nav-text">Programming assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Reading-Fitting-a-simple-linear-regression-model-on-housing-data10-min"><span class="nav-number">2.1.2.7.1.</span> <span class="nav-text">Reading: Fitting a simple linear regression model on housing data10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Fitting-a-simple-linear-regression-model-on-housing-data4-questions"><span class="nav-number">2.1.2.7.2.</span> <span class="nav-text">Quiz: Fitting a simple linear regression model on housing data4 questions</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-2-Multiple-Regression"><span class="nav-number">2.2.</span> <span class="nav-text">Week 2 Multiple Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiple-features-of-one-input"><span class="nav-number">2.2.1.</span> <span class="nav-text">Multiple features of one input</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-6"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple-regression-intro30-sec"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">Multiple regression intro30 sec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Polynomial-regression3-min"><span class="nav-number">2.2.1.3.</span> <span class="nav-text">Polynomial regression3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Modeling-seasonality8-min"><span class="nav-number">2.2.1.4.</span> <span class="nav-text">Modeling seasonality8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Where-we-see-seasonality3-min"><span class="nav-number">2.2.1.5.</span> <span class="nav-text">Where we see seasonality3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regression-with-general-features-of-1-input2-min"><span class="nav-number">2.2.1.6.</span> <span class="nav-text">Regression with general features of 1 input2 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Incorporating-multiple-inputs"><span class="nav-number">2.2.2.</span> <span class="nav-text">Incorporating multiple inputs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivating-the-use-of-multiple-inputs4-min"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">Motivating the use of multiple inputs4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Defining-notation3-min"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">Defining notation3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regression-with-features-of-multiple-inputs3-min"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">Regression with features of multiple inputs3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interpreting-the-multiple-regression-fit7-min"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">Interpreting the multiple regression fit7 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Setting-the-stage-for-computing-the-least-squares-fit"><span class="nav-number">2.2.3.</span> <span class="nav-text">Setting the stage for computing the least squares fit</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Optional-reading-review-of-matrix-algebra10-min"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">Optional reading: review of matrix algebra10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rewriting-the-single-observation-model-in-vector-notation6-min"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">Rewriting the single observation model in vector notation6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rewriting-the-model-for-all-observations-in-matrix-notation4-min"><span class="nav-number">2.2.3.3.</span> <span class="nav-text">Rewriting the model for all observations in matrix notation4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-the-cost-of-a-D-dimensional-curve9-min"><span class="nav-number">2.2.3.4.</span> <span class="nav-text">Computing the cost of a D-dimensional curve9 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Computing-the-least-squares-D-dimensional-curve"><span class="nav-number">2.2.4.</span> <span class="nav-text">Computing the least squares D-dimensional curve</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-the-gradient-of-RSS3-min"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">Computing the gradient of RSS3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approach-1-closed-form-solution3-min"><span class="nav-number">2.2.4.2.</span> <span class="nav-text">Approach 1: closed-form solution3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discussing-the-closed-form-solution4-min"><span class="nav-number">2.2.4.3.</span> <span class="nav-text">Discussing the closed-form solution4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approach-2-gradient-descent2-min"><span class="nav-number">2.2.4.4.</span> <span class="nav-text">Approach 2: gradient descent2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-by-feature-update9-min"><span class="nav-number">2.2.4.5.</span> <span class="nav-text">Feature-by-feature update9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algorithmic-summary-of-gradient-descent-approach4-min"><span class="nav-number">2.2.4.6.</span> <span class="nav-text">Algorithmic summary of gradient descent approach4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-multiple-regression"><span class="nav-number">2.2.5.</span> <span class="nav-text">Summarizing multiple regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-brief-recap1-min-1"><span class="nav-number">2.2.5.1.</span> <span class="nav-text">A brief recap1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Multiple-Regression9-questions"><span class="nav-number">2.2.5.2.</span> <span class="nav-text">Quiz: Multiple Regression9 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-assignment-1"><span class="nav-number">2.2.6.</span> <span class="nav-text">Programming assignment 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Exploring-different-multiple-regression-models-for-house-price-prediction10-min"><span class="nav-number">2.2.6.1.</span> <span class="nav-text">Reading: Exploring different multiple regression models for house price prediction10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Exploring-different-multiple-regression-models-for-house-price-prediction8-questions"><span class="nav-number">2.2.6.2.</span> <span class="nav-text">Quiz: Exploring different multiple regression models for house price prediction8 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-assignment-2"><span class="nav-number">2.2.7.</span> <span class="nav-text">Programming assignment 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Numpy-tutorial10-min"><span class="nav-number">2.2.7.1.</span> <span class="nav-text">Numpy tutorial10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Implementing-gradient-descent-for-multiple-regression10-min"><span class="nav-number">2.2.7.2.</span> <span class="nav-text">Reading: Implementing gradient descent for multiple regression10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Implementing-gradient-descent-for-multiple-regression5-questions"><span class="nav-number">2.2.7.3.</span> <span class="nav-text">Quiz: Implementing gradient descent for multiple regression5 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-3-Assessing-Performance"><span class="nav-number">2.3.</span> <span class="nav-text">Week 3 Assessing Performance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Defining-how-we-assess-performance"><span class="nav-number">2.3.1.</span> <span class="nav-text">Defining how we assess performance</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-7"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Assessing-performance-intro32-sec"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">Assessing performance intro32 sec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What-do-we-mean-by-“loss”-4-min"><span class="nav-number">2.3.1.3.</span> <span class="nav-text">What do we mean by “loss”?4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-measures-of-loss-and-their-trends-with-model-complexity"><span class="nav-number">2.3.2.</span> <span class="nav-text">3 measures of loss and their trends with model complexity</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-error-assessing-loss-on-the-training-set7-min"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">Training error: assessing loss on the training set7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Generalization-error-what-we-really-want8-min"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">Generalization error: what we really want8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Test-error-what-we-can-actually-compute4-min"><span class="nav-number">2.3.2.3.</span> <span class="nav-text">Test error: what we can actually compute4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Defining-overfitting2-min"><span class="nav-number">2.3.2.4.</span> <span class="nav-text">Defining overfitting2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-test-split1-min"><span class="nav-number">2.3.2.5.</span> <span class="nav-text">Training/test split1 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-sources-of-error-and-the-bias-variance-tradeoff"><span class="nav-number">2.3.3.</span> <span class="nav-text">3 sources of error and the bias-variance tradeoff</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Irreducible-error-and-bias6-min"><span class="nav-number">2.3.3.1.</span> <span class="nav-text">Irreducible error and bias6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Variance-and-the-bias-variance-tradeoff6-min"><span class="nav-number">2.3.3.2.</span> <span class="nav-text">Variance and the bias-variance tradeoff6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Error-vs-amount-of-data6-min"><span class="nav-number">2.3.3.3.</span> <span class="nav-text">Error vs. amount of data6 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OPTIONAL-ADVANCED-MATERIAL-Formally-defining-and-deriving-the-3-sources-of-error"><span class="nav-number">2.3.4.</span> <span class="nav-text">OPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of error</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Formally-defining-the-3-sources-of-error14-min"><span class="nav-number">2.3.4.1.</span> <span class="nav-text">Formally defining the 3 sources of error14 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Formally-deriving-why-3-sources-of-error20-min"><span class="nav-number">2.3.4.2.</span> <span class="nav-text">Formally deriving why 3 sources of error20 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Putting-the-pieces-together"><span class="nav-number">2.3.5.</span> <span class="nav-text">Putting the pieces together</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-validation-test-split-for-model-selection-fitting-and-assessment7-min"><span class="nav-number">2.3.5.1.</span> <span class="nav-text">Training/validation/test split for model selection, fitting, and assessment7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-brief-recap1-min-2"><span class="nav-number">2.3.5.2.</span> <span class="nav-text">A brief recap1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Assessing-Performance13-questions"><span class="nav-number">2.3.5.3.</span> <span class="nav-text">Quiz: Assessing Performance13 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-assignment-4"><span class="nav-number">2.3.6.</span> <span class="nav-text">Programming assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Exploring-the-bias-variance-tradeoff10-min"><span class="nav-number">2.3.6.1.</span> <span class="nav-text">Reading: Exploring the bias-variance tradeoff10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Exploring-the-bias-variance-tradeoff4-questions"><span class="nav-number">2.3.6.2.</span> <span class="nav-text">Quiz: Exploring the bias-variance tradeoff4 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-4-Ridge-Regression"><span class="nav-number">2.4.</span> <span class="nav-text">Week 4 Ridge Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Characteristics-of-overfit-models"><span class="nav-number">2.4.1.</span> <span class="nav-text">Characteristics of overfit models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-8"><span class="nav-number">2.4.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Symptoms-of-overfitting-in-polynomial-regression2-min"><span class="nav-number">2.4.1.2.</span> <span class="nav-text">Symptoms of overfitting in polynomial regression2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-notebook-and-follow-along10-min"><span class="nav-number">2.4.1.3.</span> <span class="nav-text">Download the notebook and follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-demo7-min"><span class="nav-number">2.4.1.4.</span> <span class="nav-text">Overfitting demo7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-for-more-general-multiple-regression-models3-min"><span class="nav-number">2.4.1.5.</span> <span class="nav-text">Overfitting for more general multiple regression models3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-ridge-objective"><span class="nav-number">2.4.2.</span> <span class="nav-text">The ridge objective</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Balancing-fit-and-magnitude-of-coefficients7-min"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">Balancing fit and magnitude of coefficients7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-resulting-ridge-objective-and-its-extreme-solutions5-min"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">The resulting ridge objective and its extreme solutions5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How-ridge-regression-balances-bias-and-variance1-min"><span class="nav-number">2.4.2.3.</span> <span class="nav-text">How ridge regression balances bias and variance1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-notebook-and-follow-along10-min-1"><span class="nav-number">2.4.2.4.</span> <span class="nav-text">Download the notebook and follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ridge-regression-demo9-min"><span class="nav-number">2.4.2.5.</span> <span class="nav-text">Ridge regression demo9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-ridge-coefficient-path4-min"><span class="nav-number">2.4.2.6.</span> <span class="nav-text">The ridge coefficient path4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimizing-the-ridge-objective"><span class="nav-number">2.4.3.</span> <span class="nav-text">Optimizing the ridge objective</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-the-gradient-of-the-ridge-objective5-min"><span class="nav-number">2.4.3.1.</span> <span class="nav-text">Computing the gradient of the ridge objective5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approach-1-closed-form-solution6-min"><span class="nav-number">2.4.3.2.</span> <span class="nav-text">Approach 1: closed-form solution6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discussing-the-closed-form-solution5-min"><span class="nav-number">2.4.3.3.</span> <span class="nav-text">Discussing the closed-form solution5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approach-2-gradient-descent9-min"><span class="nav-number">2.4.3.4.</span> <span class="nav-text">Approach 2: gradient descent9 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tying-up-the-loose-ends"><span class="nav-number">2.4.4.</span> <span class="nav-text">Tying up the loose ends</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Selecting-tuning-parameters-via-cross-validation3-min"><span class="nav-number">2.4.4.1.</span> <span class="nav-text">Selecting tuning parameters via cross validation3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-fold-cross-validation5-min"><span class="nav-number">2.4.4.2.</span> <span class="nav-text">K-fold cross validation5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How-to-handle-the-intercept6-min"><span class="nav-number">2.4.4.3.</span> <span class="nav-text">How to handle the intercept6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-brief-recap1-min-3"><span class="nav-number">2.4.4.4.</span> <span class="nav-text">A brief recap1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Ridge-Regression9-questions"><span class="nav-number">2.4.4.5.</span> <span class="nav-text">Quiz: Ridge Regression9 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-1"><span class="nav-number">2.4.5.</span> <span class="nav-text">Programming Assignment 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Observing-effects-of-L2-penalty-in-polynomial-regression10-min"><span class="nav-number">2.4.5.1.</span> <span class="nav-text">Reading: Observing effects of L2 penalty in polynomial regression10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Observing-effects-of-L2-penalty-in-polynomial-regression7-questions"><span class="nav-number">2.4.5.2.</span> <span class="nav-text">Quiz: Observing effects of L2 penalty in polynomial regression7 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-2"><span class="nav-number">2.4.6.</span> <span class="nav-text">Programming Assignment 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Implementing-ridge-regression-via-gradient-descent10-min"><span class="nav-number">2.4.6.1.</span> <span class="nav-text">Reading: Implementing ridge regression via gradient descent10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Implementing-ridge-regression-via-gradient-descent8-questions"><span class="nav-number">2.4.6.2.</span> <span class="nav-text">Quiz: Implementing ridge regression via gradient descent8 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-5-Feature-Selection-amp-Lasso"><span class="nav-number">2.5.</span> <span class="nav-text">Week 5 Feature Selection & Lasso</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-selection-via-explicit-model-enumeration"><span class="nav-number">2.5.1.</span> <span class="nav-text">Feature selection via explicit model enumeration</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-9"><span class="nav-number">2.5.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-feature-selection-task3-min"><span class="nav-number">2.5.1.2.</span> <span class="nav-text">The feature selection task3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#All-subsets6-min"><span class="nav-number">2.5.1.3.</span> <span class="nav-text">All subsets6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Complexity-of-all-subsets3-min"><span class="nav-number">2.5.1.4.</span> <span class="nav-text">Complexity of all subsets3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Greedy-algorithms7-min"><span class="nav-number">2.5.1.5.</span> <span class="nav-text">Greedy algorithms7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Complexity-of-the-greedy-forward-stepwise-algorithm2-min"><span class="nav-number">2.5.1.6.</span> <span class="nav-text">Complexity of the greedy forward stepwise algorithm2 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-selection-implicitly-via-regularized-regression"><span class="nav-number">2.5.2.</span> <span class="nav-text">Feature selection implicitly via regularized regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Can-we-use-regularization-for-feature-selection-3-min"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">Can we use regularization for feature selection?3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Thresholding-ridge-coefficients-4-min"><span class="nav-number">2.5.2.2.</span> <span class="nav-text">Thresholding ridge coefficients?4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-lasso-objective-and-its-coefficient-path7-min"><span class="nav-number">2.5.2.3.</span> <span class="nav-text">The lasso objective and its coefficient path7 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Geometric-intuition-for-sparsity-of-lasso-solutions"><span class="nav-number">2.5.3.</span> <span class="nav-text">Geometric intuition for sparsity of lasso solutions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Visualizing-the-ridge-cost7-min"><span class="nav-number">2.5.3.1.</span> <span class="nav-text">Visualizing the ridge cost7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Visualizing-the-ridge-solution6-min"><span class="nav-number">2.5.3.2.</span> <span class="nav-text">Visualizing the ridge solution6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Visualizing-the-lasso-cost-and-solution7-min"><span class="nav-number">2.5.3.3.</span> <span class="nav-text">Visualizing the lasso cost and solution7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Download-the-notebook-and-follow-along10-min-2"><span class="nav-number">2.5.3.4.</span> <span class="nav-text">Download the notebook and follow along10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lasso-demo5-min"><span class="nav-number">2.5.3.5.</span> <span class="nav-text">Lasso demo5 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Setting-the-stage-for-solving-the-lasso"><span class="nav-number">2.5.4.</span> <span class="nav-text">Setting the stage for solving the lasso</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-makes-the-lasso-objective-different3-min"><span class="nav-number">2.5.4.1.</span> <span class="nav-text">What makes the lasso objective different3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Coordinate-descent5-min"><span class="nav-number">2.5.4.2.</span> <span class="nav-text">Coordinate descent5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Normalizing-features3-min"><span class="nav-number">2.5.4.3.</span> <span class="nav-text">Normalizing features3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Coordinate-descent-for-least-squares-regression-normalized-features-8-min"><span class="nav-number">2.5.4.4.</span> <span class="nav-text">Coordinate descent for least squares regression (normalized features)8 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimizing-the-lasso-objective"><span class="nav-number">2.5.5.</span> <span class="nav-text">Optimizing the lasso objective</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Coordinate-descent-for-lasso-normalized-features-5-min"><span class="nav-number">2.5.5.1.</span> <span class="nav-text">Coordinate descent for lasso (normalized features)5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Assessing-convergence-and-other-lasso-solvers2-min"><span class="nav-number">2.5.5.2.</span> <span class="nav-text">Assessing convergence and other lasso solvers2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Coordinate-descent-for-lasso-unnormalized-features-1-min"><span class="nav-number">2.5.5.3.</span> <span class="nav-text">Coordinate descent for lasso (unnormalized features)1 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OPTIONAL-ADVANCED-MATERIAL-Deriving-the-lasso-coordinate-descent-update"><span class="nav-number">2.5.6.</span> <span class="nav-text">OPTIONAL ADVANCED MATERIAL: Deriving the lasso coordinate descent update</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Deriving-the-lasso-coordinate-descent-update19-min"><span class="nav-number">2.5.6.1.</span> <span class="nav-text">Deriving the lasso coordinate descent update19 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tying-up-loose-ends"><span class="nav-number">2.5.7.</span> <span class="nav-text">Tying up loose ends</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing-the-penalty-strength-and-other-practical-issues-with-lasso5-min"><span class="nav-number">2.5.7.1.</span> <span class="nav-text">Choosing the penalty strength and other practical issues with lasso5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-brief-recap3-min"><span class="nav-number">2.5.7.2.</span> <span class="nav-text">A brief recap3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Feature-Selection-and-Lasso7-questions"><span class="nav-number">2.5.7.3.</span> <span class="nav-text">Quiz: Feature Selection and Lasso7 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-1-1"><span class="nav-number">2.5.8.</span> <span class="nav-text">Programming Assignment 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Using-LASSO-to-select-features10-min"><span class="nav-number">2.5.8.1.</span> <span class="nav-text">Reading: Using LASSO to select features10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Using-LASSO-to-select-features6-questions"><span class="nav-number">2.5.8.2.</span> <span class="nav-text">Quiz: Using LASSO to select features6 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-2-1"><span class="nav-number">2.5.9.</span> <span class="nav-text">Programming Assignment 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reading-Implementing-LASSO-using-coordinate-descent10-min"><span class="nav-number">2.5.9.1.</span> <span class="nav-text">Reading: Implementing LASSO using coordinate descent10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Implementing-LASSO-using-coordinate-descent8-questions"><span class="nav-number">2.5.9.2.</span> <span class="nav-text">Quiz: Implementing LASSO using coordinate descent8 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-6"><span class="nav-number">2.6.</span> <span class="nav-text">Week 6</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Nearest-Neighbors-amp-Kernel-Regression"><span class="nav-number">2.6.1.</span> <span class="nav-text">Nearest Neighbors & Kernel Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivating-local-fits"><span class="nav-number">2.6.1.1.</span> <span class="nav-text">Motivating local fits</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-10"><span class="nav-number">2.6.1.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Limitations-of-parametric-regression3-min"><span class="nav-number">2.6.1.1.2.</span> <span class="nav-text">Limitations of parametric regression3 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Nearest-neighbor-regression"><span class="nav-number">2.6.1.2.</span> <span class="nav-text">Nearest neighbor regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Nearest-neighbor-regression-approach8-min"><span class="nav-number">2.6.1.2.1.</span> <span class="nav-text">1-Nearest neighbor regression approach8 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Distance-metrics4-min"><span class="nav-number">2.6.1.2.2.</span> <span class="nav-text">Distance metrics4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Nearest-neighbor-algorithm3-min"><span class="nav-number">2.6.1.2.3.</span> <span class="nav-text">1-Nearest neighbor algorithm3 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-Nearest-neighbors-and-weighted-k-nearest-neighbors"><span class="nav-number">2.6.1.3.</span> <span class="nav-text">k-Nearest neighbors and weighted k-nearest neighbors</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#k-Nearest-neighbors-regression7-min"><span class="nav-number">2.6.1.3.1.</span> <span class="nav-text">k-Nearest neighbors regression7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#k-Nearest-neighbors-in-practice3-min"><span class="nav-number">2.6.1.3.2.</span> <span class="nav-text">k-Nearest neighbors in practice3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Weighted-k-nearest-neighbors4-min"><span class="nav-number">2.6.1.3.3.</span> <span class="nav-text">Weighted k-nearest neighbors4 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kernel-regression"><span class="nav-number">2.6.1.4.</span> <span class="nav-text">Kernel regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#From-weighted-k-NN-to-kernel-regression6-min"><span class="nav-number">2.6.1.4.1.</span> <span class="nav-text">From weighted k-NN to kernel regression6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Global-fits-of-parametric-models-vs-local-fits-of-kernel-regression6-min"><span class="nav-number">2.6.1.4.2.</span> <span class="nav-text">Global fits of parametric models vs. local fits of kernel regression6 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-NN-and-kernel-regression-wrapup"><span class="nav-number">2.6.1.5.</span> <span class="nav-text">k-NN and kernel regression wrapup</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Performance-of-NN-as-amount-of-data-grows7-min"><span class="nav-number">2.6.1.5.1.</span> <span class="nav-text">Performance of NN as amount of data grows7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Issues-with-high-dimensions-data-scarcity-and-computational-complexity3-min"><span class="nav-number">2.6.1.5.2.</span> <span class="nav-text">Issues with high-dimensions, data scarcity, and computational complexity3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#k-NN-for-classification1-min"><span class="nav-number">2.6.1.5.3.</span> <span class="nav-text">k-NN for classification1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A-brief-recap1-min-4"><span class="nav-number">2.6.1.5.4.</span> <span class="nav-text">A brief recap1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Nearest-Neighbors-amp-Kernel-Regression7-questions"><span class="nav-number">2.6.1.5.5.</span> <span class="nav-text">Quiz: Nearest Neighbors & Kernel Regression7 questions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment"><span class="nav-number">2.6.1.6.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Reading-Predicting-house-prices-using-k-nearest-neighbors-regression10-min"><span class="nav-number">2.6.1.6.1.</span> <span class="nav-text">Reading: Predicting house prices using k-nearest neighbors regression10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Predicting-house-prices-using-k-nearest-neighbors-regression8-questions"><span class="nav-number">2.6.1.6.2.</span> <span class="nav-text">Quiz: Predicting house prices using k-nearest neighbors regression8 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Closing-Remarks"><span class="nav-number">2.6.2.</span> <span class="nav-text">Closing Remarks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-we’ve-learned"><span class="nav-number">2.6.2.1.</span> <span class="nav-text">What we’ve learned</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-11"><span class="nav-number">2.6.2.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Simple-and-multiple-regression4-min"><span class="nav-number">2.6.2.1.2.</span> <span class="nav-text">Simple and multiple regression4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Assessing-performance-and-ridge-regression7-min"><span class="nav-number">2.6.2.1.3.</span> <span class="nav-text">Assessing performance and ridge regression7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Feature-selection-lasso-and-nearest-neighbor-regression4-min"><span class="nav-number">2.6.2.1.4.</span> <span class="nav-text">Feature selection, lasso, and nearest neighbor regression4 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summary-and-what’s-ahead-in-the-specialization"><span class="nav-number">2.6.2.2.</span> <span class="nav-text">Summary and what’s ahead in the specialization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#What-we-covered-and-what-we-didn’t-cover5-min"><span class="nav-number">2.6.2.2.1.</span> <span class="nav-text">What we covered and what we didn’t cover5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Thank-you-1-min"><span class="nav-number">2.6.2.2.2.</span> <span class="nav-text">Thank you!1 min</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning-Classification"><span class="nav-number">3.</span> <span class="nav-text">Machine Learning: Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-1-1"><span class="nav-number">3.1.</span> <span class="nav-text">Week 1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Welcome-1"><span class="nav-number">3.1.1.</span> <span class="nav-text">Welcome !</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Welcome-to-the-course"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">Welcome to the course</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Important-Update-regarding-the-Machine-Learning-Specialization-10-min"><span class="nav-number">3.1.1.1.1.</span> <span class="nav-text">Important Update regarding the Machine Learning Specialization 10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module-10-min"><span class="nav-number">3.1.1.1.2.</span> <span class="nav-text">Slides presented in this module 10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Welcome-to-the-classification-course-a-part-of-the-Machine-Learning-Specialization-1-min"><span class="nav-number">3.1.1.1.3.</span> <span class="nav-text">Welcome to the classification course, a part of the Machine Learning Specialization 1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#What-is-this-course-about-6-min"><span class="nav-number">3.1.1.1.4.</span> <span class="nav-text">What is this course about? 6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Impact-of-classification-1-min"><span class="nav-number">3.1.1.1.5.</span> <span class="nav-text">Impact of classification 1 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Course-overview-and-details"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">Course overview and details</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Course-overview-3-min"><span class="nav-number">3.1.1.2.1.</span> <span class="nav-text">Course overview 3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Outline-of-first-half-of-course-5-min"><span class="nav-number">3.1.1.2.2.</span> <span class="nav-text">Outline of first half of course 5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Outline-of-second-half-of-course-5-min"><span class="nav-number">3.1.1.2.3.</span> <span class="nav-text">Outline of second half of course 5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Assumed-background-3-min"><span class="nav-number">3.1.1.2.4.</span> <span class="nav-text">Assumed background 3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Let’s-get-started-45-sec"><span class="nav-number">3.1.1.2.5.</span> <span class="nav-text">Let’s get started! 45 sec</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Reading-Software-tools-you’ll-need-10-min"><span class="nav-number">3.1.1.2.6.</span> <span class="nav-text">Reading: Software tools you’ll need 10 min</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Software-tools-you’ll-need-for-this-course"><span class="nav-number">3.1.1.2.6.1.</span> <span class="nav-text">Software tools you’ll need for this course</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Programming-assignment-format"><span class="nav-number">3.1.1.2.6.2.</span> <span class="nav-text">Programming assignment format</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Why-Python"><span class="nav-number">3.1.1.2.6.3.</span> <span class="nav-text">Why Python</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Useful-software-tools"><span class="nav-number">3.1.1.2.6.4.</span> <span class="nav-text">Useful software tools</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Upgrade-GraphLab-Create"><span class="nav-number">3.1.1.2.6.5.</span> <span class="nav-text">Upgrade GraphLab Create</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Resources"><span class="nav-number">3.1.1.2.6.6.</span> <span class="nav-text">Resources</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Installing-the-recommended-software-tools"><span class="nav-number">3.1.1.2.6.7.</span> <span class="nav-text">Installing the recommended software tools</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Github-repository-with-starter-code"><span class="nav-number">3.1.1.2.6.8.</span> <span class="nav-text">Github repository with starter code</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Using-other-software-packages"><span class="nav-number">3.1.1.2.6.9.</span> <span class="nav-text">Using other software packages</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Classifiers-amp-Logistic-Regression"><span class="nav-number">3.1.2.</span> <span class="nav-text">Linear Classifiers & Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-classifiers"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">Linear classifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module-10-min-1"><span class="nav-number">3.1.2.1.1.</span> <span class="nav-text">Slides presented in this module 10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Linear-classifiers-A-motivating-example-2-min"><span class="nav-number">3.1.2.1.2.</span> <span class="nav-text">Linear classifiers: A motivating example 2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Intuition-behind-linear-classifiers-3-min"><span class="nav-number">3.1.2.1.3.</span> <span class="nav-text">Intuition behind linear classifiers 3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Decision-boundaries-3-min"><span class="nav-number">3.1.2.1.4.</span> <span class="nav-text">Decision boundaries 3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Linear-classifier-model-5-min"><span class="nav-number">3.1.2.1.5.</span> <span class="nav-text">Linear classifier model 5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Effect-of-coefficient-values-on-decision-boundary-2-min"><span class="nav-number">3.1.2.1.6.</span> <span class="nav-text">Effect of coefficient values on decision boundary 2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Using-features-of-the-inputs-2-min"><span class="nav-number">3.1.2.1.7.</span> <span class="nav-text">Using features of the inputs 2 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Class-probabilities"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">Class probabilities</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Predicting-class-probabilities-1-min"><span class="nav-number">3.1.2.2.1.</span> <span class="nav-text">Predicting class probabilities 1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Review-of-basics-of-probabilities-6-min"><span class="nav-number">3.1.2.2.2.</span> <span class="nav-text">Review of basics of probabilities 6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Review-of-basics-of-conditional-probabilities-8-min"><span class="nav-number">3.1.2.2.3.</span> <span class="nav-text">Review of basics of conditional probabilities 8 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Using-probabilities-in-classification-2-min"><span class="nav-number">3.1.2.2.4.</span> <span class="nav-text">Using probabilities in classification 2 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic-regression"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">Logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Predicting-class-probabilities-with-generalized-linear-models-5-min"><span class="nav-number">3.1.2.3.1.</span> <span class="nav-text">Predicting class probabilities with (generalized) linear models 5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#The-sigmoid-or-logistic-link-function-4-min"><span class="nav-number">3.1.2.3.2.</span> <span class="nav-text">The sigmoid (or logistic) link function 4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Logistic-regression-model-5-min"><span class="nav-number">3.1.2.3.3.</span> <span class="nav-text">Logistic regression model 5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Effect-of-coefficient-values-on-predicted-probabilities-7-min"><span class="nav-number">3.1.2.3.4.</span> <span class="nav-text">Effect of coefficient values on predicted probabilities 7 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Overview-of-learning-logistic-regression-models-2-min"><span class="nav-number">3.1.2.3.5.</span> <span class="nav-text">Overview of learning logistic regression models 2 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Practical-issues-for-classification"><span class="nav-number">3.1.2.4.</span> <span class="nav-text">Practical issues for classification</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Encoding-categorical-inputs-4-min"><span class="nav-number">3.1.2.4.1.</span> <span class="nav-text">Encoding categorical inputs 4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multiclass-classification-with-1-versus-all-7-min"><span class="nav-number">3.1.2.4.2.</span> <span class="nav-text">Multiclass classification with 1 versus all 7 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summarizing-linear-classifiers-amp-logistic-regression"><span class="nav-number">3.1.2.5.</span> <span class="nav-text">Summarizing linear classifiers & logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Recap-of-logistic-regression-classifier-1-min"><span class="nav-number">3.1.2.5.1.</span> <span class="nav-text">Recap of logistic regression classifier 1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Linear-Classifiers-amp-Logistic-Regression-5-questions"><span class="nav-number">3.1.2.5.2.</span> <span class="nav-text">Quiz: Linear Classifiers & Logistic Regression 5 questions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment-1"><span class="nav-number">3.1.2.6.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Predicting-sentiment-from-product-reviews-10-min"><span class="nav-number">3.1.2.6.1.</span> <span class="nav-text">Predicting sentiment from product reviews 10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Predicting-sentiment-from-product-reviews-12-questions"><span class="nav-number">3.1.2.6.2.</span> <span class="nav-text">Quiz: Predicting sentiment from product reviews 12 questions</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-2"><span class="nav-number">3.2.</span> <span class="nav-text">Week 2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Linear-Classifiers"><span class="nav-number">3.2.1.</span> <span class="nav-text">Learning Linear Classifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Maximum-likelihood-estimation"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">Maximum likelihood estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-12"><span class="nav-number">3.2.1.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Goal-Learning-parameters-of-logistic-regression2-min"><span class="nav-number">3.2.1.1.2.</span> <span class="nav-text">Goal: Learning parameters of logistic regression2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Intuition-behind-maximum-likelihood-estimation4-min"><span class="nav-number">3.2.1.1.3.</span> <span class="nav-text">Intuition behind maximum likelihood estimation4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Data-likelihood8-min"><span class="nav-number">3.2.1.1.4.</span> <span class="nav-text">Data likelihood8 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Finding-best-linear-classifier-with-gradient-ascent3-min"><span class="nav-number">3.2.1.1.5.</span> <span class="nav-text">Finding best linear classifier with gradient ascent3 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-ascent-algorithm-for-learning-logistic-regression-classifier"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">Gradient ascent algorithm for learning logistic regression classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Review-of-gradient-ascent6-min"><span class="nav-number">3.2.1.2.1.</span> <span class="nav-text">Review of gradient ascent6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-algorithm-for-logistic-regression3-min"><span class="nav-number">3.2.1.2.2.</span> <span class="nav-text">Learning algorithm for logistic regression3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Example-of-computing-derivative-for-logistic-regression5-min"><span class="nav-number">3.2.1.2.3.</span> <span class="nav-text">Example of computing derivative for logistic regression5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Interpreting-derivative-for-logistic-regression5-min"><span class="nav-number">3.2.1.2.4.</span> <span class="nav-text">Interpreting derivative for logistic regression5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Summary-of-gradient-ascent-for-logistic-regression2-min"><span class="nav-number">3.2.1.2.5.</span> <span class="nav-text">Summary of gradient ascent for logistic regression2 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing-step-size-for-gradient-ascent-descent"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">Choosing step size for gradient ascent/descent</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Choosing-step-size5-min"><span class="nav-number">3.2.1.3.1.</span> <span class="nav-text">Choosing step size5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Careful-with-step-sizes-that-are-too-large4-min"><span class="nav-number">3.2.1.3.2.</span> <span class="nav-text">Careful with step sizes that are too large4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Rule-of-thumb-for-choosing-step-size3-min"><span class="nav-number">3.2.1.3.3.</span> <span class="nav-text">Rule of thumb for choosing step size3 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VERY-OPTIONAL-LESSON-Deriving-gradient-of-logistic-regression"><span class="nav-number">3.2.1.4.</span> <span class="nav-text">(VERY OPTIONAL LESSON) Deriving gradient of logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#VERY-OPTIONAL-Deriving-gradient-of-logistic-regression-Log-trick4-min"><span class="nav-number">3.2.1.4.1.</span> <span class="nav-text">(VERY OPTIONAL) Deriving gradient of logistic regression: Log trick4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#VERY-OPTIONAL-Expressing-the-log-likelihood3-min"><span class="nav-number">3.2.1.4.2.</span> <span class="nav-text">(VERY OPTIONAL) Expressing the log-likelihood3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#VERY-OPTIONAL-Deriving-probability-y-1-given-x2-min"><span class="nav-number">3.2.1.4.3.</span> <span class="nav-text">(VERY OPTIONAL) Deriving probability y=-1 given x2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#VERY-OPTIONAL-Rewriting-the-log-likelihood-into-a-simpler-form8-min"><span class="nav-number">3.2.1.4.4.</span> <span class="nav-text">(VERY OPTIONAL) Rewriting the log likelihood into a simpler form8 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#VERY-OPTIONAL-Deriving-gradient-of-log-likelihood8-min"><span class="nav-number">3.2.1.4.5.</span> <span class="nav-text">(VERY OPTIONAL) Deriving gradient of log likelihood8 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summarizing-learning-linear-classifiers"><span class="nav-number">3.2.1.5.</span> <span class="nav-text">Summarizing learning linear classifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Recap-of-learning-logistic-regression-classifiers1-min"><span class="nav-number">3.2.1.5.1.</span> <span class="nav-text">Recap of learning logistic regression classifiers1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Learning-Linear-Classifiers6-questions"><span class="nav-number">3.2.1.5.2.</span> <span class="nav-text">Quiz: Learning Linear Classifiers6 questions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment-2"><span class="nav-number">3.2.1.6.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Implementing-logistic-regression-from-scratch10-min"><span class="nav-number">3.2.1.6.1.</span> <span class="nav-text">Implementing logistic regression from scratch10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Implementing-logistic-regression-from-scratch8-questions"><span class="nav-number">3.2.1.6.2.</span> <span class="nav-text">Quiz: Implementing logistic regression from scratch8 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Overfitting-amp-Regularization-in-Logistic-Regression"><span class="nav-number">3.2.2.</span> <span class="nav-text">Overfitting & Regularization in Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-in-classification"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">Overfitting in classification</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-13"><span class="nav-number">3.2.2.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Evaluating-a-classifier-3-min"><span class="nav-number">3.2.2.1.2.</span> <span class="nav-text">Evaluating a classifier 3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Review-of-overfitting-in-regression3-min"><span class="nav-number">3.2.2.1.3.</span> <span class="nav-text">Review of overfitting in regression3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Overfitting-in-classification5-min"><span class="nav-number">3.2.2.1.4.</span> <span class="nav-text">Overfitting in classification5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Visualizing-overfitting-with-high-degree-polynomial-features3-min"><span class="nav-number">3.2.2.1.5.</span> <span class="nav-text">Visualizing overfitting with high-degree polynomial features3 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Overconfident-predictions-due-to-overfitting"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">Overconfident predictions due to overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Overfitting-in-classifiers-leads-to-overconfident-predictions5-min"><span class="nav-number">3.2.2.2.1.</span> <span class="nav-text">Overfitting in classifiers leads to overconfident predictions5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Visualizing-overconfident-predictions4-min"><span class="nav-number">3.2.2.2.2.</span> <span class="nav-text">Visualizing overconfident predictions4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#OPTIONAL-Another-perspecting-on-overfitting-in-logistic-regression8-min"><span class="nav-number">3.2.2.2.3.</span> <span class="nav-text">(OPTIONAL) Another perspecting on overfitting in logistic regression8 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-regularized-logistic-regression"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">L2 regularized logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Penalizing-large-coefficients-to-mitigate-overfitting5-min"><span class="nav-number">3.2.2.3.1.</span> <span class="nav-text">Penalizing large coefficients to mitigate overfitting5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#L2-regularized-logistic-regression4-min"><span class="nav-number">3.2.2.3.2.</span> <span class="nav-text">L2 regularized logistic regression4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Visualizing-effect-of-L2-regularization-in-logistic-regression5-min"><span class="nav-number">3.2.2.3.3.</span> <span class="nav-text">Visualizing effect of L2 regularization in logistic regression5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-L2-regularized-logistic-regression-with-gradient-ascent7-min"><span class="nav-number">3.2.2.3.4.</span> <span class="nav-text">Learning L2 regularized logistic regression with gradient ascent7 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sparse-logistic-regression"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">Sparse logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Sparse-logistic-regression-with-L1-regularization7-min"><span class="nav-number">3.2.2.4.1.</span> <span class="nav-text">Sparse logistic regression with L1 regularization7 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summarizing-overfitting-amp-regularization-in-logistic-regression"><span class="nav-number">3.2.2.5.</span> <span class="nav-text">Summarizing overfitting & regularization in logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Recap-of-overfitting-amp-regularization-in-logistic-regression58-sec"><span class="nav-number">3.2.2.5.1.</span> <span class="nav-text">Recap of overfitting & regularization in logistic regression58 sec</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Overfitting-amp-Regularization-in-Logistic-Regression8-questions"><span class="nav-number">3.2.2.5.2.</span> <span class="nav-text">Quiz: Overfitting & Regularization in Logistic Regression8 questions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment-3"><span class="nav-number">3.2.2.6.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Logistic-Regression-with-L2-regularization10-min"><span class="nav-number">3.2.2.6.1.</span> <span class="nav-text">Logistic Regression with L2 regularization10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Logistic-Regression-with-L2-regularization8-questions"><span class="nav-number">3.2.2.6.2.</span> <span class="nav-text">Quiz: Logistic Regression with L2 regularization8 questions</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-3-Decision-Trees"><span class="nav-number">3.3.</span> <span class="nav-text">Week 3 Decision Trees</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Intuition-behind-decision-trees"><span class="nav-number">3.3.1.</span> <span class="nav-text">Intuition behind decision trees</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-14"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Predicting-loan-defaults-with-decision-trees3-min"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">Predicting loan defaults with decision trees3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Intuition-behind-decision-trees1-min"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">Intuition behind decision trees1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Task-of-learning-decision-trees-from-data3-min"><span class="nav-number">3.3.1.4.</span> <span class="nav-text">Task of learning decision trees from data3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-decision-trees"><span class="nav-number">3.3.2.</span> <span class="nav-text">Learning decision trees</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recursive-greedy-algorithm4-min"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">Recursive greedy algorithm4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-a-decision-stump3-min"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">Learning a decision stump3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Selecting-best-feature-to-split-on6-min"><span class="nav-number">3.3.2.3.</span> <span class="nav-text">Selecting best feature to split on6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#When-to-stop-recursing4-min"><span class="nav-number">3.3.2.4.</span> <span class="nav-text">When to stop recursing4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Using-the-learned-decision-tree"><span class="nav-number">3.3.3.</span> <span class="nav-text">Using the learned decision tree</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Making-predictions-with-decision-trees1-min"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">Making predictions with decision trees1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiclass-classification-with-decision-trees2-min"><span class="nav-number">3.3.3.2.</span> <span class="nav-text">Multiclass classification with decision trees2 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-decision-trees-with-continuous-inputs"><span class="nav-number">3.3.4.</span> <span class="nav-text">Learning decision trees with continuous inputs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Threshold-splits-for-continuous-inputs6-min"><span class="nav-number">3.3.4.1.</span> <span class="nav-text">Threshold splits for continuous inputs6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-Picking-the-best-threshold-to-split-on3-min"><span class="nav-number">3.3.4.2.</span> <span class="nav-text">(OPTIONAL) Picking the best threshold to split on3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Visualizing-decision-boundaries5-min"><span class="nav-number">3.3.4.3.</span> <span class="nav-text">Visualizing decision boundaries5 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-decision-trees"><span class="nav-number">3.3.5.</span> <span class="nav-text">Summarizing decision trees</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recap-of-decision-trees56-sec"><span class="nav-number">3.3.5.1.</span> <span class="nav-text">Recap of decision trees56 sec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Decision-Trees11-questions"><span class="nav-number">3.3.5.2.</span> <span class="nav-text">Quiz: Decision Trees11 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-1-2"><span class="nav-number">3.3.6.</span> <span class="nav-text">Programming Assignment 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Identifying-safe-loans-with-decision-trees10-min"><span class="nav-number">3.3.6.1.</span> <span class="nav-text">Identifying safe loans with decision trees10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Identifying-safe-loans-with-decision-trees7-questions"><span class="nav-number">3.3.6.2.</span> <span class="nav-text">Quiz: Identifying safe loans with decision trees7 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-2-2"><span class="nav-number">3.3.7.</span> <span class="nav-text">Programming Assignment 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementing-binary-decision-trees10-min"><span class="nav-number">3.3.7.1.</span> <span class="nav-text">Implementing binary decision trees10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Implementing-binary-decision-trees7-questions"><span class="nav-number">3.3.7.2.</span> <span class="nav-text">Quiz: Implementing binary decision trees7 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-4"><span class="nav-number">3.4.</span> <span class="nav-text">Week 4</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Preventing-Overfitting-in-Decision-Trees"><span class="nav-number">3.4.1.</span> <span class="nav-text">Preventing Overfitting in Decision Trees</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-in-decision-trees"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">Overfitting in decision trees</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-15"><span class="nav-number">3.4.1.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#A-review-of-overfitting2-min"><span class="nav-number">3.4.1.1.2.</span> <span class="nav-text">A review of overfitting2 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Overfitting-in-decision-trees5-min"><span class="nav-number">3.4.1.1.3.</span> <span class="nav-text">Overfitting in decision trees5 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Early-stopping-to-avoid-overfitting"><span class="nav-number">3.4.1.2.</span> <span class="nav-text">Early stopping to avoid overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Principle-of-Occam’s-razor-Learning-simpler-decision-trees5-min"><span class="nav-number">3.4.1.2.1.</span> <span class="nav-text">Principle of Occam’s razor: Learning simpler decision trees5 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Early-stopping-in-learning-decision-trees6-min"><span class="nav-number">3.4.1.2.2.</span> <span class="nav-text">Early stopping in learning decision trees6 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-LESSON-Pruning-decision-trees"><span class="nav-number">3.4.1.3.</span> <span class="nav-text">(OPTIONAL LESSON) Pruning decision trees</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#OPTIONAL-Motivating-pruning8-min"><span class="nav-number">3.4.1.3.1.</span> <span class="nav-text">(OPTIONAL) Motivating pruning8 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#OPTIONAL-Pruning-decision-trees-to-avoid-overfitting6-min"><span class="nav-number">3.4.1.3.2.</span> <span class="nav-text">(OPTIONAL) Pruning decision trees to avoid overfitting6 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#OPTIONAL-Tree-pruning-algorithm3-min"><span class="nav-number">3.4.1.3.3.</span> <span class="nav-text">(OPTIONAL) Tree pruning algorithm3 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summarizing-preventing-overfitting-in-decision-trees"><span class="nav-number">3.4.1.4.</span> <span class="nav-text">Summarizing preventing overfitting in decision trees</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Recap-of-overfitting-and-regularization-in-decision-trees1-min"><span class="nav-number">3.4.1.4.1.</span> <span class="nav-text">Recap of overfitting and regularization in decision trees1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Preventing-Overfitting-in-Decision-Trees11-questions"><span class="nav-number">3.4.1.4.2.</span> <span class="nav-text">Quiz: Preventing Overfitting in Decision Trees11 questions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment-4"><span class="nav-number">3.4.1.5.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Decision-Trees-in-Practice10-min"><span class="nav-number">3.4.1.5.1.</span> <span class="nav-text">Decision Trees in Practice10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Decision-Trees-in-Practice14-questions"><span class="nav-number">3.4.1.5.2.</span> <span class="nav-text">Quiz: Decision Trees in Practice14 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Handling-Missing-Data"><span class="nav-number">3.4.2.</span> <span class="nav-text">Handling Missing Data</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic-strategies-for-handling-missing-data"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">Basic strategies for handling missing data</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Slides-presented-in-this-module10-min-16"><span class="nav-number">3.4.2.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Challenge-of-missing-data3-min"><span class="nav-number">3.4.2.1.2.</span> <span class="nav-text">Challenge of missing data3 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Strategy-1-Purification-by-skipping-missing-data4-min"><span class="nav-number">3.4.2.1.3.</span> <span class="nav-text">Strategy 1: Purification by skipping missing data4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Strategy-2-Purification-by-imputing-missing-data4-min"><span class="nav-number">3.4.2.1.4.</span> <span class="nav-text">Strategy 2: Purification by imputing missing data4 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Strategy-3-Modify-learning-algorithm-to-explicitly-handle-missing-data"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">Strategy 3: Modify learning algorithm to explicitly handle missing data</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Modifying-decision-trees-to-handle-missing-data4-min"><span class="nav-number">3.4.2.2.1.</span> <span class="nav-text">Modifying decision trees to handle missing data4 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Feature-split-selection-with-missing-data5-min"><span class="nav-number">3.4.2.2.2.</span> <span class="nav-text">Feature split selection with missing data5 min</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summarizing-handling-missing-data"><span class="nav-number">3.4.2.3.</span> <span class="nav-text">Summarizing handling missing data</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Recap-of-handling-missing-data1-min"><span class="nav-number">3.4.2.3.1.</span> <span class="nav-text">Recap of handling missing data1 min</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Quiz-Handling-Missing-Data7-questions"><span class="nav-number">3.4.2.3.2.</span> <span class="nav-text">Quiz: Handling Missing Data7 questions</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-5-Boosting"><span class="nav-number">3.5.</span> <span class="nav-text">Week 5 Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-amazing-idea-of-boosting-a-classifier"><span class="nav-number">3.5.1.</span> <span class="nav-text">The amazing idea of boosting a classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-17"><span class="nav-number">3.5.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-boosting-question3-min"><span class="nav-number">3.5.1.2.</span> <span class="nav-text">The boosting question3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ensemble-classifiers5-min"><span class="nav-number">3.5.1.3.</span> <span class="nav-text">Ensemble classifiers5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Boosting5-min"><span class="nav-number">3.5.1.4.</span> <span class="nav-text">Boosting5 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost"><span class="nav-number">3.5.2.</span> <span class="nav-text">AdaBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AdaBoost-overview3-min"><span class="nav-number">3.5.2.1.</span> <span class="nav-text">AdaBoost overview3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weighted-error4-min"><span class="nav-number">3.5.2.2.</span> <span class="nav-text">Weighted error4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-coefficient-of-each-ensemble-component4-min"><span class="nav-number">3.5.2.3.</span> <span class="nav-text">Computing coefficient of each ensemble component4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reweighing-data-to-focus-on-mistakes4-min"><span class="nav-number">3.5.2.4.</span> <span class="nav-text">Reweighing data to focus on mistakes4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Normalizing-weights2-min"><span class="nav-number">3.5.2.5.</span> <span class="nav-text">Normalizing weights2 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Applying-AdaBoost"><span class="nav-number">3.5.3.</span> <span class="nav-text">Applying AdaBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-of-AdaBoost-in-action5-min"><span class="nav-number">3.5.3.1.</span> <span class="nav-text">Example of AdaBoost in action5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-boosted-decision-stumps-with-AdaBoost4-min"><span class="nav-number">3.5.3.2.</span> <span class="nav-text">Learning boosted decision stumps with AdaBoost4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-1-3"><span class="nav-number">3.5.4.</span> <span class="nav-text">Programming Assignment 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploring-Ensemble-Methods10-min"><span class="nav-number">3.5.4.1.</span> <span class="nav-text">Exploring Ensemble Methods10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Exploring-Ensemble-Methods9-questions"><span class="nav-number">3.5.4.2.</span> <span class="nav-text">Quiz: Exploring Ensemble Methods9 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convergence-and-overfitting-in-boosting"><span class="nav-number">3.5.5.</span> <span class="nav-text">Convergence and overfitting in boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-Boosting-Theorem3-min"><span class="nav-number">3.5.5.1.</span> <span class="nav-text">The Boosting Theorem3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-in-boosting5-min"><span class="nav-number">3.5.5.2.</span> <span class="nav-text">Overfitting in boosting5 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-boosting"><span class="nav-number">3.5.6.</span> <span class="nav-text">Summarizing boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Ensemble-methods-impact-of-boosting-amp-quick-recap4-min"><span class="nav-number">3.5.6.1.</span> <span class="nav-text">Ensemble methods, impact of boosting & quick recap4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Boosting11-questions"><span class="nav-number">3.5.6.2.</span> <span class="nav-text">Quiz:Boosting11 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-2-3"><span class="nav-number">3.5.7.</span> <span class="nav-text">Programming Assignment 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Boosting-a-decision-stump10-min"><span class="nav-number">3.5.7.1.</span> <span class="nav-text">Boosting a decision stump10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Boosting-a-decision-stump5-questions"><span class="nav-number">3.5.7.2.</span> <span class="nav-text">Quiz:Boosting a decision stump5 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-6-Precision-Recall"><span class="nav-number">3.6.</span> <span class="nav-text">Week 6 Precision-Recall</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-use-precision-amp-recall-as-quality-metrics"><span class="nav-number">3.6.1.</span> <span class="nav-text">Why use precision & recall as quality metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-18"><span class="nav-number">3.6.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Case-study-where-accuracy-is-not-best-metric-for-classification3-min"><span class="nav-number">3.6.1.2.</span> <span class="nav-text">Case-study where accuracy is not best metric for classification3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What-is-good-performance-for-a-classifier-3-min"><span class="nav-number">3.6.1.3.</span> <span class="nav-text">What is good performance for a classifier?3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Precision-amp-recall-explained"><span class="nav-number">3.6.2.</span> <span class="nav-text">Precision & recall explained</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Precision-Fraction-of-positive-predictions-that-are-actually-positive5-min"><span class="nav-number">3.6.2.1.</span> <span class="nav-text">Precision: Fraction of positive predictions that are actually positive5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Recall-Fraction-of-positive-data-predicted-to-be-positive3-min"><span class="nav-number">3.6.2.2.</span> <span class="nav-text">Recall: Fraction of positive data predicted to be positive3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-precision-recall-tradeoff"><span class="nav-number">3.6.3.</span> <span class="nav-text">The precision-recall tradeoff</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Precision-recall-extremes2-min"><span class="nav-number">3.6.3.1.</span> <span class="nav-text">Precision-recall extremes2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Trading-off-precision-and-recall4-min"><span class="nav-number">3.6.3.2.</span> <span class="nav-text">Trading off precision and recall4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Precision-recall-curve5-min"><span class="nav-number">3.6.3.3.</span> <span class="nav-text">Precision-recall curve5 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-precision-recall"><span class="nav-number">3.6.4.</span> <span class="nav-text">Summarizing precision-recall</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recap-of-precision-recall1-min"><span class="nav-number">3.6.4.1.</span> <span class="nav-text">Recap of precision-recall1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Precision-Recall9-questions"><span class="nav-number">3.6.4.2.</span> <span class="nav-text">Quiz: Precision-Recall9 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-5"><span class="nav-number">3.6.5.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Exploring-precision-and-recall10-min"><span class="nav-number">3.6.5.1.</span> <span class="nav-text">Exploring precision and recall10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Exploring-precision-and-recall13-questions"><span class="nav-number">3.6.5.2.</span> <span class="nav-text">Quiz: Exploring precision and recall13 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-7-Scaling-to-Huge-Datasets-amp-Online-Learning"><span class="nav-number">3.7.</span> <span class="nav-text">Week 7 Scaling to Huge Datasets & Online Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scaling-ML-to-huge-datasets"><span class="nav-number">3.7.1.</span> <span class="nav-text">Scaling ML to huge datasets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-19"><span class="nav-number">3.7.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-ascent-won’t-scale-to-today’s-huge-datasets3-min"><span class="nav-number">3.7.1.2.</span> <span class="nav-text">Gradient ascent won’t scale to today’s huge datasets3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Timeline-of-scalable-machine-learning-amp-stochastic-gradient4-min"><span class="nav-number">3.7.1.3.</span> <span class="nav-text">Timeline of scalable machine learning & stochastic gradient4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scaling-ML-with-stochastic-gradient"><span class="nav-number">3.7.2.</span> <span class="nav-text">Scaling ML with stochastic gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Why-gradient-ascent-won’t-scale3-min"><span class="nav-number">3.7.2.1.</span> <span class="nav-text">Why gradient ascent won’t scale3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Stochastic-gradient-Learning-one-data-point-at-a-time3-min"><span class="nav-number">3.7.2.2.</span> <span class="nav-text">Stochastic gradient: Learning one data point at a time3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Comparing-gradient-to-stochastic-gradient3-min"><span class="nav-number">3.7.2.3.</span> <span class="nav-text">Comparing gradient to stochastic gradient3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-why-stochastic-gradient-works"><span class="nav-number">3.7.3.</span> <span class="nav-text">Understanding why stochastic gradient works</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Why-would-stochastic-gradient-ever-work-4-min"><span class="nav-number">3.7.3.1.</span> <span class="nav-text">Why would stochastic gradient ever work?4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Convergence-paths2-min"><span class="nav-number">3.7.3.2.</span> <span class="nav-text">Convergence paths2 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stochastic-gradient-Practical-tricks"><span class="nav-number">3.7.4.</span> <span class="nav-text">Stochastic gradient: Practical tricks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Shuffle-data-before-running-stochastic-gradient2-min"><span class="nav-number">3.7.4.1.</span> <span class="nav-text">Shuffle data before running stochastic gradient2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing-step-size3-min"><span class="nav-number">3.7.4.2.</span> <span class="nav-text">Choosing step size3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Don’t-trust-last-coefficients1-min"><span class="nav-number">3.7.4.3.</span> <span class="nav-text">Don’t trust last coefficients1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-Learning-from-batches-of-data3-min"><span class="nav-number">3.7.4.4.</span> <span class="nav-text">(OPTIONAL) Learning from batches of data3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-Measuring-convergence4-min"><span class="nav-number">3.7.4.5.</span> <span class="nav-text">(OPTIONAL) Measuring convergence4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-Adding-regularization3-min"><span class="nav-number">3.7.4.6.</span> <span class="nav-text">(OPTIONAL) Adding regularization3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Online-learning-Fitting-models-from-streaming-data"><span class="nav-number">3.7.5.</span> <span class="nav-text">Online learning: Fitting models from streaming data</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-online-learning-task3-min"><span class="nav-number">3.7.5.1.</span> <span class="nav-text">The online learning task3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Using-stochastic-gradient-for-online-learning3-min"><span class="nav-number">3.7.5.2.</span> <span class="nav-text">Using stochastic gradient for online learning3 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-scaling-to-huge-datasets-amp-online-learning"><span class="nav-number">3.7.6.</span> <span class="nav-text">Summarizing scaling to huge datasets & online learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Scaling-to-huge-datasets-through-parallelization-amp-module-recap1-min"><span class="nav-number">3.7.6.1.</span> <span class="nav-text">Scaling to huge datasets through parallelization & module recap1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Scaling-to-Huge-Datasets-amp-Online-Learning10-questions"><span class="nav-number">3.7.6.2.</span> <span class="nav-text">Quiz: Scaling to Huge Datasets & Online Learning10 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-6"><span class="nav-number">3.7.7.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-Logistic-Regression-via-Stochastic-Gradient-Ascent10-min"><span class="nav-number">3.7.7.1.</span> <span class="nav-text">Training Logistic Regression via Stochastic Gradient Ascent10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Training-Logistic-Regression-via-Stochastic-Gradient-Ascent12-questions"><span class="nav-number">3.7.7.2.</span> <span class="nav-text">Quiz: Training Logistic Regression via Stochastic Gradient Ascent12 questions</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning-Clustering-amp-Retrieval"><span class="nav-number">4.</span> <span class="nav-text">Machine Learning: Clustering & Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-1-Welcome-1"><span class="nav-number">4.1.</span> <span class="nav-text">Week 1 Welcome</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-is-this-course-about-1"><span class="nav-number">4.1.1.</span> <span class="nav-text">What is this course about?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-20"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Welcome-and-introduction-to-clustering-and-retrieval-tasks6-min"><span class="nav-number">4.1.1.2.</span> <span class="nav-text">Welcome and introduction to clustering and retrieval tasks6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Course-overview3-min"><span class="nav-number">4.1.1.3.</span> <span class="nav-text">Course overview3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Module-by-module-topics-covered8-min"><span class="nav-number">4.1.1.4.</span> <span class="nav-text">Module-by-module topics covered8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Assumed-background6-min"><span class="nav-number">4.1.1.5.</span> <span class="nav-text">Assumed background6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Software-tools-you’ll-need-for-this-course10-min"><span class="nav-number">4.1.1.6.</span> <span class="nav-text">Software tools you’ll need for this course10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-big-week-ahead-10-min"><span class="nav-number">4.1.1.7.</span> <span class="nav-text">A big week ahead!10 min</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-2-Nearest-Neighbor-Search"><span class="nav-number">4.2.</span> <span class="nav-text">Week 2 Nearest Neighbor Search</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction-to-nearest-neighbor-search-and-algorithms"><span class="nav-number">4.2.1.</span> <span class="nav-text">Introduction to nearest neighbor search and algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-21"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Retrieval-as-k-nearest-neighbor-search2-min"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">Retrieval as k-nearest neighbor search2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-NN-algorithm2-min"><span class="nav-number">4.2.1.3.</span> <span class="nav-text">1-NN algorithm2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-NN-algorithm6-min"><span class="nav-number">4.2.1.4.</span> <span class="nav-text">k-NN algorithm6 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-importance-of-data-representations-and-distance-metrics"><span class="nav-number">4.2.2.</span> <span class="nav-text">The importance of data representations and distance metrics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Document-representation5-min"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">Document representation5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Distance-metrics-Euclidean-and-scaled-Euclidean6-min"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">Distance metrics: Euclidean and scaled Euclidean6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Writing-scaled-Euclidean-distance-using-weighted-inner-products4-min"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">Writing (scaled) Euclidean distance using (weighted) inner products4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Distance-metrics-Cosine-similarity9-min"><span class="nav-number">4.2.2.4.</span> <span class="nav-text">Distance metrics: Cosine similarity9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#To-normalize-or-not-and-other-distance-considerations6-min"><span class="nav-number">4.2.2.5.</span> <span class="nav-text">To normalize or not and other distance considerations6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Representations-and-metrics6-questions"><span class="nav-number">4.2.2.6.</span> <span class="nav-text">Quiz: Representations and metrics6 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-1-4"><span class="nav-number">4.2.3.</span> <span class="nav-text">Programming Assignment 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing-features-and-metrics-for-nearest-neighbor-search10-min"><span class="nav-number">4.2.3.1.</span> <span class="nav-text">Choosing features and metrics for nearest neighbor search10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Choosing-features-and-metrics-for-nearest-neighbor-search5-questions"><span class="nav-number">4.2.3.2.</span> <span class="nav-text">Quiz: Choosing features and metrics for nearest neighbor search5 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scaling-up-k-NN-search-using-KD-trees"><span class="nav-number">4.2.4.</span> <span class="nav-text">Scaling up k-NN search using KD-trees</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Complexity-of-brute-force-search1-min"><span class="nav-number">4.2.4.1.</span> <span class="nav-text">Complexity of brute force search1 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KD-tree-representation9-min"><span class="nav-number">4.2.4.2.</span> <span class="nav-text">KD-tree representation9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NN-search-with-KD-trees7-min"><span class="nav-number">4.2.4.3.</span> <span class="nav-text">NN search with KD-trees7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Complexity-of-NN-search-with-KD-trees5-min"><span class="nav-number">4.2.4.4.</span> <span class="nav-text">Complexity of NN search with KD-trees5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Visualizing-scaling-behavior-of-KD-trees4-min"><span class="nav-number">4.2.4.5.</span> <span class="nav-text">Visualizing scaling behavior of KD-trees4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approximate-k-NN-search-using-KD-trees7-min"><span class="nav-number">4.2.4.6.</span> <span class="nav-text">Approximate k-NN search using KD-trees7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-A-worked-out-example-for-KD-trees10-min"><span class="nav-number">4.2.4.7.</span> <span class="nav-text">(OPTIONAL) A worked-out example for KD-trees10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-KD-trees5-questions"><span class="nav-number">4.2.4.8.</span> <span class="nav-text">Quiz: KD-trees5 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Locality-sensitive-hashing-for-approximate-NN-search"><span class="nav-number">4.2.5.</span> <span class="nav-text">Locality sensitive hashing for approximate NN search</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Limitations-of-KD-trees3-min"><span class="nav-number">4.2.5.1.</span> <span class="nav-text">Limitations of KD-trees3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSH-as-an-alternative-to-KD-trees4-min"><span class="nav-number">4.2.5.2.</span> <span class="nav-text">LSH as an alternative to KD-trees4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Using-random-lines-to-partition-points5-min"><span class="nav-number">4.2.5.3.</span> <span class="nav-text">Using random lines to partition points5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Defining-more-bins3-min"><span class="nav-number">4.2.5.4.</span> <span class="nav-text">Defining more bins3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Searching-neighboring-bins8-min"><span class="nav-number">4.2.5.5.</span> <span class="nav-text">Searching neighboring bins8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSH-in-higher-dimensions4-min"><span class="nav-number">4.2.5.6.</span> <span class="nav-text">LSH in higher dimensions4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-Improving-efficiency-through-multiple-tables22-min"><span class="nav-number">4.2.5.7.</span> <span class="nav-text">(OPTIONAL) Improving efficiency through multiple tables22 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Locality-Sensitive-Hashing5-questions"><span class="nav-number">4.2.5.8.</span> <span class="nav-text">Quiz: Locality Sensitive Hashing5 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-2-4"><span class="nav-number">4.2.6.</span> <span class="nav-text">Programming Assignment 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementing-Locality-Sensitive-Hashing-from-scratch10-min"><span class="nav-number">4.2.6.1.</span> <span class="nav-text">Implementing Locality Sensitive Hashing from scratch10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Implementing-Locality-Sensitive-Hashing-from-scratch5-questions"><span class="nav-number">4.2.6.2.</span> <span class="nav-text">Quiz: Implementing Locality Sensitive Hashing from scratch5 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-nearest-neighbor-search"><span class="nav-number">4.2.7.</span> <span class="nav-text">Summarizing nearest neighbor search</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-brief-recap2-min"><span class="nav-number">4.2.7.1.</span> <span class="nav-text">A brief recap2 min</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-3-Clustering-with-k-means"><span class="nav-number">4.3.</span> <span class="nav-text">Week 3 Clustering with k-means</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction-to-clustering"><span class="nav-number">4.3.1.</span> <span class="nav-text">Introduction to clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-22"><span class="nav-number">4.3.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-goal-of-clustering3-min"><span class="nav-number">4.3.1.2.</span> <span class="nav-text">The goal of clustering3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#An-unsupervised-task6-min"><span class="nav-number">4.3.1.3.</span> <span class="nav-text">An unsupervised task6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hope-for-unsupervised-learning-and-some-challenge-cases4-min"><span class="nav-number">4.3.1.4.</span> <span class="nav-text">Hope for unsupervised learning, and some challenge cases4 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Clustering-via-k-means"><span class="nav-number">4.3.2.</span> <span class="nav-text">Clustering via k-means</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-k-means-algorithm7-min"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">The k-means algorithm7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#k-means-as-coordinate-descent6-min"><span class="nav-number">4.3.2.2.</span> <span class="nav-text">k-means as coordinate descent6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Smart-initialization-via-k-means-4-min"><span class="nav-number">4.3.2.3.</span> <span class="nav-text">Smart initialization via k-means++4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Assessing-the-quality-and-choosing-the-number-of-clusters9-min"><span class="nav-number">4.3.2.4.</span> <span class="nav-text">Assessing the quality and choosing the number of clusters9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-k-means9-questions"><span class="nav-number">4.3.2.5.</span> <span class="nav-text">Quiz: k-means9 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-7"><span class="nav-number">4.3.3.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Clustering-text-data-with-k-means10-min"><span class="nav-number">4.3.3.1.</span> <span class="nav-text">Clustering text data with k-means10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Clustering-text-data-with-K-means8-questions"><span class="nav-number">4.3.3.2.</span> <span class="nav-text">Quiz: Clustering text data with K-means8 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MapReduce-for-scaling-k-means"><span class="nav-number">4.3.4.</span> <span class="nav-text">MapReduce for scaling k-means</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Motivating-MapReduce8-min"><span class="nav-number">4.3.4.1.</span> <span class="nav-text">Motivating MapReduce8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-general-MapReduce-abstraction5-min"><span class="nav-number">4.3.4.2.</span> <span class="nav-text">The general MapReduce abstraction5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MapReduce-execution-overview-and-combiners6-min"><span class="nav-number">4.3.4.3.</span> <span class="nav-text">MapReduce execution overview and combiners6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MapReduce-for-k-means7-min"><span class="nav-number">4.3.4.4.</span> <span class="nav-text">MapReduce for k-means7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-MapReduce-for-k-means5-questions"><span class="nav-number">4.3.4.5.</span> <span class="nav-text">Quiz: MapReduce for k-means5 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-clustering-with-k-means"><span class="nav-number">4.3.5.</span> <span class="nav-text">Summarizing clustering with k-means</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Other-applications-of-clustering7-min"><span class="nav-number">4.3.5.1.</span> <span class="nav-text">Other applications of clustering7 min</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-4-Mixture-Models"><span class="nav-number">4.4.</span> <span class="nav-text">Week 4 Mixture Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Motivating-and-setting-the-foundation-for-mixture-models"><span class="nav-number">4.4.1.</span> <span class="nav-text">Motivating and setting the foundation for mixture models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-23"><span class="nav-number">4.4.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Motiving-probabilistic-clustering-models8-min"><span class="nav-number">4.4.1.2.</span> <span class="nav-text">Motiving probabilistic clustering models8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Aggregating-over-unknown-classes-in-an-image-dataset6-min"><span class="nav-number">4.4.1.3.</span> <span class="nav-text">Aggregating over unknown classes in an image dataset6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Univariate-Gaussian-distributions2-min"><span class="nav-number">4.4.1.4.</span> <span class="nav-text">Univariate Gaussian distributions2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bivariate-and-multivariate-Gaussians7-min"><span class="nav-number">4.4.1.5.</span> <span class="nav-text">Bivariate and multivariate Gaussians7 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mixtures-of-Gaussians-for-clustering"><span class="nav-number">4.4.2.</span> <span class="nav-text">Mixtures of Gaussians for clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Mixture-of-Gaussians6-min"><span class="nav-number">4.4.2.1.</span> <span class="nav-text">Mixture of Gaussians6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interpreting-the-mixture-of-Gaussian-terms5-min"><span class="nav-number">4.4.2.2.</span> <span class="nav-text">Interpreting the mixture of Gaussian terms5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Scaling-mixtures-of-Gaussians-for-document-clustering5-min"><span class="nav-number">4.4.2.3.</span> <span class="nav-text">Scaling mixtures of Gaussians for document clustering5 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Expectation-Maximization-EM-building-blocks"><span class="nav-number">4.4.3.</span> <span class="nav-text">Expectation Maximization (EM) building blocks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-soft-assignments-from-known-cluster-parameters7-min"><span class="nav-number">4.4.3.1.</span> <span class="nav-text">Computing soft assignments from known cluster parameters7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-Responsibilities-as-Bayes’-rule5-min"><span class="nav-number">4.4.3.2.</span> <span class="nav-text">(OPTIONAL) Responsibilities as Bayes’ rule5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Estimating-cluster-parameters-from-known-cluster-assignments6-min"><span class="nav-number">4.4.3.3.</span> <span class="nav-text">Estimating cluster parameters from known cluster assignments6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Estimating-cluster-parameters-from-soft-assignments8-min"><span class="nav-number">4.4.3.4.</span> <span class="nav-text">Estimating cluster parameters from soft assignments8 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-EM-algorithm"><span class="nav-number">4.4.4.</span> <span class="nav-text">The EM algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#EM-iterates-in-equations-and-pictures6-min"><span class="nav-number">4.4.4.1.</span> <span class="nav-text">EM iterates in equations and pictures6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Convergence-initialization-and-overfitting-of-EM9-min"><span class="nav-number">4.4.4.2.</span> <span class="nav-text">Convergence, initialization, and overfitting of EM9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Relationship-to-k-means3-min"><span class="nav-number">4.4.4.3.</span> <span class="nav-text">Relationship to k-means3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OPTIONAL-A-worked-out-example-for-EM10-min"><span class="nav-number">4.4.4.4.</span> <span class="nav-text">(OPTIONAL) A worked-out example for EM10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-EM-for-Gaussian-mixtures9-questions"><span class="nav-number">4.4.4.5.</span> <span class="nav-text">Quiz: EM for Gaussian mixtures9 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summarizing-mixture-models"><span class="nav-number">4.4.5.</span> <span class="nav-text">Summarizing mixture models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-brief-recap1-min-5"><span class="nav-number">4.4.5.1.</span> <span class="nav-text">A brief recap1 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-1-5"><span class="nav-number">4.4.6.</span> <span class="nav-text">Programming Assignment 1</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementing-EM-for-Gaussian-mixtures10-min"><span class="nav-number">4.4.6.1.</span> <span class="nav-text">Implementing EM for Gaussian mixtures10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Implementing-EM-for-Gaussian-mixtures6-questions"><span class="nav-number">4.4.6.2.</span> <span class="nav-text">Quiz: Implementing EM for Gaussian mixtures6 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-2-5"><span class="nav-number">4.4.7.</span> <span class="nav-text">Programming Assignment 2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Clustering-text-data-with-Gaussian-mixtures10-min"><span class="nav-number">4.4.7.1.</span> <span class="nav-text">Clustering text data with Gaussian mixtures10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Clustering-text-data-with-Gaussian-mixtures4-questions"><span class="nav-number">4.4.7.2.</span> <span class="nav-text">Quiz: Clustering text data with Gaussian mixtures4 questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-5-Mixed-Membership-Modeling-via-Latent-Dirichlet-Allocation"><span class="nav-number">4.5.</span> <span class="nav-text">Week 5 Mixed Membership Modeling via Latent Dirichlet Allocation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction-to-latent-Dirichlet-allocation"><span class="nav-number">4.6.</span> <span class="nav-text">Introduction to latent Dirichlet allocation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Slides-presented-in-this-module10-min-24"><span class="nav-number">4.6.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mixed-membership-models-for-documents3-min"><span class="nav-number">4.6.2.</span> <span class="nav-text">Mixed membership models for documents3 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#An-alternative-document-clustering-model4-min"><span class="nav-number">4.6.3.</span> <span class="nav-text">An alternative document clustering model4 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Components-of-latent-Dirichlet-allocation-model2-min"><span class="nav-number">4.6.4.</span> <span class="nav-text">Components of latent Dirichlet allocation model2 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Goal-of-LDA-inference5-min"><span class="nav-number">4.6.5.</span> <span class="nav-text">Goal of LDA inference5 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quiz-Latent-Dirichlet-Allocation5-questions"><span class="nav-number">4.6.6.</span> <span class="nav-text">Quiz: Latent Dirichlet Allocation5 questions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayesian-inference-via-Gibbs-sampling"><span class="nav-number">4.7.</span> <span class="nav-text">Bayesian inference via Gibbs sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-need-for-Bayesian-inference4-min"><span class="nav-number">4.7.1.</span> <span class="nav-text">The need for Bayesian inference4 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gibbs-sampling-from-10-000-feet5-min"><span class="nav-number">4.7.2.</span> <span class="nav-text">Gibbs sampling from 10,000 feet5 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-standard-Gibbs-sampler-for-LDA9-min"><span class="nav-number">4.7.3.</span> <span class="nav-text">A standard Gibbs sampler for LDA9 min</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Collapsed-Gibbs-sampling-for-LDA"><span class="nav-number">4.8.</span> <span class="nav-text">Collapsed Gibbs sampling for LDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-is-collapsed-Gibbs-sampling-3-min"><span class="nav-number">4.8.1.</span> <span class="nav-text">What is collapsed Gibbs sampling?3 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-worked-example-for-LDA-Initial-setup4-min"><span class="nav-number">4.8.2.</span> <span class="nav-text">A worked example for LDA: Initial setup4 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-worked-example-for-LDA-Deriving-the-resampling-distribution7-min"><span class="nav-number">4.8.3.</span> <span class="nav-text">A worked example for LDA: Deriving the resampling distribution7 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Using-the-output-of-collapsed-Gibbs-sampling4-min"><span class="nav-number">4.8.4.</span> <span class="nav-text">Using the output of collapsed Gibbs sampling4 min</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summarizing-latent-Dirichlet-allocation"><span class="nav-number">4.9.</span> <span class="nav-text">Summarizing latent Dirichlet allocation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-brief-recap1-min-6"><span class="nav-number">4.9.1.</span> <span class="nav-text">A brief recap1 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quiz-Learning-LDA-model-via-Gibbs-sampling10-questions"><span class="nav-number">4.9.2.</span> <span class="nav-text">Quiz: Learning LDA model via Gibbs sampling10 questions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-8"><span class="nav-number">4.10.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Modeling-text-topics-with-Latent-Dirichlet-Allocation10-min"><span class="nav-number">4.10.1.</span> <span class="nav-text">Modeling text topics with Latent Dirichlet Allocation10 min</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quiz-Modeling-text-topics-with-Latent-Dirichlet-Allocation12-questions"><span class="nav-number">4.10.2.</span> <span class="nav-text">Quiz: Modeling text topics with Latent Dirichlet Allocation12 questions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-6-Hierarchical-Clustering-amp-Closing-Remarks"><span class="nav-number">4.11.</span> <span class="nav-text">Week 6 Hierarchical Clustering & Closing Remarks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-we’ve-learned-1"><span class="nav-number">4.11.1.</span> <span class="nav-text">What we’ve learned</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Slides-presented-in-this-module10-min-25"><span class="nav-number">4.11.1.1.</span> <span class="nav-text">Slides presented in this module10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Module-1-recap10-min"><span class="nav-number">4.11.1.2.</span> <span class="nav-text">Module 1 recap10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Module-2-recap3-min"><span class="nav-number">4.11.1.3.</span> <span class="nav-text">Module 2 recap3 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Module-3-recap6-min"><span class="nav-number">4.11.1.4.</span> <span class="nav-text">Module 3 recap6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Module-4-recap7-min"><span class="nav-number">4.11.1.5.</span> <span class="nav-text">Module 4 recap7 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hierarchical-clustering-and-clustering-for-time-series-segmentation"><span class="nav-number">4.11.2.</span> <span class="nav-text">Hierarchical clustering and clustering for time series segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Why-hierarchical-clustering-2-min"><span class="nav-number">4.11.2.1.</span> <span class="nav-text">Why hierarchical clustering?2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Divisive-clustering4-min"><span class="nav-number">4.11.2.2.</span> <span class="nav-text">Divisive clustering4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Agglomerative-clustering2-min"><span class="nav-number">4.11.2.3.</span> <span class="nav-text">Agglomerative clustering2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-dendrogram4-min"><span class="nav-number">4.11.2.4.</span> <span class="nav-text">The dendrogram4 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Agglomerative-clustering-details7-min"><span class="nav-number">4.11.2.5.</span> <span class="nav-text">Agglomerative clustering details7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hidden-Markov-models9-min"><span class="nav-number">4.11.2.6.</span> <span class="nav-text">Hidden Markov models9 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Programming-Assignment-9"><span class="nav-number">4.11.3.</span> <span class="nav-text">Programming Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Modeling-text-data-with-a-hierarchy-of-clusters10-min"><span class="nav-number">4.11.3.1.</span> <span class="nav-text">Modeling text data with a hierarchy of clusters10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Modeling-text-data-with-a-hierarchy-of-clusters3-questions"><span class="nav-number">4.11.3.2.</span> <span class="nav-text">Quiz: Modeling text data with a hierarchy of clusters3 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-and-what’s-ahead-in-the-specialization-1"><span class="nav-number">4.11.4.</span> <span class="nav-text">Summary and what’s ahead in the specialization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-we-didn’t-cover2-min"><span class="nav-number">4.11.4.1.</span> <span class="nav-text">What we didn’t cover2 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Thank-you-1-min-1"><span class="nav-number">4.11.4.2.</span> <span class="nav-text">Thank you!1 min</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SSQ</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://SSQ.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://ssq.github.io/2017/08/19/Coursera UW Machine Learning Specialization Notebook/';
          this.page.identifier = '2017/08/19/Coursera UW Machine Learning Specialization Notebook/';
          this.page.title = 'Coursera UW Machine Learning Specialization Notebook';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://SSQ.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (search_path.endsWith("json")) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("MWlUL7cwf0gV4nd1BczDGmFm-gzGzoHsz", "YwuYEA1xBo0rm9hzIjUwOm2F");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
