<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Coursera,Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="For quick searchingCourse can be found hereVideo in YouTubeLecture Slides can be found in my Github">
<meta name="keywords" content="Coursera,Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera HSE Advanced Machine Learning Specialization">
<meta property="og:url" content="https://ssq.github.io/2017/11/19/Coursera HSE Advanced Machine Learning Specialization/index.html">
<meta property="og:site_name" content="SSQ">
<meta property="og:description" content="For quick searchingCourse can be found hereVideo in YouTubeLecture Slides can be found in my Github">
<meta property="og:updated_time" content="2017-12-09T16:12:10.574Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera HSE Advanced Machine Learning Specialization">
<meta name="twitter:description" content="For quick searchingCourse can be found hereVideo in YouTubeLecture Slides can be found in my Github">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://ssq.github.io/2017/11/19/Coursera HSE Advanced Machine Learning Specialization/"/>





  <title>Coursera HSE Advanced Machine Learning Specialization | SSQ</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-91307065-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3c8ad2ecdd2387b44044b1d7cd3536a9";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SSQ</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-friends">
          <a href="/friends" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-link"></i> <br />
            
            friends
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://ssq.github.io/2017/11/19/Coursera HSE Advanced Machine Learning Specialization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="SSQ">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SSQ">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Coursera HSE Advanced Machine Learning Specialization</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-19T09:39:07+08:00">
                2017-11-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/19/Coursera HSE Advanced Machine Learning Specialization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/19/Coursera HSE Advanced Machine Learning Specialization/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/19/Coursera HSE Advanced Machine Learning Specialization/" class="leancloud_visitors" data-flag-title="Coursera HSE Advanced Machine Learning Specialization">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <div class="note primary"><p>For quick searching<br>Course can be found <a href="https://www.coursera.org/specializations/aml" target="_blank" rel="external">here</a><br>Video in <a href="">YouTube</a><br>Lecture Slides can be found in my Github</p>
</div>
<a id="more"></a>
<div class="note primary"><p><strong>About This Specialization</strong><br>This specialization gives an introduction to deep learning, reinforcement learning, natural language understanding, computer vision and Bayesian methods. Top Kaggle machine learning practitioners and CERN scientists will share their experience of solving real-world problems and help you to fill the gaps between theory and practice. Upon completion of 7 courses you will be able to apply modern machine learning methods in enterprise and understand the caveats of real-world data and settings.</p>
<p><strong>Projects Overview</strong><br>You will master your skills by solving a wide variety of real-world problems like image captioning and automatic game playing throughout the course projects. You will gain the hands-on experience of applying advanced machine learning techniques that provide the foundation to the current state-of-the art in AI.</p>
</div>
<h1 id="Introduction-to-Deep-Learning"><a href="#Introduction-to-Deep-Learning" class="headerlink" title="Introduction to Deep Learning"></a>Introduction to Deep Learning</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/intro-to-deep-learning" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="">here</a></p>
<p><strong>About this course:</strong> The goal of this course is to give learners basic understanding of modern neural networks and their applications in computer vision and natural language understanding. The course starts with a recap of linear models and discussion of stochastic optimization methods that are crucial for training deep neural networks. Learners will study all popular building blocks of neural networks including fully connected layers, convolutional and recurrent layers.<br>Learners will use these building blocks to define complex modern architectures in TensorFlow and Keras frameworks. In the course project learner will implement deep neural network for the task of image captioning which solves the problem of giving a text description for an input image.</p>
<p>The prerequisites for this course are:<br>1) Basic knowledge of Python.<br>2) Basic linear algebra and probability.</p>
<p>Please note that this is an advanced course and we assume basic knowledge of machine learning. You should understand:<br>1) Linear regression: mean squared error, analytical solution.<br>2) Logistic regression: model, cross-entropy loss, class probability estimation.<br>3) Gradient descent for linear models. Derivatives of MSE and cross-entropy loss functions.<br>4) The problem of overfitting.<br>5) Regularization for linear models.</p>
<p><strong>Who is this class for:</strong> Developers, analysts and researchers who are faced with tasks involving complex structure understanding such as image, sound and text analysis.</p>
</div>
<h2 id="Week-1-Introduction-to-optimization"><a href="#Week-1-Introduction-to-optimization" class="headerlink" title="Week 1 Introduction to optimization"></a>Week 1 Introduction to optimization</h2><div class="note primary"><p>Welcome to the “Introduction to Deep Learning” course! In the first week you’ll learn about linear models and stochatic optimization methods. Linear models are basic building blocks for many deep architectures, and stochastic optimization is used to learn every model that we’ll discuss in our course.</p>
</div>
<div class="note primary"><p><strong>Learning Objectives</strong></p>
<ul>
<li>Train a linear model for classification or regression task using stochastic gradient descent</li>
<li>Tune SGD optimization using different techniques</li>
<li>Apply regularization to train better models</li>
<li>Use linear models for classification and regression tasks</li>
</ul>
</div>
<h3 id="Course-intro"><a href="#Course-intro" class="headerlink" title="Course intro"></a>Course intro</h3><h4 id="Welcome-5-min"><a href="#Welcome-5-min" class="headerlink" title="Welcome!5 min"></a>Welcome!5 min</h4><h3 id="Linear-model-as-the-simplest-neural-network"><a href="#Linear-model-as-the-simplest-neural-network" class="headerlink" title="Linear model as the simplest neural network"></a>Linear model as the simplest neural network</h3><h4 id="Linear-regression-9-min"><a href="#Linear-regression-9-min" class="headerlink" title="Linear regression 9 min"></a>Linear regression 9 min</h4><h4 id="Linear-classification-10-min"><a href="#Linear-classification-10-min" class="headerlink" title="Linear classification 10 min"></a>Linear classification 10 min</h4><h4 id="Gradient-descent-5-min"><a href="#Gradient-descent-5-min" class="headerlink" title="Gradient descent 5 min"></a>Gradient descent 5 min</h4><h4 id="Quiz-Linear-models-3-questions"><a href="#Quiz-Linear-models-3-questions" class="headerlink" title="Quiz: Linear models 3 questions"></a>Quiz: Linear models 3 questions</h4><div class="note primary"><p>QUIZ<br>Linear models<br>3 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 26, 11:59 PM PST</p>
</div>
<p>1 point<br>1.Consider a vector (1,−2,0.5). Apply a softmax transform to it and enter the first component (accurate to 2 decimal places).</p>
<p><input type="”text”" placeholder="0.60"><br><br><br>1 point<br>2.Suppose you are solving a 5-class classification problem with 10 features. How many parameters a linear model would have? Don’t forget bias terms!</p>
<p><input type="”text”" placeholder="55"><br><br><br>1 point<br>3.There is an analytical solution for linear regression parameters and MSE loss, but we usually prefer gradient descent optimization over it. What are the reasons?</p>
<p><input type="checkbox" disabled checked><label></label><br>Gradient descent is more scalable and can be applied for problems with high number of features</p>
<p><input type="checkbox" disabled><label></label><br>Gradient descent is a method developed especially for MSE loss</p>
<p><input type="checkbox" disabled><label></label><br>Gradient descent can find parameter values that give lower MSE value than parameters from analytical solution</p>
<p><input type="checkbox" disabled checked><label></label><br>Gradient descent doesn’t require to invert a matrix<br><br></p>
<h3 id="Regularization-in-machine-learning"><a href="#Regularization-in-machine-learning" class="headerlink" title="Regularization in machine learning"></a>Regularization in machine learning</h3><h4 id="Overfitting-problem-and-model-validation-6-min"><a href="#Overfitting-problem-and-model-validation-6-min" class="headerlink" title="Overfitting problem and model validation 6 min"></a>Overfitting problem and model validation 6 min</h4><h4 id="Model-regularization-5-min"><a href="#Model-regularization-5-min" class="headerlink" title="Model regularization 5 min"></a>Model regularization 5 min</h4><h4 id="Quiz-Overfitting-and-regularization-4-questions"><a href="#Quiz-Overfitting-and-regularization-4-questions" class="headerlink" title="Quiz: Overfitting and regularization 4 questions"></a>Quiz: Overfitting and regularization 4 questions</h4><div class="note primary"><p>QUIZ<br>Overfitting and regularization<br>4 questions<br>To Pass80% or higher<br>Attempts3 every 8 hours<br>Deadline<br>November 26, 11:59 PM PST</p>
</div>
<p>1 point<br>1.Select correct statements about overfitting:</p>
<p><input type="checkbox" disabled checked><label></label><br>Overfitting is a situation where a model gives lower quality for new data compared to quality on a training sample</p>
<p><input type="checkbox" disabled><label></label><br>Overfitting happens when model is too simple for the problem</p>
<p><input type="checkbox" disabled><label></label><br>Overfitting is a situation where a model gives comparable quality on new data and on a training sample</p>
<p><input type="checkbox" disabled checked><label></label><br>Large model weights can indicate that model is overfitted<br><br><br>1 point<br>2.What disadvantages do model validation on holdout sample have?</p>
<p><input type="checkbox" disabled><label></label><br>It requires multiple model fitting</p>
<p><input type="checkbox" disabled checked><label></label><br>It is sensitive to the particular split of the sample into training and test parts</p>
<p><input type="checkbox" disabled checked><label></label><br>It can give biased quality estimates for small samples<br><br><br><div class="note primary"><p>123,1,13,|3,12,2|23</p>
</div><br>1 point<br>3.Suppose you are using k-fold cross-validation to assess model quality. How many times should you train the model during this procedure?</p>
<p><input type="radio" disabled><label></label><br>1</p>
<p><input type="radio" disabled checked><label></label><br>k</p>
<p><input type="radio" disabled><label></label><br>k(k−1)/2</p>
<p><input type="radio" disabled><label></label><br>k2<br><br><br>1 point<br>4.Select correct statements about regularization:</p>
<p><input type="checkbox" disabled><label></label><br>Weight penalty reduces the number of model parameters and leads to faster model training</p>
<p><input type="checkbox" disabled><label></label><br>Reducing the training sample size makes data simpler and then leads to better quality</p>
<p><input type="checkbox" disabled checked><label></label><br>Regularization restricts model complexity (namely the scale of the coefficients) to reduce overfitting</p>
<p><input type="checkbox" disabled checked><label></label><br>Weight penalty drives model parameters closer to zero and prevents the model from being too sensitive to small changes in features<br><br></p>
<h3 id="Stochastic-methods-for-optimization"><a href="#Stochastic-methods-for-optimization" class="headerlink" title="Stochastic methods for optimization"></a>Stochastic methods for optimization</h3><h4 id="Stochastic-gradient-descent-5-min"><a href="#Stochastic-gradient-descent-5-min" class="headerlink" title="Stochastic gradient descent 5 min"></a>Stochastic gradient descent 5 min</h4><h4 id="Gradient-descent-extensions-9-min"><a href="#Gradient-descent-extensions-9-min" class="headerlink" title="Gradient descent extensions 9 min"></a>Gradient descent extensions 9 min</h4><h4 id="Linear-models-and-optimization"><a href="#Linear-models-and-optimization" class="headerlink" title="Linear models and optimization"></a>Linear models and optimization</h4><h4 id="Programming-Assignment-Linear-models-and-optimization-3h"><a href="#Programming-Assignment-Linear-models-and-optimization-3h" class="headerlink" title="Programming Assignment: Linear models and optimization 3h"></a>Programming Assignment: Linear models and optimization 3h</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="comment"># # Programming assignment (Linear models, Optimization)</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># In this programming assignment you will implement a linear classifier and train it using stochastic gradient descent modifications and numpy.</span></div><div class="line"></div><div class="line"><span class="comment"># In[1]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">get_ipython().magic(<span class="string">'matplotlib inline'</span>)</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[2]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">".."</span>)</div><div class="line"><span class="keyword">import</span> grading</div><div class="line">grader = grading.Grader(assignment_key=<span class="string">"UaHtvpEFEee0XQ6wjK-hZg"</span>, </div><div class="line">                      all_parts=[<span class="string">"xU7U4"</span>, <span class="string">"HyTF6"</span>, <span class="string">"uNidL"</span>, <span class="string">"ToK7N"</span>, <span class="string">"GBdgZ"</span>, <span class="string">"dLdHG"</span>])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[3]:</span></div><div class="line"></div><div class="line"><span class="comment"># token expires every 30 min</span></div><div class="line">COURSERA_TOKEN = <span class="string">""</span></div><div class="line">COURSERA_EMAIL = <span class="string">""</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># ## Two-dimensional classification</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># To make things more intuitive, let's solve a 2D classification problem with synthetic data.</span></div><div class="line"></div><div class="line"><span class="comment"># In[4]:</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> open(<span class="string">'train.npy'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fin:</div><div class="line">    X = np.load(fin)</div><div class="line">    </div><div class="line"><span class="keyword">with</span> open(<span class="string">'target.npy'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> fin:</div><div class="line">    y = np.load(fin)</div><div class="line"></div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.Paired, s=<span class="number">20</span>)</div><div class="line">plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[5]:</span></div><div class="line"></div><div class="line">print(X.shape)</div><div class="line">print(y.shape)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Task</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># ## Features</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># As you can notice the data above isn't linearly separable. Since that we should add features (or use non-linear model). Note that decision line between two classes have form of circle, since that we can add quadratic features to make the problem linearly separable. The idea under this displayed on image below:</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># ![](kernel.png)</span></div><div class="line"></div><div class="line"><span class="comment"># In[6]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Adds quadratic features. </div><div class="line">    This expansion allows your linear model to make non-linear separation.</div><div class="line">    </div><div class="line">    For each sample (row in matrix), compute an expanded row:</div><div class="line">    [feature0, feature1, feature0^2, feature1^2, feature1*feature2, 1]</div><div class="line">    </div><div class="line">    :param X: matrix of features, shape [n_samples,2]</div><div class="line">    :returns: expanded features of shape [n_samples,6]</div><div class="line">    """</div><div class="line">    X_expanded = np.zeros((X.shape[<span class="number">0</span>], <span class="number">6</span>))</div><div class="line">    </div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span>&lt;your code here&gt;</span></div><div class="line">    X_expanded[:,<span class="number">0</span>], X_expanded[:,<span class="number">1</span>] = X[:,<span class="number">0</span>],X[:,<span class="number">1</span>]</div><div class="line">    X_expanded[:,<span class="number">2</span>], X_expanded[:,<span class="number">3</span>]= X[:,<span class="number">0</span>]**<span class="number">2</span>, X[:,<span class="number">1</span>]**<span class="number">2</span></div><div class="line">    X_expanded[:,<span class="number">4</span>], X_expanded[:,<span class="number">5</span>] = X[:,<span class="number">0</span>]*X[:,<span class="number">1</span>], np.ones(X.shape[<span class="number">0</span>])</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> X_expanded</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[7]:</span></div><div class="line"></div><div class="line">X_expanded = expand(X)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Here are some tests for your implementation of `expand` function.</span></div><div class="line"></div><div class="line"><span class="comment"># In[8]:</span></div><div class="line"></div><div class="line"><span class="comment"># simple test on random numbers</span></div><div class="line"></div><div class="line">dummy_X = np.array([</div><div class="line">        [<span class="number">0</span>,<span class="number">0</span>],</div><div class="line">        [<span class="number">1</span>,<span class="number">0</span>],</div><div class="line">        [<span class="number">2.61</span>,<span class="number">-1.28</span>],</div><div class="line">        [<span class="number">-0.59</span>,<span class="number">2.1</span>]</div><div class="line">    ])</div><div class="line"></div><div class="line"><span class="comment"># call your expand function</span></div><div class="line">dummy_expanded = expand(dummy_X)</div><div class="line"></div><div class="line"><span class="comment"># what it should have returned:   x0       x1       x0^2     x1^2     x0*x1    1</span></div><div class="line">dummy_expanded_ans = np.array([[ <span class="number">0.</span>    ,  <span class="number">0.</span>    ,  <span class="number">0.</span>    ,  <span class="number">0.</span>    ,  <span class="number">0.</span>    ,  <span class="number">1.</span>    ],</div><div class="line">                               [ <span class="number">1.</span>    ,  <span class="number">0.</span>    ,  <span class="number">1.</span>    ,  <span class="number">0.</span>    ,  <span class="number">0.</span>    ,  <span class="number">1.</span>    ],</div><div class="line">                               [ <span class="number">2.61</span>  , <span class="number">-1.28</span>  ,  <span class="number">6.8121</span>,  <span class="number">1.6384</span>, <span class="number">-3.3408</span>,  <span class="number">1.</span>    ],</div><div class="line">                               [<span class="number">-0.59</span>  ,  <span class="number">2.1</span>   ,  <span class="number">0.3481</span>,  <span class="number">4.41</span>  , <span class="number">-1.239</span> ,  <span class="number">1.</span>    ]])</div><div class="line"></div><div class="line"><span class="comment">#tests</span></div><div class="line"><span class="keyword">assert</span> isinstance(dummy_expanded,np.ndarray), <span class="string">"please make sure you return numpy array"</span></div><div class="line"><span class="keyword">assert</span> dummy_expanded.shape == dummy_expanded_ans.shape, <span class="string">"please make sure your shape is correct"</span></div><div class="line"><span class="keyword">assert</span> np.allclose(dummy_expanded,dummy_expanded_ans,<span class="number">1e-3</span>), <span class="string">"Something's out of order with features"</span></div><div class="line"></div><div class="line">print(<span class="string">"Seems legit!"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># ## Logistic regression</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># To classify objects we will obtain probability of object belongs to class '1'. To predict probability we will use output of linear model and logistic function:</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $$ a(x; w) = \langle w, x \rangle $$</span></div><div class="line"><span class="comment"># $$ P( y=1 \; \big| \; x, \, w) = \dfrac&#123;1&#125;&#123;1 + \exp(- \langle w, x \rangle)&#125; = \sigma(\langle w, x \rangle)$$</span></div><div class="line"><span class="comment"># </span></div><div class="line"></div><div class="line"><span class="comment"># In[9]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">probability</span><span class="params">(X, w)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Given input features and weights</div><div class="line">    return predicted probabilities of y==1 given x, P(y=1|x), see description above</div><div class="line">        </div><div class="line">    Don't forget to use expand(X) function (where necessary) in this and subsequent functions.</div><div class="line">    </div><div class="line">    :param X: feature matrix X of shape [n_samples,6] (expanded)</div><div class="line">    :param w: weight vector w of shape [6] for each of the expanded features</div><div class="line">    :returns: an array of predicted probabilities in [0,1] interval.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span>&lt;your code here&gt;</span></div><div class="line">    z = np.dot(X,w)</div><div class="line">    </div><div class="line">    a = <span class="number">1.</span>/(<span class="number">1</span>+np.exp(-z))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> np.array(a)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[10]:</span></div><div class="line"></div><div class="line">dummy_weights = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">6</span>)</div><div class="line">ans_part1 = probability(X_expanded[:<span class="number">1</span>, :], dummy_weights)[<span class="number">0</span>]</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[11]:</span></div><div class="line"></div><div class="line">print(ans_part1)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[12]:</span></div><div class="line"></div><div class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></div><div class="line">grader.set_answer(<span class="string">"xU7U4"</span>, ans_part1)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[13]:</span></div><div class="line"></div><div class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></div><div class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In logistic regression the optimal parameters $w$ are found by cross-entropy minimization:</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $$ L(w) =  - &#123;1 \over \ell&#125; \sum_&#123;i=1&#125;^\ell \left[ &#123;y_i \cdot log P(y_i \, | \, x_i,w) + (1-y_i) \cdot log (1-P(y_i\, | \, x_i,w))&#125;\right] $$</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># </span></div><div class="line"></div><div class="line"><span class="comment"># In[14]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(X, y, w)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Given feature matrix X [n_samples,6], target vector [n_samples] of 1/0,</div><div class="line">    and weight vector w [6], compute scalar loss function using formula above.</div><div class="line">    """</div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span>&lt;your code here&gt;</span></div><div class="line">    l = X.shape[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    a = probability(X, w)</div><div class="line">    </div><div class="line">    cross_entropy = y*np.log(a) +(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-a)</div><div class="line">    cost = -np.sum(cross_entropy)/float(l)</div><div class="line">    </div><div class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></div><div class="line">    <span class="keyword">assert</span>(cost.shape == ())</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> cost</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[15]:</span></div><div class="line"></div><div class="line"><span class="comment"># use output of this cell to fill answer field </span></div><div class="line">ans_part2 = compute_loss(X_expanded, y, dummy_weights)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[16]:</span></div><div class="line"></div><div class="line">print(ans_part2)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[17]:</span></div><div class="line"></div><div class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></div><div class="line">grader.set_answer(<span class="string">"HyTF6"</span>, ans_part2)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[18]:</span></div><div class="line"></div><div class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></div><div class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Since we train our model with gradient descent, we should compute gradients.</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># To be specific, we need a derivative of loss function over each weight [6 of them].</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $$ \nabla_w L = ...$$</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># We won't be giving you the exact formula this time — instead, try figuring out a derivative with pen and paper. </span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># As usual, we've made a small test for you, but if you need more, feel free to check your math against finite differences (estimate how $L$ changes if you shift $w$ by $10^&#123;-5&#125;$ or so).</span></div><div class="line"></div><div class="line"><span class="comment"># In[19]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_grad</span><span class="params">(X, y, w)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Given feature matrix X [n_samples,6], target vector [n_samples] of 1/0,</div><div class="line">    and weight vector w [6], compute vector [6] of derivatives of L over each weights.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># TODO&lt;your code here&gt;</span></div><div class="line">    m = X.shape[<span class="number">0</span>]</div><div class="line">    A = probability(X, w)</div><div class="line">    dZ = A - y</div><div class="line">    <span class="comment">#cost = compute_loss(X, y, w)</span></div><div class="line">    dW = np.dot(dZ, X) / float(m)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> dW</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[20]:</span></div><div class="line"></div><div class="line"><span class="comment"># use output of this cell to fill answer field </span></div><div class="line">ans_part3 = np.linalg.norm(compute_grad(X_expanded, y, dummy_weights))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[21]:</span></div><div class="line"></div><div class="line">print(ans_part3)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[22]:</span></div><div class="line"></div><div class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></div><div class="line">grader.set_answer(<span class="string">"uNidL"</span>, ans_part3)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[23]:</span></div><div class="line"></div><div class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></div><div class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Here's an auxiliary function that visualizes the predictions:</span></div><div class="line"></div><div class="line"><span class="comment"># In[24]:</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</div><div class="line"></div><div class="line">h = <span class="number">0.01</span></div><div class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></div><div class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></div><div class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(X, y, w, history)</span>:</span></div><div class="line">    <span class="string">"""draws classifier prediction with matplotlib magic"""</span></div><div class="line">    Z = probability(expand(np.c_[xx.ravel(), yy.ravel()]), w)</div><div class="line">    Z = Z.reshape(xx.shape)</div><div class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</div><div class="line">    plt.contourf(xx, yy, Z, alpha=<span class="number">0.8</span>)</div><div class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.Paired)</div><div class="line">    plt.xlim(xx.min(), xx.max())</div><div class="line">    plt.ylim(yy.min(), yy.max())</div><div class="line">    </div><div class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    plt.plot(history)</div><div class="line">    plt.grid()</div><div class="line">    ymin, ymax = plt.ylim()</div><div class="line">    plt.ylim(<span class="number">0</span>, ymax)</div><div class="line">    display.clear_output(wait=<span class="keyword">True</span>)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[25]:</span></div><div class="line"></div><div class="line">visualize(X, y, dummy_weights, [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.25</span>])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># ## Training</span></div><div class="line"><span class="comment"># In this section we'll use the functions you wrote to train our classifier using stochastic gradient descent.</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># You can try change hyperparameters like batch size, learning rate and so on to find the best one, but use our hyperparameters when fill answers.</span></div><div class="line"></div><div class="line"><span class="comment"># ## Mini-batch SGD</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># Stochastic gradient descent just takes a random example on each iteration, calculates a gradient of the loss on it and makes a step:</span></div><div class="line"><span class="comment"># $$ w_t = w_&#123;t-1&#125; - \eta \dfrac&#123;1&#125;&#123;m&#125; \sum_&#123;j=1&#125;^m \nabla_w L(w_t, x_&#123;i_j&#125;, y_&#123;i_j&#125;) $$</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># </span></div><div class="line"></div><div class="line"><span class="comment"># In[26]:</span></div><div class="line"></div><div class="line"><span class="comment"># please use np.random.seed(42), eta=0.1, n_iter=100 and batch_size=4 for deterministic results</span></div><div class="line"></div><div class="line">np.random.seed(<span class="number">42</span>)</div><div class="line">w = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">eta= <span class="number">0.1</span> <span class="comment"># learning rate</span></div><div class="line"></div><div class="line">n_iter = <span class="number">100</span></div><div class="line">batch_size = <span class="number">4</span></div><div class="line">loss = np.zeros(n_iter)</div><div class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_iter):</div><div class="line">    ind = np.random.choice(X_expanded.shape[<span class="number">0</span>], batch_size)</div><div class="line">    loss[i] = compute_loss(X_expanded, y, w)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</div><div class="line">        visualize(X_expanded[ind, :], y[ind], w, loss)</div><div class="line"></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span>&lt;your code here&gt;    </span></div><div class="line">    dW = compute_grad(X_expanded[ind, :], y[ind], w)</div><div class="line">    w = w - eta * dW </div><div class="line">visualize(X, y, w, loss)</div><div class="line">plt.clf()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[27]:</span></div><div class="line"></div><div class="line"><span class="comment"># use output of this cell to fill answer field </span></div><div class="line"></div><div class="line">ans_part4 = compute_loss(X_expanded, y, w)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[28]:</span></div><div class="line"></div><div class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></div><div class="line">grader.set_answer(<span class="string">"ToK7N"</span>, ans_part4)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[29]:</span></div><div class="line"></div><div class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></div><div class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># ## SGD with momentum</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in image below. It does this by adding a fraction $\alpha$ of the update vector of the past time step to the current update vector.</span></div><div class="line"><span class="comment"># &lt;br&gt;</span></div><div class="line"><span class="comment"># &lt;br&gt;</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $$ \nu_t = \alpha \nu_&#123;t-1&#125; + \eta\dfrac&#123;1&#125;&#123;m&#125; \sum_&#123;j=1&#125;^m \nabla_w L(w_t, x_&#123;i_j&#125;, y_&#123;i_j&#125;) $$</span></div><div class="line"><span class="comment"># $$ w_t = w_&#123;t-1&#125; - \nu_t$$</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># &lt;br&gt;</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># ![](sgd.png)</span></div><div class="line"><span class="comment"># </span></div><div class="line"></div><div class="line"><span class="comment"># In[30]:</span></div><div class="line"></div><div class="line"><span class="comment"># please use np.random.seed(42), eta=0.05, alpha=0.9, n_iter=100 and batch_size=4 for deterministic results</span></div><div class="line">np.random.seed(<span class="number">42</span>)</div><div class="line">w = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">eta = <span class="number">0.05</span> <span class="comment"># learning rate</span></div><div class="line">alpha = <span class="number">0.9</span> <span class="comment"># momentum</span></div><div class="line">nu = np.zeros_like(w)</div><div class="line"></div><div class="line">n_iter = <span class="number">100</span></div><div class="line">batch_size = <span class="number">4</span></div><div class="line">loss = np.zeros(n_iter)</div><div class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_iter):</div><div class="line">    ind = np.random.choice(X_expanded.shape[<span class="number">0</span>], batch_size)</div><div class="line">    loss[i] = compute_loss(X_expanded, y, w)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</div><div class="line">        visualize(X_expanded[ind, :], y[ind], w, loss)</div><div class="line"></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span>&lt;your code here&gt;</span></div><div class="line">    dW = compute_grad(X_expanded[ind, :], y[ind], w)</div><div class="line">    nu = alpha*nu+eta*dW</div><div class="line">    w = w - nu</div><div class="line">visualize(X, y, w, loss)</div><div class="line">plt.clf()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[31]:</span></div><div class="line"></div><div class="line"><span class="comment"># use output of this cell to fill answer field </span></div><div class="line"></div><div class="line">ans_part5 = compute_loss(X_expanded, y, w)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[32]:</span></div><div class="line"></div><div class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></div><div class="line">grader.set_answer(<span class="string">"GBdgZ"</span>, ans_part5)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[33]:</span></div><div class="line"></div><div class="line"><span class="comment"># you can make submission with answers so far to check yourself at this stage</span></div><div class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># ## RMSprop</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># Implement RMSPROP algorithm, which use squared gradients to adjust learning rate:</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $$ G_j^t = \alpha G_j^&#123;t-1&#125; + (1 - \alpha) g_&#123;tj&#125;^2 $$</span></div><div class="line"><span class="comment"># $$ w_j^t = w_j^&#123;t-1&#125; - \dfrac&#123;\eta&#125;&#123;\sqrt&#123;G_j^t + \varepsilon&#125;&#125; g_&#123;tj&#125; $$</span></div><div class="line"></div><div class="line"><span class="comment"># In[34]:</span></div><div class="line"></div><div class="line"><span class="comment"># please use np.random.seed(42), eta=0.1, alpha=0.9, n_iter=100 and batch_size=4 for deterministic results</span></div><div class="line">np.random.seed(<span class="number">42</span>)</div><div class="line"></div><div class="line">w = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1.</span>])</div><div class="line"></div><div class="line">eta = <span class="number">0.1</span> <span class="comment"># learning rate</span></div><div class="line">alpha = <span class="number">0.9</span> <span class="comment"># moving average of gradient norm squared</span></div><div class="line">G = np.zeros_like(w)</div><div class="line">g2 = np.zeros_like(w)</div><div class="line">eps = <span class="number">1e-8</span></div><div class="line"></div><div class="line">n_iter = <span class="number">100</span></div><div class="line">batch_size = <span class="number">4</span></div><div class="line">loss = np.zeros(n_iter)</div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_iter):</div><div class="line">    ind = np.random.choice(X_expanded.shape[<span class="number">0</span>], batch_size)</div><div class="line">    loss[i] = compute_loss(X_expanded, y, w)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</div><div class="line">        visualize(X_expanded[ind, :], y[ind], w, loss)</div><div class="line"></div><div class="line">    <span class="comment"># <span class="doctag">TODO:</span>&lt;your code here&gt;</span></div><div class="line">    dW = compute_grad(X_expanded[ind, :], y[ind], w)</div><div class="line">    g2 = dW**<span class="number">2</span></div><div class="line">    G = alpha*G+(<span class="number">1</span>-alpha)*g2</div><div class="line">    </div><div class="line">    w = w - eta*dW/np.sqrt(G+eps)</div><div class="line">    </div><div class="line">visualize(X, y, w, loss)</div><div class="line">plt.clf()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[35]:</span></div><div class="line"></div><div class="line"><span class="comment"># use output of this cell to fill answer field </span></div><div class="line">ans_part6 = compute_loss(X_expanded, y, w)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[36]:</span></div><div class="line"></div><div class="line"><span class="comment">## GRADED PART, DO NOT CHANGE!</span></div><div class="line">grader.set_answer(<span class="string">"dLdHG"</span>, ans_part6)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[37]:</span></div><div class="line"></div><div class="line">grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[ ]:</span></div></pre></td></tr></table></figure>
<h2 id="Week-2-Introduction-to-neural-networks"><a href="#Week-2-Introduction-to-neural-networks" class="headerlink" title="Week 2 Introduction to neural networks"></a>Week 2 Introduction to neural networks</h2><div class="note primary"><p>This module is an introduction to the concept of a deep neural network. You’ll begin with the linear model in numpy and finish with writing your very first deep network.</p>
</div>
<div class="note primary"><p>Learning Objectives</p>
<p>Explain the mechanics of basic building blocks for neural networks<br>Apply backpropagation algorithm to train deep neural networks using automatic differentiation<br>Implement, train and test neural networks using TensorFlow and Keras</p>
</div>
<h3 id="Multilayer-perceptron-or-the-basic-principles-of-deep-learning"><a href="#Multilayer-perceptron-or-the-basic-principles-of-deep-learning" class="headerlink" title="Multilayer perceptron, or the basic principles of deep learning"></a>Multilayer perceptron, or the basic principles of deep learning</h3><h4 id="Multilayer-perceptron6-min"><a href="#Multilayer-perceptron6-min" class="headerlink" title="Multilayer perceptron6 min"></a>Multilayer perceptron6 min</h4><p><a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning</a></p>
<h4 id="Training-a-neural-network7-min"><a href="#Training-a-neural-network7-min" class="headerlink" title="Training a neural network7 min"></a>Training a neural network7 min</h4><p><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" target="_blank" rel="external">Matrix Operation</a></p>
<h4 id="Backpropagation-primer7-min"><a href="#Backpropagation-primer7-min" class="headerlink" title="Backpropagation primer7 min"></a>Backpropagation primer7 min</h4><h4 id="Practice-Quiz-Multilayer-perceptron4-questions"><a href="#Practice-Quiz-Multilayer-perceptron4-questions" class="headerlink" title="Practice Quiz: Multilayer perceptron4 questions"></a>Practice Quiz: Multilayer perceptron4 questions</h4><div class="note primary"><p>PRACTICE QUIZ<br>Multilayer perceptron<br>4 questions<br>To Pass100% or higher<br>Deadline<br>December 3, 11:59 PM PST</p>
</div>
<p>Question 11point</p>
<ol>
<li>Question 1<br>The best nonlinearity functions to use in a Multilayer perceptron are step functions as they allow to reconstruct the decision boundary with better precision.<br><input type="radio" disabled><label>Yes</label><br><input type="radio" disabled checked><label>No</label><br><br><br>Question 21 point</li>
<li>Question 2<br>A dense layer applies a linear transformation to its input<br><input type="radio" disabled checked><label>Yes</label><br><input type="radio" disabled><label>No</label><br><br><br>Question 31 point</li>
<li>Question 3<br>For an MLP to work, the nonlinearity function must have a finite upper bound<br><input type="radio" disabled><label>Yes</label><br><input type="radio" disabled checked><label>No</label><br><br><br>1 point</li>
<li>Question 4<br>How many dimensions will a derivative of a 1-D vector by a 2-D matrix have?<br><input type="”text”" placeholder="3"><br><br></li>
</ol>
<h3 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h3><h4 id="Tensorflow-task-ipynb"><a href="#Tensorflow-task-ipynb" class="headerlink" title="Tensorflow_task.ipynb"></a>Tensorflow_task.ipynb</h4><h4 id="Going-deeper-with-Tensorflow11-min"><a href="#Going-deeper-with-Tensorflow11-min" class="headerlink" title="Going deeper with Tensorflow11 min"></a>Going deeper with Tensorflow11 min</h4><h4 id="Practice-Programming-Assignment-MSE-in-TensorFlow15-min"><a href="#Practice-Programming-Assignment-MSE-in-TensorFlow15-min" class="headerlink" title="Practice Programming Assignment: MSE in TensorFlow15 min"></a>Practice Programming Assignment: MSE in TensorFlow15 min</h4><h4 id="Gradients-amp-optimization-in-Tensorflow8-min"><a href="#Gradients-amp-optimization-in-Tensorflow8-min" class="headerlink" title="Gradients &amp; optimization in Tensorflow8 min"></a>Gradients &amp; optimization in Tensorflow8 min</h4><h4 id="Programming-Assignment-Logistic-regression-in-TensorFlow30-min"><a href="#Programming-Assignment-Logistic-regression-in-TensorFlow30-min" class="headerlink" title="Programming Assignment: Logistic regression in TensorFlow30 min"></a>Programming Assignment: Logistic regression in TensorFlow30 min</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div><div class="line">395</div><div class="line">396</div><div class="line">397</div><div class="line">398</div><div class="line">399</div><div class="line">400</div><div class="line">401</div><div class="line">402</div><div class="line">403</div><div class="line">404</div><div class="line">405</div><div class="line">406</div><div class="line">407</div><div class="line">408</div><div class="line">409</div><div class="line">410</div><div class="line">411</div><div class="line">412</div><div class="line">413</div><div class="line">414</div><div class="line">415</div><div class="line">416</div><div class="line">417</div><div class="line">418</div><div class="line">419</div><div class="line">420</div><div class="line">421</div><div class="line">422</div><div class="line">423</div><div class="line">424</div><div class="line">425</div><div class="line">426</div><div class="line">427</div><div class="line">428</div><div class="line">429</div><div class="line">430</div><div class="line">431</div><div class="line">432</div><div class="line">433</div><div class="line">434</div><div class="line">435</div><div class="line">436</div><div class="line">437</div><div class="line">438</div><div class="line">439</div><div class="line">440</div><div class="line">441</div><div class="line">442</div><div class="line">443</div><div class="line">444</div><div class="line">445</div><div class="line">446</div><div class="line">447</div><div class="line">448</div><div class="line">449</div><div class="line">450</div><div class="line">451</div><div class="line">452</div><div class="line">453</div><div class="line">454</div><div class="line">455</div><div class="line">456</div><div class="line">457</div><div class="line">458</div><div class="line">459</div><div class="line">460</div><div class="line">461</div><div class="line">462</div><div class="line">463</div><div class="line">464</div><div class="line">465</div><div class="line">466</div><div class="line">467</div><div class="line">468</div><div class="line">469</div><div class="line">470</div><div class="line">471</div><div class="line">472</div><div class="line">473</div><div class="line">474</div><div class="line">475</div><div class="line">476</div><div class="line">477</div><div class="line">478</div><div class="line">479</div><div class="line">480</div><div class="line">481</div><div class="line">482</div><div class="line">483</div><div class="line">484</div><div class="line">485</div><div class="line">486</div><div class="line">487</div><div class="line">488</div><div class="line">489</div><div class="line">490</div><div class="line">491</div><div class="line">492</div><div class="line">493</div><div class="line">494</div><div class="line">495</div><div class="line">496</div><div class="line">497</div><div class="line">498</div><div class="line">499</div><div class="line">500</div><div class="line">501</div><div class="line">502</div><div class="line">503</div><div class="line">504</div><div class="line">505</div><div class="line">506</div><div class="line">507</div><div class="line">508</div><div class="line">509</div><div class="line">510</div><div class="line">511</div><div class="line">512</div><div class="line">513</div><div class="line">514</div><div class="line">515</div><div class="line">516</div><div class="line">517</div><div class="line">518</div><div class="line">519</div><div class="line">520</div><div class="line">521</div><div class="line">522</div><div class="line">523</div><div class="line">524</div><div class="line">525</div><div class="line">526</div><div class="line">527</div><div class="line">528</div><div class="line">529</div><div class="line">530</div><div class="line">531</div><div class="line">532</div><div class="line">533</div><div class="line">534</div><div class="line">535</div><div class="line">536</div><div class="line">537</div><div class="line">538</div><div class="line">539</div><div class="line">540</div><div class="line">541</div><div class="line">542</div><div class="line">543</div><div class="line">544</div><div class="line">545</div><div class="line">546</div><div class="line">547</div><div class="line">548</div><div class="line">549</div><div class="line">550</div><div class="line">551</div><div class="line">552</div><div class="line">553</div><div class="line">554</div><div class="line">555</div><div class="line">556</div><div class="line">557</div><div class="line">558</div><div class="line">559</div><div class="line">560</div><div class="line">561</div><div class="line">562</div><div class="line">563</div><div class="line">564</div><div class="line">565</div><div class="line">566</div><div class="line">567</div><div class="line">568</div><div class="line">569</div><div class="line">570</div><div class="line">571</div><div class="line">572</div><div class="line">573</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="comment"># # Going deeper with Tensorflow</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># In this video, we're going to study the tools you'll use to build deep learning models. Namely, [Tensorflow](https://www.tensorflow.org/).</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># If you're running this notebook outside the course environment, you'll need to install tensorflow:</span></div><div class="line"><span class="comment"># * `pip install tensorflow` should install cpu-only TF on Linux &amp; Mac OS</span></div><div class="line"><span class="comment"># * If you want GPU support from offset, see [TF install page](https://www.tensorflow.org/install/)</span></div><div class="line"></div><div class="line"><span class="comment"># In[1]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line">sys.path.append(<span class="string">".."</span>)</div><div class="line"><span class="keyword">import</span> grading</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Visualization</span></div><div class="line"></div><div class="line"><span class="comment"># Plase note that if you are running on the Coursera platform, you won't be able to access the tensorboard instance due to the network setup there. If you run the notebook locally, you should be able to access TensorBoard on http://127.0.0.1:7007/</span></div><div class="line"></div><div class="line"><span class="comment"># In[ ]:</span></div><div class="line"></div><div class="line">get_ipython().system(<span class="string">' killall tensorboard'</span>)</div><div class="line"><span class="keyword">import</span> os</div><div class="line">os.system(<span class="string">"tensorboard --logdir=/tmp/tboard --port=7007 &amp;"</span>);</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[2]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">s = tf.InteractiveSession()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Warming up</span></div><div class="line"><span class="comment"># For starters, let's implement a python function that computes the sum of squares of numbers from 0 to N-1.</span></div><div class="line"></div><div class="line"><span class="comment"># In[3]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_sin</span><span class="params">(N)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.sum(np.arange(N)**<span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[4]:</span></div><div class="line"></div><div class="line">get_ipython().run_cell_magic(<span class="string">'time'</span>, <span class="string">''</span>, <span class="string">'sum_sin(10**8)'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Tensoflow teaser</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># Doing the very same thing</span></div><div class="line"></div><div class="line"><span class="comment"># In[5]:</span></div><div class="line"></div><div class="line"><span class="comment"># An integer parameter</span></div><div class="line">N = tf.placeholder(<span class="string">'int64'</span>, name=<span class="string">"input_to_your_function"</span>)</div><div class="line"></div><div class="line"><span class="comment"># A recipe on how to produce the same result</span></div><div class="line">result = tf.reduce_sum(tf.range(N)**<span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[6]:</span></div><div class="line"></div><div class="line">result</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[7]:</span></div><div class="line"></div><div class="line">get_ipython().run_cell_magic(<span class="string">'time'</span>, <span class="string">''</span>, <span class="string">'result.eval(&#123;N: 10**8&#125;)'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[8]:</span></div><div class="line"></div><div class="line">writer = tf.summary.FileWriter(<span class="string">"/tmp/tboard"</span>, graph=s.graph)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # How does it work?</span></div><div class="line"><span class="comment"># 1. Define placeholders where you'll send inputs</span></div><div class="line"><span class="comment"># 2. Make symbolic graph: a recipe for mathematical transformation of those placeholders</span></div><div class="line"><span class="comment"># 3. Compute outputs of your graph with particular values for each placeholder</span></div><div class="line"><span class="comment">#   * `output.eval(&#123;placeholder:value&#125;)`</span></div><div class="line"><span class="comment">#   * `s.run(output, &#123;placeholder:value&#125;)`</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># So far there are two main entities: "placeholder" and "transformation"</span></div><div class="line"><span class="comment"># * Both can be numbers, vectors, matrices, tensors, etc.</span></div><div class="line"><span class="comment"># * Both can be int32/64, floats, booleans (uint8) of various size.</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># * You can define new transformations as an arbitrary operation on placeholders and other transformations</span></div><div class="line"><span class="comment">#  * `tf.reduce_sum(tf.arange(N)**2)` are 3 sequential transformations of placeholder `N`</span></div><div class="line"><span class="comment">#  * There's a tensorflow symbolic version for every numpy function</span></div><div class="line"><span class="comment">#    * `a+b, a/b, a**b, ...` behave just like in numpy</span></div><div class="line"><span class="comment">#    * `np.mean` -&gt; `tf.reduce_mean`</span></div><div class="line"><span class="comment">#    * `np.arange` -&gt; `tf.range`</span></div><div class="line"><span class="comment">#    * `np.cumsum` -&gt; `tf.cumsum`</span></div><div class="line"><span class="comment">#    * If if you can't find the op you need, see the [docs](https://www.tensorflow.org/api_docs/python).</span></div><div class="line"><span class="comment">#    </span></div><div class="line"><span class="comment"># `tf.contrib` has many high-level features, may be worth a look.</span></div><div class="line"></div><div class="line"><span class="comment"># In[9]:</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"Placeholders_examples"</span>):</div><div class="line">    <span class="comment"># Default placeholder that can be arbitrary float32</span></div><div class="line">    <span class="comment"># scalar, vertor, matrix, etc.</span></div><div class="line">    arbitrary_input = tf.placeholder(<span class="string">'float32'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Input vector of arbitrary length</span></div><div class="line">    input_vector = tf.placeholder(<span class="string">'float32'</span>, shape=(<span class="keyword">None</span>,))</div><div class="line"></div><div class="line">    <span class="comment"># Input vector that _must_ have 10 elements and integer type</span></div><div class="line">    fixed_vector = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="number">10</span>,))</div><div class="line"></div><div class="line">    <span class="comment"># Matrix of arbitrary n_rows and 15 columns</span></div><div class="line">    <span class="comment"># (e.g. a minibatch your data table)</span></div><div class="line">    input_matrix = tf.placeholder(<span class="string">'float32'</span>, shape=(<span class="keyword">None</span>, <span class="number">15</span>))</div><div class="line">    </div><div class="line">    <span class="comment"># You can generally use None whenever you don't need a specific shape</span></div><div class="line">    input1 = tf.placeholder(<span class="string">'float64'</span>, shape=(<span class="keyword">None</span>, <span class="number">100</span>, <span class="keyword">None</span>))</div><div class="line">    input2 = tf.placeholder(<span class="string">'int32'</span>, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</div><div class="line"></div><div class="line">    <span class="comment"># elementwise multiplication</span></div><div class="line">    double_the_vector = input_vector*<span class="number">2</span></div><div class="line"></div><div class="line">    <span class="comment"># elementwise cosine</span></div><div class="line">    elementwise_cosine = tf.cos(input_vector)</div><div class="line"></div><div class="line">    <span class="comment"># difference between squared vector and vector itself plus one</span></div><div class="line">    vector_squares = input_vector**<span class="number">2</span> - input_vector + <span class="number">1</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[10]:</span></div><div class="line"></div><div class="line">my_vector =  tf.placeholder(<span class="string">'float32'</span>, shape=(<span class="keyword">None</span>,), name=<span class="string">"VECTOR_1"</span>)</div><div class="line">my_vector2 = tf.placeholder(<span class="string">'float32'</span>, shape=(<span class="keyword">None</span>,))</div><div class="line">my_transformation = my_vector * my_vector2 / (tf.sin(my_vector) + <span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[11]:</span></div><div class="line"></div><div class="line">print(my_transformation)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[12]:</span></div><div class="line"></div><div class="line">dummy = np.arange(<span class="number">5</span>).astype(<span class="string">'float32'</span>)</div><div class="line">print(dummy)</div><div class="line">my_transformation.eval(&#123;my_vector:dummy, my_vector2:dummy[::<span class="number">-1</span>]&#125;)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[13]:</span></div><div class="line"></div><div class="line">writer.add_graph(my_transformation.graph)</div><div class="line">writer.flush()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># TensorBoard allows writing scalars, images, audio, histogram. You can read more on tensorboard usage [here](https://www.tensorflow.org/get_started/graph_viz).</span></div><div class="line"></div><div class="line"><span class="comment"># # Summary</span></div><div class="line"><span class="comment"># * Tensorflow is based on computation graphs</span></div><div class="line"><span class="comment"># * The graphs consist of placehlders and transformations</span></div><div class="line"></div><div class="line"><span class="comment"># # Mean squared error</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># Your assignment is to implement mean squared error in tensorflow.</span></div><div class="line"></div><div class="line"><span class="comment"># In[16]:</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"MSE"</span>):</div><div class="line">    y_true = tf.placeholder(<span class="string">"float32"</span>, shape=(<span class="keyword">None</span>,), name=<span class="string">"y_true"</span>)</div><div class="line">    y_predicted = tf.placeholder(<span class="string">"float32"</span>, shape=(<span class="keyword">None</span>,), name=<span class="string">"y_predicted"</span>)</div><div class="line">    <span class="comment"># Your code goes here</span></div><div class="line">    <span class="comment"># You want to use tf.reduce_mean</span></div><div class="line">    <span class="comment"># mse = tf.&lt;...&gt;</span></div><div class="line">    mse = tf.reduce_mean(tf.squared_difference(y_true, y_predicted)) </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_mse</span><span class="params">(vector1, vector2)</span>:</span></div><div class="line">    <span class="keyword">return</span> mse.eval(&#123;y_true: vector1, y_predicted: vector2&#125;)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[17]:</span></div><div class="line"></div><div class="line">writer.add_graph(mse.graph)</div><div class="line">writer.flush()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Tests and result submission. Please use the credentials obtained from the Coursera assignment page.</span></div><div class="line"></div><div class="line"><span class="comment"># In[18]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> submit</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[19]:</span></div><div class="line"></div><div class="line">submit.submit_mse(compute_mse, &lt;your email&gt;, &lt;your token&gt;)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Variables</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># The inputs and transformations have no value outside function call. This isn't too comfortable if you want your model to have parameters (e.g. network weights) that are always present, but can change their value over time.</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># Tensorflow solves this with `tf.Variable` objects.</span></div><div class="line"><span class="comment"># * You can assign variable a value at any time in your graph</span></div><div class="line"><span class="comment"># * Unlike placeholders, there's no need to explicitly pass values to variables when `s.run(...)`-ing</span></div><div class="line"><span class="comment"># * You can use variables the same way you use transformations </span></div><div class="line"><span class="comment">#  </span></div><div class="line"></div><div class="line"><span class="comment"># In[20]:</span></div><div class="line"></div><div class="line"><span class="comment"># Creating a shared variable</span></div><div class="line">shared_vector_1 = tf.Variable(initial_value=np.ones(<span class="number">5</span>),</div><div class="line">                              name=<span class="string">"example_variable"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[21]:</span></div><div class="line"></div><div class="line"><span class="comment"># Initialize variable(s) with initial values</span></div><div class="line">s.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line"><span class="comment"># Evaluating shared variable (outside symbolicd graph)</span></div><div class="line">print(<span class="string">"Initial value"</span>, s.run(shared_vector_1))</div><div class="line"></div><div class="line"><span class="comment"># Within symbolic graph you use them just</span></div><div class="line"><span class="comment"># as any other inout or transformation, not "get value" needed</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[22]:</span></div><div class="line"></div><div class="line"><span class="comment"># Setting a new value</span></div><div class="line">s.run(shared_vector_1.assign(np.arange(<span class="number">5</span>)))</div><div class="line"></div><div class="line"><span class="comment"># Getting that new value</span></div><div class="line">print(<span class="string">"New value"</span>, s.run(shared_vector_1))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # tf.gradients - why graphs matter</span></div><div class="line"><span class="comment"># * Tensorflow can compute derivatives and gradients automatically using the computation graph</span></div><div class="line"><span class="comment"># * True to its name it can manage matrix derivatives</span></div><div class="line"><span class="comment"># * Gradients are computed as a product of elementary derivatives via the chain rule:</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $$ &#123;\partial f(g(x)) \over \partial x&#125; = &#123;\partial f(g(x)) \over \partial g(x)&#125;\cdot &#123;\partial g(x) \over \partial x&#125; $$</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># It can get you the derivative of any graph as long as it knows how to differentiate elementary operations</span></div><div class="line"></div><div class="line"><span class="comment"># In[23]:</span></div><div class="line"></div><div class="line">my_scalar = tf.placeholder(<span class="string">'float32'</span>)</div><div class="line"></div><div class="line">scalar_squared = my_scalar**<span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># A derivative of scalar_squared by my_scalar</span></div><div class="line">derivative = tf.gradients(scalar_squared, [my_scalar, ])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[24]:</span></div><div class="line"></div><div class="line">derivative</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[25]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">get_ipython().magic(<span class="string">'matplotlib inline'</span>)</div><div class="line"></div><div class="line">x = np.linspace(<span class="number">-3</span>, <span class="number">3</span>)</div><div class="line">x_squared, x_squared_der = s.run([scalar_squared, derivative[<span class="number">0</span>]],</div><div class="line">                                 &#123;my_scalar:x&#125;)</div><div class="line"></div><div class="line">plt.plot(x, x_squared,label=<span class="string">"$x^2$"</span>)</div><div class="line">plt.plot(x, x_squared_der, label=<span class="string">r"$\frac&#123;dx^2&#125;&#123;dx&#125;$"</span>)</div><div class="line">plt.legend();</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Why that rocks</span></div><div class="line"></div><div class="line"><span class="comment"># In[26]:</span></div><div class="line"></div><div class="line">my_vector = tf.placeholder(<span class="string">'float32'</span>, [<span class="keyword">None</span>])</div><div class="line"><span class="comment"># Compute the gradient of the next weird function over my_scalar and my_vector</span></div><div class="line"><span class="comment"># Warning! Trying to understand the meaning of that function may result in permanent brain damage</span></div><div class="line">weird_psychotic_function = tf.reduce_mean(</div><div class="line">    (my_vector+my_scalar)**(<span class="number">1</span>+tf.nn.moments(my_vector,[<span class="number">0</span>])[<span class="number">1</span>]) + </div><div class="line">    <span class="number">1.</span>/ tf.atan(my_scalar))/(my_scalar**<span class="number">2</span> + <span class="number">1</span>) + <span class="number">0.01</span>*tf.sin(</div><div class="line">    <span class="number">2</span>*my_scalar**<span class="number">1.5</span>)*(tf.reduce_sum(my_vector)* my_scalar**<span class="number">2</span></div><div class="line">                      )*tf.exp((my_scalar<span class="number">-4</span>)**<span class="number">2</span>)/(</div><div class="line">    <span class="number">1</span>+tf.exp((my_scalar<span class="number">-4</span>)**<span class="number">2</span>))*(<span class="number">1.</span>-(tf.exp(-(my_scalar<span class="number">-4</span>)**<span class="number">2</span>)</div><div class="line">                                    )/(<span class="number">1</span>+tf.exp(-(my_scalar<span class="number">-4</span>)**<span class="number">2</span>)))**<span class="number">2</span></div><div class="line"></div><div class="line">der_by_scalar = tf.gradients(weird_psychotic_function, my_scalar)</div><div class="line">der_by_vector = tf.gradients(weird_psychotic_function, my_vector)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[27]:</span></div><div class="line"></div><div class="line"><span class="comment"># Plotting the derivative</span></div><div class="line">scalar_space = np.linspace(<span class="number">1</span>, <span class="number">7</span>, <span class="number">100</span>)</div><div class="line"></div><div class="line">y = [s.run(weird_psychotic_function, &#123;my_scalar:x, my_vector:[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</div><div class="line">     <span class="keyword">for</span> x <span class="keyword">in</span> scalar_space]</div><div class="line"></div><div class="line">plt.plot(scalar_space, y, label=<span class="string">'function'</span>)</div><div class="line"></div><div class="line">y_der_by_scalar = [s.run(der_by_scalar,</div><div class="line">                         &#123;my_scalar:x, my_vector:[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</div><div class="line">                   <span class="keyword">for</span> x <span class="keyword">in</span> scalar_space]</div><div class="line"></div><div class="line">plt.plot(scalar_space, y_der_by_scalar, label=<span class="string">'derivative'</span>)</div><div class="line">plt.grid()</div><div class="line">plt.legend();</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Almost done - optimizers</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># While you can perform gradient descent by hand with automatic grads from above, tensorflow also has some optimization methods implemented for you. Recall momentum &amp; rmsprop?</span></div><div class="line"></div><div class="line"><span class="comment"># In[28]:</span></div><div class="line"></div><div class="line">y_guess = tf.Variable(np.zeros(<span class="number">2</span>, dtype=<span class="string">'float32'</span>))</div><div class="line">y_true = tf.range(<span class="number">1</span>, <span class="number">3</span>, dtype=<span class="string">'float32'</span>)</div><div class="line">loss = tf.reduce_mean((y_guess - y_true + tf.random_normal([<span class="number">2</span>]))**<span class="number">2</span>) </div><div class="line"><span class="comment">#loss = tf.reduce_mean((y_guess - y_true)**2) </span></div><div class="line">optimizer = tf.train.MomentumOptimizer(<span class="number">0.01</span>, <span class="number">0.5</span>).minimize(</div><div class="line">    loss, var_list=y_guess)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[29]:</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> animation, rc</div><div class="line"><span class="keyword">import</span> matplotlib_utils</div><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</div><div class="line"></div><div class="line">fig, ax = plt.subplots()</div><div class="line">y_true_value = s.run(y_true)</div><div class="line">level_x = np.arange(<span class="number">0</span>, <span class="number">2</span>, <span class="number">0.02</span>)</div><div class="line">level_y = np.arange(<span class="number">0</span>, <span class="number">3</span>, <span class="number">0.02</span>)</div><div class="line">X, Y = np.meshgrid(level_x, level_y)</div><div class="line">Z = (X - y_true_value[<span class="number">0</span>])**<span class="number">2</span> + (Y - y_true_value[<span class="number">1</span>])**<span class="number">2</span></div><div class="line">ax.set_xlim(<span class="number">-0.02</span>, <span class="number">2</span>)</div><div class="line">ax.set_ylim(<span class="number">-0.02</span>, <span class="number">3</span>)</div><div class="line">s.run(tf.global_variables_initializer())</div><div class="line">ax.scatter(*s.run(y_true), c=<span class="string">'red'</span>)</div><div class="line">contour = ax.contour(X, Y, Z, <span class="number">10</span>)</div><div class="line">ax.clabel(contour, inline=<span class="number">1</span>, fontsize=<span class="number">10</span>)</div><div class="line">line, = ax.plot([], [], lw=<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">()</span>:</span></div><div class="line">    line.set_data([], [])</div><div class="line">    <span class="keyword">return</span> (line,)</div><div class="line"></div><div class="line">guesses = [s.run(y_guess)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">animate</span><span class="params">(i)</span>:</span></div><div class="line">    s.run(optimizer)</div><div class="line">    guesses.append(s.run(y_guess))</div><div class="line">    line.set_data(*zip(*guesses))</div><div class="line">    <span class="keyword">return</span> (line,)</div><div class="line"></div><div class="line">anim = animation.FuncAnimation(fig, animate, init_func=init,</div><div class="line">                               frames=<span class="number">400</span>, interval=<span class="number">20</span>, blit=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[ ]:</span></div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    HTML(anim.to_html5_video())</div><div class="line"><span class="comment"># In case the build-in renderers are unaviable, fall back to</span></div><div class="line"><span class="comment"># a custom one, that doesn't require external libraries</span></div><div class="line"><span class="keyword">except</span> RuntimeError:</div><div class="line">    anim.save(<span class="keyword">None</span>, writer=matplotlib_utils.SimpleMovieWriter(<span class="number">0.001</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># # Logistic regression</span></div><div class="line"><span class="comment"># Your assignment is to implement the logistic regression</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># Plan:</span></div><div class="line"><span class="comment"># * Use a shared variable for weights</span></div><div class="line"><span class="comment"># * Use a matrix placeholder for `X`</span></div><div class="line"><span class="comment">#  </span></div><div class="line"><span class="comment"># We shall train on a two-class MNIST dataset</span></div><div class="line"><span class="comment"># * please note that target `y` are `&#123;0,1&#125;` and not `&#123;-1,1&#125;` as in some formulae</span></div><div class="line"></div><div class="line"><span class="comment"># In[31]:</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">mnist = load_digits(<span class="number">2</span>)</div><div class="line"></div><div class="line">X, y = mnist.data, mnist.target</div><div class="line"></div><div class="line">print(<span class="string">"y [shape - %s]:"</span> % (str(y.shape)), y[:<span class="number">10</span>])</div><div class="line">print(<span class="string">"X [shape - %s]:"</span> % (str(X.shape)))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[32]:</span></div><div class="line"></div><div class="line">print(<span class="string">'X:\n'</span>,X[:<span class="number">3</span>,:<span class="number">10</span>])</div><div class="line">print(<span class="string">'y:\n'</span>,y[:<span class="number">10</span>])</div><div class="line">plt.imshow(X[<span class="number">0</span>].reshape([<span class="number">8</span>,<span class="number">8</span>]));</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># It's your turn now!</span></div><div class="line"><span class="comment"># Just a small reminder of the relevant math:</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $$</span></div><div class="line"><span class="comment"># P(y=1|X) = \sigma(X \cdot W + b)</span></div><div class="line"><span class="comment"># $$</span></div><div class="line"><span class="comment"># $$</span></div><div class="line"><span class="comment"># \text&#123;loss&#125; = -\log\left(P\left(y_\text&#123;predicted&#125; = 1\right)\right)\cdot y_\text&#123;true&#125; - \log\left(1 - P\left(y_\text&#123;predicted&#125; = 1\right)\right)\cdot\left(1 - y_\text&#123;true&#125;\right)</span></div><div class="line"><span class="comment"># $$</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># $\sigma(x)$ is available via `tf.nn.sigmoid` and matrix multiplication via `tf.matmul`</span></div><div class="line"></div><div class="line"><span class="comment"># In[33]:</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(</div><div class="line">    X, y, random_state=<span class="number">42</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># __Your code goes here.__ For the training and testing scaffolding to work, please stick to the names in comments.</span></div><div class="line"></div><div class="line"><span class="comment"># In[102]:</span></div><div class="line"></div><div class="line"><span class="comment"># Model parameters - weights and bias</span></div><div class="line"><span class="comment"># weights = tf.Variable(...) shape should be (X.shape[1], 1)</span></div><div class="line"><span class="comment"># b = tf.Variable(...)</span></div><div class="line"></div><div class="line">weights = tf.Variable(initial_value=np.random.randn(X.shape[<span class="number">1</span>], <span class="number">1</span>)*<span class="number">0.01</span>, name=<span class="string">"weights"</span>, dtype=<span class="string">"float32"</span>)</div><div class="line">b = tf.Variable(initial_value=<span class="number">0</span>, name=<span class="string">"b"</span>, dtype=<span class="string">"float32"</span>)</div><div class="line">print(weights)</div><div class="line">print(b)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[103]:</span></div><div class="line"></div><div class="line"><span class="comment"># Placeholders for the input data</span></div><div class="line"><span class="comment"># input_X = tf.placeholder(...)</span></div><div class="line"><span class="comment"># input_y = tf.placeholder(...)</span></div><div class="line"></div><div class="line">input_X = tf.placeholder(tf.float32, name=<span class="string">"input_X"</span>)</div><div class="line">input_y = tf.placeholder(tf.float32, name=<span class="string">"input_y"</span>)</div><div class="line">print(input_X)</div><div class="line">print(input_y)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[104]:</span></div><div class="line"></div><div class="line"><span class="comment"># The model code</span></div><div class="line"></div><div class="line"><span class="comment"># Compute a vector of predictions, resulting shape should be [input_X.shape[0],]</span></div><div class="line"><span class="comment"># This is 1D, if you have extra dimensions, you can  get rid of them with tf.squeeze .</span></div><div class="line"><span class="comment"># Don't forget the sigmoid.</span></div><div class="line"><span class="comment"># predicted_y = &lt;predicted probabilities for input_X&gt;</span></div><div class="line">predicted_y = tf.squeeze(tf.nn.sigmoid(tf.add(tf.matmul(input_X, weights), b)))</div><div class="line">print(predicted_y)</div><div class="line"></div><div class="line"><span class="comment"># Loss. Should be a scalar number - average loss over all the objects</span></div><div class="line"><span class="comment"># tf.reduce_mean is your friend here</span></div><div class="line"><span class="comment"># loss = &lt;logistic loss (scalar, mean over sample)&gt;</span></div><div class="line">loss = -tf.reduce_mean(tf.log(predicted_y)*input_y + tf.log(<span class="number">1</span>-predicted_y)*(<span class="number">1</span>-input_y))</div><div class="line">print(loss)</div><div class="line"></div><div class="line"><span class="comment"># See above for an example. tf.train.*Optimizer</span></div><div class="line"><span class="comment"># optimizer = &lt;optimizer that minimizes loss&gt;</span></div><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss)</div><div class="line">print(optimizer)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># A test to help with the debugging</span></div><div class="line"></div><div class="line"><span class="comment"># In[105]:</span></div><div class="line"></div><div class="line">validation_weights = <span class="number">1e-3</span> * np.fromiter(map(<span class="keyword">lambda</span> x:</div><div class="line">        s.run(weird_psychotic_function, &#123;my_scalar:x, my_vector:[<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">2</span>]&#125;),</div><div class="line">                                   <span class="number">0.15</span> * np.arange(<span class="number">1</span>, X.shape[<span class="number">1</span>] + <span class="number">1</span>)),</div><div class="line">                                   count=X.shape[<span class="number">1</span>], dtype=np.float32)[:, np.newaxis]</div><div class="line"><span class="comment"># Compute predictions for given weights and bias</span></div><div class="line">prediction_validation = s.run(</div><div class="line">    predicted_y, &#123;</div><div class="line">    input_X: X,</div><div class="line">    weights: validation_weights,</div><div class="line">    b: <span class="number">1e-1</span>&#125;)</div><div class="line"></div><div class="line"><span class="comment"># Load the reference values for the predictions</span></div><div class="line">validation_true_values = np.loadtxt(<span class="string">"validation_predictons.txt"</span>)</div><div class="line"></div><div class="line"><span class="keyword">assert</span> prediction_validation.shape == (X.shape[<span class="number">0</span>],),       <span class="string">"Predictions must be a 1D array with length equal to the number "</span>        <span class="string">"of examples in input_X"</span></div><div class="line"><span class="keyword">assert</span> np.allclose(validation_true_values, prediction_validation)</div><div class="line">loss_validation = s.run(</div><div class="line">        loss, &#123;</div><div class="line">            input_X: X[:<span class="number">100</span>],</div><div class="line">            input_y: y[<span class="number">-100</span>:],</div><div class="line">            weights: validation_weights+<span class="number">1.21e-3</span>,</div><div class="line">            b: <span class="number">-1e-1</span>&#125;)</div><div class="line"><span class="keyword">assert</span> np.allclose(loss_validation, <span class="number">0.728689</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[106]:</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</div><div class="line">s.run(tf.global_variables_initializer())</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</div><div class="line">    s.run(optimizer, &#123;input_X: X_train, input_y: y_train&#125;)</div><div class="line">    loss_i = s.run(loss, &#123;input_X: X_train, input_y: y_train&#125;)</div><div class="line">    print(<span class="string">"loss at iter %i:%.4f"</span> % (i, loss_i))</div><div class="line">    print(<span class="string">"train auc:"</span>, roc_auc_score(y_train, s.run(predicted_y, &#123;input_X:X_train&#125;)))</div><div class="line">    print(<span class="string">"test auc:"</span>, roc_auc_score(y_test, s.run(predicted_y, &#123;input_X:X_test&#125;)))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># ### Coursera submission</span></div><div class="line"></div><div class="line"><span class="comment"># In[107]:</span></div><div class="line"></div><div class="line">grade_submitter = grading.Grader(<span class="string">"BJCiiY8sEeeCnhKCj4fcOA"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[108]:</span></div><div class="line"></div><div class="line">test_weights = <span class="number">1e-3</span> * np.fromiter(map(<span class="keyword">lambda</span> x:</div><div class="line">    s.run(weird_psychotic_function, &#123;my_scalar:x, my_vector:[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;),</div><div class="line">                               <span class="number">0.1</span> * np.arange(<span class="number">1</span>, X.shape[<span class="number">1</span>] + <span class="number">1</span>)),</div><div class="line">                               count=X.shape[<span class="number">1</span>], dtype=np.float32)[:, np.newaxis]</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># First, test prediction and loss computation. This part doesn't require a fitted model.</span></div><div class="line"></div><div class="line"><span class="comment"># In[109]:</span></div><div class="line"></div><div class="line">prediction_test = s.run(</div><div class="line">    predicted_y, &#123;</div><div class="line">    input_X: X,</div><div class="line">    weights: test_weights,</div><div class="line">    b: <span class="number">1e-1</span>&#125;)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[110]:</span></div><div class="line"></div><div class="line"><span class="keyword">assert</span> prediction_test.shape == (X.shape[<span class="number">0</span>],),       <span class="string">"Predictions must be a 1D array with length equal to the number "</span>        <span class="string">"of examples in X_test"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[111]:</span></div><div class="line"></div><div class="line">grade_submitter.set_answer(<span class="string">"0ENlN"</span>, prediction_test)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[112]:</span></div><div class="line"></div><div class="line">loss_test = s.run(</div><div class="line">    loss, &#123;</div><div class="line">        input_X: X[:<span class="number">100</span>],</div><div class="line">        input_y: y[<span class="number">-100</span>:],</div><div class="line">        weights: test_weights+<span class="number">1.21e-3</span>,</div><div class="line">        b: <span class="number">-1e-1</span>&#125;)</div><div class="line"><span class="comment"># Yes, the X/y indices mistmach is intentional</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[113]:</span></div><div class="line"></div><div class="line">grade_submitter.set_answer(<span class="string">"mMVpM"</span>, loss_test)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[114]:</span></div><div class="line"></div><div class="line">grade_submitter.set_answer(<span class="string">"D16Rc"</span>, roc_auc_score(y_test, s.run(predicted_y, &#123;input_X:X_test&#125;)))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Please use the credentials obtained from the Coursera assignment page.</span></div><div class="line"></div><div class="line"><span class="comment"># In[115]:</span></div><div class="line"></div><div class="line">grade_submitter.submit(&lt;your email&gt;, &lt;your token&gt;)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[ ]:</span></div></pre></td></tr></table></figure>
<h4 id="my1stNN-boilerplate"><a href="#my1stNN-boilerplate" class="headerlink" title="my1stNN boilerplate"></a>my1stNN boilerplate</h4><h4 id="Peer-graded-Assignment-my1stNN1h"><a href="#Peer-graded-Assignment-my1stNN1h" class="headerlink" title="Peer-graded Assignment: my1stNN1h"></a>Peer-graded Assignment: my1stNN1h</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div><div class="line">326</div><div class="line">327</div><div class="line">328</div><div class="line">329</div><div class="line">330</div><div class="line">331</div><div class="line">332</div><div class="line">333</div><div class="line">334</div><div class="line">335</div><div class="line">336</div><div class="line">337</div><div class="line">338</div><div class="line">339</div><div class="line">340</div><div class="line">341</div><div class="line">342</div><div class="line">343</div><div class="line">344</div><div class="line">345</div><div class="line">346</div><div class="line">347</div><div class="line">348</div><div class="line">349</div><div class="line">350</div><div class="line">351</div><div class="line">352</div><div class="line">353</div><div class="line">354</div><div class="line">355</div><div class="line">356</div><div class="line">357</div><div class="line">358</div><div class="line">359</div><div class="line">360</div><div class="line">361</div><div class="line">362</div><div class="line">363</div><div class="line">364</div><div class="line">365</div><div class="line">366</div><div class="line">367</div><div class="line">368</div><div class="line">369</div><div class="line">370</div><div class="line">371</div><div class="line">372</div><div class="line">373</div><div class="line">374</div><div class="line">375</div><div class="line">376</div><div class="line">377</div><div class="line">378</div><div class="line">379</div><div class="line">380</div><div class="line">381</div><div class="line">382</div><div class="line">383</div><div class="line">384</div><div class="line">385</div><div class="line">386</div><div class="line">387</div><div class="line">388</div><div class="line">389</div><div class="line">390</div><div class="line">391</div><div class="line">392</div><div class="line">393</div><div class="line">394</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="comment"># In[1]:</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> preprocessed_mnist <span class="keyword">import</span> load_dataset</div><div class="line">X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()</div><div class="line">print(X_train.shape, y_train.shape)</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">get_ipython().magic(<span class="string">'matplotlib inline'</span>)</div><div class="line">plt.imshow(X_train[<span class="number">0</span>], cmap=<span class="string">"Greys"</span>);</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[2]:</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[3]:</span></div><div class="line"></div><div class="line">print(X_train.shape, y_train.shape)</div><div class="line">print(X_val.shape, y_val.shape)</div><div class="line">print(X_test.shape, y_test.shape)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[4]:</span></div><div class="line"></div><div class="line">print(y_train[<span class="number">0</span>])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[5]:</span></div><div class="line"></div><div class="line"><span class="comment"># Reshape the training, validate and test examples </span></div><div class="line">X_train_flatten = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">-1</span>).T   <span class="comment"># The "-1" makes reshape flatten the remaining dimensions</span></div><div class="line">X_val_flatten = X_val.reshape(X_val.shape[<span class="number">0</span>], <span class="number">-1</span>).T</div><div class="line">X_test_flatten = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">-1</span>).T</div><div class="line"></div><div class="line">print(X_train_flatten.shape)</div><div class="line">print(X_val_flatten.shape)</div><div class="line">print(X_test_flatten.shape)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[6]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_matrix</span><span class="params">(labels, C)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates a matrix where the i-th row corresponds to the ith class number and the jth column</div><div class="line">                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) </div><div class="line">                     will be 1. </div><div class="line">                     </div><div class="line">    Arguments:</div><div class="line">    labels -- vector containing the labels </div><div class="line">    C -- number of classes, the depth of the one hot dimension</div><div class="line">    </div><div class="line">    Returns: </div><div class="line">    one_hot -- one hot matrix</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)</span></div><div class="line">    depth = tf.constant(C, name = <span class="string">"C"</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># Use tf.one_hot, be careful with the axis (approx. 1 line)</span></div><div class="line">    one_hot_matrix = tf.one_hot(labels, depth, axis = <span class="number">0</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># Create the session (approx. 1 line)</span></div><div class="line">    sess = tf.Session()</div><div class="line">    </div><div class="line">    <span class="comment"># Run the session (approx. 1 line)</span></div><div class="line">    one_hot = sess.run(one_hot_matrix)</div><div class="line">    </div><div class="line">    <span class="comment"># Close the session (approx. 1 line). See method 1 above.</span></div><div class="line">    sess.close()</div><div class="line">    </div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> one_hot</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[7]:</span></div><div class="line"></div><div class="line"><span class="comment"># encode y with one-hot</span></div><div class="line">y_train_one_hot = one_hot_matrix(y_train, <span class="number">10</span>)</div><div class="line">y_val_one_hot = one_hot_matrix(y_val, <span class="number">10</span>)</div><div class="line">y_test_one_hot = one_hot_matrix(y_test, <span class="number">10</span>)</div><div class="line"></div><div class="line">print(y_train_one_hot.shape)</div><div class="line">print(y_val_one_hot.shape)</div><div class="line">print(y_test_one_hot.shape)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[8]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_x, n_y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates the placeholders for the tensorflow session.</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    n_x -- scalar, size of an image vector (num_px * num_px = 28 * 28 = 784)</div><div class="line">    n_y -- scalar, number of classes (from 0 to 9, so -&gt; 10)</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">    X -- placeholder for the data input, of shape [n_x, None] and dtype "float"</div><div class="line">    Y -- placeholder for the input labels, of shape [n_y, None] and dtype "float"</div><div class="line">    </div><div class="line">    Tips:</div><div class="line">    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.</div><div class="line">      In fact, the number of examples during test/train is different.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></div><div class="line">    X = tf.placeholder(tf.float32, [n_x, <span class="keyword">None</span>], name = <span class="string">"X"</span>)</div><div class="line">    Y = tf.placeholder(tf.float32, [n_y, <span class="keyword">None</span>], name = <span class="string">"Y"</span>)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> X, Y</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[9]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Initializes parameters to build a neural network with tensorflow. The shapes are:</div><div class="line">                        W1 : [50, 784]</div><div class="line">                        b1 : [50, 1]</div><div class="line">                        W2 : [10, 50]</div><div class="line">                        b2 : [10, 1]</div><div class="line"></div><div class="line">    </div><div class="line">    Returns:</div><div class="line">    parameters -- a dictionary of tensors containing W1, b1, W2, b2</div><div class="line">    """</div><div class="line">    </div><div class="line">    tf.set_random_seed(<span class="number">1</span>)                   <span class="comment"># so that your "random" numbers match ours</span></div><div class="line">        </div><div class="line">    <span class="comment">### START CODE HERE ### (approx. 6 lines of code)</span></div><div class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">50</span>,<span class="number">784</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">1</span>))</div><div class="line">    b1 = tf.get_variable(<span class="string">"b1"</span>, [<span class="number">50</span>,<span class="number">1</span>], initializer = tf.zeros_initializer())</div><div class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, [<span class="number">10</span>, <span class="number">50</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">1</span>))</div><div class="line">    b2 = tf.get_variable(<span class="string">"b2"</span>, [<span class="number">10</span>,<span class="number">1</span>], initializer = tf.zeros_initializer())</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</div><div class="line">                  <span class="string">"b1"</span>: b1,</div><div class="line">                  <span class="string">"W2"</span>: W2,</div><div class="line">                  <span class="string">"b2"</span>: b2&#125;</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> parameters</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[10]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Implements the forward propagation for the model: LINEAR -&gt; SIGMOID -&gt; LINEAR -&gt; SOFTMAX</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    X -- input dataset placeholder, of shape (input size, number of examples)</div><div class="line">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2"</div><div class="line">                  the shapes are given in initialize_parameters</div><div class="line"></div><div class="line">    Returns:</div><div class="line">    Z2 -- the output of the last LINEAR unit</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></div><div class="line">    W1 = parameters[<span class="string">'W1'</span>]</div><div class="line">    b1 = parameters[<span class="string">'b1'</span>]</div><div class="line">    W2 = parameters[<span class="string">'W2'</span>]</div><div class="line">    b2 = parameters[<span class="string">'b2'</span>]</div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:</span></div><div class="line">    Z1 = tf.add(tf.matmul(W1, X), b1)                      <span class="comment"># Z1 = np.dot(W1, X) + b1</span></div><div class="line">    A1 = tf.nn.relu(Z1)                                    <span class="comment"># A1 = relu(Z1)</span></div><div class="line">    Z2 = tf.add(tf.matmul(W2, A1), b2)                     <span class="comment"># Z2 = np.dot(W2, a1) + b2</span></div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Z2</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[11]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z2, Y)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Computes the cost</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    Z2 -- output of forward propagation (output of the last LINEAR unit), of shape (10, number of examples)</div><div class="line">    Y -- "true" labels vector placeholder, same shape as Z2</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">    cost - Tensor of the cost function</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span></div><div class="line">    logits = tf.transpose(Z2)</div><div class="line">    labels = tf.transpose(Y)</div><div class="line">    </div><div class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></div><div class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> cost</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[12]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates a list of random minibatches from (X, Y)</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    X -- input data, of shape (input size, number of examples)</div><div class="line">    Y -- input target, of shape (10, number of examples)</div><div class="line">    mini_batch_size -- size of the mini-batches, integer</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</div><div class="line">    """</div><div class="line">    </div><div class="line">    np.random.seed(seed)            <span class="comment"># To make your "random" minibatches the same as ours</span></div><div class="line">    m = X.shape[<span class="number">1</span>]                  <span class="comment"># number of training examples</span></div><div class="line">    mini_batches = []</div><div class="line">        </div><div class="line">    <span class="comment"># Step 1: Shuffle (X, Y)</span></div><div class="line">    permutation = list(np.random.permutation(m))</div><div class="line">    shuffled_X = X[:, permutation]</div><div class="line">    shuffled_Y = Y[:, permutation]</div><div class="line"></div><div class="line">    <span class="comment"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></div><div class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># number of mini batches of size mini_batch_size in your partitionning</span></div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</div><div class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></div><div class="line">        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</div><div class="line">        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</div><div class="line">        mini_batches.append(mini_batch)</div><div class="line">    </div><div class="line">    <span class="comment"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></div><div class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</div><div class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></div><div class="line">        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : ]</div><div class="line">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size : ]</div><div class="line">        <span class="comment">### END CODE HERE ###</span></div><div class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</div><div class="line">        mini_batches.append(mini_batch)</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> mini_batches</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[27]:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_val, Y_val, learning_rate = <span class="number">0.0001</span>,</span></span></div><div class="line">          num_epochs = <span class="number">1000</span>, minibatch_size = <span class="number">32</span>, print_cost = True):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Implements a two-layer tensorflow neural network: LINEAR-&gt;SIGMOID-&gt;LINEAR-&gt;SOFTMAX.</div><div class="line">    </div><div class="line">    Arguments:</div><div class="line">    X_train -- training set, of shape (input size = 784, number of training examples = 50000)</div><div class="line">    Y_train -- training set, of shape (output size = 10, number of training examples = 50000)</div><div class="line">    X_val -- validation set, of shape (input size = 784, number of validation examples = 10000)</div><div class="line">    Y_val -- validation set, of shape (output size = 10, number of validation examples = 10000)</div><div class="line">    learning_rate -- learning rate of the optimization</div><div class="line">    num_epochs -- number of epochs of the optimization loop</div><div class="line">    minibatch_size -- size of a minibatch</div><div class="line">    print_cost -- True to print the cost every 100 epochs</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">    parameters -- parameters learnt by the model. They can then be used to predict.</div><div class="line">    """</div><div class="line">    </div><div class="line">    ops.reset_default_graph()                         <span class="comment"># to be able to rerun the model without overwriting tf variables</span></div><div class="line">    tf.set_random_seed(<span class="number">1</span>)                             <span class="comment"># to keep consistent results</span></div><div class="line">    seed = <span class="number">3</span>                                          <span class="comment"># to keep consistent results</span></div><div class="line">    (n_x, m) = X_train.shape                          <span class="comment"># (n_x: input size, m : number of examples in the train set)</span></div><div class="line">    n_y = Y_train.shape[<span class="number">0</span>]                            <span class="comment"># n_y : output size</span></div><div class="line">    costs = []                                        <span class="comment"># To keep track of the cost</span></div><div class="line">    </div><div class="line">    <span class="comment"># Create Placeholders of shape (n_x, n_y)</span></div><div class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></div><div class="line">    X, Y = create_placeholders(n_x, n_y)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line"></div><div class="line">    <span class="comment"># Initialize parameters</span></div><div class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></div><div class="line">    parameters = initialize_parameters()</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Forward propagation: Build the forward propagation in the tensorflow graph</span></div><div class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></div><div class="line">    Z2 = forward_propagation(X, parameters)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Cost function: Add cost function to tensorflow graph</span></div><div class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></div><div class="line">    cost = compute_cost(Z2, Y)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.</span></div><div class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></div><div class="line">    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)</div><div class="line">    <span class="comment">### END CODE HERE ###</span></div><div class="line">    </div><div class="line">    <span class="comment"># Initialize all the variables</span></div><div class="line">    init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">    <span class="comment"># Start the session to compute the tensorflow graph</span></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        </div><div class="line">        <span class="comment"># Run the initialization</span></div><div class="line">        sess.run(init)</div><div class="line">        </div><div class="line">        <span class="comment"># Do the training loop</span></div><div class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</div><div class="line"></div><div class="line">            epoch_cost = <span class="number">0.</span>                       <span class="comment"># Defines a cost related to an epoch</span></div><div class="line">            num_minibatches = int(m / minibatch_size) <span class="comment"># number of minibatches of size minibatch_size in the train set</span></div><div class="line">            seed = seed + <span class="number">1</span></div><div class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</div><div class="line"></div><div class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</div><div class="line"></div><div class="line">                <span class="comment"># Select a minibatch</span></div><div class="line">                (minibatch_X, minibatch_Y) = minibatch</div><div class="line">                </div><div class="line">                <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></div><div class="line">                <span class="comment"># Run the session to execute the "optimizer" and the "cost", the feedict should contain a minibatch for (X,Y).</span></div><div class="line">                <span class="comment">### START CODE HERE ### (1 line)</span></div><div class="line">                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</div><div class="line">                <span class="comment">### END CODE HERE ###</span></div><div class="line">                </div><div class="line">                epoch_cost += minibatch_cost / num_minibatches</div><div class="line"></div><div class="line">            <span class="comment"># Print the cost every epoch</span></div><div class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, epoch_cost))</div><div class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">                costs.append(epoch_cost)</div><div class="line">            </div><div class="line">            <span class="comment"># Early stoping condition </span></div><div class="line">            <span class="comment">#if np.absolute(costs[-1] - epoch_cost) &lt; 1e-12:</span></div><div class="line">            <span class="comment">#    break</span></div><div class="line">                </div><div class="line">        <span class="comment"># plot the cost</span></div><div class="line">        plt.plot(np.squeeze(costs))</div><div class="line">        plt.ylabel(<span class="string">'cost'</span>)</div><div class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</div><div class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</div><div class="line">        plt.show()</div><div class="line"></div><div class="line">        <span class="comment"># lets save the parameters in a variable</span></div><div class="line">        parameters = sess.run(parameters)</div><div class="line">        <span class="keyword">print</span> (<span class="string">"Parameters have been trained!"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Calculate the correct predictions</span></div><div class="line">        correct_prediction = tf.equal(tf.argmax(Z2), tf.argmax(Y))</div><div class="line"></div><div class="line">        <span class="comment"># Calculate accuracy on the test set</span></div><div class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</div><div class="line"></div><div class="line">        <span class="keyword">print</span> (<span class="string">"Train Accuracy:"</span>, accuracy.eval(&#123;X: X_train, Y: Y_train&#125;))</div><div class="line">        <span class="keyword">print</span> (<span class="string">"Validation Accuracy:"</span>, accuracy.eval(&#123;X: X_val, Y: Y_val&#125;))</div><div class="line">        </div><div class="line">        <span class="keyword">return</span> parameters</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[ ]:</span></div><div class="line"></div><div class="line">parameters = model(X_train_flatten, y_train_one_hot, X_val_flatten, y_val_one_hot)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[32]:</span></div><div class="line"></div><div class="line"><span class="comment"># Start the session to compute the tensorflow graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    n_x = <span class="number">784</span></div><div class="line">    n_y = <span class="number">10</span></div><div class="line">    X, Y = create_placeholders(n_x, n_y)</div><div class="line">    Z2 = forward_propagation(X, parameters)</div><div class="line">    <span class="comment"># Calculate the correct predictions</span></div><div class="line">    correct_prediction = tf.equal(tf.argmax(Z2), tf.argmax(Y))</div><div class="line"></div><div class="line">    <span class="comment"># Calculate accuracy on the test set</span></div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</div><div class="line"></div><div class="line">    <span class="keyword">print</span> (<span class="string">"Test Accuracy:"</span>, accuracy.eval(&#123;X: X_test_flatten, Y: y_test_one_hot&#125;))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># In[ ]:</span></div></pre></td></tr></table></figure>
<h4 id="Review-Your-Peers-my1stNN"><a href="#Review-Your-Peers-my1stNN" class="headerlink" title="Review Your Peers: my1stNN"></a>Review Your Peers: my1stNN</h4><h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h3><h4 id="Keras-task-ipynb"><a href="#Keras-task-ipynb" class="headerlink" title="Keras-task.ipynb"></a>Keras-task.ipynb</h4><h4 id="Keras-introduction10-min"><a href="#Keras-introduction10-min" class="headerlink" title="Keras introduction10 min"></a>Keras introduction10 min</h4><h4 id="Programming-Assignment-my1stNN-Keras-this-time1h"><a href="#Programming-Assignment-my1stNN-Keras-this-time1h" class="headerlink" title="Programming Assignment: my1stNN - Keras this time1h"></a>Programming Assignment: my1stNN - Keras this time1h</h4><div class="note primary"><p>primary</p>
</div>
<h3 id="Philosophy-of-deep-learning"><a href="#Philosophy-of-deep-learning" class="headerlink" title="Philosophy of deep learning"></a>Philosophy of deep learning</h3><h4 id="What-Deep-Learning-is-and-is-not8-min"><a href="#What-Deep-Learning-is-and-is-not8-min" class="headerlink" title="What Deep Learning is and is not8 min"></a>What Deep Learning is and is not8 min</h4><h4 id="Deep-learning-as-a-language6-min"><a href="#Deep-learning-as-a-language6-min" class="headerlink" title="Deep learning as a language6 min"></a>Deep learning as a language6 min</h4><h3 id="Optional-Honors-Content"><a href="#Optional-Honors-Content" class="headerlink" title="Optional Honors Content"></a>Optional Honors Content</h3><h3 id="Neural-networks-the-hard-way"><a href="#Neural-networks-the-hard-way" class="headerlink" title="Neural networks the hard way"></a>Neural networks the hard way</h3><h4 id="NumpyNN-honor-ipynb"><a href="#NumpyNN-honor-ipynb" class="headerlink" title="NumpyNN (honor).ipynb"></a>NumpyNN (honor).ipynb</h4><h4 id="Peer-graded-Assignment-Your-very-own-neural-network2h"><a href="#Peer-graded-Assignment-Your-very-own-neural-network2h" class="headerlink" title="Peer-graded Assignment: Your very own neural network2h"></a>Peer-graded Assignment: Your very own neural network2h</h4><div class="note primary"><p>primary</p>
</div>
<h4 id="Review-Your-Peers-Your-very-own-neural-network"><a href="#Review-Your-Peers-Your-very-own-neural-network" class="headerlink" title="Review Your Peers: Your very own neural network"></a>Review Your Peers: Your very own neural network</h4><h2 id="Week"><a href="#Week" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-1"><a href="#Week-1" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-2"><a href="#Week-2" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-3"><a href="#Week-3" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h1 id="How-to-Win-a-Data-Science-Competition-Learn-from-Top-Kagglers"><a href="#How-to-Win-a-Data-Science-Competition-Learn-from-Top-Kagglers" class="headerlink" title="How to Win a Data Science Competition: Learn from Top Kagglers"></a>How to Win a Data Science Competition: Learn from Top Kagglers</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/competitive-data-science" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="">here</a></p>
</div>
<h2 id="Week-4"><a href="#Week-4" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-5"><a href="#Week-5" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-6"><a href="#Week-6" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h1 id="Bayesian-Methods-for-Machine-Learning"><a href="#Bayesian-Methods-for-Machine-Learning" class="headerlink" title="Bayesian Methods for Machine Learning"></a>Bayesian Methods for Machine Learning</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="">here</a></p>
</div>
<h2 id="Week-7"><a href="#Week-7" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-8"><a href="#Week-8" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-9"><a href="#Week-9" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h1 id="Introduction-to-Reinforcement-Learning"><a href="#Introduction-to-Reinforcement-Learning" class="headerlink" title="Introduction to Reinforcement Learning"></a>Introduction to Reinforcement Learning</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/introduction-to-reinforcement-learning" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="">here</a></p>
</div>
<h2 id="Week-10"><a href="#Week-10" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-11"><a href="#Week-11" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-12"><a href="#Week-12" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h1 id="Deep-Learning-in-Computer-Vision"><a href="#Deep-Learning-in-Computer-Vision" class="headerlink" title="Deep Learning in Computer Vision"></a>Deep Learning in Computer Vision</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/deep-learning-in-computer-vision" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="">here</a></p>
</div>
<h2 id="Week-13"><a href="#Week-13" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-14"><a href="#Week-14" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-15"><a href="#Week-15" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h1 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/language-processing" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="">here</a></p>
</div>
<h2 id="Week-16"><a href="#Week-16" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-17"><a href="#Week-17" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-18"><a href="#Week-18" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h1 id="Addressing-Large-Hadron-Collider-Challenges-by-Machine-Learning"><a href="#Addressing-Large-Hadron-Collider-Challenges-by-Machine-Learning" class="headerlink" title="Addressing Large Hadron Collider Challenges by Machine Learning"></a>Addressing Large Hadron Collider Challenges by Machine Learning</h1><div class="note primary"><p>Course can be found <a href="https://www.coursera.org/learn/hadron-collider-machine-learning" target="_blank" rel="external">here</a><br>Lecture slides can be found <a href="">here</a></p>
</div>
<h2 id="Week-19"><a href="#Week-19" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-20"><a href="#Week-20" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
<h2 id="Week-21"><a href="#Week-21" class="headerlink" title="Week"></a>Week</h2><div class="note primary"><p>primary</p>
</div>
<div class="note primary"><p>primary</p>
</div>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera/" rel="tag"># Coursera</a>
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/26/Coursera PU 程序设计与算法 Specialization/" rel="next" title="Coursera PU 程序设计与算法 Specialization">
                <i class="fa fa-chevron-left"></i> Coursera PU 程序设计与算法 Specialization
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  
    <div class="sidebar-toggle">
      <div class="sidebar-toggle-line-wrap">
        <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
        <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
        <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
      </div>
    </div>
  

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          
            
          
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="SSQ" />
          <p class="site-author-name" itemprop="name">SSQ</p>
           
              <p class="site-description motion-element" itemprop="description">Notebook for quick search</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">36</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">33</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/SSQ" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-Deep-Learning"><span class="nav-number">1.</span> <span class="nav-text">Introduction to Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-1-Introduction-to-optimization"><span class="nav-number">1.1.</span> <span class="nav-text">Week 1 Introduction to optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Course-intro"><span class="nav-number">1.1.1.</span> <span class="nav-text">Course intro</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Welcome-5-min"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Welcome!5 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-model-as-the-simplest-neural-network"><span class="nav-number">1.1.2.</span> <span class="nav-text">Linear model as the simplest neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-regression-9-min"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Linear regression 9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-classification-10-min"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Linear classification 10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-descent-5-min"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">Gradient descent 5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Linear-models-3-questions"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">Quiz: Linear models 3 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization-in-machine-learning"><span class="nav-number">1.1.3.</span> <span class="nav-text">Regularization in machine learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting-problem-and-model-validation-6-min"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Overfitting problem and model validation 6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-regularization-5-min"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">Model regularization 5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quiz-Overfitting-and-regularization-4-questions"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">Quiz: Overfitting and regularization 4 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stochastic-methods-for-optimization"><span class="nav-number">1.1.4.</span> <span class="nav-text">Stochastic methods for optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Stochastic-gradient-descent-5-min"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">Stochastic gradient descent 5 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-descent-extensions-9-min"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">Gradient descent extensions 9 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-models-and-optimization"><span class="nav-number">1.1.4.3.</span> <span class="nav-text">Linear models and optimization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment-Linear-models-and-optimization-3h"><span class="nav-number">1.1.4.4.</span> <span class="nav-text">Programming Assignment: Linear models and optimization 3h</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-2-Introduction-to-neural-networks"><span class="nav-number">1.2.</span> <span class="nav-text">Week 2 Introduction to neural networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Multilayer-perceptron-or-the-basic-principles-of-deep-learning"><span class="nav-number">1.2.1.</span> <span class="nav-text">Multilayer perceptron, or the basic principles of deep learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multilayer-perceptron6-min"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Multilayer perceptron6 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-a-neural-network7-min"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Training a neural network7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Backpropagation-primer7-min"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Backpropagation primer7 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Practice-Quiz-Multilayer-perceptron4-questions"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">Practice Quiz: Multilayer perceptron4 questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensorflow"><span class="nav-number">1.2.2.</span> <span class="nav-text">Tensorflow</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tensorflow-task-ipynb"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Tensorflow_task.ipynb</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Going-deeper-with-Tensorflow11-min"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">Going deeper with Tensorflow11 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Practice-Programming-Assignment-MSE-in-TensorFlow15-min"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">Practice Programming Assignment: MSE in TensorFlow15 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradients-amp-optimization-in-Tensorflow8-min"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">Gradients & optimization in Tensorflow8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment-Logistic-regression-in-TensorFlow30-min"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">Programming Assignment: Logistic regression in TensorFlow30 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#my1stNN-boilerplate"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">my1stNN boilerplate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Peer-graded-Assignment-my1stNN1h"><span class="nav-number">1.2.2.7.</span> <span class="nav-text">Peer-graded Assignment: my1stNN1h</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Review-Your-Peers-my1stNN"><span class="nav-number">1.2.2.8.</span> <span class="nav-text">Review Your Peers: my1stNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Keras"><span class="nav-number">1.2.3.</span> <span class="nav-text">Keras</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Keras-task-ipynb"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">Keras-task.ipynb</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Keras-introduction10-min"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">Keras introduction10 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Programming-Assignment-my1stNN-Keras-this-time1h"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">Programming Assignment: my1stNN - Keras this time1h</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Philosophy-of-deep-learning"><span class="nav-number">1.2.4.</span> <span class="nav-text">Philosophy of deep learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-Deep-Learning-is-and-is-not8-min"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">What Deep Learning is and is not8 min</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-learning-as-a-language6-min"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">Deep learning as a language6 min</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optional-Honors-Content"><span class="nav-number">1.2.5.</span> <span class="nav-text">Optional Honors Content</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-networks-the-hard-way"><span class="nav-number">1.2.6.</span> <span class="nav-text">Neural networks the hard way</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NumpyNN-honor-ipynb"><span class="nav-number">1.2.6.1.</span> <span class="nav-text">NumpyNN (honor).ipynb</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Peer-graded-Assignment-Your-very-own-neural-network2h"><span class="nav-number">1.2.6.2.</span> <span class="nav-text">Peer-graded Assignment: Your very own neural network2h</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Review-Your-Peers-Your-very-own-neural-network"><span class="nav-number">1.2.6.3.</span> <span class="nav-text">Review Your Peers: Your very own neural network</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week"><span class="nav-number">1.3.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-1"><span class="nav-number">1.4.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-2"><span class="nav-number">1.5.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-3"><span class="nav-number">1.6.</span> <span class="nav-text">Week</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#How-to-Win-a-Data-Science-Competition-Learn-from-Top-Kagglers"><span class="nav-number">2.</span> <span class="nav-text">How to Win a Data Science Competition: Learn from Top Kagglers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-4"><span class="nav-number">2.1.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-5"><span class="nav-number">2.2.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-6"><span class="nav-number">2.3.</span> <span class="nav-text">Week</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bayesian-Methods-for-Machine-Learning"><span class="nav-number">3.</span> <span class="nav-text">Bayesian Methods for Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-7"><span class="nav-number">3.1.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-8"><span class="nav-number">3.2.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-9"><span class="nav-number">3.3.</span> <span class="nav-text">Week</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-Reinforcement-Learning"><span class="nav-number">4.</span> <span class="nav-text">Introduction to Reinforcement Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-10"><span class="nav-number">4.1.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-11"><span class="nav-number">4.2.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-12"><span class="nav-number">4.3.</span> <span class="nav-text">Week</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Learning-in-Computer-Vision"><span class="nav-number">5.</span> <span class="nav-text">Deep Learning in Computer Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-13"><span class="nav-number">5.1.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-14"><span class="nav-number">5.2.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-15"><span class="nav-number">5.3.</span> <span class="nav-text">Week</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Natural-Language-Processing"><span class="nav-number">6.</span> <span class="nav-text">Natural Language Processing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-16"><span class="nav-number">6.1.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-17"><span class="nav-number">6.2.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-18"><span class="nav-number">6.3.</span> <span class="nav-text">Week</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Addressing-Large-Hadron-Collider-Challenges-by-Machine-Learning"><span class="nav-number">7.</span> <span class="nav-text">Addressing Large Hadron Collider Challenges by Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-19"><span class="nav-number">7.1.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-20"><span class="nav-number">7.2.</span> <span class="nav-text">Week</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-21"><span class="nav-number">7.3.</span> <span class="nav-text">Week</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SSQ</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://SSQ.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://ssq.github.io/2017/11/19/Coursera HSE Advanced Machine Learning Specialization/';
          this.page.identifier = '2017/11/19/Coursera HSE Advanced Machine Learning Specialization/';
          this.page.title = 'Coursera HSE Advanced Machine Learning Specialization';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://SSQ.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (search_path.endsWith("json")) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("MWlUL7cwf0gV4nd1BczDGmFm-gzGzoHsz", "YwuYEA1xBo0rm9hzIjUwOm2F");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
