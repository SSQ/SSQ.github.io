<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2018校招笔试题解答]]></title>
    <url>%2F2017%2F08%2F26%2F2018%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AF%95%E9%A2%98%E8%A7%A3%E7%AD%94%2F</url>
    <content type="text"><![CDATA[抛砖引玉 2018阿里校招笔试Question 11234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npN = 9intersections = np.array([[0,3],[1,5],[2,7],[3,3],[4,5],[5,7],[6,9],[7,3],[8,5]])M = 14roads = np.array([[0,1,4],[0,7,8],[1,2,8],[1,7,11],[2,3,7],[2,5,4],[2,8,2],[3,4,9],[3,5,14],[4,5,10],[5,6,2],[6,8,6],[6,7,1],[7,8,7]])s = 0t = 4def WaitTime(CurrentTime,permision): if CurrentTime % (2*permision) in [x for x in range(permision)]: # Green light return 0 else: return (2*permision) - CurrentTime % (2*permision) def CanGo(source): NodeWithTime = &#123;&#125; for i in range(len(roads[roads[:,0] == source])): NodeWithTime[roads[roads[:,0] == source][i,1]] = roads[roads[:,0] == source][i,2] for i in range(len(roads[roads[:,1] == source])): NodeWithTime[roads[roads[:,1] == source][i,0]] = roads[roads[:,1] == source][i,2] return NodeWithTime def minTravelTime(N,intersections,M,roads,s,t): Nodes = [s] A = &#123;&#125; A[s] = 0 for source in Nodes: permision = intersections[source][1] NodeWithTime = CanGo(source) for determination in NodeWithTime.keys(): value = A[source] + WaitTime(A[source],permision) + NodeWithTime[determination] if determination in A.keys(): A[determination] = min(value, A[determination]) else: A[determination] = value if determination not in Nodes: Nodes.extend([determination]) #print Nodes #print A return A[t] print minTravelTime(N,intersections,M,roads,s,t)# 28 Question 212345678910def number(k): count = 1 iteration = 1 while count &lt; k: count += iteration +1 iteration += 1 startposition = count - iteration return k - startposition]]></content>
  </entry>
  <entry>
    <title><![CDATA[Udacity DLND Notebook]]></title>
    <url>%2F2017%2F08%2F14%2FUdacity%20DLND%20Notebook%2F</url>
    <content type="text"><![CDATA[Not enroll yet.Here‘s preview]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Functions Notebook]]></title>
    <url>%2F2017%2F08%2F11%2FPython%20Functions%20Notebook%2F</url>
    <content type="text"><![CDATA[Notation for errors I met.For quick search Build in functionAassertcode:assert statementexplanation:1234if statement == True: continueelse: report AssertionError Rreadline()read the first line and begin with second linedata:12Title,Released,Label,UK Chart Position,US Chart Position,BPI Certification,RIAA CertificationPlease Please Me,22 March 1963,Parlophone(UK),1,-,Gold,Platinum code:1234with open(datafile, &quot;r&quot;) as f: header = f.readline().split(&quot;,&quot;) for line in f: ... header:[&#39;Title&#39;, &#39;Released&#39;, &#39;Label&#39;, &#39;UK Chart Position&#39;, &#39;US Chart Position&#39;, &#39;BPI Certification&#39;, &#39;RIAA Certification\n&#39;]line:Please Please Me,22 March 1963,Parlophone(UK),1,-,Gold,Platinum Sstrip()can be used to delete the \n codecode:1234header = f.readline().split(&quot;,&quot;)print headerprint header[-1]print header[-1].strip() output:1234[&apos;Title&apos;, &apos;Released&apos;, &apos;Label&apos;, &apos;UK Chart Position&apos;, &apos;US Chart Position&apos;, &apos;BPI Certification&apos;, &apos;RIAA Certification\n&apos;]RIAA CertificationRIAA Certification Imported modulescsvcsv.DictReader()read csv file,default denote the first row as the field labels,line is dict data typecode:12345import csvwith open(datafile, &apos;rb&apos;) as sd: r = csv.DictReader(sd) for line in r: ... more details can be found in ud032 Using CSV Module xlrdxlrd.open_workbook()read xls filecode:1234567import xlrddatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;workbook = xlrd.open_workbook(datafile)sheet = workbook.sheet_by_index(0)data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] details can be found in ud032 Notebook ZipFilecode:12from zipfile import ZipFiledatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud032 Data Wrangling with MongoDB Notebook]]></title>
    <url>%2F2017%2F08%2F11%2FUdacity%20ud032%20Data%20Wrangling%20with%20MongoDB%20Notebook%2F</url>
    <content type="text"><![CDATA[Course can be found here, classroom.wiki Data Extraction FundamentalsIntroQuiz: Action TimeAssessing the Quality of Data Pt. 1Assessing the Quality of Data Pt. 2Tabular FormatsCSV FormatYou can download the datafiles from the Supporting Materials link on this page or the Course Materials page. Pick the data format supported by your spreadsheet application, download the file, open it in the spreadsheet, then export it as “csv” file. Compare the size of the spreadsheet file with the size of the csv file! Beatles Discography (csv) Quiz: Parsing CSV Filescode:123456789101112131415161718192021222324252627282930313233343536# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)# split each line on &quot;,&quot; and then for each line, create a dictionary# where the key is the header title of the field, and the value is the value of that field in the row.# The function parse_file should return a list of dictionaries,# each data line in the file being a single list entry.# Field names and values should not contain extra whitespace, like spaces or newline characters.# You can use the Python string method strip() to remove the extra whitespace.# You have to parse only the first 10 data lines in this exercise,# so the returned list should have 10 entries!import osDATADIR = &quot;&quot;DATAFILE = &quot;beatles-diskography.csv&quot;def parse_file(datafile): data = [] with open(datafile, &quot;r&quot;) as f: for line in f: print line return datadef test(): # a simple test of your implemetation datafile = os.path.join(DATADIR, DATAFILE) d = parse_file(datafile) firstline = &#123;&apos;Title&apos;: &apos;Please Please Me&apos;, &apos;UK Chart Position&apos;: &apos;1&apos;, &apos;Label&apos;: &apos;Parlophone(UK)&apos;, &apos;Released&apos;: &apos;22 March 1963&apos;, &apos;US Chart Position&apos;: &apos;-&apos;, &apos;RIAA Certification&apos;: &apos;Platinum&apos;, &apos;BPI Certification&apos;: &apos;Gold&apos;&#125; tenthline = &#123;&apos;Title&apos;: &apos;&apos;, &apos;UK Chart Position&apos;: &apos;1&apos;, &apos;Label&apos;: &apos;Parlophone(UK)&apos;, &apos;Released&apos;: &apos;10 July 1964&apos;, &apos;US Chart Position&apos;: &apos;-&apos;, &apos;RIAA Certification&apos;: &apos;&apos;, &apos;BPI Certification&apos;: &apos;Gold&apos;&#125; assert d[0] == firstline assert d[9] == tenthline test() solution:123456789101112131415def parse_file(datafile): data = [] with open(datafile, &quot;r&quot;) as f: header = f.readline().split(&quot;,&quot;) count = 0 for line in f: if count == 10: break field = line.split(&quot;,&quot;) entry = &#123;&#125; for i, value in enumerate(field): entry[header[i].strip()] = value.strip() data.append(entry) count += 1 return data You can check the data in the dropdown in the top-left corner of the quiz starter code or download the datafile beatles-diskography.csv in the Supporting Materials below. Python string method strip() will come in handy to get rid of the extra whitespace (that includes newline character at the end of line) Quiz: Problematic LineUsing CSV Modulecsv.DictReader()default denote the first row as the field labelsline is dict data typecode:123456import csvdatafile = &quot;.csv&quot;with open(datafile, &apos;rb&apos;) as sd: r = csv.DictReader(sd) for line in r: ... You can read more about the python csv module at the link below:http://docs.python.org/2/library/csv.html Intro to XLRDYou might find this video a lot more exciting if you try to run this code locally along the video! You can download the datafile from Course Materials. You can also install the xlrd library locally on your computer via python pip and the following command: pip install xlrdThe example code:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import xlrddatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;def parse_file(datafile): workbook = xlrd.open_workbook(datafile) sheet = workbook.sheet_by_index(0) data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] print &quot;\nList Comprehension&quot; print &quot;data[3][2]:&quot;, print data[3][2] print &quot;\nCells in a nested loop:&quot; for row in range(sheet.nrows): for col in range(sheet.ncols): if row == 50: print sheet.cell_value(row, col), ### other useful methods: print &quot;\nROWS, COLUMNS, and CELLS:&quot; print &quot;Number of rows in the sheet:&quot;, print sheet.nrows print &quot;Type of data in cell (row 3, col 2):&quot;, print sheet.cell_type(3, 2) print &quot;Value in cell (row 3, col 2):&quot;, print sheet.cell_value(3, 2) print &quot;Get a slice of values in column 3, from rows 1-3:&quot; print sheet.col_values(3, start_rowx=1, end_rowx=4) print &quot;\nDATES:&quot; print &quot;Type of data in cell (row 1, col 0):&quot;, print sheet.cell_type(1, 0) exceltime = sheet.cell_value(1, 0) print &quot;Time in Excel format:&quot;, print exceltime print &quot;Convert time to a Python datetime tuple, from the Excel float:&quot;, print xlrd.xldate_as_tuple(exceltime, 0) return datadata = parse_file(datafile) Quiz: Reading Excel FilesYou can download the “2013_ERCOT_Hourly_Load_Data.xls” datafile from the Supporting Materials section on this page or from this Course Materials page. Note that the code expects the data to be contained in an archive named “2013_ERCOT_Hourly_Load_Data.xls.zip”, so you will need to change the name of the downloaded archive or modify the code to run the code on your local computer. You can also install the xlrd library locally on your computer via python pip and the following command:pip install xlrd 2013 ERCOT hourly load data code:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/usr/bin/env python&quot;&quot;&quot;Your task is as follows:- read the provided Excel file- find and return the min, max and average values for the COAST region- find and return the time value for the min and max entries- the time values should be returned as Python tuplesPlease see the test function for the expected return format&quot;&quot;&quot;import xlrdfrom zipfile import ZipFiledatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;def open_zip(datafile): with ZipFile(&apos;&#123;0&#125;.zip&apos;.format(datafile), &apos;r&apos;) as myzip: myzip.extractall()def parse_file(datafile): workbook = xlrd.open_workbook(datafile) sheet = workbook.sheet_by_index(0) ### example on how you can get the data #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] ### other useful methods: # print &quot;\nROWS, COLUMNS, and CELLS:&quot; # print &quot;Number of rows in the sheet:&quot;, # print sheet.nrows # print &quot;Type of data in cell (row 3, col 2):&quot;, # print sheet.cell_type(3, 2) # print &quot;Value in cell (row 3, col 2):&quot;, # print sheet.cell_value(3, 2) # print &quot;Get a slice of values in column 3, from rows 1-3:&quot; # print sheet.col_values(3, start_rowx=1, end_rowx=4) # print &quot;\nDATES:&quot; # print &quot;Type of data in cell (row 1, col 0):&quot;, # print sheet.cell_type(1, 0) # exceltime = sheet.cell_value(1, 0) # print &quot;Time in Excel format:&quot;, # print exceltime # print &quot;Convert time to a Python datetime tuple, from the Excel float:&quot;, # print xlrd.xldate_as_tuple(exceltime, 0) data = &#123; &apos;maxtime&apos;: (0, 0, 0, 0, 0, 0), &apos;maxvalue&apos;: 0, &apos;mintime&apos;: (0, 0, 0, 0, 0, 0), &apos;minvalue&apos;: 0, &apos;avgcoast&apos;: 0 &#125; return datadef test(): open_zip(datafile) data = parse_file(datafile) assert data[&apos;maxtime&apos;] == (2013, 8, 13, 17, 0, 0) assert round(data[&apos;maxvalue&apos;], 10) == round(18779.02551, 10)test() solution:1234567891011121314151617181920212223242526def parse_file(datafile): workbook = xlrd.open_workbook(datafile) sheet = workbook.sheet_by_index(0) cv = sheet.col_values(1, start_rowx=1, end_rowx=None) maxvalue = max(cv) maxposition = cv.index(maxvalue) + 1 minvalue = min(cv) minposition = cv.index(minvalue) + 1 max_exceltime = sheet.cell_value(maxposition, 0) maxtime = xlrd.xldate_as_tuple(max_exceltime, 0) min_exceltime = sheet.cell_value(minposition, 0) mintime = xlrd.xldate_as_tuple(min_exceltime, 0) avgcoast = sum(cv)/len(cv) data = &#123; &apos;maxtime&apos;: maxtime, &apos;maxvalue&apos;: maxvalue, &apos;mintime&apos;: mintime, &apos;minvalue&apos;: minvalue, &apos;avgcoast&apos;: avgcoast &#125; return data Intro to JSONData Modeling in JSONJSON ResourcesExtra InfoIf you’re unfamiliar with JSON, or would just like a refresher, W3Schools has a great tutorial on the subject. JSON Tutorial You can also check out http://www.json.org/ You can find information about Python’s json module on this page of the Python documentation. Note that JSON arrays are interpreted as lists and JSON objects as dictionaries, so you can use the standard Python approaches to inspect JSON data. You’ll get some practice exploring some data of this type in the next quiz. Quiz: JSON PlaygroundYou can check the data in the dropdown in the top-left corner of the quiz starter code. ‘Run locally’ means that you have to download or copy the file contents to your local machine, modify it and run. To be able to do that you need to have Python installed, as well as the requests module. Please see Requests installation documentation. If you have “pip, you can install Requests by running the following command: pip install requestsTo learn more about the requests module, see the documentation here. Quiz: Exploring JSONCheck You Out]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Data Science Community Notebook]]></title>
    <url>%2F2017%2F07%2F24%2FCoursera%20Data%20Science%20Community%20Notebook%2F</url>
    <content type="text"><![CDATA[About this course: This “course” is a community space for Coursera learners to connect with one another to discuss all things Data Science. Talk about the latest trends in the industry, give and receive advice on getting into or progressing a Data Science career, work on some portfolio projects, or just dig into some interesting problems with fellow Data Science enthusiasts. I was invited into Coursera Data Science Community in 2017-07-24 Week 1Welcome!Welcome to the Coursera Data Science community!About the Coursera Data Science community 10 minWelcome to the Coursera Data Science community! We hope you’ll get involved with some interesting Data Science discussions, chat about the latest trends and tools within Data Science, get and give career advice, and meet some great people who share your enthusiasm for Data Science! This “course” doesn’t work in quite the same way as normal courses. There aren’t any course materials so you’ll want to head over to the discussion forums to get started. ResourcesResources to accompany webinars 10 minData Science Career Webinar 1 (for MCS-DS students) _593f80302483112d4d09763e0f9eddc7_MCSDS Data Science Career Webinar 1 (for MCS-DS students) 52 minhttps://www.coursera.org/learn/data-science-community/lecture/xx6ws/data-science-career-webinar-1-for-mcs-ds-students Data Science Career Webinar 2 (for MCS-DS students) 47 minhttps://www.coursera.org/learn/data-science-community/lecture/ayP9T/data-science-career-webinar-2-for-mcs-ds-students]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity st095 Statistics Notebook]]></title>
    <url>%2F2017%2F07%2F19%2FUdacity%20st095%20Statistics%20Notebook%2F</url>
    <content type="text"><![CDATA[This course, classroom can be divided into describe (classroom) and inference statistics.wikiCompleted in 2017/08/07 Orientation!WelcomeIntroQuiz: MnMsQuiz: Definitely blueProblem SetsQuiz: ForumsQuiz: Instructor NotesMy DriveQuiz: Practice with SpreadsheetsQuiz: Calculate sumsCourse WikiYou can get to the course wiki by clicking “Materials” or following this link. Intro to Research MethodsLauren&#39;s Intro VideoQuiz: Believe ResultsQuiz: Measure MemoryDefine ConstructsBBC Memory TestMemory Test DescriptionHere is a brief description of how the test worked, to help you follow the rest of the lesson. The test consisted of three parts: Users were shown 12 photos in the first part.Users were shown another 12 photos in the second part.Users were shown 48 photos in the third part and asked if they saw each photoin the first part, the second part, or neither. After the test, users were given two scores: A “Recognition score”, calculated as the percentage of times they correctly identified whether they saw the face at all, regardless of which part the face was from.A “Temporal memory score”, calculated as the percentage of recognized faces that were identified with the correct part (part 1 or part 2). Users were advised to take a 5 minute break between each part of the test. BBC ScoresGoogle AccountQuiz: BBC MeasurementOperational DefinitionQuiz: ConstructsQuiz: Operational DefinitionsDataQuiz: Sleep and MemoryQuiz: Influence MemoryQuiz: Control for Time of Daylurking variablesextraneous factor Quiz: Same ScoresPopulation parameters (such as mu, or μ) are values that describe the entire population. Sample statistics (such as X-bar, or $\bar{x}$​​ ) are values that describe our sample; we use statistics to estimate the population parameters. Estimates are our best guesses for the population parameters. So, for example, we would use X-bar to estimate mu. Quiz: Sample AverageQuiz: Better SampleRandomnessQuiz: Visualize RelationshipQuiz: True or Not?Golden Arches TheoryMcDonald&#39;sQuiz: World PeaceCausal InferenceQuiz: Benefits of SurveysQuiz: Downsides of SurveysQuiz: PlaceboQuiz: BlindQuiz: Double BlindQuiz: Controlled FactorsRandom AssignmentQuiz: Control for What?Quiz: Katie&#39;s HandClick this link to access the data: Height and hand length Copy and paste the data into your own spreadsheet to create the scatterplot. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Draw ConclusionsProblem Set 1: Intro to statistical research methodsQuiz: 1. All California ResidentsQuiz: 2. Sleepy College StudentsQuiz: 3. Not Enough Sleep…?Quiz: 4. Characteristic of a SampleQuiz: 5. Freshman 15Quiz: 6. Characteristic of a PopulationQuiz: 7. Sample Approximates PopulationQuiz: 8. SAT ScoresQuiz: 9. Survey to FriendsQuiz: 10. Which Are Constructs?Quiz: 11. Which Are Not Constructs?Quiz: 12. Define Operational DefinitionQuiz: 13. Research StudiesQuiz: 14. VariablesQuiz: 15. Variable RelationshipsQuiz: 16. Which Are Hypotheses?Quiz: 17. Lurking AroundQuiz: 18. Symbols = Fun Fun Fun!Quiz: 19. nQuiz: 20. Which Are True?Quiz: 21. Random SampleQuiz: 22. Convenience SamplesQuiz: 23. Sample vs. PopulationQuiz: 24. Classical MusicQuiz: 25. CaffeineQuiz: 26. Video GamesQuiz: 27. English TeachingQuiz: 28. Reading ScoreQuiz: 29. Extreme TemperatureQuiz: 30. Teaching MethodQuiz: 31. Reading ScoresQuiz: 32. Which Is Better?Quiz: 33. All Kinds of VariablesQuiz: 34. LandminesQuiz: 35. LandminesQuiz: 36. Student SatisfactionQuiz: 37. InsomniaQuiz: 38. InsomniaQuiz: 39. SADQuiz: 40. Random AssignmentQuiz: 41. Placebo Control ConditionQuiz: 42. Why Placebo?Quiz: 43. Measuring ConstructsQuiz: 44. ParticipantsQuiz: 45. What Proportion?Quiz: 46. Blind StudiesQuiz: 47. Causality?Quiz: 48. DepressionQuiz: 49. DepressionQuiz: 50. DepressionConclusionconstructopertaional definitionindependent variabledependent variableextraneous or lurking variableexperimental study - casual conclusionobservational study Visualizing DataQuiz: Where Students Are FromQuiz: FrequencyQuiz: US, China, PakistanQuiz: Relative FrequencyQuiz: Range of ProportionsQuiz: Sum Relative FrequenciesQuiz: Proportion from CountriesQuiz: Convert to PercentageQuiz: Range PercentagesQuiz: ContinentsQuiz: Number of Rowsinterval = bin = bucket Quiz: Bin SizeVisualizing DataQuiz: HistogramDifferent Bin SizesClick this link to play around with the histogram applet! (You may need to install the Java plug-in.) Interactivate Histogram Applet Udacity Student Ages Data: Make sure you have one number per line for the applet. Quiz: Smaller BinQuiz: Find Bin SizeQuiz: Most Frequent AgeQuiz: Proportion over 60Quiz: Percentage Under 60Quiz: Younger than 20Quiz: Continent GraphQuiz: Difference Between Graphshistogram: bin x-axis: numerical/quantitativebar: x-axis: categorical/qualitative Quiz: Biased GraphsChanging Bin SizeQuiz: Interpret HistogramQuiz: Skewed DistributionProblem Set 2: Visualizing dataQuiz: 1. Blood TypesThe Σ symbol means the total sum. It is the Greek letter capital sigma. f stands for frequency (count), p stands for proportion. The Σ symbol means the total sum. It is the Greek letter capital sigma. f stands for frequency (count), p stands for proportion. Enter each answer as a number without any special characters, including % signs. Enter proportions as decimals. The Σ symbol means the total sum. It is the Greek letter capital sigma. Quiz: 2. Rare BloodQuiz: 3. Common BloodQuiz: 4. Type AQuiz: 5. GuesstimateQuiz: 6. Analyze StuffQuiz: 7. Ode to nQuiz: 8. Calculate PercentagesQuiz: 9. Common DecadeQuiz: 10. How Old?Quiz: 11. Really Old!Quiz: 12. Birth Year HistogramQuiz: 13. Most Common BinQuiz: 14. When Most Were BornQuiz: 15. Type of DataQuiz: 16. Heights of BarsQuiz: 17. Which Region?Quiz: 18. Calculate Bin SizeQuiz: 19. How to Find nQuiz: 20. How to Analyze ShapeQuiz: 21. Commute TimeQuiz: 22. Commute an HourQuiz: 23. Find Bin WidthQuiz: 24. Analyze HistogramQuiz: 25. Frequency and Bin SizeQuiz: 26. Positively SkewedQuiz: 27. Distribution of What?Quiz: 28. Thinking About DistributionsQuiz: 29. Frequency AxisQuiz: 30. X-Axis Represent!Quiz: 31. Skewnesspositively skewed Quiz: 32. Negatively SkewedQuiz: 33. Normal DistributionQuiz: 34. Table vs. HistogramGoogle Spreadsheet TutorialTutorialLink to Udacians’ Facebook Friends Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” You will work with this data in Lesson 3. Central tendencyQuiz: Which Major?One Number to Describe DataQuiz: Which Number to Choose?Quiz: Mode of Datasetmode: most frequency Quiz: Mode of DistributionQuiz: Mode - Negatively Skewed DistributionQuiz: Mode - Uniform DistributionQuiz: More than One Mode?Quiz: Mode of Categorical DataQuiz: More o&#39; Mode!Quiz: Find the MeanQuiz: Procedure for Finding MeanQuiz: Iterative ProcedureHelpful SymbolsQuiz: Properties of the MeanQuiz: Mean with OutlierQuiz: What Can You Expect?UNCQuiz: Requirement for MedianQuiz: Find the MedianQuiz: Median with OutlierQuiz: Find Median with OutlierMeasures of Centermean median mode Quiz: Order Measures of Center 1Quiz: Order Measures of Center 2Use Measures of Center to CompareQuiz: Udacians&#39; Facebook Friends - MeanQuiz: Udacians&#39; Facebook Friends - MedianQuiz: Formula for Location of MedianQuiz: Wrap Up - Measures of CenterGood Job!Problem Set 3: Central tendencyQuiz: 1. BBC Memory ScoresLink to spreadsheet with sample memory scores: BBC Sample Scores Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: 2. BBC Memory ScoresQuiz: 3. BBC Memory ScoresQuiz: 4. What Distribution?Quiz: 5. What Distribution?Quiz: 6. Normal DistributionQuiz: 7. Positively SkewedQuiz: 8. MeanQuiz: 9. MedianQuiz: 10. ModeQuiz: 11. Deal or No Deal? (Median)Deal or No Deal? Quiz: 12. Deal or No Deal? (Mode)Quiz: 13. Deal or No Deal? (Number)Quiz: 14. Deal or No Deal? (Mean vs. Median)Quiz: 15. Deal or No Deal? (Mean)Quiz: 16. Deal or No Deal? (Proportion)Quiz: 17. Deal or No Deal? (Frequency)Quiz: 18. Deal or No Deal? (Distribution)Quiz: 19. Deal or No Deal? (Center)Quiz: 20. NHL (Mean)Round to the nearest hundredth (xx.xx) National Hockey League Quiz: 21. NHL (Mode)Quiz: 22. NHL (Median)Quiz: 23. Which Distributions?Quiz: 24. Median Given HistogramVariabilityQuiz: Social Networkers&#39; SalariesQuiz: Should You Get an Account?Quiz: What&#39;s the Difference?Quiz: Quantify SpreadQuiz: Does Range Change?Quiz: Mark Z the OutlierChop Off the TailsQuiz: Where Is Q1?quartile Quiz: Q3 - Q1Quiz: IQRIQR=Q3-Q1 Quiz: What Is an Outlier?Quiz: Define Outlier&lt; Q1 - 1.5 IQR> Q3 + 1.5 IQR Quiz: Match BoxplotsQuiz: Mean Within IQR?Problem with IQRQuiz: Measure VariabilityQuiz: Calculate MeanQuiz: Deviation from MeanQuiz: Average DeviationQuiz: Equation for Average DeviationQuiz: Be Happy and Get Rid of NegativesQuiz: Absolute DeviationsQuiz: Average Absolute DeviationQuiz: Formula for Avg. Abs. Dev.Quiz: Squared DeviationsQuiz: Sum of SquaresQuiz: Average Squared Deviationvariance Quiz: Avg. Squared Dev. in WordsQuiz: One DimensionStandard Deviationsquare root of variance Quiz: Calculate SDQuiz: SD Social NetworkersQuiz: SD in WordsQuiz: Spreadsheet SDClick here to access the data: Sample Social Networkers’ Salary Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Click here to access the data: Sample Social Networkers’ Salary Enter your answer as a number without any special characters, including $ or commas. Link to Sample Social Networkers’ Salary Copy and paste this data into your own spreadsheet to calculate the standard deviation. Point of SDQuiz: Find ValuesQuiz: Sample SDQuiz: Bessel&#39;s CorrectionStandard Deviation of sample is smaller than Standard Deviation of population, so we use n-1 to estimate the Standard Deviation of population(sample Standard Deviation) Sample Standard deviation(Standard deviation of population):$$s = \sqrt{\frac{\sum (x_i - \bar{x})^2}{n - 1}}$$Variance:$$\frac{\sum (x_i - \bar{x})^2}{n -1}$$Standard deviation of sample$$s = \sqrt{\frac{\sum (x_i - \bar{x})^2}{n}}$$ Clarifying Sample SDJelly BeansProblem Set 4: VariabilityQuiz: 1. Udacians&#39; Facebook Friends (Mean)Click here for the link to the data: Udacians’ Facebook Friends Quiz: 2. Udacians&#39; Facebook Friends (Avg. Dev)Quiz: 3. Udacians&#39; Facebook Friends (SS)Quiz: 4. Udacians&#39; Facebook Friends (Variance)Quiz: 5. Udacians&#39; Facebook Friends (Std Dev)Quiz: 6. Udacians&#39; Facebook Friends (mean +/- std dev)Quiz: 7. Udacians&#39; Facebook Friends (proportion)Quiz: 8. Udacians&#39; Facebook Friends (sample sd)Quiz: 9. Class ExamQuiz: 10. Where&#39;s Your Score? (sd = 5)Quiz: 11. Where&#39;s Your Score? (sd = 2.5)Quiz: 12. Where&#39;s Your Score? (sd = 10)Quiz: 13. BBC Sample Scores (Std. Dev.)Link to BBC Sample Scores Quiz: 14. BBC Sample Scores (Variability)Lessons 1-4 Review/AssessmentQuiz: IQuiz: IIQuiz: IIIQuiz: IVQuiz: VQuiz: VIQuiz: VIIQuiz: VIIIQuiz: IXQuiz: XQuiz: XIQuiz: XIIQuiz: XIIIQuiz: XIVQuiz: XVQuiz: XVIQuiz: XVIIQuiz: XVIIIQuiz: XIXQuiz: XXQuiz: XXIQuiz: XXIIQuiz: XXIIIQuiz: XXIVQuiz: XXVQuiz: XXVIQuiz: XXVIIQuiz: XXVIIIQuiz: XXIXQuiz: XXXQuiz: XXXIStandardizingQuiz: ChessUSCF DistributionQuiz: Absolute or RelativeQuiz: Relative Frequency HistogramQuiz: Proportion Between 170 and 210Quiz: Proportion Between 180 and 200Quiz: More DetailInfinitely SmallQuiz: Continuous DistributionTheoretical Normal DistributionZnumber of standard deviations away from the mean UnpopularQuiz: Katie - SDs BelowQuiz: Andy - SDs BelowQuiz: Who&#39;s More Unpopular?Quiz: Formula for Number of SDsZ-ScoreLink to poll: How many Facebook friends do you have?$z = \frac{x-\mu}{\sigma}$ Quiz: Negative Z-ScoreQuiz: Mean of Standardized DistributionQuiz: SD of Standardized DistributionStandard Normal DistributionQuiz: Popular ChrisQuiz: Convert to Z-ScoreQuiz: Convert to Popularity ScoreProblem Set 5: StandardizingQuiz: 1. Which Distribution Is Which?Quiz: 2. Z-ScoresQuiz: 3. OkCupidQuiz: 4. Social MediaQuiz: 5. Z-Scores of UsageQuiz: 6. Where on the Distribution?Quiz: 7. Mean and SDQuiz: 8. IQ 125Quiz: 9. IQ 150Quiz: 10. ScoresQuiz: 11. Grade on a CurveQuiz: 12. ExtremeQuiz: 13. SDQuiz: 14. SJSU FootballQuiz: 15. ExamsQuiz: 16. Which SD?Quiz: 17. Closest to MeanQuiz: 18. Farthest from MeanQuiz: 19. True or False?Quiz: 20. BBC - SleepQuiz: 21. BBC - RecognitionQuiz: 22. BBC - TemporalQuiz: 23. BBC - What Score?Normal DistributionIntro to the PDFprobability density function Quiz: ProbabilityGet to Know the PDFQuiz: Probability GreaterQuiz: Probability LessQuiz: 2 SDs Below or AboveQuiz: Proportion of Facebook FriendsQuiz: More than 262Quiz: Between 118 and 226Quiz: Less than 240Z-TableHere is a link to the z-table shown in the video:z-table Quiz: Using the Z-TableKarmaUdacity forums no longer use the karma system Katie describes. However, you can still access the dataset Katie created by clicking here: Average Karma points per post. Quiz: Average Karma Points per PostQuiz: SD of Karma Points per PostQuiz: Integer SDsQuiz: Less than 5Quiz: More than 20Quiz: Between 10 and 16Quiz: Top 5%Great Job!Link to visualize the area under the curve Problem Set 6: Normal DistributionQuiz: 1. HeightsQuiz: CHALLENGE 2. HeightsQuiz: 3. HousesQuiz: 4. HousesQuiz: 5. HousesQuiz: 6. HousesQuiz: 7. Greater than 108Quiz: 8. Less than 76Quiz: 9. Between 65 and 90Quiz: 10. Between 80 and 95Quiz: 11. Top 30%Quiz: 12. Greater than 1.64Quiz: 13. Less than -2.33Quiz: 14. Top 40%Quiz: 15. MeaningsQuiz: 16. 64th PercentileSampling DistributionsQuiz: Compare Sample MeansQuiz: Gambling in VegasA tetrahedral die will result in a 1, 2, 3, or 4 on each roll. If you’re curious about how this works, you can read this thread. Write your answer as a proportion. Quiz: Tetrahedral DieQuiz: Total Number of SamplesQuiz: Mean of Each SampleQuiz: Mean of Sample MeansQuiz: Sampling Distributiondistribution of sample means = Sampling DistributionMean of each sample:1, 1.5, 2, 2.5, 1.5, 2, 2.5, 3, 2, 2.5, 3, 3.5, 2.5, 3, 3.5, 4 Copy and paste the sample means into WolframAlpha and hit enter. Pay particular attention to the histogram showing the frequency of each mean. Quiz: Probability Mean &gt; or = 3Quiz: What We Need to Compare the MeansQuiz: Calculate SDsSE: standard deviation of all our sample means Quiz: Relationship Between SDsQuiz: Ratio of SDs$$SE = \frac{ \sigma }{ \sqrt{n} }$$$\sigma$: population standard deviationSE: standard deviation of distribution of sample means(sampling distribution) Quiz: SD of Sampling DistributionThe Central Limit TheoremSE: standard error Quiz: Roll 1 DieNOTE: Unfortunately, the rolling die simulation we used to link to here is no longer valid. Don’t worry! You have enough information now to answer this question without it. Give it your best shot, and you’ll see how the applet works in the solution video. An alternative simulation can be found here. Quiz: Roll 2 DiceQuiz: Find Standard ErrorThe “standard deviation of the sampling distribution” is also known as the standard error.The mean is solved for by adding up the different outcomes (1, 2, 3, 4, 5, 6) and dividing by the number of outcomes (6), yielding a result of 3.5 Quiz: Roll 5 DiceQuiz: Standard Error for Avg of 5 DiceQuiz: Standard Error When n IncreasesQuiz: Shape of Distribution When n IncreasesSimulation AppletLink to awesome simulation applet M&amp;MsQuiz: M&amp;M CLTUsing Sampling DistributionKloutWhat is Klout and how does it work? Quiz: Klout ParametersLink to Klout data Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Your calculations should automatically save in Google Drive. Quiz: Klout Sampling Distribution (Mean)Quiz: Klout Sampling Distribution (SD)Quiz: Sampling Distribution ShapeWhat Do You Get with a Good Klout Score?Quiz: Location of Mean on DistributionQuiz: Probability of Obtaining MeanQuiz: Does Low Probability = Causation?Quiz: Increase Sample SizeQuiz: Location of MeanQuiz: Probability of MeanQuiz: Something FunProblem Set 7: Sampling DistributionsQuiz: 1. Central Limit TheoremQuiz: 2. Location of Sample MeanQuiz: 3. Average DifferenceQuiz: 4. Increase Sample SizeQuiz: 5. Standard ErrorQuiz: 6. n and σQuiz: 7. n and x-barQuiz: 8. Mean of Sample MeansQuiz: 9. Standard ErrorQuiz: 10. Z-ScoreQuiz: 11. ProbabilityQuiz: 12. Mean n = 25Quiz: 13. Standard ErrorQuiz: 14. ProbabilityQuiz: 15. Probability DecreasedQuiz: 16. Population Distribution ShapeQuiz: 17. Sampling Distribution ShapeQuiz: 18. Mean of Sampling DistributionQuiz: 19. SD of Sampling DistributionQuiz: 20. Which Distribution?Quiz: 21. Greater or Less?Quiz: CHALLENGE 22. What Sample Size?Final projecthere EstimationSummaryQuiz: Mean of Treated Populationpoint estimate Quiz: Population Mean vs. Sample MeanQuiz: Percent of Sample MeansApproximate Margin of Errormargin of error: $\frac{2\sigma}{\sqrt{n}}$ Interval Estimate for Population MeanQuiz: Confidence Interval BoundsQuiz: Exact Z-ScoresSampling DistributionQuiz: 95% CI with Exact Z-ScoresQuiz: Generalize Point EstimateQuiz: Generalize CIQuiz: CI Range for Larger Sample SizeQuiz: CI When n = 250Bigger Sample, Smaller CIQuiz: Z for 98% CILink to z-table Quiz: Find 98% CICritical Values of Z-1.96 and 1.96 are the critical values of z for 95% confidence Quiz: Engagement RatioLink to Engagement Ratio data Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Enter your answer as a number without any special characters, including commas. Hypothesis Testing SongQuiz: Point Estimate Engagement RatioQuiz: Standard ErrorQuiz: CI BoundsQuiz: Generalize CIMargin of ErrorThe margin of error is half the width of the confidence interval. Quiz: Rate Engagement and LearningLink to rate your engagement and learning in this class Results from SampleQuiz: What Statistics?Quiz: Sampling DistributionsQuiz: Z-Scores of Sample MeansQuiz: Probability Sample Mean Is at Least…Quiz: What Does This Mean?Wrap-UpProblem Set 8: EstimationQuiz: 2. Larger RangeQuiz: 3. Increase Sample SizeQuiz: 4. Increase Population SDQuiz: 5. 95% CIQuiz: 6. Critical Values 95% CIQuiz: 7. Standard ErrorQuiz: 8. ProbabilityLink to z-table Write the probability as a proportion.Hint: Theoretically, z-scores can be anything from -infinity to +infinity. What does it say about the probability if you get really high or really low z-scores? Quiz: 9. Margin of ErrorQuiz: 10. 95% CIQuiz: 11. Interpret CIQuiz: 12. Critical Values 99% CIQuiz: 13. Standard ErrorQuiz: 14. ProbabilityQuiz: 15. Margin of ErrorQuiz: 16. 99% CIQuiz: 17. Interpret CIHypothesis testingQuiz: Likely or unlikelyQuiz: Likely or UnlikelyQuiz: Alpha Levels0.05 5%0.01 1%0.001 0.1% Quiz: Z-Critical Value 0.05critical regionz-critical value Quiz: Critical Values 0.01Quiz: Critical Values 0.001Critical RegionsNote that when it comes to constructing a hypothesis test, it is best to choose a significant level before you perform the test. You can report the results as significant at a certain critical level after obtaining your result, but it is important that you are not ‘fishing’ for results before you see the results in your sample. This article has some additional information about understanding significance levels and p-values in hypothesis testing. Quiz: Significance $\alpha$ level z-critical value 0.05 1.65 0.01 2.32 0.001 3.08 z-score significant at 3.14 p&lt;0.001 DartsQuiz: Z-ScoreQuiz: Two-Tailed Critical Values 0.05Quiz: Two-Tailed TestQuiz: Two-Tailed ProbabilityQuiz: Two-Tailed Critical Values 0.01Quiz: Two-Tailed Critical Values 0.001HypothesesQuiz: Fail to Reject the NullQuiz: Evidence to Reject the NullQuiz: Mean and SDLink to Learning and Engagement Data (use this data for quiz) Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Link to Full Learning and Engagement Results (optional) Quiz: Null HypothesisQuiz: Alternative HypothesisOne tailed or two tailedQuiz: Conduct Hypothesis Testwhat does it mean to reject the null? our sample mean falls within the critical region the z-score of our sample mean is greater than the z-critical value the probability of obtaining the sample mean is less than the alpha level Quiz: Critical Values 0.05Quiz: Z-Score of Sample MeanQuiz: Results of Hypothesis TestQuiz: Increase Sample SizeQuiz: Reject or Fail to RejectQuiz: Probability of Obtaining MeanQuiz: Decision ErrorsType 1 error: reject true H0Type 2 error: retain false H0 Quiz: Hot BeverageQuiz: RainingQuiz: What Happened?Quiz: What Happened?Prone to MisinterpretationsTo Finish This Lesson…Hypothesis TestingQuiz: Increase Engagement?Problem Set 9: Hypothesis testingQuiz: 1. True or False?Quiz: 2. Null HypothesisQuiz: 3. Alternative HypothesisQuiz: 4. Standard ErrorQuiz: 5. Z-ScoreQuiz: 6. Z-Critical ValueQuiz: 7. Statistical DecisionQuiz: 8. ConclusionQuiz: 9. Type I ErrorQuiz: 10. HypothesesQuiz: 11. Standard ErrorQuiz: 12. Z-ScoreQuiz: 13. Valentine&#39;s DayLessons 5-9 Review/AssessmentQuiz: 1. Find z-scoreQuiz: 2. ProbabilityQuiz: 3. Z-score furthest from meanQuiz: 4. Find x given z-scoreQuiz: 5. ProportionQuiz: 6. ProbabilityQuiz: 7. Compare using distributionsQuiz: 8. Standard Normal DistributionQuiz: 9. ProbabilityQuiz: 10. Z-scoreQuiz: 11. PercentileQuiz: 12. Distribution of meansQuiz: 13. Standard ErrorQuiz: 14. Z-score of meanQuiz: 15. Probability of sample meanQuiz: 16. UnlikelyQuiz: 17. Proportion of samplesQuiz: 18. Proportion of samplesQuiz: 19. Increase nQuiz: 20. Point estimateQuiz: 21. 95% CIQuiz: 22. HypothesesQuiz: 23. DecisionQuiz: 24. Probability of Type I errorQuiz: 25. Find sigmat-TestsQuiz: t-Distribution$t = \frac{mean difference}{S}$S: population standard deviation estimated by standard deviation of sample means GuinnessQuiz: Degrees of FreedomQuiz: DF - Choose n NumbersQuiz: DF - Add to 10Quiz: DF - Marginal TotalsDF - Sample SDn-1 in sample standard deviation can be denoted as effctive sample size t-TableLink to t-Table Quiz: One-Tailed t-TestQuiz: Two-Tailed t-TestQuiz: Bounds of Areat-statistic: t critical value Quiz: Affect t-StatisticOne-Sample t-Test$t=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}$ Quiz: Increase tQuiz: FinchesQuiz: Finches - n and DFLink to Finch beak width data Quiz: Finches - Mean and sQuiz: Finches - Find t-StatisticQuiz: Finches - Decisionp-value: probability of getting this t-statistic Quiz: P-Valuereject the null when the p-value is less than the $\alpha$ level Quiz: Visualize P-ValueQuiz: Find P-ValueLink to GraphPad Remember, our sample has 8 values, and our t-statistic is 0.977. Quiz: Rent - t-Critical ValuesQuiz: Rent - t-StatisticQuiz: Rent - DecisionQuiz: Rent - Cohen&#39;s dstandardized mean differenceCohen’s d =$$\frac{\bar{x} - \mu_0}{s}$$ Quiz: Rent - CIQuiz: Rent - Find CIQuiz: Rent - Margin of ErrorQuiz: Rent - Increase nDependent SamplesDependent t-test for paired sampleswithin-subject designs two conditions pre-test, post-test growth over time(longitudinal study) Quiz: KeyboardsLink to Keyboards data Quiz: Keyboards: Point Estimate for Differencepoint estimate is based on samples Quiz: Keyboards - SD of DifferencesQuiz: Keyboards - t-StatisticQuiz: Keyboards - t-Critical ValuesQuiz: Keyboards - DecisionQuiz: Keyboards - Cohen&#39;s dCohen’s d = difference of means / sample standard deviation estimated by samples Quiz: Keyboards - CI for Dependent SamplesNotation for DifferenceTypes of Designsrepeated measures design Effect Sizeeffect size=mean difference Quiz: Everyday MeaningTypes of Effect-Size Measures difference measures mean difference standarized difference Cohen’s d Correlation measures $r^2$ Statistical SignificanceCohen&#39;s d$d=\frac{\bar{x}-\mu}{s}$s: standard deviation of sample means r^2coefficieng of determination0-1$$r^2 = \frac{t^2}{t^2 + df}$$t: not the t-criticaldf: degrees of freedom0: the variables are not at all related Quiz: Compute r^2Report ResultsAPA stylet(df) = x.xx, p=.xx, directiont(24) = -2.50, p&lt;.05, one-tailed Report CI ResultsReport CI Results 2APA style-CIsconfidence interval on the mean difference; 95%CI = (4,6) Report Results Effect SizeCohen’s d, $r^2$d = x.xx$r^2$ = .xx One-Sample t-Testformulas:df = n-1$SEM = \frac{s}{\sqrt{n}}$s: sample standard deviation$t = \frac{\bar{x} - \mu}{SEM}$$\bar{x}$: sample mean$\mu$: population meanCI = $\bar{x}\pm$ margin of errormargin of error = t-critical * SEMCohen’s d = $\frac{\bar{x}-\mu}{s}$$r^2 = \frac{t^2}{t^2 + df}$t: not the t-critical MuQuiz: Dependent VariableQuiz: TreatmentQuiz: Null HypothesisQuiz: Alternative HypothesisHypothesesQuiz: Which-Tailed Test?Quiz: Degrees of FreedomQuiz: t-Criticalcan be found directly via t-table Quiz: SEMstandard error of meansample standard deviation / sqrt(n) Quiz: Mean Differencesample mean and population mean Quiz: t-StatisticQuiz: Critical RegionQuiz: P-ValueQuiz: Statistically SignificantQuiz: Meaningful ResultsQuiz: Cohen&#39;s dQuiz: r^2Quiz: Margin of ErrorQuiz: Compute CIProblem Set 10: t-TestsQuiz: 1. Normal vs. t-DistributionQuiz: 2. Z vs. tQuiz: 3. Vocabularies - Type of StudyQuiz: 4. Vocabularies - Independent VariableQuiz: 5. Vocabularies - Dependent VariableQuiz: 6. Vocabularies - Null HypothesisQuiz: 7. Vocabularies - Alternative HypothesisQuiz: 8. Vocabularies - One- or Two-Tailed?Quiz: 9. Vocabularies - t-Critical ValueQuiz: 10. Vocabularies - Mean and SDQuiz: 11. Vocabularies - t-StatisticQuiz: 12. Cell Phone Law: Type of StudyQuiz: 13. Cell Phone Law: Dependent VariableQuiz: 14. Cell Phone Law: Independent VariableQuiz: 15. Cell Phone Law: Null HypothesisQuiz: 16. Cell Phone Law: Alt. HypothesisQuiz: 17. Cell Phone Law: t-TestQuiz: 18. Cell Phone Law: Difference ScoresQuiz: 19. Cell Phone Law: t-Critical ValueQuiz: 20. Cell Phone Law: Standard ErrorQuiz: 21. Cell Phone Law: t-StatisticQuiz: 22. Cell Phone Law: DecisionQuiz: 23. Cell Phone Law: Cohen&#39;s dQuiz: 24. Cell Phone Law: CIMake sure you use the t-critical value for a two-tailed test, even if you are doing a one-tailed test. This is because you center the confidence interval around your point estimate, rather than letting one bound go to positive or negative infinity depending on the direction of your test. t-Tests continuedIndependent Samplesindependent samples experimental observational Standard Errorstandard error =$$\frac{s}{\sqrt{n}} = \frac{\sqrt{s^2_1 + s^2_2}}{\sqrt{n}} = \sqrt{\frac{s^2_1 + s^2_2}{n}} = \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}}$$$$df = n_1 + n_2 -2$$t-statistic = $\frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{SE}$ Quiz: Meal PricesClick here to access the data: Meal prices in Gettysburg and Wilma If you would like to perform any calculations, copy and paste the data into your own spreadsheet. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Average Meal PriceQuiz: SD for Meal PriceQuiz: Meal Price SEMQuiz: Meal Price t-StatisticQuiz: Calculate t-StatisticQuiz: t-Critical ValuesQuiz: Gettysburg or Wilma?Acne MedicationQuiz: Acne Medication t-StatisticQuiz: Acne Medication - t-Critical ValuesQuiz: Acne Medication - DecisionWho Has More Shoes?Quiz: Mean Number of ShoesClick here to access the data: Pairs of shoes owned by males and females Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Shoes - Standard ErrorQuiz: Shoes - t-StatisticQuiz: Shoes - DecisionQuiz: Shoes - 95% CIQuiz: Shoes - Calculate CIQuiz: Gender and ShoesQuiz: Pooled Variance Sum of SquaresAssumes samples are approximately the same size$$SS^2_p = \frac{SS_1 + SS_2}{df_1 +df_2}$$SS: sum of squares$SS^2_p$: pooled variance Quiz: Calculate Pooled VarianceQuiz: Corrected Standard Errorstandard error :$$\sqrt{\frac{S^2_1}{n_1} + \frac{S^2_2}{n_2}}$$corrected Standard Error:$$\sqrt{\frac{S^2_p}{n_1} + \frac{S^2_p}{n_2}}$$ Quiz: t-StatisticQuiz: t-Critical and DecisionAssumptionsstandard error:$$\sqrt{\frac{S^2_1}{n_1} + \frac{S^2_2}{n_2}}$$corrected Standard Error:$$\sqrt{\frac{S^2_p}{n_1} + \frac{S^2_p}{n_2}}$$T-test assumption X and Y should be random samples from two different independent populations Populations are approximately normal Sample data can extimate population variances Population variances are roughly equal Problem Set 11: t-Tests continuedQuiz: 1. t-TestQuiz: 2. t-TestQuiz: 3. t-TestQuiz: 4. t-Critical ValueQuiz: 5. P-ValueQuiz: 6. P-ValueQuiz: 7. t-StatisticQuiz: 8. True or False?Quiz: 9. Sum of SquaresQuiz: 10. Pooled VarianceQuiz: 11. Standard ErrorQuiz: 12. t-StatisticQuiz: 13. DecisionQuiz: 14. Independent VariableQuiz: 15. Dependent VariableQuiz: 16. Null HypothesisQuiz: 17. Degrees of FreedomQuiz: 18. t-Critical ValuesQuiz: 19. t-StatisticQuiz: 20. DecisionQuiz: 21. Cohen&#39;s dQuiz: 22. Percent of VariabilityOne-way ANOVAQuiz: IntuitionQuiz: Number of t-TestsQuiz: Extended t-Test Numeratorgrand mean Quiz: Grand Meangrand mean sometimes equal to mean of sample means Quiz: Between-Group VariabilityQuiz: Significantly Different MeansQuiz: Sample Variability and Significancewithin-group variability: the variability of indivisual samples ANOVAANOVA: analysis of varianceone-way ANOVA: one independent vaiable Quiz: HypothesesQuiz: Within-Group VariabilityWithin-Group Variability greater, test statistics smalller Quiz: Between-Group VariabilityBetween-Group Variability greater, test statistics greater Quiz: F-RatioF-Ratio = Between-Group Variability / Within-Group Variability Quiz: Visualize Statistical OutcomeQuiz: Formalize Within-Group VariabilityFormula for F-Ratio$$F = \frac{Between-Group Variability}{Within-Group Variability} = \frac{n\sum(\bar{x}_k - \bar{x}_G)^2/(k-1)}{\sum(\bar{x}_i - \bar{x}_k)^2/(N-k)} = \frac{SS_{between}/df_{between}}{SS_{within}/df_{within}} = \frac{MS_{between}}{MS_{within}}$$ Quiz: Degrees of Freedom$$df_{total} = N-1$$$$df_{between} + df_{within} = (k-1) + (N-k) =N-1$$ Total Variation$$SS_{between} + SS_{within} = SS_{total} = \sum(\bar{x}_i - \bar{x}_G)^2$$ Quiz: F-DistributionF-Distribution ShapeQuiz: Table for F-CriticalClick here if you’d like to check out the F-table! Quiz: Sample Means and Grand MeanQuiz: SS BetweenQuiz: SS WithinQuiz: Degrees of FreedomQuiz: Mean SquaresQuiz: F-StatisticQuiz: F-CriticalQuiz: DecisionProblem Set 12: One-way ANOVAQuiz: 1. Within-Group VariabilityQuiz: 2. Between-Group Variability and F-RatioQuiz: 3. Source of VariationQuiz: 4. Between-Group VariabilityQuiz: 5. True or False?Quiz: 6. Large F-StatisticQuiz: 7. Degrees of FreedomQuiz: 8. DecisionQuiz: 9. Degrees of FreedomQuiz: 10. DecisionQuiz: 11. Null HypothesisQuiz: 12. Alternative HypothesisQuiz: 13. SS BetweenQuiz: 14. SS WithinQuiz: 15. Degrees of FreedomQuiz: 16. MSQuiz: 17. F-StatisticQuiz: 18. F-Critical Value3.8853 Quiz: 19. DecisionQuiz: 20. DF Total26 Quiz: 21. Null HypothesisQuiz: 22. N28=3+24+1 Quiz: 23. DecisionQuiz: 24. SS Between150=200-50 Quiz: 25. True or False?True ANOVA continuedQuiz: Cows and foodQuiz: Grand meanQuiz: Group meansQuiz: SS betweenQuiz: SS withinQuiz: Degrees of FreedomQuiz: Mean squaresQuiz: F statisticQuiz: F critical and decisionQuiz: Deviation from grand meanQuiz: SS totalQuiz: ConclusionMultiple Comparison Testscompare all of the means with each other Quiz: Tukeys HSDTukey’s Honostly Significant Difference(HSD) =$$q^{\star} \sqrt{\frac{MS_{within}}{n}} = q^{\star} \frac{S_p}{\sqrt{n}}$$q: Studentized Range Statisticn: sample sizeLink to Studentized Range Statistic (q) Table for alpha = 0.05df is the degrees of freedom for within-group variability; k is the number of groups/samples in your study Quiz: Which differences are significantif difference between sample means greater than Tukey’s HSD, we can conclude that two sample is Honostly Significant Difference Quiz: Cohens d for multiple comparisons$$d = \frac{\bar{x_1} - \bar{x_2}}{S_p} = \frac{\bar{x_1} - \bar{x_2}}{\sqrt{MS_{within}}}$$ Quiz: Eta squaredpropotion of total variation that is due to between-group differences(explained variation)$$\eta^2 = \frac{SS_{between}}{SS_{total}}$$ Quiz: Calculate eta squaredQuiz: Range of eta squaredQuiz: Software outputQuiz: Missing mean differencesQuiz: Different sample sizesQuiz: Grand meanQuiz: SS between$$SS_{between} = \sum n_k (\bar{x_k} - \bar{x_G})^2$$sample mean minus grand mean Quiz: SS withinQuiz: Degrees of freedomQuiz: MS and FQuiz: Proportion due to drug type$$\eta^2 = \frac{SS_{between}}{SS_{total}}$$$\eta^2$ propotion of the difference between tumor reduction can be explained by the different treatments Quiz: Powertype II error: failing to reject the null when we should have larger samples result in higher power lower within-group variability leads to higher power choosing treatments with strong effect sizes will increase power ANOVA Assumptions and Wrap Up$$F = \frac{Between-Group Variability}{Within-Group Variability} = \frac{n\sum(\bar{x}_k - \bar{x}_G)^2/(k-1)}{\sum(\bar{x}_i - \bar{x}_k)^2/(N-k)} = \frac{SS_{between}/df_{between}}{SS_{within}/df_{within}} = \frac{MS_{between}}{MS_{within}}$$$$q^{\star} \sqrt{\frac{MS_{within}}{n}} = q^{\star} \frac{S_p}{\sqrt{n}}$$$$\eta^2 = \frac{SS_{between}}{SS_{total}}$$ Problem Set 13: ANOVA continuedQuiz: 1. Decision3.3158retain Quiz: 2. Decision3.2874reject Quiz: 3. SS WithinSStotal - SSbetween Quiz: 4. Symbolize SS WithinSS{within} is also called SS{error} because this is the variation that can’t be explained by the independent variable. Quiz: 5. Degrees of Freedomk-1N-k Quiz: 6. HypothesesQuiz: 7. Post-hoc TestsWe conduct post-hoc tests in order to determine which pair(s) of groups were significantly different, since ANOVA only tells us that at least one pair is significantly different. Quiz: 8. Tukey&#39;s HSDsmaller than HSD, retain Quiz: 9. Tukey&#39;s HSDQuiz: 10. η^2Quiz: 11. η^2 MeaningQuiz: 12. Decision0.05 F(1,30)-critical = 4.1709 Quiz: 13. DecisionQuiz: 14. SS BetweenQuiz: 15. Normality AssumptionQuiz: 16. Homogeneity of VarianceQuiz: 17. Grand MeanQuiz: 18. SS BetweenQuiz: 19. Degrees of FreedomQuiz: 20. MS BetweenQuiz: 21. DecisionQuiz: 22. ConclusionQuiz: 23. η^2Quiz: 24. η^2 MeaningQuiz: 25. Explained ProportionQuiz: 26. Tukey&#39;s HSDCorrelationRelationshipsQuiz: The Variables x and yQuiz: Show RelationshipQuiz: ScatterplotQuiz: Stronger RelationshipQuiz: As x IncreasesQuiz: Strength and DirectionCorrelation CoefficientCorrelation Coefficient r: Pearson’s r$$r = \frac{cov(x,y)}{S_x \cdot S_y} = \frac{cov_{x,y}}{S_x \cdot S_y}$$$r^2$ = % of the variation in Y explained by the variation in X.$r^2$: coefficient of determination Quiz: Match with rQuiz: Age in Months and YearsQuiz: Hours Asleep vs. AwakeQuiz: Create ScatterplotData from poll Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Calculate rGoogle Spreadsheets function to calculate r:=PEARSON(start cell for variable x:end cell for variable x, start cell for variable y:end cell for variable y) Quiz: StrongerQuiz: Hypothesis Testing for ρ$\rho$: true correlation for population.correlation between variable and population Quiz: Testing for SignificanceQuiz: CI for ρQuiz: Find pGraphPad QuickCalcs Page Quiz: Add OutlierAdd the outlier (20, 8) to the data for age and number of pets (i.e., age = 20, number of pets = 8). Calculate the new correlation coefficient. Correlation vs. CausationHere is a link to the original comic Fallaciesambigous temporal precedencethird variable problempost hoc fallacy Problem Set 14: CorrelationQuiz: 1. Positive DirectionsQuiz: 2. Order Correlation CoefficientsQuiz: 3. Interpret ResultsQuiz: 4. Unlikely Value of ρQuiz: 5. ρ CIQuiz: 6. Decision Based on t-StatisticQuiz: 7. Writing Down Dreams0.95Dream Data Quiz: 8. RelationshipsQuiz: 9. No CorrelationQuiz: 10. Direction of RelationshipQuiz: 11. Approximate CorrelationQuiz: 12. Approximate CorrelationQuiz: 13. Increase Sample SizeWhen we were doing t-tests, as the sample size increased, we could reach significance with a smaller difference between the means. That’s because increasing the sample size makes it harder to get a large difference by chance. What’s the analogous idea here? Quiz: 14. Direction of RelationshipQuiz: 15. Estimate r0.58 Quiz: 16. Compute rQuiz: 17. Coefficient of DeterminationQuiz: 18. Calculate pQuiz: 19. DecisionQuiz: 20. ConclusionQuiz: 21. Decision Based on pRegressionIntro to Linear RegressionAirplane FlightsQuiz: Symbolize Regression EquationTry not to get confused with the typical way that the equation for the regression line is often presented: y = mx + b. There are a variety of standard ways for symbolizing the slope and y-intercept, and in this class we’ll use a to represent the y-intercept and b to represent the slope. In general, try not to memorize equations or formulas, because this will hinder your ability to understand the same formulas when different symbols are used to symbolize the same coefficients. Instead, it’s far better to understand the format of an equation or formula so you can recognize what each symbol means. Quiz: Guess Best Fit LineMinimize Sum of Squares$$b = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = r(\frac{S_y}{S_x})$$r: Pearson’s rWe have decided to symbolize the regression line by y = bx + a, where b represents the slope and a represents the y-intercept.Since b = r(standard deviation of y-values)/(standard deviation of x-values), we can also symbolize the regression line like this:y = r(standard deviation of y-values)/(standard deviation of x-values)x + a Quiz: Calculate rQuiz: Calculate Standard DeviationsQuiz: Calculate SlopeQuiz: Find y-InterceptQuiz: What Point Does the Line Go Through?Quiz: Calculate MeansQuiz: Calculate y-InterceptQuiz: Travel 4000 MilesQuiz: Additional Cost per MileQuiz: Cost to Travel 0 MilesQuiz: Travel on a BudgetQuiz: Which Has More Error?Standard Error of EstimateStandard Error Of Estimate =$$\frac{\sum(y - \hat{y})^2}{N - 2}$$N: number of points Confidence IntervalsQuiz: Hypothesis Testing for SlopeQuiz: t-Test for Slopedf = N -2N: number of x points R OutputFactors Affecting Linear RegressionSummary of Linear RegressionIntro to Multiple RegressionR: multiple correlation coefficient$R^2$: propotion of variability in Y explained by out set of predictors Quiz: Alcohol, Religiosity, &amp; Self-EsteemQuiz: Make PredictionsQuiz: RelationshipQuiz: CausationAppletsProblem Set 15: RegressionQuiz: 1. Stem Cells &amp; Vision - SlopeQuiz: 2. Stem Cells &amp; Vision - r^2Quiz: 3. Stem Cells &amp; Vision - y-InterceptQuiz: 4. Stem Cells &amp; Vision - Regression EqnQuiz: 5. Stem Cells &amp; Vision - # of Stem CellsQuiz: 6. Stem Cells &amp; Vision - PhotoreceptorsQuiz: 7. Estimate SlopeQuiz: 8. Indicate SlopeQuiz: 9. CI for SlopeQuiz: 10. SymbolsQuiz: 11. Make PredictionsQuiz: 12. Standard ErrorQuiz: 13. r^2Quiz: 14. Definition of SlopeQuiz: 15. Definition of y-InterceptQuiz: 16. Definition of y-HatQuiz: 17. Definition of Std Error of EstimateQuiz: 18. Describe RelationshipQuiz: 19. Describe RelationshipQuiz: 20. Describe RelationshipChi-Squared testsQuiz: Scales of MeasurementRatio dataOrdinal dataInterval data If you would like some additional discussion on the scales of measurement, then take a look at this very nice little tutorial. As you watch the tutorial, think about what properties must a measurement have for us to consider it a nominal scale or an ordinal scale, etc. Quiz: Choose Type of DataNon-Parametric TestsQuiz: Mount ShastaQuiz: Expected FrequenciesQuiz: Observed FrequencyQuiz: Hypotheses PercentQuiz: Hypotheses FrequencyQuiz: Expected Frequenciesχ^2 Goodness-of-Fit TestQuiz: χ^2 Statistic successful unsuccessful expected frequency 33 67 observed frequency 41 59 $$\chi^2 = \sum \frac{(f_o - f_e)^2}{f_e}$$$f_o$: observed frequency$f_e$: expected frequency Quiz: Observed Equals ExpectedQuiz: χ^2 ValuesQuiz: Degrees of FreedomQuiz: Which Has More df?the more categories we have, the more degree of freedoms we have and the larger $\chi^2$ statistics will be. We need higher critical value to reject the null hpyothesis. Quiz: Calculate χ^2 StatisticQuiz: Find dfdf = 1 = Number of categories - 1 Quiz: Calculate pP value equals 0.0891Not significant at either level GraphPad QuickCalcs The instructor should have said, “Calculate the one-tailed p-value.” GraphPad’s Chi-Square calculator reports the p-value as two-sided, but should be taken as the one-sided p-value we desire. χ^2 Test for IndependenceQuiz: Remember DetailsElizabeth Loftus: Eyewitness Testimony Source: Loftus, E.F. &amp; Palmer, J.C. (1974). Reconstruction of automobile destruction: An example of the interaction between language and memory. Journal of Verbal Learning and Verbal Behavior. 13, 585-589. Quiz: Broken Glass Hit Smashed Control Total Yes 7 16 6 29 No 43 34 44 121 Total 50 50 50 150 Null Hypothesis: Student’s response for whether or not they saw broken glass is independent of the wording used in questionExpected frequencies:For (Yes, Hit) is (29/150) * 50For (No, Hit) is (121/150) * 50 Quiz: Expected FrequenciesQuiz: Calculate χ^2 StatisticQuiz: Degrees of Freedomdf = 2 = (number of rows - 1)(number of columns - 1) Quiz: DecisionGraphPad QuickCalcsChi-Square Table Quiz: Effect SizeChi-square Test for Independence$$Cramer’s V (\phi_c) = \sqrt{\frac{\chi^2}{n(k - 1)}}$$k: smaller of number of rows or columns (2)n: total number (150) Quiz: Calculate Cramér&#39;s VAssumptions and RestrictionsAvoid dependent observationsAvoid small expected frequencies (large n) SummaryCongratsProblem Set 16: Chi-Squared testsQuiz: 1. Dependent VariableRanking Quiz: 2. Scale of MeasurementOrdinal Quiz: 3. Dependent Variablehelp Quiz: 4. Scale of MeasurementNominal Quiz: 5. Dependent Variabletime Quiz: 6. Scale of Measurementratio Quiz: 7. Dependent Variableerror Quiz: 8. Scale of Measurementratio Quiz: 9. Specify Level of MeasurementQuiz: 10. Select χ^2 TestQuiz: 11. Null Hypothesisnot Quiz: 12. Which χ^2 Test? outcome frequencies expected frequencies 1 8 4 2 4 4 3 1 4 4 8 4 5 3 4 6 0 4 good nominal Quiz: 13. Degrees of Freedom5 Quiz: 14. χ^2 Critical ValueQuiz: 15. Calculate χ^2 StatisticQuiz: 16. DecisionQuiz: 17. ConclusionQuiz: 18. Null Hypothesis Cabin Steerage Total Yes 299 186 485 No 280 526 806 Total 579 712 1291 independent Quiz: 19. Which χ^2 Test?Indenpence nominal Quiz: 20. Degrees of Freedom1 Quiz: 21. χ^2 Critical ValueQuiz: 22. Calculate χ^2Quiz: 23. DecisionQuiz: 24. ConclusionLessons 10-16 Review/AssessmentQuiz: 1Quiz: 2Quiz: 3Quiz: 4Quiz: 5Quiz: 6Quiz: 70.3/15 Quiz: 8120*0.02+5.24 Quiz: 9Quiz: 10Quiz: 11Quiz: 12Quiz: 13Quiz: 14Quiz: 15Quiz: 16Quiz: 17Quiz: 18Quiz: 19Quiz: 20Quiz: 21Quiz: 22Quiz: 23Quiz: 24Quiz: 25Quiz: 26Quiz: 27Quiz: 28Quiz: 29Quiz: 30Final ProjectFinal project details can be found [here](https://www.udacity.com/wiki/ud201/final_project. Here are helpful tips and suggestions as you work on your final project!]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity Courses]]></title>
    <url>%2F2017%2F07%2F19%2FUdacity%20Courses%2F</url>
    <content type="text"><![CDATA[Almost done… Courses are not Learned Course Classroom Code Complete Data Analyst Data Analysis and Visualization classroom ud404 0 Deploying a Hadoop Cluster classroom ud1000 0 A/B Testing classroom ud257 0 Data Visualization and D3.js classroom ud507 0 Data Analysis with R classroom ud651 0 Data Wrangling with MongoDB classroom ud032 0 Real-Time Analytics with Apache Storm classroom ud381 0 Database Systems Concepts &amp; Design classroom ud150 0 Business Analyst A/B Testing for Business Analysts classroom ud979 0 Time Series Forecasting classroom ud980 0 Segmentation and Clustering classroom ud981 0 Classification Models classroom ud978 0 Problem Solving with Advanced Analytics classroom ud976 0 Creating an Analytical Dataset classroom ud977 0 Digital Marketing Intro to Data Science classroom ud359 0 How to Build a Startup classroom ep245 0 Health Informatics in the Cloud classroom ud809 0 Algorithm Computability, Complexity &amp; Algorithms classroom ud061 0 High Performance Computing classroom ud281 0 Machine Learning Model Building and Validation classroom ud919 0 Deploying a Hadoop Cluster classroom ud1000 0 Intro to Hadoop and MapReduce classroom ud617 0 Intro to Artificial Intelligence classroom cs271 0 Big Data Analytics in Healthcare classroom ud758 0 Human-Computer Interaction classroom ud400 0 Self Driving Car Artificial Intelligence for Robotics classroom cs373 0 Artificial Intelligence Knowledge-Based AI: Cognitive Systems classroom ud409 0 Artificial Intelligence classroom ud954 0 Computer Science Computer Networking classroom ud436 0 Intro to Algorithms classroom cs215 0 Technical Interview classroom ud513 0 Intro to Information Security classroom ud459 0 Compilers: Theory and Practice classroom ud168 0 Python Design of Computer Programs classroom cs212 0 Git and GitHub Version Control with Git classroom ud123 0 GitHub &amp; Collaboration classroom ud456 0 React Client-Server Communication classroom ud897 0 Front End Nano Degree Localization Essentials classroom ud610 0 Full Stack Developing Scalable Apps in Python classroom ud858 0 Linux Command Line Basics classroom ud595 0 Configuring Linux Web Servers classroom ud299 0 Dynamic Web Applications with Sinatra classroom ud268 0 Intro to Relational Databases classroom ud197 0 Shell Workshop classroom ud206 0 Network Security classroom ud199 0 Deploying Applications with Heroku classroom ud272 0 Android Java Programming Basics classroom cs046 0 classroom ud 0 classroom ud 0 classroom ud 0 Have learned Course Classroom Code Complete(.0-1.) Statistics Intro to Statistics classroom st101 1 Statistics st095 st095 1 Intro to Descriptive Statistics classroom ud827 1 Intro to Inferential Statistics classroom ud201 0.9 Data Analyst Intro to Data Analysis classroom ud170 1 Data Visualization in Tableau classroom ud1006 1 Machine Learning Intro to Machine Learning classroom ud120 1 Machine Learning classroom ud262 1 Reinforcement Learning classroom ud600 1 Deep Learning classroom ud730 1 Front End Writing READMEs classroom ud777 1 Full Stack Web Development classroom ud253 0.5 Programming Foundations with Python classroom ud036 1 Programming Languages classroom cs262 1 Networking for Web Developers classroom ud256 1 Git and GitHub How to Use Git and GitHub classroom ud775 1 Python Introduction to Python classroom ud1110 0 Intro to Computer Science classroom cs101 1 | classroom | ud | 1 | classroom | ud | 1 | classroom | ud | 1]]></content>
      <tags>
        <tag>Course</tag>
        <tag>Udacity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Courses Wanted to Learn]]></title>
    <url>%2F2017%2F07%2F17%2FCoursera%20Courses%2F</url>
    <content type="text"><![CDATA[Stay stuned… Course University Number Completed Notebook Python for Everybody Specialization University of Michigan: 5 0 None Applied Data Science with Python Specialization University of Michigan: 5 0 None Data Science Specialization Johns Hopkins University: 10 0 None Data Visualization with Tableau Specialization UC, Davis: 5 0 None Machine Learning Stanford 1 1 here Machine Learning Specialization UW 4 2 here Algorithms Specialization Stanford 4 2 here Data Mining Specialization UIUC 6 2 here Probabilistic Graphical Models Specialization Stanford 3 0 None Data Structures and Algorithms Specialization UCSanDiego 6 0 None Data Analysis and Interpretation Specialization Wesleyan University 5 0 None Recommender Systems Specialization University of Minnesota 5 0 None Deep Learning Specialization deeplearning.ai 5 0 None Cloud Computing Specialization UIUC 6 0 None Fundamentals of Computing Specialization Rice University 7 0 None Data Science at Scale Specialization UW 4 0 None Data Science Math Skills Duke University 1 0 None Recommender Systems Specialization University of Minnesota 5 0 None Excel to MySQL: Analytic Techniques for Business Specialization Duke 5 0 None 0 None 0 None 0 None 0 None]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud651 DA with R Notebook]]></title>
    <url>%2F2017%2F06%2F22%2FUdacity%20ud651%20DA%20with%20R%20Notebook%2F</url>
    <content type="text"><![CDATA[on the waystay stuned…course can be found here]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud170 Intro to DA Notebook]]></title>
    <url>%2F2017%2F06%2F20%2FUdacity%20ud170%20Intro%20to%20DA%20Notebook%2F</url>
    <content type="text"><![CDATA[Final ProjectThis course can be found in udacity ud170. Data Analysis ProcessSetting Up Your SystemOtherwise, you can find the free course here. Intro to CSVsIf you’d like to learn more about data wrangling, check out the Udacity course Data Wrangling with MongoDB. CSVs in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/22sQCo6ovH0.mp4 12345678910111213141516171819202122232425262728import unicodecsvenrollments_filename = &apos;/datasets/ud170/udacity-students/enrollments.csv&apos;## Longer version of code (replaced with shorter, equivalent version below)# enrollments = []# f = open(enrollments_filename, &apos;rb&apos;)# reader = unicodecsv.DictReader(f)# for row in reader:# enrollments.append(row)# f.close()with open(enrollments_filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) enrollments = list(reader) ### Write code similar to the above to load the engagement### and submission data. The data is stored in files with### the given filenames. Then print the first row of each### table to make sure that your code works. You can use the### &quot;Test Run&quot; button to see the output of your code.engagement_filename = &apos;/datasets/ud170/udacity-students/daily_engagement.csv&apos;submissions_filename = &apos;/datasets/ud170/udacity-students/project_submissions.csv&apos; daily_engagement = None # Replace this with your codeproject_submissions = None # Replace this with your code 123456789with open(engagement_filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) daily_engagement = list(reader) print daily_engagement[0]with open(submissions_filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) project_submissions = list(reader)print project_submissions[0] Python’s csv ModuleThis page contains documentation for Python’s csv module. Instead of csv, you’ll be using unicodecsv in this course. unicodecsv works exactly the same as csv, but it comes with Anaconda and has support for unicode. The csv documentation page is still the best way to learn how to use the unicodecsv library, since the two libraries work exactly the same way. Iterators in PythonThis page explains the difference between iterators and lists in Python, and how to use iterators. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Fixing Data Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7NSYtdVrlRE.mp4 Questions about Student Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AO8vSyAtfV4.mp4 Investigating the DataNow you’ve started the data wrangling process by loading the data and making sure it’s in a good format. The next step is to investigate a bit and see if there are any inconsistencies or problems in the data that you’ll need to clean up. For each of the three files you’ve loaded, find the total number of rows in the csv and the number of unique students. To find the number of unique students in each table, you might want to try creating a set of the account keys. Again, in case you’re not finished with your local setup, you can complete this exercise in the Udacity code editor. You’ll need to run the next exercise locally, though, so if you haven’t finished setting up, you should do that now.1234567891011121314151617181920212223import unicodecsvdef read_csv(filename): with open(filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) return list(reader)enrollments = read_csv(&apos;/datasets/ud170/udacity-students/enrollments.csv&apos;)daily_engagement = read_csv(&apos;/datasets/ud170/udacity-students/daily_engagement.csv&apos;)project_submissions = read_csv(&apos;/datasets/ud170/udacity-students/project_submissions.csv&apos;) ### For each of these three tables, find the number of rows in the table and### the number of unique students in the table. To find the number of unique### students, you might want to create a set of the account keys in each table.enrollment_num_rows = 0 # Replace this with your codeenrollment_num_unique_students = 0 # Replace this with your codeengagement_num_rows = 0 # Replace this with your codeengagement_num_unique_students = 0 # Replace this with your codesubmission_num_rows = 0 # Replace this with your codesubmission_num_unique_students = 0 # Replace this with your code 123456789101112131415161718192021222324252627def unique_num(data): unique_data = set() for element in data: if &apos;acct&apos; in element: element[&apos;account_key&apos;] = element[&apos;acct&apos;] del element[&apos;acct&apos;] unique_data.add(element[&apos;account_key&apos;]) return len(unique_data)print enrollments[0]enrollment_num_rows = len(enrollments) # Replace this with your codeenrollment_num_unique_students = unique_num(enrollments) # Replace this with your codeprint enrollment_num_rowsprint enrollment_num_unique_students print daily_engagement[0]engagement_num_rows = len(daily_engagement) # Replace this with your codeprint engagement_num_rowsengagement_num_unique_students = unique_num(daily_engagement) # Replace this with your codeprint engagement_num_unique_studentsprint project_submissions[0]submission_num_rows = len(project_submissions) # Replace this with your codesubmission_num_unique_students = unique_num(project_submissions) # Replace this with your codeprint submission_num_rowsprint submission_num_unique_students Problems in the DataRemoving an Element from a DictionaryIf you’re not sure how to remove an element from a dictionary, this post might be helpful. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Updated Code for Previous ExerciseAfter running the above code, Caroline also shows rewriting the solution from the previous exercise to the following code:1234567891011121314def get_unique_students(data): unique_students = set() for data_point in data: unique_students.add(data_point[&apos;account_key&apos;]) return unique_studentslen(enrollments)unique_enrolled_students = get_unique_students(enrollments)len(unique_enrolled_students)len(daily_engagement)unique_engagement_students = get_unique_students(daily_engagement)len(unique_engagement_students)len(project_submissions)unique_project_submitters = get_unique_students(project_submissions)len(unique_project_submitters) Missing Engagement RecordsPrinting a Single RowThis page describes how to use Python’s break statement, which might be helpful for printing only a single problem record. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Checking for More Problem RecordsTracking Down the Remaining ProblemsRefining the QuestionExploratory Data AnalysisIf you’d like to learn more about the exploratory phase of the data analysis process, check out the Udacity course Data Analysis with R. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Getting Data from First Weekhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/adqc5fF5B8Y.mp4https://classroom.udacity.com/courses/ud170/lessons/5430778793/concepts/53961386350923 Note that paid students may have canceled from other courses before paying, and the suggested solution will retain records from these other enrollments. Indulge CuriosityExploring Student EngagementDebugging Data Analysis CodeLessons Completed in First WeekNumber of Visits in the First Weekhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5GYA5j1fqBU.mp4https://classroom.udacity.com/courses/ud170/lessons/5430778793/concepts/53961386450923 Splitting out Passing StudentsQuiz: Comparing the Two Student GroupsQuiz: Making HistogramsVisualizing dataEven though you know the mean, standard deviation, maximum, and minimum of various metrics, there are a lot of other facts about each metric that would be nice to know. Are more values close to the minimum or the maximum? What is the median? And so on. Instead of printing out more statistics, at this point it makes sense to visualize the data using a histogram. Making histograms in PythonTo make a histogram in Python, you can use the matplotlib library, which comes with Anaconda. The following code will make a histogram of an example list of data points called data.12345data = [1, 2, 1, 3, 3, 1, 4, 2]%matplotlib inlineimport matplotlib.pyplot as pltplt.hist(data) The line %matplotlib inline is specifically for IPython notebook, and causes your plots to appear in your notebook rather than a new window. If you are not using IPython notebook, you should not include this line, and instead you should add the line plt.show() at the bottom to show the plot in a new window. Making histograms of student dataNow use this method to make a histogram of each of the three metrics we looked at for both students who pass the subway project and students who don’t. That is, you should create 6 histograms. Do any of the metrics have histograms with very different shapes for students who pass the subway project vs. those who don’t? You can also create histograms of the metrics you explored on your own if you’d like. Are your Results Just Noise?StatisticsIf you’d like to learn more about statistics, which you can use to rigorously determine how likely it is that your results are due to chance, check out the Udacity courses Intro to Descriptive Statistics and Intro to Inferential Statistics. Correlation Does Not Imply CausationCheese and Bedsheet TanglingTo see the plot shown in the video, as well as many other amusing or strange correlations, check out this website. A/B TestingTo learn more about using online experiments to determine whether one change causes another, take the Udacity course A/B Testing. Predicting Based on Many FeaturesMachine LearningTo learn more about using machine learning to automatically make predictions, take the Udacity course Intro to Machine Learning. CommunicationQuiz: Improving Plots and Sharing FindingsAdding labels and titlesIn matplotlib, you can add axis labels using plt.xlabel(&quot;Label for x axis&quot;) and plt.ylabel(&quot;Label for y axis&quot;). For histograms, you usually only need an x-axis label, but for other plot types a y-axis label may also be needed. You can also add a title using plt.title(&quot;Title of plot&quot;). Making plots look nicer with seabornYou can automatically make matplotlib plots look nicer using the seaborn library. This library is not automatically included with Anaconda, but Anaconda includes something called a package manager to make it easier to add new libraries. The package manager is called conda, and to use it, you should open the Command Prompt (on a PC) or terminal (on Mac or Linux), and type the command conda install seaborn. If you are using a different Python installation than Anaconda, you may have a different package manager. The most common ones are pip and easy_install, and you can use them with the commands pip install seaborn or easy_install seaborn respectively. Once you have installed seaborn, you can import it anywhere in your code using the line import seaborn as sns. Then any plot you make afterwards will automatically look better. Give it a try! If you’re wondering why the abbreviation for seaborn is sns, it’s because seaborn was named after the character Samuel Norman Seaborn from the show The West Wing, and sns are his initials. The seaborn package also includes some extra functions you can use to make complex plots that would be difficult in matplotlib. We won’t be covering those in this course, but if you’d like to see what functions seaborn has available, you can look through the documentation. Adding extra arguments to your plotYou’ll also frequently want to add some arguments to your plot to tune how it looks. You can see what arguments are available on the documentation page for the hist function. One common argument to pass is the bins argument, which sets the number of bins used by your histogram. For example, plt.hist(data, bins=20) would make sure your histogram has 20 bins. Improving one of your plotsUse these techniques to improve at least one of the plots you made earlier. Sharing your findingsFinally, decide which of the discoveries you made this lesson you would most want to communicate to someone else, and write a forum post sharing your findings. Data Analysis and Related TermsConclusionL1_Solution_Code.ipynb Quiz SolutionsCSVs in Python12345678910import unicodecsvdef read_csv(filename): with open(filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) return list(reader)enrollments = read_csv(&apos;enrollments.csv&apos;)daily_engagement = read_csv(&apos;daily_engagement.csv&apos;)project_submissions = read_csv(&apos;project_submissions.csv&apos;) Investigating the Data1234567891011121314151617181920len(enrollments)unique_enrolled_students = set()for enrollment in enrollments: unique_enrolled_students.add(enrollment[&apos;account_key&apos;])len(unique_enrolled_students)len(daily_engagement)unique_engagement_students = set()for engagement_record in daily_engagement: unique_engagement_students.add(engagement_record[&apos;acct&apos;])len(unique_engagement_students)len(project_submissions)unique_project_submitters = set()for submission in project_submissions: unique_project_submitters.add(submission[&apos;account_key&apos;])len(unique_project_submitters) Problems in the Data123for engagement_record in daily_engagement: engagement_record[&apos;account_key&apos;] = engagement_record[&apos;acct&apos;] del[engagement_record[&apos;acct&apos;]] Missing engagement records12345for enrollment in enrollments: student = enrollment[&apos;account_key&apos;] if student not in unique_engagement_students: print enrollment break Checking for more problem records1234567num_problem_students = 0for enrollment in enrollments: student = enrollment[&apos;account_key&apos;] if (student not in unique_engagement_students and enrollment[&apos;join_date&apos;] != enrollment[&apos;cancel_date&apos;]): print enrollment num_problem_students += 1 num_problem_students Refining the Question12345678910paid_students = &#123;&#125;for enrollment in non_udacity_enrollments: if (not enrollment[&apos;is_canceled&apos;] or enrollment[&apos;days_to_cancel&apos;] &gt; 7): account_key = enrollment[&apos;account_key&apos;] enrollment_date = enrollment[&apos;join_date&apos;] if (account_key not in paid_students or enrollment_date &gt; paid_students[account_key]): paid_students[account_key] = enrollment_datelen(paid_students) Note that if you switch the order of the second if statement like so if (enrollment_date &gt; paid_students[account_key] oraccount_key not in paid_students)you will most likely get an error. Why do you think that is? Check out this Stackoverflow discussion to find out more: http://stackoverflow.com/questions/13960657/does-python-evaluate-ifs-conditions-lazily Getting Data from First Week1234567891011121314151617181920212223242526272829def within_one_week(join_date, engagement_date): time_delta = engagement_date - join_date return time_delta.days &lt; 7def remove_free_trial_cancels(data): new_data = [] for data_point in data: if data_point[&apos;account_key&apos;] in paid_students: new_data.append(data_point) return new_datapaid_enrollments = remove_free_trial_cancels(non_udacity_enrollments)paid_engagement = remove_free_trial_cancels(non_udacity_engagement)paid_submissions = remove_free_trial_cancels(non_udacity_submissions)print len(paid_enrollments)print len(paid_engagement)print len(paid_submissions)paid_engagement_in_first_week = []for engagement_record in paid_engagement: account_key = engagement_record[&apos;account_key&apos;] join_date = paid_students[account_key] engagement_record_date = engagement_record[&apos;utc_date&apos;] if within_one_week(join_date, engagement_record_date): paid_engagement_in_first_week.append(engagement_record)len(paid_engagement_in_first_week) Debugging Data Analysis CodeHere is the code Caroline shows in the solution video:12345678910111213student_with_max_minutes = Nonemax_minutes = 0for student, total_minutes in total_minutes_by_account.items(): if total_minutes &gt; max_minutes: max_minutes = total_minutes student_with_max_minutes = studentmax_minutesfor engagement_record in paid_engagement_in_first_week: if engagement_record[&apos;account_key&apos;] == student_with_max_minutes: print engagement_record Alternatively, you can find the account key with the maximum minutes using this shorthand notation:1max(total_minutes_by_account.items(), key=lambda pair: pair[1]) Fixing Bug in within_one_week()She also updated the code for the within_one_week function to the following:123def within_one_week(join_date, engagement_date): time_delta = engagement_date - join_date return time_delta.days &gt;= 0 and time_delta.days &lt; 7 Lessons Completed in First WeekFirst, Caroline refactors the given code to analyze total minutes spent in the first week into the following:123456789101112131415161718192021222324252627282930313233from collections import defaultdictdef group_data(data, key_name): grouped_data = defaultdict(list) for data_point in data: key = data_point[key_name] grouped_data[key].append(data_point) return grouped_dataengagement_by_account = group_data(paid_engagement_in_first_week, &apos;account_key&apos;)def sum_grouped_items(grouped_data, field_name): summed_data = &#123;&#125; for key, data_points in grouped_data.items(): total = 0 for data_point in data_points: total += data_point[field_name] summed_data[key] = total return summed_datatotal_minutes_by_account = sum_grouped_items(engagement_by_account, &apos;total_minutes_visited&apos;)import numpy as npdef describe_data(data): print &apos;Mean:&apos;, np.mean(data) print &apos;Standard deviation:&apos;, np.std(data) print &apos;Minimum:&apos;, np.min(data) print &apos;Maximum:&apos;, np.max(data)describe_data(total_minutes_by_account.values()) Then she called the functions she created to analyze the lessons completed in the first week as follows:123lessons_completed_by_account = sum_grouped_items(engagement_by_account, &apos;lessons_completed&apos;)describe_data(lessons_completed_by_account.values()) Number of Visits in the First WeekHere is the code Caroline shows in the solution video. First she ran this code to create the has_visited field:12345for engagement_record in paid_engagement: if engagement_record[&apos;num_courses_visited&apos;] &gt; 0: engagement_record[&apos;has_visited&apos;] = 1 else: engagement_record[&apos;has_visited&apos;] = 0 Then, after recreating the engagement_by_account dictionary with the updated data, she ran the following code to analyze days visited in the first week:123days_visited_by_account = sum_grouped_items(engagement_by_account, &apos;has_visited&apos;)describe_data(days_visited_by_account.values()) Splitting out Passing StudentsHere is the code Caroline shows in the solution video:12345678910111213141516171819202122232425subway_project_lesson_keys = [&apos;746169184&apos;, &apos;3176718735&apos;]pass_subway_project = set()for submission in paid_submissions: project = submission[&apos;lesson_key&apos;] rating = submission[&apos;assigned_rating&apos;] if ((project in subway_project_lesson_keys) and (rating == &apos;PASSED&apos; or rating == &apos;DISTINCTION&apos;)): pass_subway_project.add(submission[&apos;account_key&apos;])len(pass_subway_project)passing_engagement = []non_passing_engagement = []for engagement_record in paid_engagement_in_first_week: if engagement_record[&apos;account_key&apos;] in pass_subway_project: passing_engagement.append(engagement_record) else: non_passing_engagement.append(engagement_record)print len(passing_engagement)print len(non_passing_engagement) Comparing the Two Student GroupsHere is the code Caroline shows in the solution video:12345678910111213141516171819202122232425262728293031323334353637383940414243444546passing_engagement_by_account = group_data(passing_engagement, &apos;account_key&apos;)non_passing_engagement_by_account = group_data(non_passing_engagement, &apos;account_key&apos;)print &apos;non-passing students:&apos;non_passing_minutes = sum_grouped_items( non_passing_engagement_by_account, &apos;total_minutes_visited&apos;)describe_data(non_passing_minutes.values())​print &apos;passing students:&apos;passing_minutes = sum_grouped_items( passing_engagement_by_account, &apos;total_minutes_visited&apos;)describe_data(passing_minutes.values())print &apos;non-passing students:&apos;non_passing_lessons = sum_grouped_items( non_passing_engagement_by_account, &apos;lessons_completed&apos;)describe_data(non_passing_lessons.values())print &apos;passing students:&apos;passing_lessons = sum_grouped_items( passing_engagement_by_account, &apos;lessons_completed&apos;)describe_data(passing_lessons.values())print &apos;non-passing students:&apos;non_passing_visits = sum_grouped_items( non_passing_engagement_by_account, &apos;has_visited&apos;)describe_data(non_passing_visits.values())print &apos;passing students:&apos;passing_visits = sum_grouped_items( passing_engagement_by_account, &apos;has_visited&apos;)describe_data(passing_visits.values()) Making HistogramsHere is the code Caroline shows in the solution video:123456789101112%pylab inlineimport matplotlib.pyplot as pltimport numpy as np# Summarize the given datadef describe_data(data): print &apos;Mean:&apos;, np.mean(data) print &apos;Standard deviation:&apos;, np.std(data) print &apos;Minimum:&apos;, np.min(data) print &apos;Maximum:&apos;, np.max(data) plt.hist(data) Fixing the Number of BinsTo change how many bins are shown for each plot, try using the bins argument to the hist function. You can find documentation for the hist function and the arguments it takes here. Improving Plots and Sharing FindingsHere is the code Caroline shows in the solution video:1234567891011import seaborn as snsplt.hist(non_passing_visits.values(), bins=8)plt.xlabel(&apos;Number of days&apos;)plt.title(&apos;Distribution of classroom visits in the first week &apos; + &apos;for students who do not pass the subway project&apos;)plt.hist(passing_visits.values(), bins=8)plt.xlabel(&apos;Number of days&apos;)plt.title(&apos;Distribution of classroom visits in the first week &apos; + &apos;for students who pass the subway project&apos;) Quiz: Survey Says!Numpy and Pandas for 1D DataIntroductionQuiz: Gapminder DataGapminder dataThe data in this lesson was obtained from the site gapminder.org. The variables included are: Aged 15+ Employment Rate (%)Life Expectancy (years)GDP/capita (US$, inflation adjusted)Primary school completion (% of boys)Primary school completion (% of girls)You can also obtain the data to anlayze on your own from the Downloadables section. One-Dimensional Data in NumPy and PandasQuiz: NumPy Arrays Pandas Numpy Series Array similarity and difference between numpy array and python list similarity difference for loop numpy array have the same type 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as np# First 20 countries with employment datacountries = np.array([ &apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;, &apos;Bosnia and Herzegovina&apos;])# Employment data in 2007 for those 20 countriesemployment = np.array([ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076])# Change False to True for each block of code to see what it does# Accessing elementsif False: print countries[0] print countries[3]# Slicingif False: print countries[0:3] print countries[:3] print countries[17:] print countries[:]# Element typesif False: print countries.dtype print employment.dtype print np.array([0, 1, 2, 3]).dtype print np.array([1.0, 1.5, 2.0, 2.5]).dtype print np.array([True, False, True]).dtype print np.array([&apos;AL&apos;, &apos;AK&apos;, &apos;AZ&apos;, &apos;AR&apos;, &apos;CA&apos;]).dtype# Loopingif False: for country in countries: print &apos;Examining country &#123;&#125;&apos;.format(country) for i in range(len(countries)): country = countries[i] country_employment = employment[i] print &apos;Country &#123;&#125; has employment &#123;&#125;&apos;.format(country, country_employment)# Numpy functionsif False: print employment.mean() print employment.std() print employment.max() print employment.sum()def max_employment(countries, employment): &apos;&apos;&apos; Fill in this function to return the name of the country with the highest employment in the given employment data, and the employment in that country. &apos;&apos;&apos; max_country = None # Replace this with your code max_value = None # Replace this with your code return (max_country, max_value) solution12345678910def max_employment(countries, employment): &apos;&apos;&apos; Fill in this function to return the name of the country with the highest employment in the given employment data, and the employment in that country. &apos;&apos;&apos; max_country = countries[employment.argmax()] # Replace this with your code max_value = employment.max() # Replace this with your code return (max_country, max_value) argmax() return the position of max() Quiz: Vectorized Operations+ operation:python | numpy—|—list concatenation | vector addition Quiz: Multiplying by a ScalarQuiz: Calculate Overall Completion RateBitwise OperationsSee this article for more information about bitwise operations. In NumPy, a &amp; b performs a bitwise and of a and b. This is not necessarily the same as a logical and, if you wanted to see if matching terms in two integer vectors were non-zero. However, if a and b are both arrays of booleans, rather than integers, bitwise and and logical and are the same thing. If you want to perform a logical and on integer vectors, then you can use the NumPy function np.logical_and(a, b) or convert them into boolean vectors first. Similarly, a | b performs a bitwise or, and ~a performs a bitwise not. However, if your arrays contain booleans, these will be the same as performing logical or and logical not. NumPy also has similar functions for performing these logical operations on integer-valued arrays. For the quiz, assume that the number of males and females are equal i.e. we can take a simple average to get an overall completion rate. In the solution, we may want to / 2. instead of just / 2. This is because in Python 2, dividing an integer by another integer (2) drops fractions, so if our inputs are also integers, we may end up losing information. If we divide by a float (2.) then we will definitely retain decimal values. Erratum: The output of cell [3] in the solution video is incorrect: it appears that the male variable has not been set to the proper value set in cell [2]. All values except for the first will be different. The correct output in cell Out[3]: should instead start with: array([ 192.83205, 205.28855, 202.82258, 186.63257, 206.91115, 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import numpy as np# Change False to True for each block of code to see what it does# Arithmetic operations between 2 NumPy arraysif False: a = np.array([1, 2, 3, 4]) b = np.array([1, 2, 1, 2]) print a + b print a - b print a * b print a / b print a ** b # Arithmetic operations between a NumPy array and a single numberif False: a = np.array([1, 2, 3, 4]) b = 2 print a + b print a - b print a * b print a / b print a ** b # Logical operations with NumPy arraysif False: a = np.array([True, True, False, False]) b = np.array([True, False, True, False]) print a &amp; b print a | b print ~a print a &amp; True print a &amp; False print a | True print a | False # Comparison operations between 2 NumPy Arraysif False: a = np.array([1, 2, 3, 4, 5]) b = np.array([5, 4, 3, 2, 1]) print a &gt; b print a &gt;= b print a &lt; b print a &lt;= b print a == b print a != b # Comparison operations between a NumPy array and a single numberif False: a = np.array([1, 2, 3, 4]) b = 2 print a &gt; b print a &gt;= b print a &lt; b print a &lt;= b print a == b print a != b # First 20 countries with school completion datacountries = np.array([ &apos;Algeria&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Aruba&apos;, &apos;Austria&apos;,&apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Bolivia&apos;, &apos;Botswana&apos;, &apos;Brunei&apos;, &apos;Bulgaria&apos;, &apos;Burkina Faso&apos;, &apos;Burundi&apos;, &apos;Cambodia&apos;, &apos;Cameroon&apos;, &apos;Cape Verde&apos;])# Female school completion rate in 2007 for those 20 countriesfemale_completion = np.array([ 97.35583, 104.62379, 103.02998, 95.14321, 103.69019, 98.49185, 100.88828, 95.43974, 92.11484, 91.54804, 95.98029, 98.22902, 96.12179, 119.28105, 97.84627, 29.07386, 38.41644, 90.70509, 51.7478 , 95.45072])# Male school completion rate in 2007 for those 20 countriesmale_completion = np.array([ 95.47622, 100.66476, 99.7926 , 91.48936, 103.22096, 97.80458, 103.81398, 88.11736, 93.55611, 87.76347, 102.45714, 98.73953, 92.22388, 115.3892 , 98.70502, 37.00692, 45.39401, 91.22084, 62.42028, 90.66958])def overall_completion_rate(female_completion, male_completion): &apos;&apos;&apos; Fill in this function to return a NumPy array containing the overall school completion rate for each country. The arguments are NumPy arrays giving the female and male completion of each country in the same order. &apos;&apos;&apos; return None solution12345678def overall_completion_rate(female_completion, male_completion): &apos;&apos;&apos; Fill in this function to return a NumPy array containing the overall school completion rate for each country. The arguments are NumPy arrays giving the female and male completion of each country in the same order. &apos;&apos;&apos; return (female_completion + male_completion ) /2. Quiz: Standardizing Dataquiz123456789101112131415161718192021222324252627282930313233343536import numpy as np# First 20 countries with employment datacountries = np.array([ &apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;, &apos;Bosnia and Herzegovina&apos;])# Employment data in 2007 for those 20 countriesemployment = np.array([ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076])# Change this country name to change what country will be printed when you# click &quot;Test Run&quot;. Your function will be called to determine the standardized# score for this country for each of the given 5 Gapminder variables in 2007.# The possible country names are available in the Downloadables section.country_name = &apos;United States&apos;def standardize_data(values): &apos;&apos;&apos; Fill in this function to return a standardized version of the given values, which will be in a NumPy array. Each value should be translated into the number of standard deviations that value is away from the mean of the data. (A positive number indicates a value higher than the mean, and a negative number indicates a value lower than the mean.) &apos;&apos;&apos; return None solution12def standardize_data(values): return (values - values.mean()) / values.std() Quiz: NumPy Index Arraysquiz1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import numpy as np# Change False to True for each block of code to see what it does# Using index arraysif False: a = np.array([1, 2, 3, 4]) b = np.array([True, True, False, False]) print a[b] print a[np.array([True, False, True, False])] # Creating the index array using vectorized operationsif False: a = np.array([1, 2, 3, 2, 1]) b = (a &gt;= 2) print a[b] print a[a &gt;= 2] # Creating the index array using vectorized operations on another arrayif False: a = np.array([1, 2, 3, 4, 5]) b = np.array([1, 2, 3, 2, 1]) print b == 2 print a[b == 2]def mean_time_for_paid_students(time_spent, days_to_cancel): &apos;&apos;&apos; Fill in this function to calculate the mean time spent in the classroom for students who stayed enrolled at least (greater than or equal to) 7 days. Unlike in Lesson 1, you can assume that days_to_cancel will contain only integers (there are no students who have not canceled yet). The arguments are NumPy arrays. time_spent contains the amount of time spent in the classroom for each student, and days_to_cancel contains the number of days until each student cancel. The data is given in the same order in both arrays. &apos;&apos;&apos; return None# Time spent in the classroom in the first week for 20 studentstime_spent = np.array([ 12.89697233, 0. , 64.55043217, 0. , 24.2315615 , 39.991625 , 0. , 0. , 147.20683783, 0. , 0. , 0. , 45.18261617, 157.60454283, 133.2434615 , 52.85000767, 0. , 54.9204785 , 26.78142417, 0.])# Days to cancel for 20 studentsdays_to_cancel = np.array([ 4, 5, 37, 3, 12, 4, 35, 38, 5, 37, 3, 3, 68, 38, 98, 2, 249, 2, 127, 35]) sloution12def mean_time_for_paid_students(time_spent, days_to_cancel): return time_spent[days_to_cancel &gt;=7].mean() Quiz: + vs. +=notice12345import numpy as npa = np.array([1,2,3,4])b = aa += np.array([1,1,1,])print b array([2,3,4,5])12345import numpy as npa = np.array([1,2,3,4])b = aa = a + np.array([1,1,1,])print b array([1,2,3,4]) Quiz: In-Place vs. Not In-Placenotice12345import numpy as npa = np.array([1,2,3,4])slice = a[:3]slice[0] = 100print a array([100,2,3,4]) Quiz: Pandas Seriesquiz12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import pandas as pdcountries = [&apos;Albania&apos;, &apos;Algeria&apos;, &apos;Andorra&apos;, &apos;Angola&apos;, &apos;Antigua and Barbuda&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;]life_expectancy_values = [74.7, 75. , 83.4, 57.6, 74.6, 75.4, 72.3, 81.5, 80.2, 70.3, 72.1, 76.4, 68.1, 75.2, 69.8, 79.4, 70.8, 62.7, 67.3, 70.6]gdp_values = [ 1681.61390973, 2155.48523109, 21495.80508273, 562.98768478, 13495.1274663 , 9388.68852258, 1424.19056199, 24765.54890176, 27036.48733192, 1945.63754911, 21721.61840978, 13373.21993972, 483.97086804, 9783.98417323, 2253.46411147, 25034.66692293, 3680.91642923, 366.04496652, 1175.92638695, 1132.21387981]# Life expectancy and gdp data in 2007 for 20 countrieslife_expectancy = pd.Series(life_expectancy_values)gdp = pd.Series(gdp_values)# Change False to True for each block of code to see what it does# Accessing elements and slicingif False: print life_expectancy[0] print gdp[3:6] # Loopingif False: for country_life_expectancy in life_expectancy: print &apos;Examining life expectancy &#123;&#125;&apos;.format(country_life_expectancy) # Pandas functionsif False: print life_expectancy.mean() print life_expectancy.std() print gdp.max() print gdp.sum()# Vectorized operations and index arraysif False: a = pd.Series([1, 2, 3, 4]) b = pd.Series([1, 2, 1, 2]) print a + b print a * 2 print a &gt;= 3 print a[a &gt;= 3] def variable_correlation(variable1, variable2): &apos;&apos;&apos; Fill in this function to calculate the number of data points for which the directions of variable1 and variable2 relative to the mean are the same, and the number of data points for which they are different. Direction here means whether each value is above or below its mean. You can classify cases where the value is equal to the mean for one or both variables however you like. Each argument will be a Pandas series. For example, if the inputs were pd.Series([1, 2, 3, 4]) and pd.Series([4, 5, 6, 7]), then the output would be (4, 0). This is because 1 and 4 are both below their means, 2 and 5 are both below, 3 and 6 are both above, and 4 and 7 are both above. On the other hand, if the inputs were pd.Series([1, 2, 3, 4]) and pd.Series([7, 6, 5, 4]), then the output would be (0, 4). This is because 1 is below its mean but 7 is above its mean, and so on. &apos;&apos;&apos; num_same_direction = None # Replace this with your code num_different_direction = None # Replace this with your code return (num_same_direction, num_different_direction) solution12345678910def variable_correlation(variable1, variable2): both_above = (variable1 &gt; variable1.mean()) &amp; \ (variable2 &gt; variable2.mean()) both_below = (variable1 &lt; variable1.mean()) &amp; \ (variable2 &lt; variable2.mean()) is_same_direction = both_above | both_below num_same_direction = is_same_direction.sum() # Replace this with your code num_different_direction = len(variable1) - num_same_direction # Replace this with your code return (num_same_direction, num_different_direction) Quiz: Series Indexess.describe() s.loc[INDEX] s.iloc[0]Pandas idxmax()Note: The argmax() function mentioned in the videos has been realiased to idxmax(), and returns the index of the first maximally-valued element. You can find documentation for the idxmax() function in Pandas here.quiz123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdcountries = [ &apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;, &apos;Bosnia and Herzegovina&apos;,]employment_values = [ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076,]# Employment data in 2007 for 20 countriesemployment = pd.Series(employment_values, index=countries)def max_employment(employment): &apos;&apos;&apos; Fill in this function to return the name of the country with the highest employment in the given employment data, and the employment in that country. The input will be a Pandas series where the values are employment and the index is country names. Try using the Pandas idxmax() function. Documention can be found here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html &apos;&apos;&apos; max_country = None # Replace this with your code max_value = None # Replace this with your code return (max_country, max_value) solution12345def max_employment(employment): max_country = employment.idxmax() # Replace this with your code max_value = employment.loc[max_country] # Replace this with your code return (max_country, max_value) Quiz: Vectorized Operations and Series Indexesquiz123456789101112131415161718192021222324252627import pandas as pd# Change False to True for each block of code to see what it does# Addition when indexes are the sameif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) print s1 + s2# Indexes have same elements in a different orderif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;b&apos;, &apos;d&apos;, &apos;a&apos;, &apos;c&apos;]) print s1 + s2# Indexes overlap, but do not have exactly the same elementsif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;]) print s1 + s2# Indexes do not overlapif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;e&apos;, &apos;f&apos;, &apos;g&apos;, &apos;h&apos;]) print s1 + s2 Quiz: Filling Missing ValuesRemember that Jupyter notebooks will just print out the results of the last expression run in a code cell as though a print expression was run. If you want to save the results of your operations for later, remember to assign the results to a variable or, for some Pandas functions like .dropna(), use inplace = True to modify the starting object without needing to reassign it.quiz123456789import pandas as pds1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;])# Try to write code that will add the 2 previous series together,# but treating missing values from either series as 0. The result# when printed out should be similar to the following line:# print pd.Series([1, 2, 13, 24, 30, 40], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;]) solution1s1.add(s2, fill_value=0) Quiz: Pandas Series apply()Note: The grader will execute your finished reverse_names(names) function on some test names Series when you submit your answer. Make sure that this function returns another Series with the transformed names. split()You can find documentation for Python’s split() function here.quiz12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import pandas as pd# Change False to True to see what the following block of code does# Example pandas apply() usage (although this could have been done# without apply() using vectorized operations)if False: s = pd.Series([1, 2, 3, 4, 5]) def add_one(x): return x + 1 print s.apply(add_one)names = pd.Series([ &apos;Andre Agassi&apos;, &apos;Barry Bonds&apos;, &apos;Christopher Columbus&apos;, &apos;Daniel Defoe&apos;, &apos;Emilio Estevez&apos;, &apos;Fred Flintstone&apos;, &apos;Greta Garbo&apos;, &apos;Humbert Humbert&apos;, &apos;Ivan Ilych&apos;, &apos;James Joyce&apos;, &apos;Keira Knightley&apos;, &apos;Lois Lane&apos;, &apos;Mike Myers&apos;, &apos;Nick Nolte&apos;, &apos;Ozzy Osbourne&apos;, &apos;Pablo Picasso&apos;, &apos;Quirinus Quirrell&apos;, &apos;Rachael Ray&apos;, &apos;Susan Sarandon&apos;, &apos;Tina Turner&apos;, &apos;Ugueth Urbina&apos;, &apos;Vince Vaughn&apos;, &apos;Woodrow Wilson&apos;, &apos;Yoji Yamada&apos;, &apos;Zinedine Zidane&apos;])def reverse_names(names): &apos;&apos;&apos; Fill in this function to return a new series where each name in the input series has been transformed from the format &quot;Firstname Lastname&quot; to &quot;Lastname, FirstName&quot;. Try to use the Pandas apply() function rather than a loop. &apos;&apos;&apos; return None solution12345678910111213141516171819202122def reverse_name(name): name = name.split(&quot; &quot;) print &quot;name: &quot; print name first_name = name[1] last_name = name[0] return first_name + &apos;, &apos; + last_nameprint &quot;reverse_name: &quot;print reverse_name(names.iloc[0])def reverse_names(names): &apos;&apos;&apos; Fill in this function to return a new series where each name in the input series has been transformed from the format &quot;Firstname Lastname&quot; to &quot;Lastname, FirstName&quot;. Try to use the Pandas apply() function rather than a loop. &apos;&apos;&apos; return names.apply(reverse_name)print &quot;reverse: &quot;reverse_names(names) Quiz: Plotting in PandasIf the variable data is a NumPy array or a Pandas Series, just like if it is a list, the code12import matplotlib.pyplot as pltplt.hist(data) will create a histogram of the data. Pandas also has built-in plotting that uses matplotlib behind the scenes, so if data is a Series, you can create a histogram using data.hist(). There’s no difference between these two in this case, but sometimes the Pandas wrapper can be more convenient. For example, you can make a line plot of a series using data.plot(). The index of the Series will be used for the x-axis and the values for the y-axis. In the following quiz, we’ve created Series containing the various variables we’ve been looking at this lesson. Pick a country you’re interested in, and make a plot of each variable over time. The Udacity editor will only show one plot each time you click “Test Run”, so you can look at multiple plots by clicking “Test Run” multiple times. If you’re running plotting code locally, you may need to add the line plt.show() depending on your setup.quiz12345678910111213141516171819202122232425262728import pandas as pdimport seaborn as sns# The following code reads all the Gapminder data into Pandas DataFrames. You&apos;ll# learn about DataFrames next lesson.path = &apos;/datasets/ud170/gapminder/&apos;employment = pd.read_csv(path + &apos;employment_above_15.csv&apos;, index_col=&apos;Country&apos;)female_completion = pd.read_csv(path + &apos;female_completion_rate.csv&apos;, index_col=&apos;Country&apos;)male_completion = pd.read_csv(path + &apos;male_completion_rate.csv&apos;, index_col=&apos;Country&apos;)life_expectancy = pd.read_csv(path + &apos;life_expectancy.csv&apos;, index_col=&apos;Country&apos;)gdp = pd.read_csv(path + &apos;gdp_per_capita.csv&apos;, index_col=&apos;Country&apos;)# The following code creates a Pandas Series for each variable for the United States.# You can change the string &apos;United States&apos; to a country of your choice.employment_us = employment.loc[&apos;United States&apos;]female_completion_us = female_completion.loc[&apos;United States&apos;]male_completion_us = male_completion.loc[&apos;United States&apos;]life_expectancy_us = life_expectancy.loc[&apos;United States&apos;]gdp_us = gdp.loc[&apos;United States&apos;]# Uncomment the following line of code to see the available country names# print employment.index.values# Use the Series defined above to create a plot of each variable over time for# the country of your choice. You will only be able to display one plot at a time# with each &quot;Test Run&quot;. solution12345# employment_us.plot()# female_completion_us.plot()# male_completion_us.plot()# life_expectancy_us.plot()gdp_us.plot() ConclusionNumpy and Pandas for 2D DataIntroductionQuiz: Subway DataQuiz: Two-Dimensional NumPy Arrayspython: list of listsnumpy: 2D arraypandas: DataFrameThis page describes the memory layout of 2D NumPy arrays.quiz123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as np# Subway ridership for 5 stations on 10 different daysridership = np.array([ [ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]])# Change False to True for each block of code to see what it does# Accessing elementsif False: print ridership[1, 3] print ridership[1:3, 3:5] print ridership[1, :] # Vectorized operations on rows or columnsif False: print ridership[0, :] + ridership[1, :] print ridership[:, 0] + ridership[:, 1] # Vectorized operations on entire arraysif False: a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) b = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]) print a + bdef mean_riders_for_max_station(ridership): &apos;&apos;&apos; Fill in this function to find the station with the maximum riders on the first day, then return the mean riders per day for that station. Also return the mean ridership overall for comparsion. Hint: NumPy&apos;s argmax() function might be useful: http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html &apos;&apos;&apos; overall_mean = None # Replace this with your code mean_for_max = None # Replace this with your code return (overall_mean, mean_for_max) solution12345678def mean_riders_for_max_station(ridership): overall_mean = ridership.mean() # Replace this with your code station = ridership[0,:].argmax() mean_for_max = ridership[:,station].mean() # Replace this with your code return (overall_mean, mean_for_max) Quiz: NumPy Axisaxis = 0 column1 rowquiz123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# Change False to True for this block of code to see what it does# NumPy axis argumentif False: a = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) print a.sum() print a.sum(axis=0) print a.sum(axis=1) # Subway ridership for 5 stations on 10 different daysridership = np.array([ [ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]])def min_and_max_riders_per_day(ridership): &apos;&apos;&apos; Fill in this function. First, for each subway station, calculate the mean ridership per day. Then, out of all the subway stations, return the maximum and minimum of these values. That is, find the maximum mean-ridership-per-day and the minimum mean-ridership-per-day for any subway station. &apos;&apos;&apos; max_daily_ridership = None # Replace this with your code min_daily_ridership = None # Replace this with your code return (max_daily_ridership, min_daily_ridership) solution12345def min_and_max_riders_per_day(ridership): max_daily_ridership = ridership.mean(axis=0).max() # Replace this with your code min_daily_ridership = ridership.mean(axis=0).min() # Replace this with your code return (max_daily_ridership, min_daily_ridership) NumPy and Pandas Data typesQuiz: Accessing Elements of a DataFramequiz12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pandas as pd# Subway ridership for 5 stations on 10 different daysridership_df = pd.DataFrame( data=[[ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]], index=[&apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-06-11&apos;, &apos;05-07-11&apos;, &apos;05-08-11&apos;, &apos;05-09-11&apos;, &apos;05-10-11&apos;], columns=[&apos;R003&apos;, &apos;R004&apos;, &apos;R005&apos;, &apos;R006&apos;, &apos;R007&apos;])# Change False to True for each block of code to see what it does# DataFrame creationif False: # You can create a DataFrame out of a dictionary mapping column names to values df_1 = pd.DataFrame(&#123;&apos;A&apos;: [0, 1, 2], &apos;B&apos;: [3, 4, 5]&#125;) print df_1 # You can also use a list of lists or a 2D NumPy array df_2 = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]) print df_2 # Accessing elementsif False: print ridership_df.iloc[0] print ridership_df.loc[&apos;05-05-11&apos;] print ridership_df[&apos;R003&apos;] print ridership_df.iloc[1, 3] # Accessing multiple rowsif False: print ridership_df.iloc[1:4] # Accessing multiple columnsif False: print ridership_df[[&apos;R003&apos;, &apos;R005&apos;]] # Pandas axisif False: df = pd.DataFrame(&#123;&apos;A&apos;: [0, 1, 2], &apos;B&apos;: [3, 4, 5]&#125;) print df.sum() print df.sum(axis=1) print df.values.sum() def mean_riders_for_max_station(ridership): &apos;&apos;&apos; Fill in this function to find the station with the maximum riders on the first day, then return the mean riders per day for that station. Also return the mean ridership overall for comparsion. This is the same as a previous exercise, but this time the input is a Pandas DataFrame rather than a 2D NumPy array. &apos;&apos;&apos; overall_mean = None # Replace this with your code mean_for_max = None # Replace this with your code return (overall_mean, mean_for_max) solution123456def mean_riders_for_max_station(ridership): station = ridership.iloc[0].argmax() overall_mean = ridership.values.mean() # Replace this with your code mean_for_max = ridership[station].mean() # Replace this with your code return (overall_mean, mean_for_max) Loading Data into a DataFrameQuiz: Calculating CorrelationUnderstand and Interpreting CorrelationsThis page contains some scatterplots of variables with different values of correlation.This page lets you use a slider to change the correlation and see how the data might look.Pearson’s r only measures linear correlation! This image shows some different linear and non-linear relationships and what Pearson’s r will be for those relationships. Corrected vs. Uncorrected Standard DeviationBy default, Pandas’ std() function computes the standard deviation using Bessel’s correction. Calling std(ddof=0) ensures that Bessel’s correction will not be used. Previous ExerciseThe exercise where you used a simple heuristic to estimate correlation was the “Pandas Series” exercise in the previous lesson, “NumPy and Pandas for 1D Data”. Pearson’s r in NumPyNumPy’s corrcoef() function can be used to calculate Pearson’s r, also known as the correlation coefficient.quiz123456789101112131415161718192021222324252627import pandas as pdfilename = &apos;/datasets/ud170/subway/nyc_subway_weather.csv&apos;subway_df = pd.read_csv(filename)def correlation(x, y): &apos;&apos;&apos; Fill in this function to compute the correlation between the two input variables. Each input is either a NumPy array or a Pandas Series. correlation = average of (x in standard units) times (y in standard units) Remember to pass the argument &quot;ddof=0&quot; to the Pandas std() function! &apos;&apos;&apos; return Noneentries = subway_df[&apos;ENTRIESn_hourly&apos;]cum_entries = subway_df[&apos;ENTRIESn&apos;]rain = subway_df[&apos;meanprecipi&apos;]temp = subway_df[&apos;meantempi&apos;]print correlation(entries, rain)print correlation(entries, temp)print correlation(rain, temp)print correlation(entries, cum_entries) solution1234def correlation(x, y): std_x = (x - x.mean()) / x.std(ddof=0) std_y = (y - y.mean()) / y.std(ddof=0) return ( std_x * std_y ).mean() Pandas Axis Names12axis=0 axis=1axis=&apos;index&apos; axis=&apos;columns&apos; Quiz: DataFrame Vectorized OperationsPandas shift()Documentation for the Pandas shift() function is here. If you’re still not sure how the function works, try it out and see! Alternative SolutionAs an alternative to using vectorized operations, you could also use the code return entries_and_exits.diff() to calculate the answer in a single step.quiz123456789101112131415161718192021222324252627282930313233343536373839404142import pandas as pd# Examples of vectorized operations on DataFrames:# Change False to True for each block of code to see what it does# Adding DataFrames with the column namesif False: df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;) df2 = pd.DataFrame(&#123;&apos;a&apos;: [10, 20, 30], &apos;b&apos;: [40, 50, 60], &apos;c&apos;: [70, 80, 90]&#125;) print df1 + df2 # Adding DataFrames with overlapping column names if False: df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;) df2 = pd.DataFrame(&#123;&apos;d&apos;: [10, 20, 30], &apos;c&apos;: [40, 50, 60], &apos;b&apos;: [70, 80, 90]&#125;) print df1 + df2# Adding DataFrames with overlapping row indexesif False: df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;, index=[&apos;row1&apos;, &apos;row2&apos;, &apos;row3&apos;]) df2 = pd.DataFrame(&#123;&apos;a&apos;: [10, 20, 30], &apos;b&apos;: [40, 50, 60], &apos;c&apos;: [70, 80, 90]&#125;, index=[&apos;row4&apos;, &apos;row3&apos;, &apos;row2&apos;]) print df1 + df2# --- Quiz ---# Cumulative entries and exits for one station for a few hours.entries_and_exits = pd.DataFrame(&#123; &apos;ENTRIESn&apos;: [3144312, 3144335, 3144353, 3144424, 3144594, 3144808, 3144895, 3144905, 3144941, 3145094], &apos;EXITSn&apos;: [1088151, 1088159, 1088177, 1088231, 1088275, 1088317, 1088328, 1088331, 1088420, 1088753]&#125;)def get_hourly_entries_and_exits(entries_and_exits): &apos;&apos;&apos; Fill in this function to take a DataFrame with cumulative entries and exits (entries in the first column, exits in the second) and return a DataFrame with hourly entries and exits (entries in the first column, exits in the second). &apos;&apos;&apos; return None solution12def get_hourly_entries_and_exits(entries_and_exits): return entries_and_exits - entries_and_exits.shift(1) Quiz: DataFrame applymap()Note: The grader will execute your finished convert_grades(grades) function on some test grades DataFrames when you submit your answer. Make sure that this function returns a DataFrame with the converted grades. ​Hint​: You may need to define a helper function to use with .applymap().quiz1234567891011121314151617181920212223242526272829303132333435363738import pandas as pd# Change False to True for this block of code to see what it does# DataFrame applymap()if False: df = pd.DataFrame(&#123; &apos;a&apos;: [1, 2, 3], &apos;b&apos;: [10, 20, 30], &apos;c&apos;: [5, 10, 15] &#125;) def add_one(x): return x + 1 print df.applymap(add_one) grades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;]) def convert_grades(grades): &apos;&apos;&apos; Fill in this function to convert the given DataFrame of numerical grades to letter grades. Return a new DataFrame with the converted grade. The conversion rule is: 90-100 -&gt; A 80-89 -&gt; B 70-79 -&gt; C 60-69 -&gt; D 0-59 -&gt; F &apos;&apos;&apos; return None solution123456789101112131415def convert(grades): if (grades &gt;= 90) &amp; (grades &lt;= 100): grades = &apos;A&apos; elif (grades &gt;= 80) &amp; (grades &lt;= 89): grades = &apos;B&apos; elif (grades &gt;= 70) &amp; (grades &lt;= 79): grades = &apos;C&apos; elif (grades &gt;= 60) &amp; (grades &lt;= 69): grades = &apos;D&apos; else: grades = &apos;F&apos; return grades def convert_grades(grades): return grades.applymap(convert) Quiz: DataFrame apply()Note: In order to get the proper computations, we should actually be setting the value of the “ddof” parameter to 0 in the .std() function. Note that the type of standard deviation calculated by default is different between numpy’s .std() and pandas’ .std() functions. By default, numpy calculates a population standard deviation, with “ddof = 0”. On the other hand, pandas calculates a sample standard deviation, with “ddof = 1”. If we know all of the scores, then we have a population - so to standardize using pandas, we need to set “ddof = 0”..apply() used to convert columns(default) to columns and convert rows(with axis) to rows.applymap() used to elementsquiz123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdgrades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;])# Change False to True for this block of code to see what it does# DataFrame apply()if False: def convert_grades_curve(exam_grades): # Pandas has a bult-in function that will perform this calculation # This will give the bottom 0% to 10% of students the grade &apos;F&apos;, # 10% to 20% the grade &apos;D&apos;, and so on. You can read more about # the qcut() function here: # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html return pd.qcut(exam_grades, [0, 0.1, 0.2, 0.5, 0.8, 1], labels=[&apos;F&apos;, &apos;D&apos;, &apos;C&apos;, &apos;B&apos;, &apos;A&apos;]) # qcut() operates on a list, array, or Series. This is the # result of running the function on a single column of the # DataFrame. print convert_grades_curve(grades_df[&apos;exam1&apos;]) # qcut() does not work on DataFrames, but we can use apply() # to call the function on each column separately print grades_df.apply(convert_grades_curve) def standardize(df): &apos;&apos;&apos; Fill in this function to standardize each column of the given DataFrame. To standardize a variable, convert each value to the number of standard deviations it is above or below the mean. &apos;&apos;&apos; return None solution12345def std_col(col): return (col - col.mean()) / col.std(ddof=0) def standardize(df): return df.apply(std_col) Quiz: DataFrame apply() Use Case 2.apply() convert columns to elementdf.apply(np.max):=df.max()quiz12345678910111213141516171819202122import numpy as npimport pandas as pddf = pd.DataFrame(&#123; &apos;a&apos;: [4, 5, 3, 1, 2], &apos;b&apos;: [20, 10, 40, 50, 30], &apos;c&apos;: [25, 20, 5, 15, 10]&#125;)# Change False to True for this block of code to see what it does# DataFrame apply() - use case 2if False: print df.apply(np.mean) print df.apply(np.max) def second_largest(df): &apos;&apos;&apos; Fill in this function to return the second-largest value of each column of the input DataFrame. &apos;&apos;&apos; return None solution123456def second_max(col): sorted_col = col.sort_values(ascending=False) return sorted_col.iloc[1] def second_largest(df): return df.apply(second_max) Quiz: Adding a DataFrame to a Seriescode1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import pandas as pd# Change False to True for each block of code to see what it does# Adding a Series to a square DataFrameif False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding a Series to a one-row DataFrame if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123;0: [10], 1: [20], 2: [30], 3: [40]&#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s# Adding a Series to a one-column DataFrameif False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123;0: [10, 20, 30, 40]&#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding when DataFrame column names match Series indexif False: s = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) df = pd.DataFrame(&#123; &apos;a&apos;: [10, 20, 30, 40], &apos;b&apos;: [50, 60, 70, 80], &apos;c&apos;: [90, 100, 110, 120], &apos;d&apos;: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding when DataFrame column names don&apos;t match Series indexif False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; &apos;a&apos;: [10, 20, 30, 40], &apos;b&apos;: [50, 60, 70, 80], &apos;c&apos;: [90, 100, 110, 120], &apos;d&apos;: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s Quiz: Standardizing Each Column Againquiz123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import pandas as pd# Adding using +if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding with axis=&apos;index&apos;if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df.add(s, axis=&apos;index&apos;) # The functions sub(), mul(), and div() work similarly to add() # Adding with axis=&apos;columns&apos;if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df.add(s, axis=&apos;columns&apos;) # The functions sub(), mul(), and div() work similarly to add() grades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;])def standardize(df): &apos;&apos;&apos; Fill in this function to standardize each column of the given DataFrame. To standardize a variable, convert each value to the number of standard deviations it is above or below the mean. This time, try to use vectorized operations instead of apply(). You should get the same results as you did before. &apos;&apos;&apos; return Nonedef standardize_rows(df): &apos;&apos;&apos; Optional: Fill in this function to standardize each row of the given DataFrame. Again, try not to use apply(). This one is more challenging than standardizing each column! &apos;&apos;&apos; return None solution123456def standardize(df): return (df - df.mean()) / df.std(ddof=0)def standardize_rows(df): mean_diff = df.sub(df.mean(axis=&apos;columns&apos;), axis=&apos;index&apos;) return mean_diff.div(df.std(ddof=0, axis=&apos;columns&apos;), axis=&apos;index&apos;) Quiz: Pandas groupby()code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport seaborn as snsvalues = np.array([1, 3, 2, 4, 1, 6, 4])example_df = pd.DataFrame(&#123; &apos;value&apos;: values, &apos;even&apos;: values % 2 == 0, &apos;above_three&apos;: values &gt; 3 &#125;, index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;])# Change False to True for each block of code to see what it does# Examine DataFrameif False: print example_df # Examine groupsif False: grouped_data = example_df.groupby(&apos;even&apos;) # The groups attribute is a dictionary mapping keys to lists of row indexes print grouped_data.groups # Group by multiple columnsif False: grouped_data = example_df.groupby([&apos;even&apos;, &apos;above_three&apos;]) print grouped_data.groups # Get sum of each groupif False: grouped_data = example_df.groupby(&apos;even&apos;) print grouped_data.sum() # Limit columns in resultif False: grouped_data = example_df.groupby(&apos;even&apos;) # You can take one or more columns from the result DataFrame print grouped_data.sum()[&apos;value&apos;] print &apos;\n&apos; # Blank line to separate results # You can also take a subset of columns from the grouped data before # collapsing to a DataFrame. In this case, the result is the same. print grouped_data[&apos;value&apos;].sum() filename = &apos;/datasets/ud170/subway/nyc_subway_weather.csv&apos;subway_df = pd.read_csv(filename)### Write code here to group the subway data by a variable of your choice, then### either print out the mean ridership within each group or create a plot. Quiz: Calculating Hourly Entries and ExitsIn the quiz where you calculated hourly entries and exits, you did so for a single set of cumulative entries. However, in the original data, there was a separate set of numbers for each station. Thus, to correctly calculate the hourly entries and exits, it was necessary to group by station and day, then calculate the hourly entries and exits within each day. Write a function to do that. You should use the apply() function to call the function you wrote previously. You should also make sure you restrict your grouped data to just the entries and exits columns, since your function may cause an error if it is called on non-numerical data types. If you would like to learn more about using groupby() in Pandas, this page contains more details. Note: You will not be able to reproduce the ENTRIESn_hourly and EXITSn_hourly columns in the full dataset using this method. When creating the dataset, we did extra processing to remove erroneous values. quizTo clarify the structure of the data, the original data recorded the cumulative number of entries on each station at four-hour intervals. For the quiz, you just need to look at the differences between consecutive measurements on each station: by computing “hourly entries”, we just mean recording the number of new tallies between each recording period as a contrast to “cumulative entries”.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport pandas as pdvalues = np.array([1, 3, 2, 4, 1, 6, 4])example_df = pd.DataFrame(&#123; &apos;value&apos;: values, &apos;even&apos;: values % 2 == 0, &apos;above_three&apos;: values &gt; 3 &#125;, index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;])# Change False to True for each block of code to see what it does# Standardize each groupif False: def standardize(xs): return (xs - xs.mean()) / xs.std() grouped_data = example_df.groupby(&apos;even&apos;) print grouped_data[&apos;value&apos;].apply(standardize) # Find second largest value in each groupif False: def second_largest(xs): sorted_xs = xs.sort(inplace=False, ascending=False) return sorted_xs.iloc[1] grouped_data = example_df.groupby(&apos;even&apos;) print grouped_data[&apos;value&apos;].apply(second_largest)# --- Quiz ---# DataFrame with cumulative entries and exits for multiple stationsridership_df = pd.DataFrame(&#123; &apos;UNIT&apos;: [&apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;], &apos;TIMEn&apos;: [&apos;00:00:00&apos;, &apos;02:00:00&apos;, &apos;04:00:00&apos;, &apos;06:00:00&apos;, &apos;08:00:00&apos;, &apos;10:00:00&apos;, &apos;12:00:00&apos;, &apos;14:00:00&apos;, &apos;16:00:00&apos;], &apos;ENTRIESn&apos;: [3144312, 8936644, 3144335, 8936658, 3144353, 8936687, 3144424, 8936819, 3144594], &apos;EXITSn&apos;: [1088151, 13755385, 1088159, 13755393, 1088177, 13755598, 1088231, 13756191, 1088275]&#125;)def get_hourly_entries_and_exits(entries_and_exits): &apos;&apos;&apos; Fill in this function to take a DataFrame with cumulative entries and exits and return a DataFrame with hourly entries and exits. The hourly entries and exits should be calculated separately for each station (the &apos;UNIT&apos; column). Hint: Take a look at the `get_hourly_entries_and_exits()` function you wrote in a previous quiz, DataFrame Vectorized Operations. If you copy it here and rename it, you can use it and the `.apply()` function to help solve this problem. &apos;&apos;&apos; return None solution123456def hourly_entries_and_exits(entries_and_exits): return entries_and_exits - entries_and_exits.shift(1) def get_hourly_entries_and_exits(entries_and_exits): grouped_data = entries_and_exits.groupby(&apos;UNIT&apos;) return grouped_data[[&apos;ENTRIESn&apos;, &apos;EXITSn&apos;]].apply(hourly_entries_and_exits) Quiz: Combining Pandas DataFramesIn the merged table on the right, the join dates in the third and fourth rows should be 5/19 and 5/11, reflecting the account key mapping in the enrollments table.quiz1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdsubway_df = pd.DataFrame(&#123; &apos;UNIT&apos;: [&apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;], &apos;DATEn&apos;: [&apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;], &apos;hour&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;ENTRIESn&apos;: [ 4388333, 4388348, 4389885, 4391507, 4393043, 14656120, 14656174, 14660126, 14664247, 14668301], &apos;EXITSn&apos;: [ 2911002, 2911036, 2912127, 2913223, 2914284, 14451774, 14451851, 14454734, 14457780, 14460818], &apos;latitude&apos;: [ 40.689945, 40.689945, 40.689945, 40.689945, 40.689945, 40.69132 , 40.69132 , 40.69132 , 40.69132 , 40.69132 ], &apos;longitude&apos;: [-73.872564, -73.872564, -73.872564, -73.872564, -73.872564, -73.867135, -73.867135, -73.867135, -73.867135, -73.867135]&#125;)weather_df = pd.DataFrame(&#123; &apos;DATEn&apos;: [&apos;05-01-11&apos;, &apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-05-11&apos;], &apos;hour&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;latitude&apos;: [ 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 ], &apos;longitude&apos;: [-73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135], &apos;pressurei&apos;: [ 30.24, 30.24, 30.32, 30.32, 30.14, 30.14, 29.98, 29.98, 30.01, 30.01], &apos;fog&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;rain&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;tempi&apos;: [ 52. , 52. , 48.9, 48.9, 54. , 54. , 57.2, 57.2, 48.9, 48.9], &apos;wspdi&apos;: [ 8.1, 8.1, 6.9, 6.9, 3.5, 3.5, 15. , 15. , 15. , 15. ]&#125;)def combine_dfs(subway_df, weather_df): &apos;&apos;&apos; Fill in this function to take 2 DataFrames, one with subway data and one with weather data, and return a single dataframe with one row for each date, hour, and location. Only include times and locations that have both subway data and weather data available. &apos;&apos;&apos; return None solution12def combine_dfs(subway_df, weather_df): return subway_df.merge(weather_df, on=[&apos;DATEn&apos;, &apos;hour&apos;, &apos;latitude&apos;, &apos;longitude&apos;], how=&apos;inner&apos;) Quiz: Plotting for DataFramesJust like Pandas Series, DataFrames also have a plot() method. If df is a DataFrame, then df.plot() will produce a line plot with a different colored line for each variable in the DataFrame. This can be a convenient way to get a quick look at your data, especially for small DataFrames, but for more complicated plots you will usually want to use matplotlib directly. In the following quiz, create a plot of your choice showing something interesting about the New York subway data. For example, you might create: Histograms of subway ridership on both days with rain and days without rainA scatterplot of subway stations with latitude and longitude as the x and y axes and ridership as the bubble sizeIf you choose this option, you may wish to use the as_index=False argument to groupby(). There is example code in the following quiz.A scatterplot with subway ridership on one axis and precipitation or temperature on the otherIf you’re not sure how to make the plot you want, try searching on Google or take a look at the matplotlib documentation. Once you’ve created a plot you’re happy with, share what you’ve found on the forums!quiz1234567891011121314151617181920212223242526272829303132import matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport seaborn as snsvalues = np.array([1, 3, 2, 4, 1, 6, 4])example_df = pd.DataFrame(&#123; &apos;value&apos;: values, &apos;even&apos;: values % 2 == 0, &apos;above_three&apos;: values &gt; 3 &#125;, index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;])# Change False to True for this block of code to see what it does# groupby() without as_indexif False: first_even = example_df.groupby(&apos;even&apos;).first() print first_even print first_even[&apos;even&apos;] # Causes an error. &apos;even&apos; is no longer a column in the DataFrame # groupby() with as_index=Falseif False: first_even = example_df.groupby(&apos;even&apos;, as_index=False).first() print first_even print first_even[&apos;even&apos;] # Now &apos;even&apos; is still a column in the DataFramefilename = &apos;/datasets/ud170/subway/nyc_subway_weather.csv&apos;subway_df = pd.read_csv(filename)## Make a plot of your choice here showing something interesting about the subway data.## Matplotlib documentation here: http://matplotlib.org/api/pyplot_api.html## Once you&apos;ve got something you&apos;re happy with, share it on the forums! solution123location = subway_df.groupby([&apos;latitude&apos;, &apos;longitude&apos;], as_index=False).mean()scale = location[&apos;ENTRIESn_hourly&apos;] / location[&apos;ENTRIESn_hourly&apos;].std(ddof=0)plt.scatter(location[&apos;latitude&apos;], location[&apos;longitude&apos;], s=scale) Three-Dimensional DataThree-Dimensional DataNow that you’ve worked with one-dimensional and two-dimensional data, you might be wondering how to work with three or more dimensions. 3D data in NumPyNumPy arrays can have arbitrarily many dimensions. Just like you can create a 1D array from a list, and a 2D array from a list of lists, you can create a 3D array from a list of lists of lists, and so on. For example, the following code would create a 3D array:1234a = np.array([ [[&apos;A1a&apos;, &apos;A1b&apos;, &apos;A1c&apos;], [&apos;A2a&apos;, &apos;A2b&apos;, &apos;A2c&apos;]], [[&apos;B1a&apos;, &apos;B1b&apos;, &apos;B1c&apos;], [&apos;B2a&apos;, &apos;B2b&apos;, &apos;B2c&apos;]]]) 3D data in PandasPandas has a data structure called a Panel, which is similar to a DataFrame or a Series, but for 3D data. If you would like, you can learn more about Panels here. ConclusionFinal Project: Investigate a Dataset]]></content>
      <categories>
        <category>Udacity</category>
        <category>Data Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Udacity</tag>
        <tag>Data Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity DAND Notebook]]></title>
    <url>%2F2017%2F06%2F20%2FUdacity%20DAND%20Notebook%2F</url>
    <content type="text"><![CDATA[Stay stunedPreview https://cn.udacity.com/course/statistics--st095https://cn.udacity.com/course/intro-to-descriptive-statistics--ud827https://cn.udacity.com/course/intro-to-inferential-statistics--ud201https://classroom.udacity.com/courses/ud1111https://www.udacity.com/course/intro-to-data-analysis--ud170https://cn.udacity.com/course/data-visualization-in-tableau--ud1006https://www.udacity.com/course/ab-testing--ud257https://www.udacity.com/course/ab-testing--ud979 https://cn.udacity.com/course/data-wrangling-with-mongodb--ud032https://cn.udacity.com/course/data-analysis-with-r--ud651https://cn.udacity.com/course/data-visualization-and-d3js--ud507]]></content>
      <tags>
        <tag>Data Analyst</tag>
        <tag>NanoDegree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud1111 Anaconda and Jupyter Notebooks notebook]]></title>
    <url>%2F2017%2F06%2F20%2FUdacity%20ud1111%20Anaconda%20and%20Jupyter%20Notebooks%20notebook%2F</url>
    <content type="text"><![CDATA[completed in 2017-06-20course can be found here LessonAnacondaManaging environmentsManaging environmentsAs I mentioned before, conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal. Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy. When creating an environment, you can specify which version of Python to install in the environment. This is useful when you’re working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python=3 or conda create -n py2 python=2. I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python=3.3 for Python 3.3. Entering an environmentOnce you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env. When you’re in the environment, you’ll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name. Only this time, the specific packages you install will only be available when you’re in the environment. To leave the environment, type source deactivate (on OSX/Linux). On Windows, use deactivate. More environment actionsSaving and loading environmentsA really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export &gt; environment.yaml. The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, &gt; environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml. This will create a new environment with the same name listed in environment.yaml. Listing environmentsIf you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you’ve created. You should see a list of environments, there will be an asterisk next to the environment you’re currently in. The default environment, the environment used when you aren’t in one, is called root. Removing environmentsIf there are environments you don’t use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name). Best practicesUsing environmentsOne thing that’s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python=2 and conda create -n py3 python=3 to create two separate environments, py2 and py3. Now I have a general use environment for each Python version. In each of those environments, I’ve installed most of the standard data science packages (numpy, scipy, pandas, etc.) I’ve also found it useful to create environments for each project I’m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican. Sharing environmentsWhen sharing your code on GitHub, it’s good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze (learn more here) for people not using conda. More to learnTo learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here’s the conda documentation you can reference later. On Python versions at UdacityMost Nanodegree programs at Udacity will be (or are already) using Python 3 almost exclusively. Why we’re switching to Python 3 Jupyter is switching to Python 3 only Python 2.7 is being retired Python 3.6 has great features such as formatted strings At this point, there are enough new features in Python 3 that it doesn’t make much sense to stick with Python 2 unless you’re working with old code. All new Python code should be written for version 3. The main breakage between Python 2 and 3For the most part, Python 2 code will work with Python 3. Of course, most new features introduced with Python 3 versions won’t be backwards compatible. The place where your Python 2 code will fail most often is the print statement. For most of Python’s history including Python 2, printing was done like so:12print &quot;Hello&quot;, &quot;world!&quot;&gt; Hello world! This was changed in Python 3 to a function.12print(&quot;Hello&quot;, &quot;world!&quot;)&gt; Hello world! The print function was back-ported to Python 2 in version 2.6 through the __future__ module:1234# In Python 2.6+from __future__ import print_functionprint(&quot;Hello&quot;, &quot;world!&quot;)&gt; Hello world! The print statement doesn’t work in Python 3. If you want to print something and have it work in both Python versions, you’ll need to import print_function in your Python 2 code. Note for students in the Data Analyst Nanodegree programCurrently, most of the materials for this Nanodegree program are still guaranteed to work only for Python 2.7. You can quickly set up an environment for the current DAND program by opening the Resources tab and downloading an appropriate YAML file. Note for students in the Machine Learning Engineer Nanodegree programCurrently, Machine Learning Engineer Nanodegree requires Python 2.7 to finish all the projects. Jupyter NotebooksWhat are Jupyter notebooks?Markdown cellsso if you don’t have experience with LaTeX please read this primer on using it to create math expressions. Magic keywordshttps://classroom.udacity.com/courses/ud1111/lessons/b15ba0a2-015d-4c5a-87ae-9efba2cabb43/concepts/256cdd36-17d4-442a-a033-7c64ce83f7f8 here’s the list of all available magic commands. Converting notebooksFor example, to convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb As always, learn more about nbconvert from the documentation. Creating a slideshowYou can see an example of a slideshow here Slides are full slides that you move through left to right. Sub-slides show up in the slideshow by pressing up or down. Fragments are hidden at first, then appear with a button press. You can skip cells in the slideshow with Skip and Notes leaves the cell as speaker notes. Running the slideshowTo create the slideshow from the notebook file, you’ll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it.]]></content>
      <categories>
        <category>Udacity</category>
        <category>Anaconda and Jupyter Notebooks</category>
      </categories>
      <tags>
        <tag>jupyter notebook</tag>
        <tag>Udacity</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Daily Easy English Expression Notebook]]></title>
    <url>%2F2017%2F06%2F09%2FDaily%20Easy%20English%20Expression%20Notebook%2F</url>
    <content type="text"><![CDATA[on the way SourceFollow this video in YouTube Content How are you doing ha yuh doen What do you do for a living wha d yuh do fere living My knee went out my knee wen nout s n l cancel d t th What’s up this weekend wassup thi sweekend s cancel t]]></content>
      <tags>
        <tag>Speaking</tag>
        <tag>Listening</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity FEND Notebook]]></title>
    <url>%2F2017%2F06%2F04%2FUdacity%20FEND%20Notebook%2F</url>
    <content type="text"><![CDATA[FEND Completed in 2016 Article to Mockuporiginal siteAnimal Trading Cardsoriginal siteBuild a Portfolio Siteoriginal siteOnline Resumeoriginal siteClassic Arcade Game Cloneoriginal siteWebsite Optimizationoriginal siteNeighborhood Maporiginal siteFeed Reader Testingoriginal site]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>NanoDegree</tag>
        <tag>Front End</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter Notebook Note]]></title>
    <url>%2F2017%2F05%2F25%2FJupyter%20Notebook%20Note%2F</url>
    <content type="text"><![CDATA[stay stuned… 05-25-2017activate python278 conda install notebook ipykernel ipython kernel install --user cd G:\Udacity\MLND\P1 Model Evaluation and Validation\program files jupyter notebook boston_housing.ipynb delete python278conda env remove -n python278 show conda envs infoconda info --envs conda create -n py27 python=2.7 anaconda 1234activate py27conda listconda install numpy pandas matplotlib jupyter notebookdeactivate To uninstall, useconda remove package_name. To update a package conda update package_name. If you want to update all packages in an environment, which is often useful, use conda update --all If you don’t know the exact name of the package you’re looking for, you can try searching with conda search search_term.]]></content>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity cs253 How to Build a Blog Notebook]]></title>
    <url>%2F2017%2F05%2F01%2FUdacity%20cs253%20How%20to%20Build%20a%20Blog%20Notebook%2F</url>
    <content type="text"><![CDATA[Components of the Webupdated in 2017-05-01no more updated Course can be found herehttps://classroom.udacity.com/courses/cs253/lessons/48737165/concepts/483298540923wikihttps://developer.mozilla.org/en-US/docs/Learn How the Web Workshttps://www.udacity.com/wiki/cs253/unit-1 Introduction to the Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AkjMCbSvTto.mp4 World Wide Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NBI9kXzMHS0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uPTMmyZB7tw.mp4 File Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ND8jAv7WrmU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VngVBqQYxVg.mp4 Components of the Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kzyfIiVZPJA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cVU7hYn-B8I.mp4 Best Browserhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/57kH7Yole2k.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eQDNuWOxaJ0.mp4 HTML Basicshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5Kjx-NOwcSc.mp4 Intro to HTML Tagshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VsxbuJWcxqA.mp4 Bold Tag&lt;b&gt;content&lt;/b&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/irJ9o1Uv6U8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JWld9DM-La4.mp4 Italics&lt;em&gt;content&lt;/em&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4H23hC78J_A.mp4Instructions: Make the phrase ‘HTML is reasonably straightforward’ italic. Note: the textbox should have default text in it for you to edit. In the meantime, it should look like this: HTML is &lt;b&gt;reasonably straightforward&lt;/b&gt; Your job is to make entire phase italicized by using the &lt;em&gt; tag.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1079BoYQUD8.mp4 Missing End Taghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OWjh74s_uT4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pKouF71QK_c.mp4 Making Links&lt;tag attr=&quot;value&quot;&gt;content&lt;/tag&gt; &lt;a href=&quot;www.reddit.com&quot;&gt;derp&lt;/a&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-0S9lBFHeo0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IjdvSD1tX8U.mp4Errata:URLs need the protocolThe URL in the link should be “http://udacity.com“ and not “udacity.com”. If the default text is missingThe textbox should have default text in it for you to edit. In the meantime, it should look like this: This website is my favorite Your job is to make the words my favorite a link to udacity.com. Adding Images&lt;img src=&quot;url&quot; alt=&quot;text&quot;&gt;https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Le8i8TtDleU.mp4 Whitespace&lt;br&gt; &lt;p&gt;content&lt;/p&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dEtVii1eYYY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/at-mlJ0KeMQ.mp4 Paragraph Taghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fSnKsMM6DRI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BH46s-hYLXg.mp4 Inline vs Blockinline: &lt;b&gt;&lt;/b&gt; &lt;em&gt;&lt;/em&gt; &lt;img src=&quot;&quot;&gt; block: &lt;p&gt;&lt;/p&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bOh9WjucNsA.mp4 Span and Divinline:&lt;span&gt;text&lt;/span&gt; block: &lt;div&gt;text&lt;/div&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rv1Oy-EduCk.mp4We now recommend using scratchpad.io instead of the HTML playground Steve refers to.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jCdjfORR7BI.mp4 Document Structure&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;context&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VL7Wm0UzY6s.mp4 Introducing URLsUniform Resource LocatorFor those of you wondering what an IP address is, wikipedia has this to say: An Internet Protocol address (IP address) is a numerical label assigned to each device (e.g., computer, printer) participating in a computer network that uses the Internet Protocol for communication. An IP address serves two principal functions: host or network interface identification and location addressing. Its role has been characterized as follows: &quot;A name indicates what we seek. An address indicates where it is. A route indicates how to get there.&quot; The URL is the human readable locator which resolves to a numerical IP Address and represents, as Steve says, &quot;the location of the physical machine which has the document we want to fetch.&quot; An example IPv4 address looks like this: 172.16.254.1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yKKGg6ihUCs.mp4 Correct URLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kUMasWPRKE4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4_nLjYoN8ks.mp4 Query Parametershttp://example.com/foo?p=1&amp;q=neat name=value https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qv5XK91OhFo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e7N_4uaoNeI.mp4 Fragmentshttp://www.example.com/foo#fragment http://example.com/foo?p=1#fragment https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xq95EZdiOQc.mp4 Porthttp://localhost:8000/ port-default=80 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3P7e6R9LsRY.mp4The url got slightly cut off at the end. It should read: http://example.com:80/toys?p=foo#blah Note that urls are, in general, case-sensitive, as are most subsections of urls. Keep that in mind when you answer! The full url ishttp://example.com:80/toys?p=foo#blah https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8hiQyL6lcBs.mp4 Gethttp://www.example.com/foo request line: GET /foo HTTP/1.1 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8QjYUp3w5U0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rByBs2dt4dg.mp4 Most Common MethodGET POST https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/F9Fp-LtY7So.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pJNxDTa5uP0.mp4 Making Requestsrequest line: GET /foo?p=1 HTTP/1.1 Headers: Name: value Host: www.example.com User-Agent: chrome v.17 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5X7wcZuO5mU.mp4 User Agent Headerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oJ6MBvzgPcQ.mp4 Valid Headershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RdZDK0121kI.mp4Here is some information about headers that you might find useful.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/li-vh81yxKI.mp4 HTTP Responsesrequest: request-line: GET /foo HTTP/1.1 response: status-line: HTTP/1.1 200 Ok status code: 200 Ok 302 Found 404 Not found 500 Server Error https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/I4kUB17pTno.mp4 Response Headerstelnet www.udacity.com 80 GET / HTTP/1.0 Host: www.udacity.com https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ie0ONOZzNlw.mp4 NOTE:This example does not work properly at the moment, as http://www.udacity.com redirects to https://www.udacity.com. The former uses the protocol HTTP while the site now actually uses the protocol HTTPS. The extra “S” in HTTPS stands for “secure”, and you will hear more about it later. Opening A Terminal WindowMacMethod 1:Open Finder, then go to Applications -&gt; Utilities -&gt; Terminal Method 2:Open Spotlight, type ‘Terminal’, and the correct program should be the top result. LinuxHow you open the terminal varies depending on the specific distro and desktop environment. For Ubuntu, this page has a good introduction to the terminal and how to open it. WindowsOpen the Start Menu, then go to All Programs -&gt; Accessories -&gt; Command Prompt Running TelnetRunning telnet in Windows requires enabling it in “Windows Features” first. This page.aspx) has a good introduction to enabling the telnet client in Windows up to version 8. Windows 10 is similar but you can find “Windows Features” with the search box in the taskbar. Telnet should already be installed on Mac and Linux machines, so you should be able to copy the video exactly. Telnet Requesthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CIDBvFiccXE.mp4Since the video was recorded, the location header has changed to http://iana.org You’ll need to use telnet iana.org 80 and GET /domains/example HTTP/1.0 Host: iana.org instead of using domain example.com Since the video was recorded, the location header has changed to http://example.iana.org GET /domains/example HTTP/1.0 Host: iana.orghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ly95z-I1qmA.mp4 Web Applicationshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_E8qXBHI4cg.mp4 Dynamic Contenthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/543bDFHQKTs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4MlMQ5RJ4cc.mp4 Problem Set 1 - Creating Your First CiteGoogle App Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Sj_OmHLEdEA.mp4Check out the next lesson, “Problem Set 1 Help” for help getting Google App Engine running. In the current version of Google App Engine (1.9.1 - 2014-03-19), the command line argument to choose a port in dev_appserver.py is no longer “-p”. The argument is now “–port”.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PNmji8qNL0s.mp4tutorial in the forum tutorialhttps://console.cloud.google.com/home/dashboard?project=ssq-udacity-cs253 set pathhttp://docs.python-guide.org/en/latest/starting/install/win/ Welcome to Cloud Shell! Type &quot;help&quot; to get started. ssq6554@ssq-udacity-cs253:~$ TUTORIALDIR=~/src/ssq-udacity-cs253/ type: TUTORIALDIR=~/src/ssq-udacity-cs253/ python_gae_quickstart-2017-05-03-15-34 ssq6554@ssq-udacity-cs253:~$ git clone https://github.com/GoogleCloudPlatform/appengine-try-python-flask.git $TUTORIALDIR type: git clone https://github.com/GoogleCloudPlatform/appengine-try-python-flask.git $TUTORIALDIR Cloning into &apos;/home/ssq6554/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34&apos;... remote: Counting objects: 386, done. remote: Total 386 (delta 0), reused 0 (delta 0), pack-reused 386 Receiving objects: 100% (386/386), 776.84 KiB | 355.00 KiB/s, done. Resolving deltas: 100% (53/53), done. ssq6554@ssq-udacity-cs253:~$ cd $TUTORIALDIR type: cd $TUTORIALDIR ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ git checkout gcloud type: git checkout gcloud Branch gcloud set up to track remote branch gcloud from origin. Switched to a new branch &apos;gcloud&apos; ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ dev_appserver.py $PWD type: dev_appserver.py $PWD INFO 2017-05-03 07:45:50,733 devappserver2.py:692] Skipping SDK update check. WARNING 2017-05-03 07:45:50,973 simple_search_stub.py:1152] Could not read search indexes from /tmp/appengine.None.ssq6554/search_indexes INFO 2017-05-03 07:45:50,977 api_server.py:272] Starting API server at: http://0.0.0.0:48288 INFO 2017-05-03 07:45:50,980 dispatcher.py:205] Starting module &quot;default&quot; running at: http://0.0.0.0:8080 INFO 2017-05-03 07:45:50,981 admin_server.py:116] Starting admin server at: http://0.0.0.0:8000 INFO 2017-05-03 07:46:22,190 module.py:813] default: &quot;GET /?authuser=0 HTTP/1.0&quot; 200 12 ^CINFO 2017-05-03 07:46:39,143 shutdown.py:45] Shutting down. INFO 2017-05-03 07:46:39,144 api_server.py:863] Applying all pending transactions and saving the datastore INFO 2017-05-03 07:46:39,144 api_server.py:866] Saving search indexes ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ gcloud app deploy app.yaml --project ssq-udacity-cs253 type: gcloud app deploy app.yaml --project ssq-udacity-cs253 You are about to deploy the following services: - ssq-udacity-cs253/default/20170503t155355 (from [/home/ssq6554/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34/app.yaml]) Deploying to URL: [https://ssq-udacity-cs253.appspot.com] Do you want to continue (Y/n)? y type: y Beginning deployment of service [default]... Some files were skipped. Pass `--verbosity=info` to see which ones. You may also view the gcloud log file, found at remote: Counting objects: 386, done. [/tmp/tmp.QsMJk5uBEu/logs/2017.05.03/15.53.52.611133.log]. ╔════════════════════════════════════════════════════════════╗ ╠═ Uploading 77 files to Google Cloud Storage ═╣ ╚════════════════════════════════════════════════════════════╝ File upload done. Updating service [default]...done. Deployed service [default] to [https://ssq-udacity-cs253.appspot.com] You can stream logs from the command line by running: $ gcloud app logs tail -s default To view your application in the web browser run: $ gcloud app browse ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ succeed in deployinghttp://ssq-udacity-cs253.appspot.com/ (success)Try locallytype: gcloud init gcloud components list Your current Cloud SDK version is: 153.0.0 The latest available version is: 153.0.0 +------------------------------------------------------------------------------- ------------------------------+ | Components | +---------------+------------------------------------------------------+-------- ------------------+-----------+ | Status | Name | ID | Size | +---------------+------------------------------------------------------+-------- ------------------+-----------+ | Not Installed | App Engine Go Extensions | app-eng ine-go | 48.4 MiB | | Not Installed | Bigtable Command Line Tool | cbt | 3.9 MiB | | Not Installed | Cloud Datalab Command Line Tool | datalab | &lt; 1 MiB | | Not Installed | Cloud Datastore Emulator | cloud-d atastore-emulator | 15.4 MiB | | Not Installed | Cloud Datastore Emulator (Legacy) | gcd-emu lator | 38.1 MiB | | Not Installed | Cloud Pub/Sub Emulator | pubsub- emulator | 21.0 MiB | | Not Installed | Emulator Reverse Proxy | emulato r-reverse-proxy | 14.5 MiB | | Not Installed | Google Container Registry&apos;s Docker credential helper | docker- credential-gcr | 3.3 MiB | | Not Installed | gcloud Alpha Commands | alpha | &lt; 1 MiB | | Not Installed | gcloud Beta Commands | beta | &lt; 1 MiB | | Not Installed | gcloud app Java Extensions | app-eng ine-java | 128.6 MiB | | Not Installed | gcloud app PHP Extensions (Windows) | app-eng ine-php-windows | 19.1 MiB | | Not Installed | gcloud app Python Extensions | app-eng ine-python | 6.1 MiB | | Not Installed | kubectl | kubectl | 14.9 MiB | | Installed | BigQuery Command Line Tool | bq | &lt; 1 MiB | | Installed | Cloud SDK Core Libraries | core | 5.9 MiB | | Installed | Cloud Storage Command Line Tool | gsutil | 2.9 MiB | | Installed | Default set of gcloud commands | gcloud | | +---------------+------------------------------------------------------+-------- ------------------+-----------+ To install or remove components at your current SDK version [153.0.0], run: $ gcloud components install COMPONENT_ID $ gcloud components remove COMPONENT_ID To update your SDK installation to the latest version [153.0.0], run: $ gcloud components update type:gcloud components install app-engine-python Restarting command: $ gcloud components install app-engine-python There appears a new cmd window Your current Cloud SDK version is: 153.0.0 Installing components from version: 153.0.0 +--------------------------------------------------+ | These components will be installed. | +------------------------------+---------+---------+ | Name | Version | Size | +------------------------------+---------+---------+ | gcloud app Python Extensions | 1.9.52 | 6.1 MiB | +------------------------------+---------+---------+ For the latest full release notes, please visit: https://cloud.google.com/sdk/release_notes Do you want to continue (Y/n)? type: y #============================================================# #= Creating update staging area =# #============================================================# #= Installing: gcloud app Python Extensions =# #============================================================# #= Creating backup and activating new installation =# #============================================================# Performing post processing steps...done. Update done! 请按任意键继续. . . open git bashtype: git clone https://github.com/GoogleCloudPlatform/python-docs-samplestype: cd G:/\Udacity/\cs253 Cloning into &apos;python-docs-samples&apos;... remote: Counting objects: 10835, done. remote: Compressing objects: 100% (161/161), done. remote: Total 10835 (delta 58), reused 1 (delta 1), pack-reused 10649 Receiving objects: 100% (10835/10835), 4.60 MiB | 190.00 KiB/s, done. Resolving deltas: 100% (5742/5742), done. type: cd python-docs-samples/\appengine/\standard/\hello_world type: dev_appserver.py .look in http://localhost:8080/ (failed)or open cmd Go to cloud.google.com Login and create a project Copy the project id and create a project in App Engine on your machine with same id Install Google Cloud SDK Open Google Cloud SDK shell Type &apos;gcloud init &apos; It will show ur gmail id OR ask u to log in Select the project you created in step 2 Type &apos;gcloud beta app create&apos; It will ask for region - choose the closest region Hit Deploy button in App Engine OR Type &apos;gcloud app deploy &apos; in the shell type: gcloud beta app create You do not currently have this command group installed. Using it requires the installation of components: [beta] Restarting command: $ gcloud components install beta Installing component in a new window. Please re-run this command when installation is complete. $ C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\..\lib\gc loud.py beta app create there opens a new window Your current Cloud SDK version is: 153.0.0 Installing components from version: 153.0.0 +---------------------------------------------+ | These components will be installed. | +----------------------+------------+---------+ | Name | Version | Size | +----------------------+------------+---------+ | gcloud Beta Commands | 2017.03.24 | &lt; 1 MiB | +----------------------+------------+---------+ For the latest full release notes, please visit: https://cloud.google.com/sdk/release_notes Do you want to continue (Y/n)? y #============================================================# #= Creating update staging area =# #============================================================# #= Installing: gcloud Beta Commands =# #============================================================# #= Creating backup and activating new installation =# #============================================================# Performing post processing steps...done. Update done! 请按任意键继续. . . type: gcloud beta app create ERROR: (gcloud.beta.app.create) The project [ssq-udacity-helloworld] already con tains an App Engine application in region [us-central]. You can deploy your app lication using `gcloud app deploy`. type: cd G:\Udacity\cs253\helloworld g:type: gcloud app deploy ERROR: gcloud crashed (UnicodeEncodeError): &apos;ascii&apos; codec can&apos;t encode character u&apos;\xad&apos; in position 22: ordinal not in range(128) If you would like to report this issue, please run the following command: gcloud feedback To check gcloud for common problems, please run the following command: gcloud info --run-diagnostics failedtype: C:\Users\SSQ&gt;python &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\dev_appserver.py&quot; G:\Udacity\cs253\python-docs-samples\appengine\standard\hello_world\app.yaml success (Fail)try locally againopen Google Cloud SDK Shelltype: python &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\dev_appserver.py&quot; G:\Udacity\cs253\helloworld\app.yaml it appears: Updates are available for some Cloud SDK components. To install them, please run: $ gcloud components update Traceback (most recent call last): File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 103, in &lt;module&gt; _run_file(__file__, globals()) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 97, in _run_file execfile(_PATHS.script_file(script_name), globals_) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 899, in &lt;module&gt; main() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 895, in main dev_server.stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 775, in stop metrics.GetMetricsLogger().Stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\metrics.py&quot;, line 119, in St op total_run_time = int((Now() - self._start_time).total_seconds()) TypeError: unsupported operand type(s) for -: &apos;datetime.datetime&apos; and &apos;NoneType&apos; type: gcloud components updatethere’s a new window: Your current Cloud SDK version is: 153.0.0 You will be upgraded to version: 154.0.1 +-------------------------------------------------+ | These components will be updated. | +--------------------------+------------+---------+ | Name | Version | Size | +--------------------------+------------+---------+ | Cloud SDK Core Libraries | 2017.05.04 | 6.0 MiB | | gcloud cli dependencies | 2017.05.01 | 1.6 MiB | +--------------------------+------------+---------+ The following release notes are new in this upgrade. Please read carefully for information about new features, breaking changes, and bugs fixed. The latest full release notes can be viewed at: https://cloud.google.com/sdk/release_notes 154.0.1 (2017-05-04) Cloud SDK o Fixed issue in for gcloud init command. See https://issuetracker.google.com/37968909. 154.0.0 (2017-05-03) Cloud SDK o Added support for project creation during the gcloud init flow. Google Cloud Logging o BUG FIX: gcloud beta logging sinks update would remove any start time or end time from a sink o gcloud beta logging sinks describe now reports values of start_time, end_time and include_children o The --include-children flag is now available for gcloud beta logging sinks create to create sinks that apply to an organization or folder and also to all of its child projects and folders. Google Compute Engine o Workaround problems with alpha and beta versions of compute ssh command fail an attempt to use clouduseraccounts APIs. Google Container Engine o Promote --cluster-version from beta to GA in gcloud container clusters create. o &apos;--no-source&apos; flag for &apos;gcloud container builds submit&apos; allows builds with no source input. Google Cloud ML Engine o Added --config parameter to gcloud ml-engine versions create; this parameter allows specifying scaling settings for a version. Google Cloud Speech o The gcloud ml speech commands to recognize spoken words in recorded speech using the Cloud Speech API are now available in beta. Please run gcloud beta ml speech --help or visit https://cloud.google.com/speech/docs/ to learn more. Google App Engine o gcloud beta app deploy now attempts to use the Service Management API to enable the Appengine Flexible Environment API for Flexible deployments, if needed. Before deploying a Flexible app, please ensure that the Flexible Environment API is enabled on the app&apos;s project. o The new Node.js Runtime Builder pipeline will now be used to deploy apps when using gcloud beta app deploy. Google Cloud SQL o Promote gcloud sql operations to GA. The beta surface still remains and is identical. Google Cloud Source Repositories o Add a source repos describe command to describe a repository to the beta track. Do you want to continue (Y/n)? type: y #============================================================# #= Creating update staging area =# #============================================================# #= Uninstalling: Cloud SDK Core Libraries =# #============================================================# #= Uninstalling: gcloud cli dependencies =# #============================================================# #= Installing: Cloud SDK Core Libraries =# #============================================================# #= Installing: gcloud cli dependencies =# #============================================================# #= Creating backup and activating new installation =# #============================================================# Performing post processing steps...done. Update done! To revert your SDK to the previously installed version, you may run: $ gcloud components update --version 153.0.0 请按任意键继续. . . type: `` Traceback (most recent call last): File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 103, in &lt;module&gt; _run_file(__file__, globals()) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 97, in _run_file execfile(_PATHS.script_file(script_name), globals_) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 899, in &lt;module&gt; main() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 895, in main dev_server.stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 775, in stop metrics.GetMetricsLogger().Stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\metrics.py&quot;, line 119, in St op total_run_time = int((Now() - self._start_time).total_seconds()) TypeError: unsupported operand type(s) for -: &apos;datetime.datetime&apos; and &apos;NoneType&apos; Deploy your app to the cloudcd to hello_world dir and type gcloud app deploy G:\Udacity\cs253\python-docs-samples\appengine\standard\hello_world&gt;gcloud app deploy You are about to deploy the following services: - ssq-udacity-cs253/default/20170503t210424 (from [G:\Udacity\cs253\python-docs -samples\appengine\standard\hello_world\app.yaml]) Deploying to URL: [https://ssq-udacity-cs253.appspot.com] Do you want to continue (Y/n)? y type y Beginning deployment of service [default]... Some files were skipped. Pass `--verbosity=info` to see which ones. You may also view the gcloud log file, found at [C:\Users\SSQ\AppData\Roaming\gcloud\logs\2017.05.03\21.04.21.920000.log]. #============================================================# #= Uploading 0 files to Google Cloud Storage =# #============================================================# File upload done. Updating service [default]...done. Deployed service [default] to [https://ssq-udacity-cs253.appspot.com] You can stream logs from the command line by running: $ gcloud app logs tail -s default To view your application in the web browser run: $ gcloud app browse type gcloud app browse G:\Udacity\cs253\python-docs-samples\appengine\standard\hello_world&gt;gcloud app browse Opening [https://ssq-udacity-cs253.appspot.com] in a new tab in your default bro wser. Problem Set 1 HelpIntrohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IFYmvf_RNT0.mp4 Install Google App Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/euox_iFW2eM.mp4 Install Google App EngineMake sure you are downloading Python 2 rather than Python 3, as this is what Google App Engine uses! Changes to Google App EngineThe Google App Engine system (now part of Google Cloud) has changed a lot since this course was created. You will need to create an app with a new, unique name using the Developer Console before you can deploy it. For more information, take a look at these guides written by Udacity mentor Steven Wooding: (Windows), (Mac and Linux) Installing Python on LinuxIf you’re on a Mac or Linux machine, open a terminal and type ‘python -v’ at the prompt. If it returns some information about Python, then you already have Python installed! If not, then Mac users can follow the instructions in the video, and Linux users can use their package managers instead. For Ubuntu or Debian, type ‘sudo apt-get install python’ into your terminal, then your password. For Fedora, type ‘sudo yum install python’ into your terminal, then your password. For other Linux distros, their should be documentation available for your specific package manager. Running Google App Engine on Mac OSUnfortunately Google no longer supports the GoogleAppengineLauncher program that Steve demonstrates in this video. You follow use the instructions below for Linux, or you can download the deprecated GoogleAppengineLauncher installer from the “Supporting Materials” list below. Warning: Use the GoogleAppengineLauncher program at your own peril! As Google no longer supports the tool it may stop working in the future. The gcloud command-line program is the currently supported tool. Running Google App Engine on LinuxTo run the Development Web Server locally, run: dev_appserver.py myapp Where myapp is the name you want your app to have. To upload your code to Google App Engine, run: appcfg.py update helloworld/ Where helloworld/ is the directory you’re running your web app from. Further help is available here and hereHave questions? Head to the forums for discussion with the Udacity Community. Supporting MaterialsWindows Installation Guide for App Engine.pdfGoogleAppEngineLauncher-1.9.40-OSXhttps://discussions.udacity.com/t/problem-set-1-seems-to-be-nothing-like-help-video-shows/215184/3 Office Hours 1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QKTIOnISsCw.mp4 Forms and Inputhttps://www.udacity.com/wiki/cs253/unit-2 Introductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3qgaev6_ZkY.mp4 Forms https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OnptThLQb_k.mp4 User Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qNdg3ZKoKnA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PmF0C6dWzFk.mp4 Naming Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IOkzztrmxbg.mp4how to implement play.html:https://discussions.udacity.com/t/unit2-this-is-fustrating/68505/2 Entering Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JDwulMznybc.mp4Try the HTML code &lt;form&gt; &lt;input name=&quot;q&quot;&gt; &lt;/form&gt; Note that if you chose the text disappears in addition to the the URL changes with my text, that you are also correct. However, we’re focused on the latter, and arguably more interesting, of these two events. But, you may always re-submit your answer to any quiz.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vUHabH7bsTc.mp4 Submitting Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/easPpLBGf2w.mp4 &lt;form&gt; &lt;input name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GMNVeltn0ms.mp4 The Action Attributehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UR-I9lkV3No.mp4 &lt;form action=&quot;http://www.google.com/search&quot;&gt; &lt;input name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7YawG07y9lg.mp4 URL Encodinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Z3udiqgW1VA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lSjOr6_LaMs.mp4 Hello Webapp Worldhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t9qqgCvHxgs.mp4 Content Typehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/a0k61hlrO9M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TGHZjKuxTB0.mp4 More Handlershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9624RoIt2pk.mp4 GET /testform?q=hello+world%21 HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Language: zh-CN,zh;q=0.8 Content-Type: ; charset=&quot;utf-8&quot; Cookie: _xsrf=2|69d62867|9814fd50905531e52e958841989ec6fb|1491708993; username-localhost-8890=&quot;2|1:0|10:1491841902|23:username-localhost-8890|44:NzI4YzlkMzc5MzIyNGQ5YTgxMWMzMDAyZDZiY2M4NTY=|69e8bf9e2ec4a47c684886256bda319f88bbbbe1cc4bf5c9193aa52c9449bab1&quot;; username-localhost-8891=&quot;2|1:0|10:1491842087|23:username-localhost-8891|44:YThkM2ViMzc0NmJjNDVhNGE4YWI4OTgwZGE5M2RlYzk=|cd0850025b097f89ae64b4bda7911166954a2384ecd7eb8a17991270ad4111a4&quot;; username-localhost-8889=&quot;2|1:0|10:1492581579|23:username-localhost-8889|44:NGM1MDZlMTUzNWE1NGUzNmFkYTNhMzRiMjdmOGYyNWY=|a15a04a3925f6c3954789ffbdb5b01f5bda3a0c4e9e47420ac1564c5302aaf9d&quot;; username-localhost-8888=&quot;2|1:0|10:1493640422|23:username-localhost-8888|44:NjYzZDYzYThhNmQwNDZiODk2ZTVlZDU0YjkxOTI5MWM=|fb58980d83c777e3d64786ba156f12ab1834a9c404151d3d5dacb943ca4e0616&quot;; Hm_lvt_3c8ad2ecdd2387b44044b1d7cd3536a9=1492997775,1493349600,1493631556,1493712288; _ga=GA1.1.1682530332.1487774100 Host: localhost:8080 Referer: http://localhost:8080/ Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36 X-Appengine-Country: ZZ https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6BpCLwYGwBA.mp4 The Method Attributehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eDotptZ1Nyo.mp4 405 Method Not Allowed The method POST is not allowed for this resource. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BOnkFj9uMkg.mp4 http://localhost:8080/testform hello world! Methods and Parametershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E_OiAN5VKOM.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot; action=&quot;/testform&quot;&gt; &lt;input name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.write(form) class TestHandler(webapp2.RequestHandler): def post(self): #q=self.request.get(&quot;q&quot;) #self.response.out.write(q) self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.out.write(self.request) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage), (&apos;/testform&apos;,TestHandler) ], debug=True) POST /testform HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Language: zh-CN,zh;q=0.8 Cache-Control: max-age=0 Content-Length: 16 Content-Type: application/x-www-form-urlencoded; charset=&quot;utf-8&quot; Content_Length: 16 Content_Type: application/x-www-form-urlencoded Cookie: _xsrf=2|69d62867|9814fd50905531e52e958841989ec6fb|1491708993; username-localhost-8890=&quot;2|1:0|10:1491841902|23:username-localhost-8890|44:NzI4YzlkMzc5MzIyNGQ5YTgxMWMzMDAyZDZiY2M4NTY=|69e8bf9e2ec4a47c684886256bda319f88bbbbe1cc4bf5c9193aa52c9449bab1&quot;; username-localhost-8891=&quot;2|1:0|10:1491842087|23:username-localhost-8891|44:YThkM2ViMzc0NmJjNDVhNGE4YWI4OTgwZGE5M2RlYzk=|cd0850025b097f89ae64b4bda7911166954a2384ecd7eb8a17991270ad4111a4&quot;; username-localhost-8889=&quot;2|1:0|10:1492581579|23:username-localhost-8889|44:NGM1MDZlMTUzNWE1NGUzNmFkYTNhMzRiMjdmOGYyNWY=|a15a04a3925f6c3954789ffbdb5b01f5bda3a0c4e9e47420ac1564c5302aaf9d&quot;; username-localhost-8888=&quot;2|1:0|10:1493640422|23:username-localhost-8888|44:NjYzZDYzYThhNmQwNDZiODk2ZTVlZDU0YjkxOTI5MWM=|fb58980d83c777e3d64786ba156f12ab1834a9c404151d3d5dacb943ca4e0616&quot;; Hm_lvt_3c8ad2ecdd2387b44044b1d7cd3536a9=1492997775,1493349600,1493631556,1493712288; _ga=GA1.1.1682530332.1487774100 Host: localhost:8080 Origin: http://localhost:8080 Referer: http://localhost:8080/ Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36 X-Appengine-Country: ZZ q=hello+world%21 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9Hj83VRhOQY.mp4 Differences Between Get and Posthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UObINRj2EGY.mp4 Problems with Gethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cIliEo0zOwg.mp4 When to Use Get and Posthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CprytP12okM.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KYPi9loZE-M.mp4 Passwordshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xS71dGcER_s.mp4 &lt;form&gt; &lt;input type=&quot;password&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=hellohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oWvmNCuI47k.mp4 Checkboxeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bFK7ZIwnVik.mp4 &lt;form&gt; &lt;input type=&quot;checkbox&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html? Multiple Checkboxeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Td_6JT-1MQ0.mp4 &lt;form&gt; &lt;input type=&quot;checkbox&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;checkbox&quot; name=&quot;r&quot;&gt; &lt;input type=&quot;checkbox&quot; name=&quot;s&quot;&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=on&amp;r=onhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HEACkANsj9o.mp4 Radio Buttonshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/di_FzsEzyQk.mp4 &lt;form&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;r&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;s&quot;&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=on&amp;r=on Grouping Radio Buttonshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QKxqWDx2bcs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5GqK_VmlrAY.mp4 Radio Button Valueshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xPTL73OT5IY.mp4 &lt;form&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hTDenQDEvqU.mp4 Label Elementshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XnBMnDGQqtw.mp4 &lt;form&gt; &lt;label&gt; One &lt;input type=&quot;radio&quot; name=&quot;q&quot; value=&quot;one&quot;&gt; &lt;/label&gt; &lt;label&gt; Two &lt;input type=&quot;radio&quot; name=&quot;q&quot; value=&quot;two&quot;&gt; &lt;/label&gt; &lt;label&gt; Three &lt;input type=&quot;radio&quot; name=&quot;q&quot; value=&quot;three&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; Dropdownshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gVtTvYfc5NA.mp4 &lt;form&gt; &lt;select name=&quot;q&quot;&gt; &lt;option&gt;One&lt;/option&gt; &lt;option&gt;Two&lt;/option&gt; &lt;option&gt;Three&lt;/option&gt; &lt;/select&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=Twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/joUhCZw_zG0.mp4 Dropdowns and Valueshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/q2jmRGrAixc.mp4 The Number Onehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mRTBZNTmimA.mp4 &lt;form&gt; &lt;select name=&quot;q&quot;&gt; &lt;option value=&quot;1&quot;&gt;the number One&lt;/option&gt; &lt;option&gt;Two&lt;/option&gt; &lt;option&gt;Three&lt;/option&gt; &lt;/select&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zCBUF8oDJPE.mp4 Validationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uC8ny0rTzIE.mp4 What Is Your Birthday?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Gx16pZ7crqU.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.write(form) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) 405 Method Not Allowed The method POST is not allowed for this resource. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/psynonn163k.mp4 Handling Postshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NyIz4ht6NGI.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.out.write(form) def post(self): self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) Handling Bad Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fcrTA3_iHLY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/u7vPsxvUNMA.mp4 Valid Monthhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mOAcUVFqIO0.mp4 Hint: You may want to look into the string.capitalize() function In this answer video, Steve is using list comprehensions to create the dictionary of month abbreviations. If we were to translate this code: month_abbvs = dict((m[:3].lower(),m) for m in months) it would be: month_abbvs = {} for m in months: month_abbvs[m[:3].lower()] = m You can read more about list comprehensions here # ----------- # User Instructions # # Modify the valid_month() function to verify # whether the data a user enters is a valid # month. If the passed in parameter &apos;month&apos; # is not a valid month, return None. # If &apos;month&apos; is a valid month, then return # the name of the month with the first letter # capitalized. # months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] def valid_month(month): # print valid_month(&quot;january&quot;) # =&gt; &quot;January&quot; # print valid_month(&quot;January&quot;) # =&gt; &quot;January&quot; # print valid_month(&quot;foo&quot;) # =&gt; None # print valid_month(&quot;&quot;) # =&gt; None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/a2sLiEgBl9k.mp4 month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) Valid Dayhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jj44KNeNK3A.mp4 # ----------- # User Instructions # # Modify the valid_day() function to verify # whether the string a user enters is a valid # day. The valid_day() function takes as # input a String, and returns either a valid # Int or None. If the passed in String is # not a valid day, return None. # If it is a valid day, then return # the day as an Int, not a String. Don&apos;t # worry about months of different length. # Assume a day is valid if it is a number # between 1 and 31. # Be careful, the input can be any string # at all, you don&apos;t have any guarantees # that the user will input a sensible # day. # # Hint: The string function isdigit() might be helpful. def valid_day(day): # print valid_day(&apos;0&apos;) # =&gt; None # print valid_day(&apos;1&apos;) # =&gt; 1 # print valid_day(&apos;15&apos;) # =&gt; 15 # print valid_day(&apos;500&apos;) # =&gt; None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6CdFM4grsXc.mp4 def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day Valid Yearhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/g1f2nOw7a9M.mp4 # ----------- # User Instructions # # Modify the valid_year() function to verify # whether the string a user enters is a valid # year. If the passed in parameter &apos;year&apos; # is not a valid year, return None. # If &apos;year&apos; is a valid year, then return # the year as a number. Assume a year # is valid if it is a number between 1900 and # 2020. # def valid_year(year): #print valid_year(&apos;0&apos;) #=&gt; None #print valid_year(&apos;-11&apos;) #=&gt; None #print valid_year(&apos;1950&apos;) #=&gt; 1950 #print valid_year(&apos;2000&apos;) #=&gt; 2000 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YHEqWp6R0IA.mp4 def valid_year(year): if year and year.isdigit(): year=int(year) if year&gt;1900 and year&lt;=2020: return year Checking Validationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WbBDizcryiA.mp4 Responding Based On Verificationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3EgN3NxgH-Y.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.out.write(form) def post(self): user_month=valid_month(self.request.get(&apos;month&apos;)) user_day=valid_day(self.request.get(&apos;day&apos;)) user_year=valid_year(self.request.get(&apos;year&apos;)) if not (user_month and user_day and user_year): #self.response.out.write(user_month) self.response.out.write(form) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) String Substitutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/DYncK4Tuthc.mp4 # User Instructions # # Write a function &apos;sub1&apos; that, given a # string, embeds that string in # the string: # &quot;I think X is a perfectly normal thing to do in public.&quot; # where X is replaced by the given # string. # The function should return the new string. given_string = &quot;I think %s is a perfectly normal thing to do in public.&quot; def sub1(s): #print sub1(&quot;running&quot;) # =&gt; &quot;I think running is a perfectly normal thing to do in public.&quot; #print sub1(&quot;sleeping&quot;) # =&gt; &quot;I think sleeping is a perfectly normal thing to do in public.&quot; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0h0RYI_MGtU.mp4 def sub1(s): return given_string % s Substituting Multiple Stringshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ahNxN_Ly5sE.mp4 # User Instructions # # Write a function &apos;sub2&apos; that, given two # strings, embeds those strings in the string: # &quot;I think X and Y are perfectly normal things to do in public.&quot; # where X and Y are replaced by the given # strings. # The function should return the new string. given_string2 = &quot;I think %s and %s are perfectly normal things to do in public.&quot; def sub2(s1, s2): # print sub2(&quot;running&quot;, &quot;sleeping&quot;) # =&gt; &quot;I think running and sleeping are perfectly normal things to do in public.&quot; # print sub2(&quot;sleeping&quot;, &quot;running&quot;) # =&gt; &quot;I think sleeping and running are perfectly normal things to do in public.&quot; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K7mx3o3-X3Y.mp4 def sub2(s1, s2): return given_string2 %(s1,s2) Advanced String Substitutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4OXzeuhwM-E.mp4 # User Instructions # # Write a function &apos;sub_m&apos; that takes a # name and a nickname, and returns a # string of the following format: # &quot;I&apos;m NICKNAME. My real name is NAME, but my friends call me NICKNAME.&quot; # given_string2 = &quot;I&apos;m %(nickname)s. My real name is %(name)s, but my friends call me %(nickname)s.&quot; def sub_m(name, nickname): #print sub_m(&quot;Mike&quot;, &quot;Goose&quot;) # =&gt; &quot;I&apos;m Goose. My real name is Mike, but my friends call me Goose.&quot; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3iHZY-xJJUY.mp4 def sub_m(name, nickname): return given_string2 % {&quot;nickname&quot;:nickname,&quot;name&quot;:name} Substituting into Our Formhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/trxe2U-OplI.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error}) def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; #self.response.out.write(form) self.write_form() def post(self): user_month=valid_month(self.request.get(&apos;month&apos;)) user_day=valid_day(self.request.get(&apos;day&apos;)) user_year=valid_year(self.request.get(&apos;year&apos;)) if not (user_month and user_day and user_year): #self.response.out.write(user_month) #self.response.out.write(form) self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) Preserving User Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BpNhr-xc0Nw.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lwHWqkMijss.mp4 Problems with HTML Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/iWX-XylQsH8.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot; value=&quot;%(month)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot; value=&quot;%(day)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot; value=&quot;%(year)s&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;, month=&quot;&quot;, day=&quot;&quot;, year=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error, &quot;month&quot;: month, &quot;day&quot;: day, &quot;year&quot;: year}) def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; #self.response.out.write(form) self.write_form() def post(self): user_month = self.request.get(&apos;month&apos;) user_day = self.request.get(&apos;day&apos;) user_year = self.request.get(&apos;year&apos;) month = valid_month(user_month) day = valid_day(user_day) year = valid_year(user_year) if not (month and day and year): #self.response.out.write(user_month) #self.response.out.write(form) self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;, user_month, user_day, user_year) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OpOOJcP-fY0.mp4 Handling HTML Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K37Ldm4GSPo.mp4 HTML Escapinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/teNvqTRP5EY.mp4 Using HTML Escapinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Hp-z-SVf3fw.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2T-ubUA5xLk.mp4 Implementing HTML Escapinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NmCRWl-OZ60.mp4 # User Instructions # # Implement the function escape_html(s), which replaces # all instances of: # &gt; with &amp;gt; # &lt; with &amp;lt; # &quot; with &amp;quot; # &amp; with &amp;amp; # and returns the escaped string # Note that your browser will probably automatically # render your escaped text as the corresponding symbols, # but the grading script will still correctly evaluate it. # def escape_html(s): # print escape_html(&apos;&gt;&apos;) # print escape_html(&apos;&lt;&apos;) # print escape_html(&apos;&quot;&apos;) # print escape_html(&quot;&amp;&quot;) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4NKBjlnV530.mp4 def escape_html(s): for (i,o) in ((&quot;&amp;&quot;,&quot;&amp;amp;&quot;),(&quot;&gt;&quot;,&quot;&amp;gl;&quot;),(&quot;&lt;&quot;,&quot;&amp;lt;&quot;),(&apos;&quot;&apos;,&quot;&amp;quot;&quot;)): s=s.replace(i,o) return s or import cgi def escape_html(s): return cgi.escape(s, quote = True) Problems Reinventing the Wheelhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4YEcl5g3ADA.mp4 Current Limitationshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_lRKZMPh5R8.mp4 import webapp2 import cgi def escape_html(s): return cgi.escape(s, quote = True) form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot; value=&quot;%(month)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot; value=&quot;%(day)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot; value=&quot;%(year)s&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;, month=&quot;&quot;, day=&quot;&quot;, year=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error, &quot;month&quot;: escape_html(month), &quot;day&quot;: escape_html(day), &quot;year&quot;: escape_html(year)}) def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; #self.response.out.write(form) self.write_form() def post(self): user_month = self.request.get(&apos;month&apos;) user_day = self.request.get(&apos;day&apos;) user_year = self.request.get(&apos;year&apos;) month = valid_month(user_month) day = valid_day(user_day) year = valid_year(user_year) if not (month and day and year): self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;, user_month, user_day, user_year) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) Redirectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gjPdQ-ywbPM.mp4 Redirection Advantageshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/k84dleLQ-WI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eygchvjmP60.mp4 Implementing Redirectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/46HPScqA0CA.mp4 import webapp2 import cgi def escape_html(s): return cgi.escape(s, quote = True) form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot; value=&quot;%(month)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot; value=&quot;%(day)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot; value=&quot;%(year)s&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;, month=&quot;&quot;, day=&quot;&quot;, year=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error, &quot;month&quot;: escape_html(month), &quot;day&quot;: escape_html(day), &quot;year&quot;: escape_html(year)}) def get(self): self.write_form() def post(self): user_month = self.request.get(&apos;month&apos;) user_day = self.request.get(&apos;day&apos;) user_year = self.request.get(&apos;year&apos;) month = valid_month(user_month) day = valid_day(user_day) year = valid_year(user_year) if not (month and day and year): self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;, user_month, user_day, user_year) else: self.redirect(&quot;/thanks&quot;) class ThanksHandler(webapp2.RequestHandler): def get(self): self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage), (&apos;/thanks&apos;, ThanksHandler) ], debug=True) Problem Set 2Rot13https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uMGNwoFHfB4.mp4In order to be graded correctly for this homework, there are a few things to keep in mind. We’ll be grading your web app by POSTing to your form and retrieving the text that has been encoded with ROT13. There are a few main issues you need to keep in mind in order for this to work: The textarea form element where the user inputs the text to encode must be named ‘text’. In other words, you must have ‘textarea name=”text”‘ for us to post to. The form method must be POST, not GET. You must enter the full url into the supplied textbox above, including the path. For example, our example app is running at http://udacity-cs253.appspot.com/unit2/rot13, but if we instead only entered http://udacity-cs253.appspot.com/ then the grading script would not work. Don’t forget to escape your output! User Signuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SkSJp6IIZfw.mp4In order to be graded correctly for this homework, there are a few things to keep in mind. We’ll be grading your web app by posting to your form and then checking the HTTP Response we receive. There are a few main issues you need to keep in mind in order for this to work: The form elements where the user inputs their username, password, password again, and email address must be named “username”, “password”, “verify”, and “email”, respectively. The form method must be POST, not GET. Upon invalid user input, your web app should re-render the form for the user. Upon valid user input, your web app should redirect to a welcome page for the user. This page must include both “Welcome” and the user’s username. You must enter the full url into the supplied textbox above, including the path. For example, our example app is running at http://udacity-cs253.appspot.com/unit2/signup, but if we instead only entered http://udacity-cs253.appspot.com/ then the grading script would not work. Regular ExpressionsA regular expression is a handy tool for matching text to a pattern. The regular expressions that we’re using to validate you input are as follows: Username: “^[a-zA-Z0-9_-]{3,20}$” Password: “^.{3,20}$” Email: “^[\S]+@[\S]+.[\S]+$” Example code for validating a username is as follows: import re USER_RE = re.compile(r&quot;^[a-zA-Z0-9_-]{3,20}$&quot;) def valid_username(username): return USER_RE.match(username) More information on using regular expressions in Python can be found here NOTE: When you go off to make real applications that require form validation, remember that using regex to check an email address is not quite as simple as we make it seem here. See this Stack Overflow question for more on email validation. Rot13 Solutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NYzMnbSjOWA.mp4The solution files are here. In order to open them, Windows users will need an archive utility that opens tgz files, like 7zip. Linux and Mac users can use the built-in functionality of their archive managers. Barring that, Linux and Mac users can directly use the command: tar xf hw2.tgz User Signup SolutionThe solution files are here. In order to open them, Windows users will need an archive utility that opens tgz files, like 7zip. Linux and Mac users can use the built-in functionality of their archive managers. Barring that, Linux and Mac users can directly use the command: tar xf hw2.tgz Office Hours 2Question 1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0uBFVZRVnrQ.mp4Jinja2 Documentation Question 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5-UWdqZQOIw.mp4 Question 3https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YY7joMbML8k.mp4 Question 4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9vTJkEjXw6Q.mp4We now have a course for beginner Object Oriented Programming in Python: https://www.udacity.com/course/ud036 Question 5https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rLPeR3A3CnM.mp4 Lesson 8: Lesson 2a-TemplatesWriting a Basic Formhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ScXJ5au8q_w.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Bnh2EnFQdkw.mp4 Hidden Inputshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dfFiYpxh4js.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EHACuozhDQA.mp4http://localhost:8080/?food=steak&amp;food=eggs Shopping List Take 1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h_EkzhX0D0I.mp412&quot;hello %s&quot; % &quot;SSQ&quot;&apos;hello SSQ&apos; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/87ieO3hrkDI.mp4 Introducing Templateshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nT0wOJ3pQMw.mp4jinja Template Refactorhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9HoEid5G4I4.mp4 Variable Substitutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/D6q5TePkyo0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ppsNOLB7p4M.mp4 ErrataDon’t include quotation marks in your solution. For additional clarification on the different render methods, see this discussion thread.&lt;h2&gt;Hello, &lt;/h2&gt; Statement Syntaxhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/13P8ncVwL_o.mp412345&#123;% if name==&quot;steve&quot; %&#125; hello, steve!&#123;% else %&#125; who are you?&#123;% endif %&#125; Testing Statement Syntaxhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zI4rmjjICmU.mp4 Templates and Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4VYTtyHnKEs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/d-PAPeH0bd0.mp412345&#123;% if n==1 %&#125;n=1&#123;% else %&#125;n!=1&#123;% endif %&#125; 12345def get(self): n=self.request.get(&quot;n&quot;) if n: n=int(n) self.render(&quot;shopping_list.html&quot;, n=n) Templates and Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4VYTtyHnKEs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/d-PAPeH0bd0.mp412345&#123;% if n==1 %&#125;n=1&#123;% else %&#125;n!=1&#123;% endif %&#125; 12345def get(self): n=self.request.get(&quot;n&quot;) if n: n=int(n) self.render(&quot;shopping_list.html&quot;, n=n) For Loop Syntaxhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KzZwbKZweKA.mp4123&#123;% for statement %&#125; body&#123;% endfor %&#125; 12345&lt;ol&gt; &#123;% for x in range(1,n+1) %&#125; &lt;li&gt;&#123;&#123;x ** 2&#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ol&gt; FizzBuzzhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nxoQoSYJryc.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KhrAMbp2MRA.mp4123n=self.request.get(&quot;n&quot;)if n: n=int(n) 123456789101112131415&lt;ol&gt; &#123;% for i in range(1,n+1) %&#125; &lt;li&gt; &#123;% if i % 3 ==0 and i % 5 ==0 %&#125; fizzbuzz &#123;% elif i % 3 ==0 %&#125; fizz &#123;% elif i % 5 ==0 %&#125; buzz &#123;% else %&#125; &#123;&#123;i&#125;&#125; &#123;% endif %&#125; &lt;/li&gt; &#123;% endfor %&#125;&lt;/ol&gt; Shopping List Take 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BkqW2HKdL0M.mp4get rid of12345678910111213141516output = form_htmloutput_hidden=&quot;&quot;items=self.request.get_all(&quot;food&quot;)if items: output_items=&quot;&quot; for item in items: output_hidden+=hidden_html % item output_items+=item_html % item output_shopping = shopping_list_html % output_items output += output_shoppingoutput=output % output_hiddenself.write(output) and1234567891011121314hidden_html=&quot;&quot;&quot;&lt;input type=&quot;hidden&quot; name=&quot;food&quot; value=&quot;%s&quot;&gt;&quot;&quot;&quot;item_html=&quot;&lt;li&gt;%s&lt;/li&gt;&quot;shopping_list_html=&quot;&quot;&quot;&lt;br&gt;&lt;br&gt;&lt;h2&gt;Shopping List&lt;/h2&gt;&lt;ul&gt;%s&lt;/ul&gt;&quot;&quot;&quot; change following code in your shopping_list.html12345678910111213141516171819202122&lt;form&gt; &lt;h2&gt;Add a food&lt;/h2&gt; &lt;input type=&quot;text&quot; name=&quot;food&quot;&gt; &#123;% if items %&#125; &#123;% for item in items %&#125; &lt;input type=&quot;hidden&quot; name=&quot;food&quot; value=&quot;&#123;&#123;item&#125;&#125;&quot;&gt; &#123;% endfor %&#125; &#123;% endif %&#125; &lt;button&gt;Add&lt;/button&gt; &#123;% if items %&#125; &lt;br&gt; &lt;br&gt; &lt;h2&gt;Shopping List&lt;/h2&gt; &lt;ul&gt; &#123;% for item in items %&#125; &lt;li&gt;&#123;&#123;item&#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &#123;% endif %&#125;&lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9DaCBZImS54.mp4 self.request.get_all()self.request.get_all() is a function that returns a list of all values that belong to string that matches a key in our query parameter. For example, if we pass in these query parameters: mysite.com?food=chips&amp;food=fruit&amp;food=milk, then get_all() will return a list that contains the strings chips,food,milk Jinja2 TemplatesNote that the bracket and the percent sign needs to be next to each other such as:1&#123;% keyword %&#125; AND NOT1&#123; % keyword % &#125; Escaping Templateshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oQJCuxvH_VM.mp4(recommand)first way12jinja_env = jinja2.Environment(loader = jinja2.FileSystemLoader(template_dir), autoescape=True) except:&lt;li&gt;&lt;/li&gt;second way:&lt;li&gt;&lt;/li&gt; Helpful Tipshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4eXgr5beSkU.mp4 Template Inheritancehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/v81yoy1L1Bo.mp4 FizzBuzz Inheritancehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ls9qRh8LVts.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nmEgJJYhW-I.mp4 Conclusionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/O60XPmSy1OY.mp4 Lesson 9: Lesson 3-Databasesnote:https://www.udacity.com/wiki/CS253%20Unit%203 Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EK0lMxBKRRw.mp4 Databases Continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Y1av5nYgQKM.mp4 What Is a Database?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7yEjSZvZOAo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ngXxI8Q0WTg.mp4 Tableshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7ldYR0Uis0I.mp4 Implementing Tables in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/j-KIhN7LKJ8.mp41234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192from collections import namedtuple# make a basic Link classLink = namedtuple(&apos;Link&apos;, [&apos;id&apos;, &apos;submitter_id&apos;, &apos;submitted_time&apos;, &apos;votes&apos;, &apos;title&apos;, &apos;url&apos;])# list of Links to work withlinks = [ Link(0, 60398, 1334014208.0, 109, &quot;C overtakes Java as the No. 1 programming language in the TIOBE index.&quot;, &quot;http://pixelstech.net/article/index.php?id=1333969280&quot;), Link(1, 60254, 1333962645.0, 891, &quot;This explains why technical books are all ridiculously thick and overpriced&quot;, &quot;http://prog21.dadgum.com/65.html&quot;), Link(23, 62945, 1333894106.0, 351, &quot;Learn Haskell Fast and Hard&quot;, &quot;http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/&quot;), Link(2, 6084, 1333996166.0, 81, &quot;Announcing Yesod 1.0- a robust, developer friendly, high performance web framework for Haskell&quot;, &quot;http://www.yesodweb.com/blog/2012/04/announcing-yesod-1-0&quot;), Link(3, 30305, 1333968061.0, 270, &quot;TIL about the Lisp Curse&quot;, &quot;http://www.winestockwebdesign.com/Essays/Lisp_Curse.html&quot;), Link(4, 59008, 1334016506.0, 19, &quot;The Downfall of Imperative Programming. Functional Programming and the Multicore Revolution&quot;, &quot;http://fpcomplete.com/the-downfall-of-imperative-programming/&quot;), Link(5, 8712, 1333993676.0, 26, &quot;Open Source - Twitter Stock Market Game - &quot;, &quot;http://www.twitstreet.com/&quot;), Link(6, 48626, 1333975127.0, 63, &quot;First look: Qt 5 makes JavaScript a first-class citizen for app development&quot;, &quot;http://arstechnica.com/business/news/2012/04/an-in-depth-look-at-qt-5-making-javascript-a-first-class-citizen-for-native-cross-platform-developme.ars&quot;), Link(7, 30172, 1334017294.0, 5, &quot;Benchmark of Dictionary Structures&quot;, &quot;http://lh3lh3.users.sourceforge.net/udb.shtml&quot;), Link(8, 678, 1334014446.0, 7, &quot;If It&apos;s Not on Prod, It Doesn&apos;t Count: The Value of Frequent Releases&quot;, &quot;http://bits.shutterstock.com/?p=165&quot;), Link(9, 29168, 1334006443.0, 18, &quot;Language proposal: dave&quot;, &quot;http://davelang.github.com/&quot;), Link(17, 48626, 1334020271.0, 1, &quot;LispNYC and EmacsNYC meetup Tuesday Night: Large Scale Development with Elisp &quot;, &quot;http://www.meetup.com/LispNYC/events/47373722/&quot;), Link(101, 62443, 1334018620.0, 4, &quot;research!rsc: Zip Files All The Way Down&quot;, &quot;http://research.swtch.com/zip&quot;), Link(12, 10262, 1334018169.0, 5, &quot;The Tyranny of the Diff&quot;, &quot;http://michaelfeathers.typepad.com/michael_feathers_blog/2012/04/the-tyranny-of-the-diff.html&quot;), Link(13, 20831, 1333996529.0, 14, &quot;Understanding NIO.2 File Channels in Java 7&quot;, &quot;http://java.dzone.com/articles/understanding-nio2-file&quot;), Link(15, 62443, 1333900877.0, 1244, &quot;Why vector icons don&apos;t work&quot;, &quot;http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html&quot;), Link(14, 30650, 1334013659.0, 3, &quot;Python - Getting Data Into Graphite - Code Examples&quot;, &quot;http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html&quot;), Link(16, 15330, 1333985877.0, 9, &quot;Mozilla: The Web as the Platform and The Kilimanjaro Event&quot;, &quot;https://groups.google.com/forum/?fromgroups#!topic/mozilla.dev.planning/Y9v46wFeejA&quot;), Link(18, 62443, 1333939389.0, 104, &quot;github is making me feel stupid(er)&quot;, &quot;http://www.serpentine.com/blog/2012/04/08/github-is-making-me-feel-stupider/&quot;), Link(19, 6937, 1333949857.0, 39, &quot;BitC Retrospective: The Issues with Type Classes&quot;, &quot;http://www.bitc-lang.org/pipermail/bitc-dev/2012-April/003315.html&quot;), Link(20, 51067, 1333974585.0, 14, &quot;Object Oriented C: Class-like Structures&quot;, &quot;http://cecilsunkure.blogspot.com/2012/04/object-oriented-c-class-like-structures.html&quot;), Link(10, 23944, 1333943632.0, 188, &quot;The LOVE game framework version 0.8.0 has been released - with GLSL shader support!&quot;, &quot;https://love2d.org/forums/viewtopic.php?f=3&amp;amp;t=8750&quot;), Link(22, 39191, 1334005674.0, 11, &quot;An open letter to language designers: Please kill your sacred cows. (megarant)&quot;, &quot;http://joshondesign.com/2012/03/09/open-letter-language-designers&quot;), Link(21, 3777, 1333996565.0, 2, &quot;Developers guide to Garage48 hackatron&quot;, &quot;http://martingryner.com/developers-guide-to-garage48-hackatron/&quot;), Link(24, 48626, 1333934004.0, 17, &quot;An R programmer looks at Julia&quot;, &quot;http://www.r-bloggers.com/an-r-programmer-looks-at-julia/&quot;)]# links is a list of Link objects. Links have a handful of properties. For# example, a Link&apos;s number of votes can be accessed by link.votes if &quot;link&quot; is a# Link.# make the function query() return the number of votes for the link whose ID is# 15def query(): Here is more information on namedtuple used in this exercise.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0jE0n6deDpk.mp4 Queryinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2TFA879U_pc.mp4123456789def query(): submision=[] for l in links: if l.submitter_id==62443: submision.append(l) submision.sort(key=lambda x: x.submitted_time) return submision print query() https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aXwzyv11_4Q.mp4Here is more information on sorting and Lambda expressions in Python . Why Databases?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bGvBewhooJc.mp4 Types of Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/MORw_tCy42A.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Q3tsjEH9W-c.mp4 SQLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vgP5bfu0na4.mp4 Databases in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oleOUdXSfs0.mp4123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121from collections import namedtupleimport sqlite3# make a basic Link classLink = namedtuple(&apos;Link&apos;, [&apos;id&apos;, &apos;submitter_id&apos;, &apos;submitted_time&apos;, &apos;votes&apos;, &apos;title&apos;, &apos;url&apos;])# list of Links to work withlinks = [ Link(0, 60398, 1334014208.0, 109, &quot;C overtakes Java as the No. 1 programming language in the TIOBE index.&quot;, &quot;http://pixelstech.net/article/index.php?id=1333969280&quot;), Link(1, 60254, 1333962645.0, 891, &quot;This explains why technical books are all ridiculously thick and overpriced&quot;, &quot;http://prog21.dadgum.com/65.html&quot;), Link(23, 62945, 1333894106.0, 351, &quot;Learn Haskell Fast and Hard&quot;, &quot;http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/&quot;), Link(2, 6084, 1333996166.0, 81, &quot;Announcing Yesod 1.0- a robust, developer friendly, high performance web framework for Haskell&quot;, &quot;http://www.yesodweb.com/blog/2012/04/announcing-yesod-1-0&quot;), Link(3, 30305, 1333968061.0, 270, &quot;TIL about the Lisp Curse&quot;, &quot;http://www.winestockwebdesign.com/Essays/Lisp_Curse.html&quot;), Link(4, 59008, 1334016506.0, 19, &quot;The Downfall of Imperative Programming. Functional Programming and the Multicore Revolution&quot;, &quot;http://fpcomplete.com/the-downfall-of-imperative-programming/&quot;), Link(5, 8712, 1333993676.0, 26, &quot;Open Source - Twitter Stock Market Game - &quot;, &quot;http://www.twitstreet.com/&quot;), Link(6, 48626, 1333975127.0, 63, &quot;First look: Qt 5 makes JavaScript a first-class citizen for app development&quot;, &quot;http://arstechnica.com/business/news/2012/04/an-in-depth-look-at-qt-5-making-javascript-a-first-class-citizen-for-native-cross-platform-developme.ars&quot;), Link(7, 30172, 1334017294.0, 5, &quot;Benchmark of Dictionary Structures&quot;, &quot;http://lh3lh3.users.sourceforge.net/udb.shtml&quot;), Link(8, 678, 1334014446.0, 7, &quot;If It&apos;s Not on Prod, It Doesn&apos;t Count: The Value of Frequent Releases&quot;, &quot;http://bits.shutterstock.com/?p=165&quot;), Link(9, 29168, 1334006443.0, 18, &quot;Language proposal: dave&quot;, &quot;http://davelang.github.com/&quot;), Link(17, 48626, 1334020271.0, 1, &quot;LispNYC and EmacsNYC meetup Tuesday Night: Large Scale Development with Elisp &quot;, &quot;http://www.meetup.com/LispNYC/events/47373722/&quot;), Link(101, 62443, 1334018620.0, 4, &quot;research!rsc: Zip Files All The Way Down&quot;, &quot;http://research.swtch.com/zip&quot;), Link(12, 10262, 1334018169.0, 5, &quot;The Tyranny of the Diff&quot;, &quot;http://michaelfeathers.typepad.com/michael_feathers_blog/2012/04/the-tyranny-of-the-diff.html&quot;), Link(13, 20831, 1333996529.0, 14, &quot;Understanding NIO.2 File Channels in Java 7&quot;, &quot;http://java.dzone.com/articles/understanding-nio2-file&quot;), Link(15, 62443, 1333900877.0, 1244, &quot;Why vector icons don&apos;t work&quot;, &quot;http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html&quot;), Link(14, 30650, 1334013659.0, 3, &quot;Python - Getting Data Into Graphite - Code Examples&quot;, &quot;http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html&quot;), Link(16, 15330, 1333985877.0, 9, &quot;Mozilla: The Web as the Platform and The Kilimanjaro Event&quot;, &quot;https://groups.google.com/forum/?fromgroups#!topic/mozilla.dev.planning/Y9v46wFeejA&quot;), Link(18, 62443, 1333939389.0, 104, &quot;github is making me feel stupid(er)&quot;, &quot;http://www.serpentine.com/blog/2012/04/08/github-is-making-me-feel-stupider/&quot;), Link(19, 6937, 1333949857.0, 39, &quot;BitC Retrospective: The Issues with Type Classes&quot;, &quot;http://www.bitc-lang.org/pipermail/bitc-dev/2012-April/003315.html&quot;), Link(20, 51067, 1333974585.0, 14, &quot;Object Oriented C: Class-like Structures&quot;, &quot;http://cecilsunkure.blogspot.com/2012/04/object-oriented-c-class-like-structures.html&quot;), Link(10, 23944, 1333943632.0, 188, &quot;The LOVE game framework version 0.8.0 has been released - with GLSL shader support!&quot;, &quot;https://love2d.org/forums/viewtopic.php?f=3&amp;amp;t=8750&quot;), Link(22, 39191, 1334005674.0, 11, &quot;An open letter to language designers: Please kill your sacred cows. (megarant)&quot;, &quot;http://joshondesign.com/2012/03/09/open-letter-language-designers&quot;), Link(21, 3777, 1333996565.0, 2, &quot;Developers guide to Garage48 hackatron&quot;, &quot;http://martingryner.com/developers-guide-to-garage48-hackatron/&quot;), Link(24, 48626, 1333934004.0, 17, &quot;An R programmer looks at Julia&quot;, &quot;http://www.r-bloggers.com/an-r-programmer-looks-at-julia/&quot;)]# links is a list of Link objects. Links have a handful of properties. For# example, a Link&apos;s number of votes can be accessed by link.votes if &quot;link&quot; is a# Link.# make and populate a tabledb = sqlite3.connect(&apos;:memory:&apos;)db.execute(&apos;create table links &apos; + &apos;(id integer, submitter_id integer, submitted_time integer, &apos; + &apos;votes integer, title text, url text)&apos;)for l in links: db.execute(&apos;insert into links values (?, ?, ?, ?, ?, ?)&apos;, l)# db is an in-memory sqlite database that can respond to sql queries using the# execute() function.## For example. If you run## c = db.execute(&quot;select * from links&quot;)## c will be a &quot;cursor&quot; to the results of that query. You can use the fetchmany()# function on the cursor to convert that cursor into a list of results. These# results won&apos;t be Links; they&apos;ll be tuples, but they can be passed turned into# a Link.## For example, to print all the votes for all of the links, do this:## c = db.execute(&quot;select * from links&quot;)# for link_tuple in c:# link = Link(*link_tuple)# print link.votes## QUIZ - make the function query() return the number of votes the link with ID = 2 hasdef query(): c = db.execute(&quot;PUT YOUR SQL HERE&quot;) link = Link(*c.fetchone()) return link.votes https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/F35zP0xOMFM.mp4123456def query(): c = db.execute(&quot;select * from links where id=2&quot;) link = Link(*c.fetchone()) return link.votesprint query() More Advanced SQLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aWIoeUm-HVA.mp4 Advanced SQL in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/u2nQqxMfLE4.mp412345def query(): c = db.execute(&quot;select * from links where submitter_id=62443 AND votes &gt; 1000&quot;) link = Link(*c.fetchone()) return link.idprint query() or123456def query(): c = db.execute(&quot;select * from links where submitter_id=62443 AND votes &gt; 1000&quot;) for link_tuple in c: link = Link(*link_tuple) return link.idprint query() https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KsGqEVJsmO4.mp4 Order Byhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L55IfVfNvL8.mp4You can find out more information about list comprehensions in the Python documentation and a supplementary lesson found here123456789def query(): results=[] c=db.execute(&quot;select * from links where submitter_id=62443 ORDER BY submitted_time ASC&quot;) for link_tuple in c: link = Link(*link_tuple) print link results.append(link.id) return resultsprint query() better12345def query(): c=db.execute(&quot;select id from links where submitter_id=62443 order by submitted_time asc&quot;) results=[t[0] for t in c] return resultsprint query() https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kWvI1pvxYZ4.mp4 Joinshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KddpLKB5JYA.mp4 Indexeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kgC5ZwM9BS8.mp4 Querying Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dLAZvKNkZcs.mp412345678# QUIZ - implement the function link_by_id() that takes a link&apos;s ID and returns# the Link object itselfdef link_by_id(link_id): for l in links: if l.id==link_id: return lprint link_by_id(24) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ASrkP2-oBuM.mp4 Using Dictionaries As Indiceshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4c3leET2nzg.mp41234567# QUIZ - implement the function build_link_index() that creates a python dictionary# the maps a link&apos;s ID to the link itselfdef build_link_index(): result=&#123;&#125; for l in links: result[l.id]=l return result https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/snkHGcSRRsk.mp4 Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pq5cH9fx524.mp41234567891011121314def build_link_index(): index = &#123;&#125; for l in links: index[l.id] = l return indexlink_index = build_link_index()def link_by_id(link_id): return link_index.get(link_id)# QUIZ - implement the function add_new_link() that both adds a link to the # &quot;links&quot; list and updates the link_index dictionary. def add_new_link(link): https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/v-nktgcT2A4.mp412345678def add_new_link(link): links.append(link) link_index[link.id]=link l=Link(50,1,1,1,&apos;title&apos;,&apos;url&apos;)add_new_link(l)print links[-1]print link_by_id(50) output12Link(id=50, submitter_id=1, submitted_time=1, votes=1, title=&apos;title&apos;, url=&apos;url&apos;)Link(id=50, submitter_id=1, submitted_time=1, votes=1, title=&apos;title&apos;, url=&apos;url&apos;) Advantages of Indiceshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9CoBxepbSR4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/da2uj4wK10Y.mp4 Real-World Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/31XAKJmp0sk.mp4 Indices for Sortinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/REgL9DoFMdU.mp4 Another Real-World Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XkuT8x6Y94A.mp4 Scaling Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dkhOZOmV7Fo.mp4 note:method | replicate | shard— | — | —downside: | 1. doesn’t increase the write speed. 2. replication lag | 1. complex queries(range queries). 2. joins become difficult https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NTpHCHAD2tM.mp4 Growing Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cFFM_GCvrYs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/R9T3hQ1axaw.mp4 ACIDhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/j-r6Fmzr4PM.mp4note:atumicityconsistencyisolationdurability Google App Engine Datastorehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/r-wl-VZhNXo.mp4 GQLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SnaxUDryqaU.mp4note:GQL: all queries begin with SELECT * no joins all queries must be indexedSQL:run arbitrary queriesAutomatic Sharding and Replicationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i4Fgm_XQ10M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TmPtKXgjZwQ.mp4ASCII Chanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dtlAAUtkvgQ.mp4Getting Started on ASCII Chanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hiyj6fDFm3c.mp4Creating the Formhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yx4SvoTbiuI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gHsG6aYIaXo.mp4Textareahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/u5iR0i5KOMY.mp4Form Handlinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/odSexP9bScI.mp4Form Handling Continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/iB5cwOZK0JU.mp4Creating Entitieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dFX5Dp-v04s.mp4Datastore Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_opjCcw2Al0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/y4o26tQcAqc.mp4Creating Entities Continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gmgKRL1wiCk.mp4Working with Entitieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JUDj8fXp3eY.mp4Running Querieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h7LyH4cvYJo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cbpxkj6DGIg.mp4Stylinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1pvAum4H7-Y.mp4Lesson 10: Problem Set 3-Building a Basic BlogBasic Bloghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ysFzedoB3Rg.mp4]]></content>
      <categories>
        <category>Udacity</category>
        <category>How to Build a Blog</category>
      </categories>
      <tags>
        <tag>Udacity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 notebook]]></title>
    <url>%2F2017%2F04%2F14%2FCS229%2F</url>
    <content type="text"><![CDATA[update in 2017-04-14stay tuned… ML StanfordCS 229 Machine Learning Course MaterialsCourses Supervised learning. (7 classes) Supervised learning setup. LMS. Logistic regression. Perceptron. Exponential family. Generative learning algorithms. Gaussian discriminant analysis. Naive Bayes. Support vector machines. Model selection and feature selection. Ensemble methods: Bagging, boosting. Evaluating and debugging learning algorithms. Learning theory. (3 classes) Bias/variance tradeoff. Union and Chernoff/Hoeffding bounds. VC dimension. Worst case (online) learning. Practical advice on how to use learning algorithms. Unsupervised learning. (5 classes) Clustering. K-means. EM. Mixture of Gaussians. Factor analysis. PCA (Principal components analysis). ICA (Independent components analysis). Reinforcement learning and control. (4 classes) MDPs. Bellman equations. Value iteration and policy iteration. Linear quadratic regulation (LQR). LQG. Q-learning. Value function approximation. Policy search. Reinforce. POMDPs.]]></content>
      <tags>
        <tag>Stanford</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prepare for TOEFL]]></title>
    <url>%2F2017%2F04%2F13%2FPrepare%20for%20TOEFL%2F</url>
    <content type="text"><![CDATA[update in 2017-04-13stay tuned… cd g:hexohexo new post &quot;Prepare for TOEFL&quot; #TOEFL Strategies with English native-speaker MagooshTOEFLThe Format of the TOEFLHow to Study for the TOEFLTOEFL Study Plan (1 Month)TOEFL Listening Practice TestBest Free TOEFL ResourcesEstimating Your TOEFL ScoreTOEFL Study Plans and GuidesTwo Month TOEFL Study Plan edXTOEFL Test Preparation: The Insider’s Guide ETSTOEFL iBT® Quick Prep准备 TOEFL iBT® 考试Inside the TOEFL® Test: Speaking Questions 1&amp;2 A GUIDE TO UNDERSTANDING TOEFL IBT® SCORESSpeaking_RubricsWriting_Rubrics #清北托福备考联盟慕课——托福听力（超清） ETSx: TOEFLx TOEFL Test PreparationOverview of the CourseWelcome to TOEFL® Test Preparation: The Insider’s Guide. In this six-week course, we will explain the TOEFL test and take you through an in-depth look at all four sections of the TOEFL test: Reading, Listening, Speaking and Writing. Each week you will have the opportunity to learn more about the question types, scoring guidelines and resources to help you prepare for test day. We will give you test preparation tips and practice materials. You should expect to spend 2 to 4 hours per week to get the most out of this course. The course includes video lectures, discussion forums, weekly quizzes, practice tests and recommended readings. ETS instructors and staff will moderate the discussions and provide feedback where possible on a weekly basis. Please note: Passing this course is not a predictor of how well you might perform on the actual TOEFL iBT® test. Learning Objectives Gain a broad understanding of the four sections of the TOEFL test: Reading, - Listening, Speaking and Writing Acquire helpful tips to prepare you for the TOEFL test Improve your English language skills Learn how the TOEFL test is scored Learn how to use your TOEFL test scores for employment, school, visas, scholarships Find test prep resources for the TOEFL test Learn how to register for the TOEFL test Weekly TopicsWeek 1 – Welcome to TOEFL Test Preparation: The Insider’s Guide 1.1 Welcome 1.2 Survey 1.3 Course Information and Support 1.4 Around the World With the TOEFL Test 1.5 A Look Inside the TOEFL Test 1.6 Accommodations and Accessibility 1.7 Week 1 Quiz 1.8 This Week in Review and Getting Ready for Week 2 Week 2 – Reading Section 2.1 Challenges of Reading 2.2 About the Reading Section 2.3 Factual/Negative Factual Information Questions 2.4 Inference and Rhetorical Purpose Questions 2.5 Vocabulary Questions 2.6 Reference Questions 2.7 Sentence Simplification Questions 2.8 Insert Text Questions 2.9 Prose Summary and Fill in a Table Questions 2.10 How the Reading Section is Scored 2.11 Reading Practice Test 2.12 Practice Activities and Resources 2.13 This Week in Review and Getting Ready for Week 3 Week 3 – Listening Section 3.1 Challenges of Listening 3.2 About the Listening Section 3.3 Gist-Content and Gist-Purpose Questions 3.4 Detail Questions 3.5 Function Questions 3.6 Attitude Questions 3.7 Organization Questions 3.8 Connecting Content Questions 3.9 Inference Questions 3.10 How the Listening Section is Scored 3.11 Listening Practice Test 3.12 Practice Activities and Resources 3.13 This Week in Review and Getting Ready for Week 4 Week 4 – Speaking Section 4.1 Challenges of Speaking 4.2 About the Speaking Section 4.3 Independent Questions 1 and 2 4.4 Integrated Questions 3 and 5 4.5 Integrated Questions 4 and 6 4.6 How the Speaking Section is Scored 4.7 Speaking Practice Test 4.8 Review of Speaking Responses 4.9 Practice Activities and Resources 4.10 This Week in Review and Getting Ready for Week 5 Week 5 – Writing Section 5.1 Challenges of Writing 5.2 About the Writing Section 5.3 Integrated Writing Question 1 5.4 Independent Writing Question 2 5.5 How the Writing Section is Scored 5.6 Writing Practice Test 5.7 Review of Writing Responses 5.8 Practice Activities and Resources 5.9 This Week in Review and Final Steps for Week 6 Week 6 – About Test Day and Beyond 6.1 Putting It All Together: Preparing for Test Day 6.2 Live Event 6.3 The Test Center 6.4 How to Register 6.5 Receiving and Sending Scores 6.6 Week 6 Quiz 6.7 Official TOEFL Prep Resources 6.8 End-of-Course Survey 6.9 Share with a Friend 6.10 Go Pursue Your Dreams]]></content>
      <tags>
        <tag>TOEFL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Duet display between Win7 and Ipad Air 2]]></title>
    <url>%2F2017%2F03%2F04%2FDuet%20display%20between%20Win7%20and%20Ipad%20Air%202%2F</url>
    <content type="text"><![CDATA[updated 03-22-2017Fix error “”duet” 不再可用” Source videoFollow this video in YouTube. Quote from YouTubeLinks:Duet Display IPA: http://bit.ly/2jvH3WRCydia Impactor Link: http://bit.ly/1bv4PDK Download Duet IPADuet Display IPA, copy the follows url and download.http://down5.dailyuploads.net:182/d/zicch26joghlriinqndkca2paz7zppcgdlhoa6tbe2u4334fbte4z2h2/Duet%20Display%20[Duet%20Inc.]%20(v1.3.7%20v6%203GS%20Univ%20LP%20os80)-Widow.rc334d_902.ipa Download Cydia Impactor (Win7)From this Cydia Impactor website, I choose the windows version Download Duet (Windows OS)Install Duet in window 7, PC download here) Successful instance Fix errorhttp-win.cpp:158 Peer cetificate cannot be authenticated]]></content>
      <tags>
        <tag>Duet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud595 Linux Command Line Basics Notebook]]></title>
    <url>%2F2017%2F02%2F28%2FUdacity%20ud595%20Linux%20Command%20Line%20Basics%20Notebook%2F</url>
    <content type="text"><![CDATA[Linum Command Line BasicsShell Commandsstay tuned… Get Into the ShellWhat’s a Virtual MachineVirtual MachineA virtual machine (VM) is a computer program that simulates a computer. The VM software we’re using in this course is called VirtualBox. When you set up your virtual machine you installed Linux on the VM, making Linux the guest operating system. The operating system (OS) that’s installed directly on your physical computer is called the host OS. We use a virtual machine in this course to ensure that everyone is working in an identical environment with the correct programs installed, but there are many other reasons programmers use VMs. VMs isolate programming projects from everything else on a programmer’s computer. The programmer can configure the guest OS by installing programs and customizing settings without disrupting their day-to-day environment. VMs are also used to simulate the environment that software will be deployed to. Most developers use Windows or Mac OS, but often deploy their code to servers running Linux. Using a Linux VM lets programmers run code on their target platform, without leaving the comfort of their preferred host OS. VagrantVagrant is a program that makes VMs more convenient to use. For example when you ran vagrant up Vagrant created a VM, installed a guest OS, and configured the guest OS. Vagrant did all of this automatically by following instructions in the Vagrantfile. Automating this process saves time and ensures consistent results. Vagrant also makes it easy to edit files that are in the VM from programs installed on the host OS. We won’t use this feature in this class, but it’s very helpful in other Udacity courses and on the job. Command Line InterfaceSetting Up Your Own (Virtual) Linux BoxYour own Linux boxTo learn the Linux shell, you need a Linux machine to run it on. But we can’t really ship a new Linux computer to every one of you. So instead you will set up a Linux virtual machine (VM) on your own computer. You’ll be using the VirtualBox application to run the virtual machine, and the vagrant software to configure it. This virtual-machine setup is very similar to the ones you will use in later Udacity courses on the Linux platform. So when you get to those courses, you will not need to re-install this software. Setting the virtual machine up is not complicated, but it will take some time when your computer downloads the Linux OS. Follow the instructions below to set it up before proceeding on in this course. What’s a virtual machine?A virtual machine is a program that runs on your Windows or Mac computer, and that can run a different operating system inside it. In this case, you’ll be running an Ubuntu Linux server system. Install GitYou can skip this step if you are not running Windows, but many other courses use Git, so you may want to install it now. Download Git from git-scm.com. Install the version for your operating system. On Windows, Git will provide you with the Git Bash terminal program, which you will use to run and connect to your Linux VM. Find your terminal programTo take this course you will need to use a terminal program, which presents the shell user interface and lets you log in to your Linux VM. Windows: Use the Git Bash program, which is installed with Git (above).Mac OS X: Use the Terminal program, located in your Applications/Utilities folder.Linux: Use any terminal program (e.g. xterm or gnome-terminal). Install VirtualBoxVirtualBox is the software that actually runs the VM. You can download it from virtualbox.org, here. Install the platform package for your operating system. You do not need the extension pack or the SDK. You do not need to launch VirtualBox after installing it. Ubuntu 14.04 Note: If you are running Ubuntu 14.04, install VirtualBox using the Ubuntu Software Center, not the virtualbox.org web site. Due to a reported bug, installing VirtualBox from the site may uninstall other software you need. Install VagrantVagrant is the software that configures the VM and lets you share files between your host computer and the VM’s filesystem. You can download it from vagrantup.com. Install the version for your operating system. Windows Note: The Installer may ask you to grant network permissions to Vagrant or make a firewall exception. Be sure to allow this. Download the VM configuration fileMake a new folder to keep your workspace for this course. You might call it Shell, but whatever name you pick is OK. Keep track of what folder you created it in (for instance, Desktop). In the Supporting Materials section of this page, below, you’ll find a link to the configuration file, called Vagrantfile. Download this file into the new folder you just created. Run the virtual machine!Open your terminal program. Type this shell command and press Enter: cd Desktop/Shell(for me cd G:/Udacity/ud595/Shell) (If your new folder is called something other than “Shell”, or is located somewhere other than “Desktop”, change those.) Then start the VM by running the command vagrant up. This will make your system download the Linux OS and start up the virtual machine. Unfortunately, this will take a long time on most network connections. Fortunately, you only have to do it once, and the same Linux OS image will work for later Udacity courses too. Once it is done, run the command vagrant ssh. And you will be logged in to the virtual machine and ready to do the course exercises!The Udacity VM is the official shell for this class, but if your computer already has a Unix* shell you can use it if you prefer. Caveat: Your computer’s own shell may differ from the VM in unanticipated ways, and may not have all the programs installed which the VM provides. The recommended environment is the VM. if you’re running Linux or Mac OS X for instance Log In and Break Stuffexitvagrant ssh In the VM or out of the VM?We’ve set this course’s exercises up to work in the virtual machine (VM) that you set up using the vagrant program. If you get logged out of the VM, you may end up typing shell commands in to your regular operating system instead of to the Linux system that we’ve set up for the course. Some commands won’t work, and some files probably won’t be where the course expects them to be. Getting logged outIf you type the command exit into the shell, or if you type Control-D, you will see a message like this: logout Connection to 127.0.0.1 closed.This just means that you got logged out. After logging out, you won’t be in the VM any more. To get back into the VM, use the command vagrant ssh. If vagrant ssh doesn’t workIf you get a message like this: VM must be running to open SSH connection. Run vagrant up to start the virtual machine.This means that the VM program is not running, for instance because you rebooted your computer. This is just fine and it doesn’t mean you’ve lost any work. Just run vagrant up to bring the VM back up, then vagrant ssh to log in. This will not take as long as the first time you ran it, because it won’t need to download the Linux OS. If vagrant up doesn’t workIf you get a message like this: A Vagrant environment or target machine is required to run this command. Run `vagrant init` to create a new Vagrant environment. Or, get an ID of a target machine from `vagrant global-status` to run this command on. A final option is to change to a directory with a Vagrantfile and to try again. That means that vagrant can’t find the configuration file you downloaded. Go back to the instructions, check to be sure that you did step 5, and then do step 6 again. Multiple terminalsIf you open up more than one terminal window, only the one(s) that you ran vagrant ssh in will be connected to your Linux OS for this course. The others will be connected to your regular OS. (It’s actually really normal for Linux users to have to carefully keep track of which terminal windows are connected to which machines. Don’t panic. Just look for whether “vagrant” appears on the command line.) Commands That Worklscurl http://udacity.github.io/ud595-shell/stuff.zip -o things.zipls Try More Commandsdate expr 2+2 echo you rock uname hostname host udacity.com bash --version history Shell CommandsThe Linux Filesystem]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>VirtualBox</tag>
        <tag>Vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud256 Networking for Web Developers Notebook]]></title>
    <url>%2F2017%2F02%2F27%2FUdacity%20ud256%20Networking%20for%20Web%20Developers%20Notebook%2F</url>
    <content type="text"><![CDATA[Networking for Web DevelopersclassroomAim to build a VM.stay tuned… From Ping to HTTPIntro Linum Command Line Basics Configuring Linux Web Servers Designing RESTful APIs Setting Up For This CourseLocal VMInstall two pieces of software: VirtualBox, which you can get from this download page. Vagrant, which you can get from this download page. Use Git Bash mkdir networking cd networking vagrant init ubuntu/trusty64 vagrant upWhen it is complete, you can log into the Linux instance with vagrant ssh. .I have install it before so for me cd G:/Udacity/ud256/from\ ping\ to\ http/networking remove files in networking vagrant up(need about 6 hours to download) When it is complete, you can log into the Linux instance with vagrant ssh. If you log out of the Linux instance or close the terminal, the next time you want to use it you only need to run cd networking and vagrant ssh. Installing networking tools SSH into your Linux machine. Then take a moment to bring it up to date with any package updates: sudo apt-get update &amp;&amp; sudo apt-get upgrade Note:1W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/trusty-security/main/source/Sources Hash Sum mismatch You’ll be using several network utility programs in this course. Some of them may already be installed, but just to make sure, let’s install them all: sudo apt-get install netcat-openbsd tcpdump traceroute mtr]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>VirtualBox</tag>
        <tag>Vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTex]]></title>
    <url>%2F2017%2F02%2F23%2FLaTex%2F</url>
    <content type="text"><![CDATA[stay tuned… Original videos can be found in LaTeX Tutorials (featuring Texmaker) made by Michelle Krummel. Installation for windows 7 Go to LaTex Download MiKTeX and run Download TexMaker and run Tutorial 1In TexMaker, type1234567891011\documentclass[11pt]&#123;article&#125;\begin&#123;document&#125;This is my first LaTex document.Suppose we are given a rectangle with side lengths $(x+1)$ and $(x+3)$.Then the equation $A=x^2+4X+3$ represents the area of the rectangle.Suppose we are given a rectangle with side lengths $(x+1)$ and $(x+3)$.Then the equation $$A=x^2+4X+3$$ represents the area of the rectangle.\end&#123;document&#125; It shows:\documentclass[11pt]{article} \begin{document}This is my first LaTex document. Suppose we are given a rectangle with sidelengths $(x+1)$ and $(x+3)$.Then the equation $A=x^2+4X+3$ represents the area of the rectangle. Suppose we are given a rectangle with sidelengths $(x+1)$ and $(x+3)$.Then the equation $$A=x^2+4X+3$$ represents the area of the rectangle.\end{document} Tutorial 2In TexMaker, type1234567891011121314151617181920212223242526272829303132333435363738394041424344\documentclass[11pt]&#123;article&#125;\begin&#123;document&#125;superscripts: $$2x^3$$$$2x^&#123;34&#125;$$$$2x^&#123;3x+4&#125;$$$$2x^&#123;3x^4+5&#125;$$subscripts: $$x_1$$$$x_&#123;12&#125;$$$$&#123;x_1&#125;_2$$greek letters:$$\pi$$$$\alpha$$$$A=\pi r^2$$trig functions:$$y=\sin&#123;x&#125;$$log functions:$$\log_5&#123;x&#125;$$$$\ln&#123;x&#125;$$square roots:$$\sqrt&#123;2&#125;$$$$\sqrt[3]&#123;2&#125;$$$$\sqrt&#123;x^2+y^2&#125;$$$$\sqrt&#123;1+\sqrt&#123;x&#125;&#125;$$fractions:About $\displaystyle&#123;\frac&#123;2&#125;&#123;3&#125;&#125;$ of the glass is full.$$\frac&#123;x&#125;&#123;x^+x+1&#125;$$$$\frac&#123;\sqrt&#123;x+1&#125;&#125;&#123;\sqrt&#123;x-1&#125;&#125;$$$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;x&#125;&#125;$$$$\sqrt&#123;\frac&#123;x&#125;&#123;x^2+x+1&#125;&#125;$$\end&#123;document&#125; Note : In hexo, need to use 1234subscripts: $$x\_1$$$$x\_&#123;12&#125;$$$$&#123;x\_1&#125;\_2$$ It shows:\documentclass[11pt]{article} \begin{document} superscripts: $$2x^3$$$$2x^{34}$$$$2x^{3x+4}$$$$2x^{3x^4+5}$$ subscripts:$$x_1$$$$x_{12}$$$${x_1}_2$$ greek letters:$$\pi$$$$\alpha$$$$A=\pi r^2$$ trig functions:$$y=\sin{x}$$ log functions:$$\log_5{x}$$$$\ln{x}$$ square roots:$$\sqrt{2}$$$$\sqrt[3]{2}$$$$\sqrt{x^2+y^2}$$$$\sqrt{1+\sqrt{x}}$$ fractions: About $\displaystyle{\frac{2}{3}}$ of the glass is full. $$\frac{x}{x^+x+1}$$ $$\frac{\sqrt{x+1}}{\sqrt{x-1}}$$ $$\frac{1}{1+\frac{1}{x}}$$ $$\sqrt{\frac{x}{x^2+x+1}}$$\end{document} Paper formula$Z{normal}=[z{normal1}, z{normal2}…z{normal_n}]\in R^{n\times m}$ \begin{equation}\label{eq:xznormal}x{normal} = \frac{z{normali}-\bar{z}{normali}}{\sigma(z{normal_i})}\end{equation} $\bar{z}_{normali}$$z{normali}$$\sigma(z{normali})$$x{normal}$ \begin{equation}\label{eq:snormalk}s_{normalk} = W{normalk}\cdot x{normal_i}\end{equation} $W_{normalk}\in R^{k\times m}$$x{normal_i}\in R^{m\times 1}$ \begin{equation}\label{eq:snormale}s_{normale} = W{normale}\cdot x{normal_i}\end{equation} $W_{normal_e}\in R^{(d-k)\times m}$ $E{normal}$${I^2}{normal}$${I^2}_{normal_e}$ \begin{equation}\label{eq:inormal}{I^2}{normal}= s{normalk}^T\cdot s{normalk}\end{equation}\begin{equation}\label{eq:inormale}I{normale}^2= s{normale}^T\cdot s{normale}\end{equation}\begin{equation}\label{eq:xase}X{normal}= A{normal}\cdot S{normal}+E_{normal}\end{equation} $E_{normal}$ \begin{equation}\label{eq:enormal}E{normal}=T{normal}\cdot P{normal}^T+F{normal}\end{equation} $E{normal}=[e{normal1}, e{normal2}…e{normal_m}]\in R^{n\times m}$ $T{normal}^2$$Q{normal}$ \begin{equation}\label{eq:t2normal}T{normal}^2=e{normali}^T\cdot P{normal}\cdot \Lambda ^{-1}\cdot P{normal}^T\cdot e{normali}\end{equation}\begin{equation}\label{eq:qnormal}Q{normal}= r{normal}^T r{normal}\end{equation} $r{normal}=(I-P{normal}\cdot P{normal}^T )\cdot e{normal_i}\in R^{m\times 1}$$I\in R^{m\times m}$ \begin{equation}\label{eq:inputmatrix}X{normal}= [{I^2}{normal},I_{normale}^2,T{normal}^2,Q_{normal}]\end{equation} $Z{fault}=[z{fault1}, z{fault2}…z{faultm}]\in R^{n\times m}$$X{fault}=[x_{fault1}, x{fault2}…x{faultm}]\in R^{n\times m}$Calculate Statistics$y{normal}=0$ \begin{equation}\label{eq:ifault}{I^2}{fault}= s{faultk}^T\cdot s{faultk}\end{equation}\begin{equation}\label{eq:ifaulte}I{faulte}^2= s{faulte}^T\cdot s{faulte}\end{equation}\begin{equation}\label{eq:t2fault}T{fault}^2=e_{faulti}^T\cdot P{fault}\cdot \Lambda ^{-1}\cdot P{fault}^T\cdot e{faulti}\end{equation}\begin{equation}\label{eq:qfault}Q{fault}= r{fault}^T r{fault}\end{equation} \begin{equation}\label{eq:inputmatrixf}X{fault}= [{I^2}{fault},I_{faulte}^2,T{fault}^2,Q_{fault}]\end{equation} \begin{equation}\label{eq:inew}{I^2}{new}= s{newk}^T\cdot s{newk}\end{equation}\begin{equation}\label{eq:inewe}I{newe}^2= s{newe}^T\cdot s{newe}\end{equation}\begin{equation}\label{eq:t2new}T{new}^2=e_{newi}^T\cdot P\cdot \Lambda ^{-1}\cdot P^T\cdot e{newi}\end{equation}\begin{equation}\label{eq:qnew}Q{new}= r{new}^T r{new}\end{equation} $X{new}= [{I^2}{new},I_{newe}^2,T{new}^2,Q_{new}]$ \multicolumn{1}{c}{ICA-PCA-SVM} &amp; \multicolumn{1}{c}{PCA-RVM} &amp; \multicolumn{1}{c}{ICA-RVM} &amp; \multicolumn{1}{c}{Proposed Method} &amp; ICA-SVM &amp; ICA-PCA-SVM &amp; PCA-RVM &amp; ICA-RVM &amp; ICA-PCA-RVM]]></content>
      <tags>
        <tag>LaTex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity MLND Notebook]]></title>
    <url>%2F2017%2F02%2F06%2FUdacity%20MLND%20Notebook%2F</url>
    <content type="text"><![CDATA[Capstone Projectupdate in 2017-04-8stay tuned… nd009syllabus [TOC] Welcome to the NanodegreeGet started with learning about your Nanodegree. Introduction to Decision Trees, Naive Bayes, Linear and Logistic Regression and Support Vector Machines. You can join the MLND student community by following this link and registering your email - https://mlnd-slack.udacity.com WELCOME TO THE NANODEGREEWelcome to MLNDhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HG5IYufgDAo.mp4 Program Readinesshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dc9CmcGTnr0.mp4 What is Machine Learning?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K45QM8Wi7BU.mp4 Machine Learning vs. Traditional Codinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_N2iIB_bLXA.mp4 Applications of Machine Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kIM5D_W6Mh8.mp4 Connections to GA Techhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/DysCmGKRpvs.mp4 Program Outlinehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/m0cIDrRWyLw.mp4 What is MLIntroduction to Machine Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bYeteZQrUcE.mp4 Decision Treeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1RonLycEJ34.mp4 Decision Trees QuizQUIZ QUESTION Between Gender and Age, which one seems more decisive for predicting what app will the users download? Gender Age Decision Trees Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h8zH47iFhCo.mp4 Naive Bayeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jsLkVYXmr3E.mp4 Naive Bayes QuizQUIZ QUESTION If an e-mail contains the word “cheap”, what is the probability of it being spam? 40% 60%80% Naive Bayes Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YKN-fjuZ1VU.mp4 Gradient Descenthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BEC0uH1fuGU.mp4 Linear Regression Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sf51L0RN6zc.mp4QUIZ QUESTION What’s the best estimate for the price of a house? 80k 120k 190kSUBMIT Linear Regression Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L5QBqYDNJn0.mp4 Logistic Regression Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wQXKdeVHTmc.mp4QUIZ QUESTION Does the student get Accepted? Yes NoSUBMIT Logistic Regression Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JuAJd9Qvs6U.mp4 Support Vector Machineshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Fwnjx0s_AIw.mp4 Support Vector Machines QuizQUIZ QUESTION Which one is a better line?The yellow line The blue lineSUBMIT Support Vector Machines Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JrUtTwfnsfM.mp4 Neural Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xFu1_2K2D2U.mp4 Kernel Methodhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/x0JqH6-Dhvw.mp4 Kernel Method QuizQUIZ QUESTION Which equation could come to our rescue? x+yxy x^2SUBMIT Kernel Method Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dRFd6HaAXys.mp4 Recap and Challengehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ecREasTrKu4.mp4 K-means Clusteringhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pv_i08zjpQw.mp4 Hierarchical Clusteringhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1PldDT8AwMA.mp4 Practice Project: Detect SpamPractice Project: Using Naive Bayes to detect spam.From time to time you will be encouraged to work on practice projects which are aimed at deepening your understanding of the concepts being taught. In this practice project, you will be implementing the Naive Bayes algorithm to detect spam text messages(as taught by Luis earlier in the lesson) from an open source dataset. Here is the notebook, the solutions are included. Summaryhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hJEuaOUu2yA.mp4 MLND Program OrientationBefore the Program Orientationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/73CdKtS-IwU.mp4 Introductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fxNSn63xFvA.mp4 Projects and Progresshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Z9ZLMQWsbsk.mp4 Career DevelopmentBeing enrolled in one of Udacity’s Nanodegree programs has many careers-based perks. Our goal is to help you take your learning from this program and apply it in the real world and in your career. As you venture through the Machine Learning Engineer Nanodegree program, you’ll have the opportunity to Update your resume through a peer-reviewed process using conventions that recruiters expect and get tips on how to best represent yourself to pass the “6 second screen”; Create a cover letter that portrays your soft and hard skills, and most importantly your passion for a particular job that you are interested in applying to; Get your GitHub and LinkedIn profiles reviewed through the lens of a recruiter or hiring manager, focusing on how your profile, projects, code, education and past experiences represent you as a potential candidate; Practice for a technical interview with a professional reviewer on a variety of topics; And more! You can also find career workshops that Udacity has hosted over the years, where you can gain a plethora of information to prepare you for your ventures into a career. Udacity also provides job placement opportunities with many of our industry partners. To take advantage of this opportunity, fill out the career section of your Udacity professional profile, so we know more about you and your career goals! If all else fails, you can always default to emailing the career team at career-support@udacity.com. Connecting with Your CommunityYour Nanodegree community will play a huge role in supporting you when you get stuck and in helping you deepen your learning. Getting to know your fellow students will also make your experience a lot more fun! To ask and answer questions, and to contribute to discussions, head to your program forum. You can get there by clicking the Discussion link in the classroom and in the Resources tab in your Udacity Home. You can search to see if someone has already asked a question related to yours, or you can make a new post if no one has. Chances are, someone else is wondering about the same thing you are, so don’t be shy! In addition, students may connect with one another through Slack, a team-oriented chat program. You can join the MLND Slack student community by following this link and registering your email. There are many content-related channels where you can speak with students about a particular concept, and even discuss your first week in the program using the #first-week-experience channel. In addition, you can talk with MLND graduates and alumni to get a live perspective on the program in the #ask-alumni channel! You can find the student-run community wiki here. Support from the Udacity TeamThe Udacity team is here to help you reach your Nanodegree program goals! You can interact with us in the following ways: Forums: Along with your student community, the Udacity team maintains a strong presence in the forum to help make sure your questions get answered and to connect you with other useful resources. 1-on-1 Appointments: If you get stuck working on a project in the program, our mentors are here to help! You can set up a half-hour appointment with a mentor available for the project at a time you choose to get assistance. Project Reviews: During the project submission process, your submissions will be reviewed by a qualified member of our support team, who will provide comments and helpful feedback on where your submission is strongest, and where your submission needs improvement. The reviews team will support your submissions all the way up to meeting specifications! By email: You can always contact the Machine Learning team with support-related questions using machine-support@udacity.com. Please make sure that you have exhausted all other options before doing so!Find out more about the support we offer using the Resources tab in your Udacity Nanodegree Home. How Does Project Submission Work?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jCJa_VP6qgg.mp4 Integrity and Mindsethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zCOr3O50gQM.mp4 How Do I Find Time for My Nanodegree?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/d-VfUw7wNEQ.mp4All calendar applications now let you set up a weekly reminder. I have included a screen capture below of how to set one up in Google Calendar. We recommend coming into the classroom at least twice a week. It is a best practice to set up at least one repeating weekly reminder to continue the Nanodegree program. Final Tipshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1ZVBvM54hQw.mp4 Wrapping Up the Program Orientationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xujb3Rqxuog.mp4You now have all the info you need to proceed on your Nanodegree journey! If you have any further questions, perhaps about payment or enrollment, read your Nanodegree Student Handbook for more details. Download Udacity’s mobile app to learn on the go! Remember to put in time consistently, engage with your community, take advantage of the resources available to you, and give us feedback throughout the program.We are so glad to have you with us! Return to your Udacity home to keep learning. Good luck! Professional Profile Career Development Workshops MLND Slack Forums MLND wiki (Optional) Exploratory ProjectSoftware Requirements windows + R and type pip install --user pandas jupyter, oops, error: Microsoft Visual C++ 9.0 is required. Get it from http://aka.ms/vcpython27 download and install successfully Starting the ProjectFirst try windows + R typecd &lt;path&gt;my&lt;path&gt;isG:\Udacity\MLND\machine-learning-master\projects\titanic_survival_exploration type&lt;path&gt; g: typebash jupyter notebook titanic_survival_exploration.ipynb,show &#39;bash&#39; 不是内部或外部命令，也不是可运行的程序 Failed Second try opengit bash cd&lt;path&gt;with/``G:/Udacity/MLND/machine-learning-master/projects/titanic_survival_exploration typebash jupyter notebook titanic_survival_exploration.ipynb failed Third try install Anaconda windows + R typecd &lt;path&gt;my&lt;path&gt;isG:\Udacity\MLND\machine-learning-master\projects\titanic_survival_exploration type&lt;path&gt;``g: typejupyter notebook titanic_survival_exploration.ipynb done Fourth try opengit bash cd&lt;path&gt;with/``G:/Udacity/MLND/machine-learning-master/projects/titanic_survival_exploration typejupyter notebook titanic_survival_exploration.ipynb done ipython notebookMarkdown Question 4(stay tuned): Pclass == 3 Career: OrientationThroughout your Nanodegree program, you will see Career Development Lessons and Projects that will help ensure you’re presenting your new skills best during your job search. In this short lesson, meet the Careers team and learn about the career resources available to you as a Nanodegree student. If you are a Nanodegree Plus student, Career Content and Career Development Projects are required for graduation. If you are enrolled in a standard Nanodegree program, Career Content and Career Development Projects are optional and do not affect your graduation. ORIENTATIONCareer Services Available to YouMeet the Careers Teamhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oR1IxPTTz0U.mp4 Resources Career Resource Center Career Services Get Hired Your Udacity ProfileConnect to Hiring Partners through your Udacity Professional ProfileIn addition to the Career Lessons and Projects you’ll find in your Nanodegree program, you have a Udacity Professional Profile linked in the left sidebar.Your Udacity Professional Profile features important, professional information about yourself. When you make your profile public, it becomes accessible to our Hiring Partners, as well as to recruiters and hiring managers who come to Udacity to hire skilled Nanodegree graduates. As you complete projects in your Nanodegree program, they will be automatically added to your Udacity Professional Profile to ensure you’re able to show employers the skills you’ve gained through the program. In order to differentiate yourself from other candidates, make sure to go in and customize those project cards. In addition to these projects, be sure to: Keep your profile updated with your basic info and job preferences, such as location Ensure you upload your latest resume Return regularly to your Profile to update your projects and ensure you’re showcasing your best work If you are looking for a job, make sure to keep your Udacity Professional Profile updated and visible to recruiters!EDIT YOUR PROFILE NOW ! Model Evaluation and ValidationApply statistical analysis tools to model observed data, and gauge how well your models perform. Project: Predicting Boston Housing Prices For most students, this project takes approximately 8 - 15 hours to complete (about 1 - 3 weeks). P1 Predicting Boston Housing Prices STATISTICAL ANALYSISIntro: Model Evaluation and ValidationIntro to Model Evaluation and Validationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cseqEWRDs5Q.mp4 Model Evaluation What You’ll Watchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jYZO17CeZDI.mp4 Model Evaluation What You’ll Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ZLOucNwuqCk.mp4 Course Outline - Fitting It All TogetherThe ultimate goal of Machine Learning is to have data models that can learn and improve over time. In essence machine learning is making inferences on data from previous examples. In this first section we review some basic statistics and numerical tools to manipulate &amp; process our data. Then we will move on to modeling data; reviewing different data types and seeing how they play out in the case of one specific dataset. The section ends by introducing the basic tool of a supervised learning algorithm. Next, we’ll see how to use our dataset for both training and testing data, and review various tools for how to evaluate how well an algorithm performs. Finally, we’ll look at the reasons that errors arise, and the relationship between adding more data and adding more complexity in getting good predictions. The last section ends by introducing cross validation, a powerful meta-tool for helping us use our tools correctly. Model Evaluation What You’ll Dohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kJCAuHjWiOA.mp4 PrerequisitesStatistics Review &amp; Supporting LibrariesIn this section we will go over some prerequisites for this course, review basic statistics concepts and problem sets, and finally teach you how to use some useful data analysis Python libraries to explore real-life datasets using the concepts you reviewed earlier. Prerequisiteshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ANDJ8i_deE.mp4Here are shortcuts to the prerequisite statistics courses: Udacity’s descriptive stats course [link] Udacity’s inferential stats course [link] You will also need to have just a little bit of git experience — enough to check out our code repository. If you’ve ever used git before, you should be fine. If this is truly your first time with git, once you get to the first mini-project, you may want to quickly look at the first lesson of Udacity’s git course. mode mean variance``standard deviation Bessel&#39;s Correction use n-1 instead of n sample SD Measures of Central TendencyIntroduction: Topics CoveredMeasures of Central TendencyIn this lesson, we will cover the following topics: Mean Median Mode This lesson is meant to be a refresher for those who have no statistics background and therefore if you are familiar with these concepts you may skip this lesson. Which Major?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mIzPoh_kqw4.mp4Quiz: Which Major?Enter your answers as a number with no commas or symbols ($). Enter the number in thousands (5 digits) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t0yIetl9ZxI.mp4 One Number to Describe Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6QfvhQ0En0E.mp4 Which Number to Choose?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7E7Czixpviw.mp4 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TSw2AAaKxBA.mp4 Mode of Datasethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/80BAbiEWsaY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HvOjcTFlVTI.mp4 Mode of Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/47JDwoDUxP8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/s9dHF4MMGx0.mp4 Mode - Negatively Skewed Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xBnhUJENAtk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oNRtSJvtkJc.mp4 Mode - Uniform Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TE2BZql64XY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mFm0FfWHlXw.mp4 More than One Mode?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1GuHNqJNY2M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9bsmO7cKzPk.mp4 Mode of Categorical DataQuizThe data doesn’t need to be numeric to find a mode: we can also compute the mode for categorical data as well! On the next slide, you’ll be asked to find the mode of a categorical data set: the preferred M&amp;M flavor of 8,000 Udacity students.START QUIZ AnswerRemember, the mode occurs on the X-axis, so you are looking for whatever value has the highest frequency. The numbers 7,000 and 1,000 are the actual frequencies. The mode, itself, is “Plain.” More o’ Mode!https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wBduq7St2Ak.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/74mudn321tA.mp4 Find the Meanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/S4KbzIyEwV8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/El5cY2jlzuM.mp4 Procedure for Finding Meanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nRurXCTYxG4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lK2lDLdE6iA.mp4 Iterative Procedurehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/r2s9INGd-Ls.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NcWS_BvM3IU.mp4 Helpful Symbolshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/w4_8YCp-9fI.mp4 Properties of the Meanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uSXJtEpwEVM.mp4Quiz: Properties Of The MeanPlease note: The last option “The mean will change if we add an extreme value to the dataset.” is not necessarily a property of the mean, more a behavioral tendency. But for the purposes of this quiz, you can mark it as a property https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AqlvTMZg6HY.mp4 Mean with Outlier What Can You Expect? UNC Requirement for Median Find the Median Median with Outlier Find Median with Outlier Measures of Center Order Measures of Center 1 Order Measures of Center 2 Use Measures of Center to CompareLink to poll: How many Facebook friends do you have?Link to poll results Mashable: Who is an Average Facebook User?Zeebly Social Me (optional but fun use of statistics) Udacians’ Facebook Friends - MeanLink to Udacians’ Facebook FriendsCopy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Please round your answer to two decimal places. Udacians’ Facebook Friends - MedianLink to Udacians’ Facebook Friends Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Formula for Location of Median Wrap Up - Measures of CenterQuiz: Wrap Up - Measures Of CenterHere is a short doc outlining Mean, Median, and Mode. http://tinyurl.com/measureOfCenter Good Job! Variability of DataIntroduction: Topics CoveredIn this lesson, we will cover the following topics: Inter Quartile Range Outliers Standard Deviation Bessel’s Correction This lesson is meant to be a refresher for those who have no statistics background and therefore if you are familiar with these concepts you may skip this lesson. Social Networkers’ Salaries Should You Get an Account? What’s the Difference? Quantify Spread Does Range Change? Mark Z the Outlier Chop Off the Tails Where Is Q1? Q3 - Q1 IQR What Is an Outlier? Define Outlier Match Boxplots Mean Within IQR? Problem with IQR Measure Variability Calculate Mean Deviation from Mean Average Deviation Equation for Average Deviation Be Happy and Get Rid of Negatives Absolute Deviations Average Absolute Deviation Formula for Avg. Abs. Dev. Squared Deviations Sum of Squares Average Squared Deviation #### #### #### #### #### #### #### #### Numpy&amp;Pandas TutorialsNumpy and Pandas TutorialsNow that you reviewed some basic statistics, lets go over some Python libraries that allow you to explore data and process large datasets. Specifically we will go over numpy which will allow us to process large amount of numerical data and panda series and dataframes which allow us to store large datasets and extract information from them. Numpy Library Documentation: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html Pandas Library Documentation: http://pandas.pydata.org/pandas-docs/version/0.17.0/ We highly recommend going through this resource by Justin Johnson if you have not worked with Numpy before. Another great resource is the SciPy-lectures series on this topic. Numpyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/l_Tzjxfa_5g.mp4 Numpy PlaygroundHere’s the code import numpy as np &apos;&apos;&apos; The following code is to help you play with Numpy, which is a library that provides functions that are especially useful when you have to work with large arrays and matrices of numeric data, like doing matrix matrix multiplications. Also, Numpy is battle tested and optimized so that it runs fast, much faster than if you were working with Python lists directly. &apos;&apos;&apos; &apos;&apos;&apos; The array object class is the foundation of Numpy, and Numpy arrays are like lists in Python, except that every thing inside an array must be of the same type, like int or float. &apos;&apos;&apos; # Change False to True to see Numpy arrays in action if False: array = np.array([1, 4, 5, 8], float) print array print &quot;&quot; array = np.array([[1, 2, 3], [4, 5, 6]], float) # a 2D array/Matrix print array &apos;&apos;&apos; You can index, slice, and manipulate a Numpy array much like you would with a a Python list. &apos;&apos;&apos; # Change False to True to see array indexing and slicing in action if False: array = np.array([1, 4, 5, 8], float) print array print &quot;&quot; print array[1] print &quot;&quot; print array[:2] print &quot;&quot; array[1] = 5.0 print array[1] # Change False to True to see Matrix indexing and slicing in action if False: two_D_array = np.array([[1, 2, 3], [4, 5, 6]], float) print two_D_array print &quot;&quot; print two_D_array[1][1] print &quot;&quot; print two_D_array[1, :] print &quot;&quot; print two_D_array[:, 2] &apos;&apos;&apos; Here are some arithmetic operations that you can do with Numpy arrays &apos;&apos;&apos; # Change False to True to see Array arithmetics in action if False: array_1 = np.array([1, 2, 3], float) array_2 = np.array([5, 2, 6], float) print array_1 + array_2 print &quot;&quot; print array_1 - array_2 print &quot;&quot; print array_1 * array_2 # Change False to True to see Matrix arithmetics in action if False: array_1 = np.array([[1, 2], [3, 4]], float) array_2 = np.array([[5, 6], [7, 8]], float) print array_1 + array_2 print &quot;&quot; print array_1 - array_2 print &quot;&quot; print array_1 * array_2 &apos;&apos;&apos; In addition to the standard arthimetic operations, Numpy also has a range of other mathematical operations that you can apply to Numpy arrays, such as mean and dot product. Both of these functions will be useful in later programming quizzes. &apos;&apos;&apos; if True: array_1 = np.array([1, 2, 3], float) array_2 = np.array([[6], [7], [8]], float) print np.mean(array_1) print np.mean(array_2) print &quot;&quot; print np.dot(array_1, array_2) Pandashttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8ay7tX26YxE.mp4 Pandas Playground – SeriesHere’s the code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pd&apos;&apos;&apos;The following code is to help you play with the concept of Series in Pandas.You can think of Series as an one-dimensional object that is similar toan array, list, or column in a database. By default, it will assign anindex label to each item in the Series ranging from 0 to N, where N isthe number of items in the Series minus one.Please feel free to play around with the concept of Series and see what it does*This playground is inspired by Greg Reda&apos;s post on Intro to Pandas Data Structures:http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/&apos;&apos;&apos;# Change False to True to create a Series objectif False: series = pd.Series([&apos;Dave&apos;, &apos;Cheng-Han&apos;, &apos;Udacity&apos;, 42, -1789710578]) print series&apos;&apos;&apos;You can also manually assign indices to the items in the Series whencreating the series&apos;&apos;&apos;# Change False to True to see custom index in actionif False: series = pd.Series([&apos;Dave&apos;, &apos;Cheng-Han&apos;, 359, 9001], index=[&apos;Instructor&apos;, &apos;Curriculum Manager&apos;, &apos;Course Number&apos;, &apos;Power Level&apos;]) print series&apos;&apos;&apos;You can use index to select specific items from the Series&apos;&apos;&apos;# Change False to True to see Series indexing in actionif False: series = pd.Series([&apos;Dave&apos;, &apos;Cheng-Han&apos;, 359, 9001], index=[&apos;Instructor&apos;, &apos;Curriculum Manager&apos;, &apos;Course Number&apos;, &apos;Power Level&apos;]) print series[&apos;Instructor&apos;] print &quot;&quot; print series[[&apos;Instructor&apos;, &apos;Curriculum Manager&apos;, &apos;Course Number&apos;]]&apos;&apos;&apos;You can also use boolean operators to select specific items from the Series&apos;&apos;&apos;# Change False to True to see boolean indexing in actionif True: cuteness = pd.Series([1, 2, 3, 4, 5], index=[&apos;Cockroach&apos;, &apos;Fish&apos;, &apos;Mini Pig&apos;, &apos;Puppy&apos;, &apos;Kitten&apos;]) print cuteness &gt; 3 print &quot;&quot; print cuteness[cuteness &gt; 3] Pandas Playground – DataframeHere’s the code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import numpy as npimport pandas as pd&apos;&apos;&apos;The following code is to help you play with the concept of Dataframe in Pandas.You can think of a Dataframe as something with rows and columns. It issimilar to a spreadsheet, a database table, or R&apos;s data.frame object.*This playground is inspired by Greg Reda&apos;s post on Intro to Pandas Data Structures:http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/&apos;&apos;&apos;&apos;&apos;&apos;To create a dataframe, you can pass a dictionary of lists to the Dataframeconstructor:1) The key of the dictionary will be the column name2) The associating list will be the values within that column.&apos;&apos;&apos;# Change False to True to see Dataframes in actionif False: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football&apos;&apos;&apos;Pandas also has various functions that will help you understand some basicinformation about your data frame. Some of these functions are:1) dtypes: to get the datatype for each column2) describe: useful for seeing basic statistics of the dataframe&apos;s numerical columns3) head: displays the first five rows of the dataset4) tail: displays the last five rows of the dataset&apos;&apos;&apos;# Change False to True to see these functions in actionif True: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football.dtypes print &quot;&quot; print football.describe() print &quot;&quot; print football.head() print &quot;&quot; print football.tail() Create a DataFramehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hMxOuWJaVDA.mp4 Here is a link to the pandas documentation. Here’s also an excellent series of tutorials as IPython notebooks (Thank you to Dominique Luna for sharing!) Also note: you do not need to use pandas.Series, you can pass in python lists as the values in this case: olympic_medal_counts_df = DataFrame( {&apos;country_name&apos;: countries, &apos;gold&apos;: gold, &apos;silver&apos;: silver, &apos;bronze&apos;: bronze}) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from pandas import DataFrame, Series################## Syntax Reminder:## The following code would create a two-column pandas DataFrame# named df with columns labeled &apos;name&apos; and &apos;age&apos;:## people = [&apos;Sarah&apos;, &apos;Mike&apos;, &apos;Chrisna&apos;]# ages = [28, 32, 25]# df = DataFrame(&#123;&apos;name&apos; : Series(people),# &apos;age&apos; : Series(ages)&#125;)def create_dataframe(): &apos;&apos;&apos; Create a pandas dataframe called &apos;olympic_medal_counts_df&apos; containing the data from the table of 2014 Sochi winter olympics medal counts. The columns for this dataframe should be called &apos;country_name&apos;, &apos;gold&apos;, &apos;silver&apos;, and &apos;bronze&apos;. There is no need to specify row indexes for this dataframe (in this case, the rows will automatically be assigned numbered indexes). You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] # your code here olympic_medal_counts_df = DataFrame( &#123;&apos;country_name&apos;: countries, &apos;gold&apos;: gold, &apos;silver&apos;: silver, &apos;bronze&apos;: bronze&#125;) return olympic_medal_counts_df https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-6Zw3Y4iXRY.mp4 Dataframe Columnshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/G4gFryXnrr8.mp4 Pandas Playground - Indexing DataframesHere’s the code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import pandas as pd&apos;&apos;&apos;You can think of a DataFrame as a group of Series that share an index.This makes it easy to select specific columns that you want from the DataFrame. Also a couple pointers:1) Selecting a single column from the DataFrame will return a Series2) Selecting multiple columns from the DataFrame will return a DataFrame*This playground is inspired by Greg Reda&apos;s post on Intro to Pandas Data Structures:http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/&apos;&apos;&apos;# Change False to True to see Series indexing in actionif False: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football[&apos;year&apos;] print &apos;&apos; print football.year # shorthand for football[&apos;year&apos;] print &apos;&apos; print football[[&apos;year&apos;, &apos;wins&apos;, &apos;losses&apos;]]&apos;&apos;&apos;Row selection can be done through multiple ways.Some of the basic and common methods are: 1) Slicing 2) An individual index (through the functions iloc or loc) 3) Boolean indexingYou can also combine multiple selection requirements through booleanoperators like &amp; (and) or | (or)&apos;&apos;&apos;# Change False to True to see boolean indexing in actionif True: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football.iloc[[0]] print &quot;&quot; print football.loc[[0]] print &quot;&quot; print football[3:5] print &quot;&quot; print football[football.wins &gt; 10] print &quot;&quot; print football[(football.wins &gt; 10) &amp; (football.team == &quot;Packers&quot;)] Pandas Vectorized Methodshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hvSEZxcH9PM.mp4As a refresher on lambda, lambda functions are small inline functions that are defined on-the-fly in Python. lambda x: x&gt;= 1 will take an input x and return x&gt;=1, or a boolean that equals True or False. In this example, map() and applymap() create a new Series or DataFrame by applying the lambda function to each element. Note that map() can only be used on a Series to return a new Series and applymap() can only be used on a DataFrame to return a new DataFrame. For further reference, please refer to the official documentation on lambda: Lambda Function Average Bronze Medalshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AjniaYFfCeg.mp4You might find using “boolean indexing“ helpful for this problem. Here is a link to the pandas documentation. Here’s also an excellent series of tutorials as IPython notebooks (Thank you to Dominique Luna for sharing!) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pandas import DataFrame, Seriesimport numpydef avg_medal_count(): &apos;&apos;&apos; Compute the average number of bronze medals earned by countries who earned at least one gold medal. Save this to a variable named avg_bronze_at_least_one_gold. You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. HINT-1: You can retrieve all of the values of a Pandas column from a data frame, &quot;df&quot;, as follows: df[&apos;column_name&apos;] HINT-2: The numpy.mean function can accept as an argument a single Pandas column. For example, numpy.mean(df[&quot;col_name&quot;]) would return the mean of the values located in &quot;col_name&quot; of a dataframe df. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] olympic_medal_counts = &#123;&apos;country_name&apos;:Series(countries), &apos;gold&apos;: Series(gold), &apos;silver&apos;: Series(silver), &apos;bronze&apos;: Series(bronze)&#125; df = DataFrame(olympic_medal_counts) # YOUR CODE HERE #print df[df.gold&gt;=1] #print df[df.gold&gt;=1][&apos;bronze&apos;] #print (df[df.gold&gt;=1][&apos;bronze&apos;]).apply(numpy.mean) avg_bronze_at_least_one_gold = numpy.mean(df[df.gold&gt;=1][&apos;bronze&apos;]) return avg_bronze_at_least_one_gold https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kibtHtPgWvs.mp4 Average Gold, Silver, and Bronze Medalshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/19b96U6dLtY.mp412345678910111213141516171819202122232425262728293031323334353637383940import numpyfrom pandas import DataFrame, Seriesdef avg_medal_count(): &apos;&apos;&apos; Using the dataframe&apos;s apply method, create a new Series called avg_medal_count that indicates the average number of gold, silver, and bronze medals earned amongst countries who earned at least one medal of any kind at the 2014 Sochi olympics. Note that the countries list already only includes countries that have earned at least one medal. No additional filtering is necessary. You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] olympic_medal_counts = &#123;&apos;country_name&apos;:countries, &apos;gold&apos;: Series(gold), &apos;silver&apos;: Series(silver), &apos;bronze&apos;: Series(bronze)&#125; df = DataFrame(olympic_medal_counts) # YOUR CODE HERE #print df[(df.gold&gt;=1)|(df.silver&gt;=1)|(df.bronze&gt;=1)] #print df[(df.gold&gt;=1)|(df.silver&gt;=1)|(df.bronze&gt;=1)][&apos;bronze&apos;,&apos;silver&apos;,&apos;gold&apos;] print df[&apos;bronze&apos;,&apos;silver&apos;,&apos;gold&apos;].apply(numpy.mean) #return 0 #return avg_medal_count https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HbCmZuVp548.mp4 Matrix Multiplication and Numpy Dothttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yAgqlTfWc9E.mp4The second line in the green vector on the top line starting at 1:15 should read: “1 x 4 + 2 x 5”. This vector should also be a row vector (1 x 3 matrix) instead of a column vector (3 x 1 matrix). You can read more about numpy.dot or matrix multiplication with numpy below:http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html Olympics Medal Pointshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uKvbguVQYh4.mp412345678910111213141516171819202122232425262728293031323334353637383940import numpyfrom pandas import DataFrame, Seriesdef numpy_dot(): &apos;&apos;&apos; Imagine a point system in which each country is awarded 4 points for each gold medal, 2 points for each silver medal, and one point for each bronze medal. Using the numpy.dot function, create a new dataframe called &apos;olympic_points_df&apos; that includes: a) a column called &apos;country_name&apos; with the country name b) a column called &apos;points&apos; with the total number of points the country earned at the Sochi olympics. You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] # YOUR CODE HERE olympic_medal_counts = &#123;&apos;country_name&apos;:countries, &apos;gold&apos;: Series(gold), &apos;silver&apos;: Series(silver), &apos;bronze&apos;: Series(bronze)&#125; df = DataFrame(olympic_medal_counts) df[&apos;points&apos;] = df[[&apos;gold&apos;,&apos;silver&apos;,&apos;bronze&apos;]].dot([4, 2, 1]) olympic_points_df = df[[&apos;country_name&apos;,&apos;points&apos;]] return olympic_points_df https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EQMUh4Id4Po.mp4 numpy which will allow us to process large amount of numerical data. pandas which allow us to store large datasets and extract information from them. Numpy Playground Pandas Playground – Series Pandas Playground – Dataframe Pandas Playground - Indexing Dataframes map()andapplymap()create a new Series or DataFrame by applying the lambda function to each element. Note thatmap()can only be used on a Series to return a new Series andapplymap()can only be used on a DataFrame to return a new DataFrame. Here is a link to the pandas documentation. Here’s also an excellent series of tutorials as IPython notebooks intro-to-pandas-data-structures DATA MODELINGscikit-learn Tutorial windows + r typepip show scikit-learn --version,I have installed scikit-learn typepip install --upgrade &#39;scikit-learn&gt;=0.17,&lt;0.18&#39;It shows系统找不到指定的文件。 typeconda -c install scikit-learn=0.17It showsconda-script.py: error: unrecognized arguments: -c opengit bash typepip install --upgrade &#39;scikit-learn&gt;=0.17,&lt;0.18&#39;,success typeconda -c install scikit-learn=0.17failedconda-script.py: error: unrecognized arguments: -c Introduction to scikit-learnScikit-LearnScikit-learn is an open source Machine Learning library that is built on NumPy, SciPy and matplotlib. It uses a Python interface and supports various regression, classification and clustering algorithms. You will be using this library throughout this program to implement in your projects. Example using Scikit-learnTo show you how we can leverage sklearn(short for scikit-learn), here is an example of a simple Linear Regression Classifier being used to make predictions on the Boston housing prices dataset which comes as a preloaded dataset with sklearn. We will not dive deep into the dataset per se, nor will we split our dataset into training and testing splits just yet(you will learn about the importance of this in the next lesson), the goal of this node is to give you a high level view of how with just a few lines of code, you can make predictions on a dataset using the sklearn tool. This data sets consists of 506 samples with a dimensionality of 13. We will run a Linear Regression classifier on the feature set to make predictions on the prices. We start by getting the necessary imports. from sklearn import datasets # sklearn comes with a variety of preloaded datasets from sklearn import metrics # calculate how well our model is doing from sklearn.linear_model import LinearRegression There are several ways in which we can load datasets in sklearn. For now, we will start the most basic way using a dataset which is pre loaded. # Load the dataset housing_data = datasets.load_boston() We now define the model we want to use and herein lies one of the main advantages of using this library. linear_regression_model = LinearRegression() Next, we can fit our Linear Regression model on our feature set to make predictions for our labels(the price of the houses). Here, housing_data.data is our feature set and housing_data.target are the labels we are trying to predict. linear_regression_model.fit(housing_data.data, housing_data.target)Once our model is fit, we make predictions as follows: predictions = linear_regression_model.predict(housing_data.data) Lastly, we want to check how our model does by comparing our predictions with the actual label values. Since this is a regression problem, we will use the r2 score metric. You will learn about the various classification and regression metrics in future lessons. score = metrics.r2_score(housing_data.target, predictions) And there we have it. We have trained a regression model on a dataset and calculated how well our model does all with just a few lines of code and with all the math abstracted from us. In the next nodes, we will walk you through installing sklearn on your system, and you will work with Katie on a sample problem. scikit-learn Installationscikit-learn InstallationFirst, check that you have a working python installation. Udacity uses python 2.7 for our code templates and in-browser exercises. We recommend using pip to install packages. First get and install pip from here. If you are using Anaconda, you can also use the conda command to install packages. To install scikit-learn via pip or anaconda: open your terminal (terminal on a mac or cmd on a PC) install sklearn with the command: pip install scikit-learn or conda install scikit-learn If you do not use pip or conda, further installation instructions can be found here. Important note about scikit-learn versioningscikit-learn has recently come out with a stable release of its library with version v0.18. With this version comes a few changes to some of the functions we will talk about extensively in this course, such as train_test_split, gridSearchCV, ShuffleSplit, and learning_curves. The documentation available on scikit-learn’s website will reference v0.18, however Katie, Udacity’s quizzes, and our projects, are still written in v0.17. Please make sure that when using the documentation and scikit-learn, you reference version v0.17 and not version v0.18. In the near future, we will be updating our content to match the most current version. Please see this forum post that provides more detail on this topic. If you have any additional questions or concerns, feel free to discuss them in the forums or email machine-support@udacity.com. If you’ve accidentally installed version v0.18 through pip, not to worry! Use the command below to downgrade your scikit-learn version to v0.17: pip install --upgrade &apos;scikit-learn&gt;=0.17,&lt;0.18&apos; If you are using the Anaconda distribution of Python and have scikit-learn installed as version v0.18, you can also use the command below to downgrade your scikit-learn version to v0.17: conda -c install scikit-learn=0.17 scikit-learn CodeIn this next section Katie will walk through using the scikit-learn (or sklearn) documentation with a Gaussian Naive Bayes model. For this exercise it is not important to know all of the details of Naive Bayes or the code Katie is demonstrating. Focus on taking in the basic layout of sklearn, which we can then use to evaluate and validate any data model. We will cover Naive Bayes along with other useful supervised models in much more detail in the upcoming Supervised Machine Learning course and use what we learn from this course to evaluate each model’s strengths and weaknesses. If you want a sneak peak into Naive Bayes, you can check out the documentation here. Getting Started With sklearnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/olGPVtH7KGU.mp4 Gaussian NB Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wpnDwiqTCJA.mp4 GaussianNB Deployment on Terrain Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VBs6D4ggnYY.mp4Quiz: GaussianNB Deployment On Terrain DataTo find the ClassifyNB.py script that you need to update for the quiz, you can click on the dropdown in the classroom code editor to get a list of files that will be used. In the quiz that follows, the line that readspred = clf.predict(features_test)is not necessary for drawing the decision boundary, at least as we’ve written the code. However, the whole point of making a classifier is that you can make predictions with it, so be sure to keep it in mind since you’ll be using it in the quiz after this one. 1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/python&quot;&quot;&quot; Complete the code in ClassifyNB.py with the sklearn Naive Bayes classifier to classify the terrain data. The objective of this exercise is to recreate the decision boundary found in the lesson video, and make a plot that visually shows the decision boundary &quot;&quot;&quot;from prep_terrain_data import makeTerrainDatafrom class_vis import prettyPicture, output_imagefrom ClassifyNB import classifyimport numpy as npimport pylab as plfeatures_train, labels_train, features_test, labels_test = makeTerrainData()### the training data (features_train, labels_train) have both &quot;fast&quot; and &quot;slow&quot; points mixed### in together--separate them so we can give them different colors in the scatterplot,### and visually identify themgrade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]# You will need to complete this function imported from the ClassifyNB script.# Be sure to change to that code tab to complete this quiz.clf = classify(features_train, labels_train)### draw the decision boundary with the text points overlaidprettyPicture(clf, features_test, labels_test)output_image(&quot;test.png&quot;, &quot;png&quot;, open(&quot;test.png&quot;, &quot;rb&quot;).read()) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TcSnd3_hAy8.mp4 Nature of Data one-hot encoding This article describes seven different possible encodings of categorical data. find the answer of One-Hot Encoding in forum that I have written Enron Email Dataset Data Types 1 - Numeric Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xmuWRRTPS4k.mp4 Data Types 2 - Categorical Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uc_FRItxRMs.mp4 Data Types 3 - Time Series Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6OmiG5zzZoA.mp4 Treatment of categorical dataMany algorithms assume that input data is numerical. For categorical data, this often means converting categorical data into numerical data that represents the same patterns. One standard way of doing this is with one-hot encoding. There are built-in methods for this in scikit-learn. Essentially, a categorical feature with 3 possible values is converted into three binary features corresponding to the values. The new feature corresponding to the category a datum belongs to has value 1, while the other new features have value 0. For example, in a dataset on baseball players, one feature might be “Handedness” which can take values “left” or “right”. Then the data: Joe Handedness: right Jim Handedness: leftWould become: Joe Handedness/right: 1 Handedness/left: 0 Jim Handedness/right: 0 Handedness/left: 1For ordinal data, it often makes sense to simply assign the values to integers. So the following data: JoeSkill: lowJimSkill: mediumJaneSkill: highWould become: JoeSkill: 0JimSkill: 1JaneSkill: 2These approaches are not all that is possible. However in general these simple approaches suffice, and if there is a reason to use another encoding that will be subject to the nature of the data. Encoding using sklearnEncoding in sklearn is done using the preprocessing module which comes with a variety of options of manipulating data before going into the analysis of data. We will focus on two forms of encoding for now, the LabelEncoder and the OneHotEncoder. Label EncoderFirst, we have to import the preprocessing library. from sklearn import preprocessing Let’s create a dummy dataframe named data with a column whose values we want to transform from categories to integers. # creating sample data sample_data = {&apos;name&apos;: [&apos;Ray&apos;, &apos;Adam&apos;, &apos;Jason&apos;, &apos;Varun&apos;, &apos;Xiao&apos;], &apos;health&apos;:[&apos;fit&apos;, &apos;slim&apos;, &apos;obese&apos;, &apos;fit&apos;, &apos;slim&apos;]} # storing sample data in the form of a dataframe data = pandas.DataFrame(sample_data, columns = [&apos;name&apos;, &apos;health&apos;]) We have 3 different labels that we are looking to categorize: slim, fit, obese. To do this, we will call LabelEncoder() and fit it to the column we are looking to categorize. label_encoder = preprocessing.LabelEncoder() label_encoder.fit(data[&apos;health&apos;]) Once you have fit the label encoder to the column you want to encode, you can then transform that column to integer data based on the categories found in that column. That can be done as follows: label_encoder.transform(data[&apos;health&apos;]) This will give you the output: array([0, 2, 1, 0, 2]) You can combine the fit and transform statements above by using label_encoder.fit_transform(data[&#39;health&#39;]). The string categorical health data has been mapped as follows: fit : 0 obese: 1 slim: 2 One thing to keep in mind when encoding data is the fact that you do not want to skew your analysis because of the numbers that are assigned to your categories. For example, in the above example, slim is assigned a value 2 and obese a value 1. This is not to say that the intention here is to have slim be a value that is empirically twice is likely to affect your analysis as compared to obese. In such situations it is better to one-hot encode your data as all categories are assigned a 0 or a 1 value thereby removing any unwanted biases that may creep in if you simply label encode your data. One-hot EncoderIf we were to apply the one-hot transformation to the same example we had above, we’d do it in Pandas using get_dummies as follows: pandas.get_dummies(data[&apos;health&apos;]) We could do this in sklearn on the label encoded data using OneHotEncoder as follows: ohe = preprocessing.OneHotEncoder() # creating OneHotEncoder object label_encoded_data = label_encoder.fit_transform(data[&apos;health&apos;]) ohe.fit_transform(label_encoded_data.reshape(-1,1)) One-Hot Encoding12345678910111213141516171819202122232425262728293031323334353637383940# In this exercise we&apos;ll load the titanic data (from Project 0)# And then perform one-hot encoding on the feature namesimport numpy as npimport pandas as pd# Load the datasetX = pd.read_csv(&apos;titanic_data.csv&apos;)# Limit to categorical dataX = X.select_dtypes(include=[object])from sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import OneHotEncoder# TODO: Create a LabelEncoder object, which will turn all labels present in# in each feature to numbers. For example, the labels [&apos;cat&apos;, &apos;dog&apos;, &apos;fish&apos;]# might be transformed into [0, 1, 2]le = LabelEncoder()# TODO: For each feature in X, apply the LabelEncoder&apos;s fit_transform# function, which will first learn the labels for the feature (fit)# and then change the labels to numbers (transform). for feature in X: le.fit(X[feature]) X[feature] = le.transform(X[feature])# TODO: Create a OneHotEncoder object, which will create a feature for each# label present in the data. For example, for a feature &apos;animal&apos; that had# the labels [&apos;cat&apos;,&apos;dog&apos;,&apos;fish&apos;], the new features (instead of &apos;animal&apos;) # could be [&apos;animal_cat&apos;, &apos;animal_dog&apos;, &apos;animal_fish&apos;]ohe = OneHotEncoder()# TODO: Apply the OneHotEncoder&apos;s fit_transform function to all of X, which will# first learn of all the (now numerical) labels in the data (fit), and then# change the data to one-hot encoded entries (transform).ohe.fit(X)onehotlabels = ohe.transform(X).toarray() Quiz: One-Hot EncodingHaving trouble? Here is a useful forum discussion about this quiz.Here are some other links you may find helpful - LabelEncoder, OneHotEncoder Time series data leakageWhen dealing with time-series data, it can be tempting to simply disregard the timing structure and simply treat it as the appropriate form of categorical or numerical data. One important concern, however, is that if you are building a predictive project looking at forecasting future data points. In this case, it is important NOT to use the future as a source of information! Since “hindsight is 20/20” and retrodictions are much easier than predictions, in predictive tasks it’s generally a good idea to use a training set made up of data from before a certain point, a validation set of data from some dates beyond that, and testing data leading up to the present. This way your algorithm won’t overfit by learning future trends. A Hands-on ExampleIn the next section we’ll explore the famous Enron Email Dataset which was the focus of much of the Introduction to Machine Learning course. While this specific dataset will play a less central role in this Nanodegree program, we will return to it a few times as an example to get practice with various techniques as they are introduced. You can download our copy of the dataset here, along with the starting code for a variety of mini-projects. None of these mini-projects are required for completing the Nanodegree program, but they are great practice! #### Datasets and QuestionsIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TdopVWltgqM.mp4 What Is A POIhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wDQhif-MWuY.mp4 EVALUATION AND VALIDATIONTraining &amp; Testing Train/Test Split in sklearn Benefits of Testinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7LGaeYfvRug.mp4 Features and LabelsAs you continue your journey into designing ML algorithms using sklearn, you will come across two new terms namely, Features and Labels. Features are individual measurable properties that you will be using to make predictions about your labels. To understand this better, let’s use an example. Let’s say you are trying to design a model that will be able to predict whether you will like a particular kind of cuisine or not. For this case, the label is a Yes for when the model thinks you will like said cuisine and No for when it thinks otherwise. The features here could be things like Sweetness, Spicyness, Bitterness, Tangyness and the like. One thing to note here is that when using our features we have to make sure that they are represented in a way that doesn’t skew one feature over another, in other words it’s usually a good idea to normalize or standardize your features; you will learn about these concepts in future lessons. For now, as long as you understand the premise of what features and labels are and how they are used, you can proceed to the next node where Sebastian will explain this concept using a visual example. Features and Labels Musical Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rnv0-lG9yKU.mp4 Evaluation Metrics Classification metrics Accuracy Confusion Matrix F1 ScoreF1 = 2 * (precision * recall) / (precision + recall) Regression metrics mean absolute error mean squared error R2 score explained variance score Welcome to Evaluation Metrics Lessonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IHuFWRM9f9Q.mp4 Overview of Lessonn this lesson we’ll look at a small selection of common performance metrics and evaluate some algorithms with them on the Titanic dataset you used earlier. There are a few important things to keep in mind here: There is a big difference in performance based on whether a train/test split is used. In general, performance on all metrics is correlated. But some algorithms may end up doing better or worse in different situations. The practical coding of any metric looks almost exactly the same! The difficulty comes in how to make the choice, not in how to implement it. The topics covered in this lesson are: Accuracy Precision Recall Confusion Matrix F1 score Mean Absolute Error Mean Squared Error If you are familiar with these concepts you can skip ahead, but we do recommend completing this lesson as a refresher nonetheless. MANAGING ERROR AND COMPLEXITYCauses of Error matplotlib.pyplot.plot 12plt.plot(training_sizes,training_scores,&apos;go&apos;,label=&apos;training_scores&apos;)plt.plot(training_sizes,testing_scores,&apos;rs&apos;,label=&apos;testing_scores&apos;) Bias-Variance Tradeoff Causes of ErrorNow that we have covered some basic metrics for measuring model performance, let us turn our attention to reasons why models exhibit errors in the first place. In model prediction there are two main sources of errors that a model can suffer from. BiasBias due to a model being unable to represent the complexity of the underlying data. A high Bias model is said to underfit the data. VarianceVariance due to a model being overly sensitive to the limited data it has been trained on. A high Variance model is said to overfit the data. In the coming videos, we will go over each in detail. Error due to BiasError due to Bias - Accuracy and UnderfittingBias occurs when a model has enough data but is not complex enough to capture the underlying relationships. As a result, the model consistently and systematically misrepresents the data, leading to low accuracy in prediction. This is known as underfitting. Simply put, bias occurs when we have an inadequate model. Example 1An example might be when we have objects that are classified by color and shape, for example easter eggs, but our model can only partition and classify objects by color. It would therefore consistently mislabel future objects–for example labeling rainbows as easter eggs because they are colorful. Example 2Another example would be continuous data that is polynomial in nature, with a model that can only represent linear relationships. In this case it does not matter how much data we feed the model because it cannot represent the underlying relationship. To overcome error from bias, we need a more complex model. Error due to VarianceError due to Variance - Precision and OverfittingWhen training a model, we typically use a limited number of samples from a larger population. If we repeatedly train a model with randomly selected subsets of data, we would expect its predictons to be different based on the specific examples given to it. Here variance is a measure of how much the predictions vary for any given test sample. Some variance is normal, but too much variance indicates that the model is unable to generalize its predictions to the larger population. High sensitivity to the training set is also known as overfitting, and generally occurs when either the model is too complex or when we do not have enough data to support it. We can typically reduce the variability of a model’s predictions and increase precision by training on more data. If more data is unavailable, we can also control variance by limiting our model’s complexity. Learning CurveLearning CurveNow that you have understood the Bias and Variance concepts let us learn about ways we can identify when our model performs well. The Learning Curve functionality from sklearn can help us in this respect. It allows us to study the behavior of our model with respect to the number of data points being considered to understand if our model is performing well or not. To start with , we have to import the module: from sklearn.learning_curve import learning_curve # sklearn 0.17 from sklearn.model_selection import learning_curve # sklearn 0.18 From the documentation, a reasonable implementation of the function would be as follows: learning_curve( estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) Here, estimator is the model which we are using to make our predictions with, for example it could be defined as GaussianNB(), X and y are the features and label respectively, cv is the cross validation generator, for example KFold(), n_jobs is the parameter that decides if we want to run multiple operations in parallel and train_sizes is the number of training examples that will be considered to generate the curve. In the following quiz, you will define your learning curve for a model that we have designed for you and you will observe the results. Noisy Data, Complex ModelHere’s the code123456789101112131415161718192021222324252627282930313233343536373839404142# In this exercise we&apos;ll examine a learner which has high variance, and tries to learn# nonexistant patterns in the data.# Use the learning curve function from sklearn.learning_curve to plot learning curves# of both training and testing error.from sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltfrom sklearn.learning_curve import learning_curvefrom sklearn.cross_validation import KFoldfrom sklearn.metrics import explained_variance_score, make_scorerimport numpy as np# Set the learning curve parameters; you&apos;ll need this for learning_curvessize = 1000cv = KFold(size,shuffle=True)score = make_scorer(explained_variance_score)# Create a series of data that forces a learner to have high varianceX = np.round(np.reshape(np.random.normal(scale=5,size=2*size),(-1,2)),2)y = np.array([[np.sin(x[0]+np.sin(x[1]))] for x in X])def plot_curve(): reg = DecisionTreeRegressor() reg.fit(X,y) print &quot;Regressor score: &#123;:.4f&#125;&quot;.format(reg.score(X,y)) # TODO: Use learning_curve imported above to create learning curves for both the # training data and testing data. You&apos;ll need &apos;size&apos;, &apos;cv&apos; and &apos;score&apos; from above. training_sizes, training_scores, testing_scores = learning_curve(DecisionTreeRegressor(),X,y, cv=cv, scoring=score) # TODO: Plot the training curves and the testing curves # Use plt.plot twice -- one for each score. Be sure to give them labels! plt.plot(training_sizes,training_scores,&apos;go&apos;,label=&apos;training_scores&apos;) plt.plot(training_sizes,testing_scores,&apos;rs&apos;,label=&apos;testing_scores&apos;) # Plot aesthetics plt.ylim(-0.1, 1.1) plt.ylabel(&quot;Curve Score&quot;) plt.xlabel(&quot;Training Points&quot;) plt.legend(bbox_to_anchor=(1.1, 1.1)) plt.show() Improving the Validity of a ModelThere is a trade-off in the value of simplicity or complexity of a model given a fixed set of data. If it is too simple, our model cannot learn about the data and misrepresents the data. However if our model is too complex, we need more data to learn the underlying relationship. Otherwise it is very common for a model to infer relationships that might not actually exist in the data. The key is to find the sweet spot that minimizes bias and variance by finding the right level of model complexity. Of course with more data any model can improve, and different models may be optimal. To learn more about bias and variance, we recommend this essay by Scott Fortmann-Roe. In addition to the subset of data chosen for training, what features you use from a given dataset can also greatly affect the bias and variance of your model. Bias, Variance, and Number of Featureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OurfO1ZR2GU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mpYpT6nZVEo.mp4 Bias, Variance &amp; Number of Features Pt 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1lNAvDubBfI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/X_AS8NBngsk.mp4 Overfitting by Eyehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sJgPnuiHrs8.mp4 Representative Power of a ModelIntroductionCurse of DimensionalityIn this short lesson, we have Charles Isbell, Senior Associate Dean at Georgia Tech School of Computing and Michael Littman, former CS department chair at Rutgers University and current Professor at Brown University teach you about the curse of dimensionality. These videos are from the OMSCS program at Georgia Tech. Curse of Dimensionalityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QZ0DtNFdDko.mp4 Curse of Dimensionality Twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OyPcbeiwps8.mp4 MODEL EVALUATION AND VALIDATION PROJECTPredicting Boston Housing prices.OverviewProject OverviewIn this project, you will apply basic machine learning concepts on data collected for housing prices in the Boston, Massachusetts area to predict the selling price of a new home. You will first explore the data to obtain important features and descriptive statistics about the dataset. Next, you will properly split the data into testing and training subsets, and determine a suitable performance metric for this problem. You will then analyze performance graphs for a learning algorithm with varying parameters and training set sizes. This will enable you to pick the optimal model that best generalizes for unseen data. Finally, you will test this optimal model on a new sample and compare the predicted selling price to your statistics. Project HighlightsThis project is designed to get you acquainted to working with datasets in Python and applying basic machine learning techniques using NumPy and Scikit-Learn. Before being expected to use many of the available algorithms in the sklearn library, it will be helpful to first practice analyzing and interpreting the performance of your model. Things you will learn by completing this project: How to use NumPy to investigate the latent features of a dataset. How to analyze various learning performance plots for variance and bias. How to determine the best-guess model for predictions from unseen data. How to evaluate a model’s performance on unseen data using previous data. Software RequirementsDescriptionThe Boston housing market is highly competitive, and you want to be the best real estate agent in the area. To compete with your peers, you decide to leverage a few basic machine learning concepts to assist you and a client with finding the best selling price for their home. Luckily, you’ve come across the Boston Housing dataset which contains aggregated data on various features for houses in Greater Boston communities, including the median value of homes for each of those areas. Your task is to build an optimal model based on a statistical analysis with the tools available. This model will then be used to estimate the best selling price for your clients’ homes. Software and LibrariesThis project uses the following software and Python libraries: Python 2.7 NumPy pandas scikit-learn (v0.17) matplotlib Jupyter Notebook If you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. Starting the ProjectFor this assignment, you can find the boston_housing folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three files: boston_housing.ipynb: This is the main file where you will be performing your work on the project. housing.csv: The project dataset. You’ll load this data in the notebook. visuals.py: This Python script contains helper functions that create the necessary visualizations. In the Terminal or Command Prompt, navigate to the folder containing the project files, and then use the command jupyter notebook boston_housing.ipynb to open up a browser window or tab to work with your notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the project. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Predicting Boston Housing Prices project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named boston_housing for ease of access: The boston_housing.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionPredicting Boston Housing PricesThe Boston housing market is highly competitive, and you want to be the best real estate agent in the area. To compete with your peers, you decide to leverage a few basic machine learning concepts to assist you and a client with finding the best selling price for their home. Luckily, you’ve come across the Boston Housing dataset which contains aggregated data on various features for houses in Greater Boston communities, including the median value of homes for each of those areas. Your task is to build an optimal model based on a statistical analysis with the tools available. This model will then be used to estimate the best selling price for your clients’ homes. Project FilesFor this assignment, you can find the boston_housing folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Predicting Boston Housing Prices project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named boston_housing for ease of access: The boston_housing.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! PROJECT Predicting Boston Housing Prices project rubric open jupyterwithWindows + rtypecd &lt;path&gt;``&lt;path&gt; Question 9 Do not forget to modify the parameter cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)to fit 0.17 version Project modification statistics with numpy second submit@viadanna third submit Kaggle(stay tuned)Career: Job Search StrategiesOpportunity can come when you least expect it, so when your dream job comes along, you want to be ready! After completing these lessons, be sure to complete the Cover Letter Review project and 1 of the 3 Resume Review projects. If you are a Nanodegree Plus student, Career Content and Career Development Projects are required for graduation. If you are enrolled in a standard Nanodegree program, Career Content and Career Development Projects are optional and do not affect your graduation. JOB SEARCH STRATEGIESBuild Your Resume Cover LetterResume Review (Entry-level)Resume Review (Career Change)Resume Review (Prior Industry Experience)Cover Letter ReviewConduct a Job SearchIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/axcFtHK6If4.mp4 NVIDIAhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/C6Rt9lxMqHs.mp4 Job Search Mindsethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cBk7bno3KS0.mp4 Target Your Application to An Employerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/X9JBzbrkcvs.mp4 Open Yourself Up to Opportunityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1OamTNkk1xM.mp4 Refine Your ResumeConvey Your Skills Conciselyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xnQr3ohml9s.mp4 Effective Resume Componentshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AiFcaHRGdEA.mp4 Resume Structurehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/POM0MqLTj98.mp4 Describe Your Work Experienceshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B1LED4txinI.mp4Description bullet points should convey: Action Numbers Success Resume Reflectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8Cj_tCp8mls.mp4 Resume Reviewhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L3F2BFGYMtI.mp4Types of Resume FormatsResume formats can be split into three categories, depending on the job candidate: Entry-level (about 0-3 years of experience) Career Change (3+ years of work experience, looking to change career paths) Prior Industry Experience (3+ years of work experience; looking to level up in their career path by upskilling) Build Your ResumeResumes are required in job applications and recruiting. The most effective resumes are ones aimed at a specific job. In this project, you will find a job posting and target your resume to those job duties and requirements. Once you complete this project, you’ve successfully applied targeted job search strategies and are ready to look for work! Receive a review of your resume that is tailored to your level of experience. The format and organization of your resume will vary dependent on your experience in the field. To ensure you’re highlighting your most relevant skills, you will submit your resume to 1 of 3 review projects that match your experience level best. Entry-level: For entry-level job applicants with 0-3 years experience in the field. Best suited for applicants who have recently graduated from their formal education and have limited work experience. Career Change: For those seeking a career change with 3+ years experience in an unrelated field. For example, if you’re a teacher looking for work as a data analyst, or even from project management to front-end development. Prior Industry Experience: For applicants with 3+ years of prior experience in a related field. This would include those with experience in software development looking for work in mobile development, or even from data science to machine learning. Project Resources Project Rubrics: Your project will be reviewed by a Udacity Career Reviewer against these rubrics. Entry-level Rubric Career Change Rubric Prior Industry Experience Rubric Project Checklists: Based on the project rubric, this is a handy checklist to use during your resume building. Entry-level Checklist Career Change Checklist Prior Industry Experience Checklist Career Resource Center: Find additional tips and guides on developing your resume. Resume Template Options* Build your own! This will ensure your resume is unique. * [Resume Genius: Resume Templates](https://resumegenius.com/resume-templates) * [Resume Builder](https://www.livecareer.com/resume-builder) Tips for Bullet Points Describe the following in your projects and experiences bullet points: Action Numbers Results UC Berkeley Action Verb List for Resumes &amp; Cover Letters Submit Your Resume for ReviewSubmission Instructions Find a job posting that you would apply to now or after your Nanodegree graduation. Judge if you would be a good fit for the role. (Note: If you’re more than 75% qualified for the job on paper, you’re probably a good candidate and should give applying a shot!) Refine your resume to target it to that job posting. Copy and paste, or link, the job posting in “Notes to reviewer” during submission. Optional: Remove any sensitive information, such as your phone number, from the submission. Submit your targeted resume as a .pdf to one of the following project submission pages dependent on your experience: Entry-level Project Submission Career Change Project Submission Prior Industry Experience Project Submission Share your Resume with Udacity Hiring PartnersUdacity partners with employers, who are able to contact Udacity students and alumni via your Professional Profile. Once you’ve completed the resume review project, make sure to upload your reviewed resume to your Profile! #### Supervised LearningLearn how Supervised Learning models such as Decision Trees, SVMs, Neural Networks, etc. are trained to model and predict labeled data. Project: Finding Donors for CharityML For most students, this project takes approximately 8 - 21 hours to complete (about 1 - 3 weeks).17 LESSONS, 1 PROJECT P2 Finding Donors for CharityML SUPERVISED LEARNING TASKS polyfit sklearn.preprocessing.PolynomialFeatures DECISION TREESID3(stay tuned) ARTIFICIAL NEURAL NETWORKSSUPPORT VECTOR MACHINESNONPARAMETRIC MODELSBAYESIAN METHODS sklearn.naive_bayes.GaussianNB sklearn.metrics.accuracy_score this Kaggle project Joint Distribution Analysis(stay tuned) ENSEMBLE OF LEARNERS weak leanrner boosting-survey INTRODUCTION TO SUPERVISED LEARNINGSupervised Learning IntroSupervised Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Vc6KuGcfVPM.mp4 What You’ll Watch and Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zoN8QJYFka4.mp4 ML in The Google Self-Driving Carhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lL16AQItG1g.mp4 Supervised Learning What You’ll Dohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jgYqzo7UFsU.mp4 Acerous Vs. Non-Aceroushttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TeFF9wXiFfs.mp4 Supervised Classification Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/buxApBhZCO0.mp4 Features and Labels Musical Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rnv0-lG9yKU.mp4 Features Visualization Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t0iflCpBUDA.mp4 Classification By Eyehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xeMDpSRTLWc.mp4 Introduction to RegressionMore RegressionsIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_CznJ6phPsg.mp4 Parametric regressionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6EC1w_fs5u8.mp4 K nearest neighborhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CWCLQ6eu2Do.mp4 How to predicthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/go7ITLl79h8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/r8PsDjf9scc.mp4 Kernel regressionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ZhJTGBbR18o.mp4 Parametric vs non parametrichttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wKT8Ztzt6r0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PVOWHYJV8P4.mp4 Which problems are regression? Are Polynomials Linear? Regressions in sklearnContinuous Output Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/udJvijJvs1M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FOwEL4S-SVo.mp4 Continuous Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Bp6oBbLw8qE.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IC-fo_A0PxQ.mp4 DECISION TREESDecision TreesDifference between Classification and Regressionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i04Pfrb71vk.mp4 More Decision TreeLinearly Separable Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lCWGV6ZuXt0.mp4 Multiple Linear Questionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t1Y-nzgI1L4.mp4 ARTIFICIAL NEURAL NETWORKSNeural NetworksNeural Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L_6idb3ZXB0.mp4 Neural Nets Mini-ProjectIntroductionThis section has a mix of coding assignments, multiple choice questions and fill in the blank type questions. Please do check the instructor notes as we have included relevant forum posts that will help you work through these problems. You can find the instructor notes below the text/video nodes in the classroom. Build a Perceptron12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# ----------# # In this exercise, you will put the finishing touches on a perceptron class.## Finish writing the activate() method by using np.dot to compute signal# strength and then add in a threshold for perceptron activation.## ----------import numpy as npclass Perceptron: &quot;&quot;&quot; This class models an artificial neuron with step activation function. &quot;&quot;&quot; def __init__(self, weights = np.array([1]), threshold = 0): &quot;&quot;&quot; Initialize weights and threshold based on input arguments. Note that no type-checking is being performed here for simplicity. &quot;&quot;&quot; self.weights = weights self.threshold = threshold def activate(self,inputs): &quot;&quot;&quot; Takes in @param inputs, a list of numbers equal to length of weights. @return the output of a threshold perceptron with given inputs based on perceptron weights and threshold. &quot;&quot;&quot; # INSERT YOUR CODE HERE # TODO: calculate the strength with which the perceptron fires strength = np.dot(inputs,self.weights) # TODO: return 0 or 1 based on the threshold if strength&gt;self.threshold: result = 1 else: result = 0 return resultdef test(): &quot;&quot;&quot; A few tests to make sure that the perceptron class performs as expected. Nothing should show up in the output if all the assertions pass. &quot;&quot;&quot; p1 = Perceptron(np.array([1, 2]), 0.) assert p1.activate(np.array([ 1,-1])) == 0 # &lt; threshold --&gt; 0 assert p1.activate(np.array([-1, 1])) == 1 # &gt; threshold --&gt; 1 assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --&gt; 0if __name__ == &quot;__main__&quot;: test() Here is relevant forum post for this quiz. Note that here, and the rest of the mini-project, that signal strength equal to the threshold results in a 0 being output (rather than 1). It is required that the dot product be strictly greater than the threshold, rather than greater than or equal to the threshold, to pass the assertion tests. Threshold Meditation Where to train Perceptrons Perceptron Inputs Neural Net Outputs Perceptron Update Rule123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# ----------## In this exercise, you will update the perceptron class so that it can update# its weights.## Finish writing the update() method so that it updates the weights according# to the perceptron update rule. Updates should be performed online, revising# the weights after each data point.# # ----------import numpy as npclass Perceptron: &quot;&quot;&quot; This class models an artificial neuron with step activation function. &quot;&quot;&quot; def __init__(self, weights = np.array([1]), threshold = 0): &quot;&quot;&quot; Initialize weights and threshold based on input arguments. Note that no type-checking is being performed here for simplicity. &quot;&quot;&quot; self.weights = weights.astype(float) self.threshold = threshold def activate(self, values): &quot;&quot;&quot; Takes in @param values, a list of numbers equal to length of weights. @return the output of a threshold perceptron with given inputs based on perceptron weights and threshold. &quot;&quot;&quot; # First calculate the strength with which the perceptron fires strength = np.dot(values,self.weights) # Then return 0 or 1 depending on strength compared to threshold return int(strength &gt; self.threshold) def update(self, values, train, eta=.1): &quot;&quot;&quot; Takes in a 2D array @param values consisting of a LIST of inputs and a 1D array @param train, consisting of a corresponding list of expected outputs. Updates internal weights according to the perceptron training rule using these values and an optional learning rate, @param eta. &quot;&quot;&quot; # YOUR CODE HERE #self.weights = np.transpose(np.zeros(values.shape)) # TODO: for each data point... for i in range(len(values)): # TODO: obtain the neuron&apos;s prediction for that point prediction = self.activate(values[i]) # TODO: update self.weights based on prediction accuracy, learning # rate and input value self.weights += eta * (train[i] - prediction) * values[i] print values print self.weightsdef test(): &quot;&quot;&quot; A few tests to make sure that the perceptron class performs as expected. Nothing should show up in the output if all the assertions pass. &quot;&quot;&quot; def sum_almost_equal(array1, array2, tol = 1e-6): return sum(abs(array1 - array2)) &lt; tol p1 = Perceptron(np.array([1,1,1]),0) p1.update(np.array([[2,0,-3]]), np.array([1])) assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7])) p2 = Perceptron(np.array([1,2,3]),0) p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0])) assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9])) p3 = Perceptron(np.array([3,0,2]),0) p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0])) assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))if __name__ == &quot;__main__&quot;: test() This is relevant forum post for this quiz. Layered Network Example Linear Representational Power Activation Function Quiz Perceptron Vs Sigmoid Sigmoid Learning Gradient Descent Issues SUPPORT VECTOR MACHINESMath behind SVMsIntroductionIn this lesson, Charles and Mike will walk you through the Math behind Support Vector Machines. If you would like to jump straight to the higher level concepts and start coding it up using scikit-learn, you can head to the next lesson where Sebastian and Katie will walk you through everything you will need to get up and running with working SVM model. The Best Linehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5yzSv4jYMyI.mp4 SVMs in PracticeWelcome to SVMhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gnAmmyQ_ZcQ.mp4 Separating Linehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mzKPXz-Yhwk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NTm_mA4akP4.mp4 NONPARAMETRIC MODELSInstance Based LearningInstance Based Learning Beforehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ZTjot416e-g.mp4 BAYESIAN METHODSNaive BayesSpeed Scatterplot: Grade and Bumpinesshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IMWsjjIeOrY.mp4 Bayesian LearningIntrohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PfJoHBLjkR8.mp4 Bayes Rulehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kv38EnIXSkY.mp4 Bayesian InferenceIntrohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AL9LH06uztM.mp4 Joint Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RN7drTE2_oI.mp4 Bayes NLP Mini-ProjectBad Handwriting ExpositionImagine your boss has left you a message from a location with terrible reception. Several words are impossible to hear. Based on some transcriptions of previous messages he’s left, you want to fill in the remaining words. To do this, we will use Bayes’ Rule to find the probability that a given word is in the blank, given some other information about the message. Recall Bayes Rule: P(A|B) = P(B|A)*P(A)/P(B) Or in our case P(a certain word|surrounding words) = P(surrounding words|a certain word)*P(a certain word) / P(surrounding words) Calculations Maximum LikelihoodHere’s the code12345678910111213141516171819202122232425262728293031323334sample_memo = &apos;&apos;&apos;Milt, we&apos;re gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.Oh, oh, and I almost forgot. Ahh, I&apos;m also gonna need you to go ahead and come in on Sunday, too...Hello Peter, whats happening? Ummm, I&apos;m gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I&apos;m also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.&apos;&apos;&apos;## Maximum Likelihood Hypothesis### In this quiz we will find the maximum likelihood word based on the preceding word## Fill in the NextWordProbability procedure so that it takes in sample text and a word,# and returns a dictionary with keys the set of words that come after, whose values are# the number of times the key comes after that word.# # Just use .split() to split the sample_memo text into words separated by spaces.def NextWordProbability(sampletext,word): wordlist = sampletext.split() if word in wordlist: indecies = [i for i,x in enumerate(wordlist) if x == word] else: pass indecies_after = [i+1 for i in indecies] newwordlist = [wordlist[i] for i in indecies_after] wordcount = &#123;&#125; for word in newwordlist: if word in wordcount: wordcount[word] += 1 else: wordcount[word] = 1 return wordcount NLP DisclaimerIn the previous exercise, you may have thought of some ways we might want to clean up the text available to us. For example, we would certainly want to remove punctuation, and generally want to make all strings lowercase for consistency. In most language processing tasks we will have a much larger corpus of data, and will want to remove certain features. Overall, just keep in mind that this mini-project is about Bayesian probability. If you’re interested in the details of language processing, you might start with this Kaggle project, which introduces a more detailed and standard approach to text processing very different from what we cover here. Optimal Classifier Example Optimal Classifier ExerciseHere’s the code123456789101112131415161718192021222324252627282930313233343536373839404142434445#------------------------------------------------------------------## Bayes Optimal Classifier## In this quiz we will compute the optimal label for a second missing word in a row# based on the possible words that could be in the first blank## Finish the procedurce, LaterWords(), below## You may want to import your code from the previous programming exercise!#sample_memo = &apos;&apos;&apos;Milt, we&apos;re gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.Oh, oh, and I almost forgot. Ahh, I&apos;m also gonna need you to go ahead and come in on Sunday, too...Hello Peter, whats happening? Ummm, I&apos;m gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I&apos;m also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.&apos;&apos;&apos;corrupted_memo = &apos;&apos;&apos;Yeah, I&apos;m gonna --- you to go ahead --- --- complain about this. Oh, and if you could --- --- and sit at the kids&apos; table, that&apos;d be --- &apos;&apos;&apos;data_list = sample_memo.strip().split()words_to_guess = [&quot;ahead&quot;,&quot;could&quot;]def LaterWords(sample,word,distance): &apos;&apos;&apos;@param sample: a sample of text to draw from @param word: a word occuring before a corrupted sequence @param distance: how many words later to estimate (i.e. 1 for the next word, 2 for the word after that) @returns: a single word which is the most likely possibility &apos;&apos;&apos; # TODO: Given a word, collect the relative probabilities of possible following words # from @sample. You may want to import your code from the maximum likelihood exercise. # TODO: Repeat the above process--for each distance beyond 1, evaluate the words that # might come after each word, and combine them weighting by relative probability # into an estimate of what might appear next. return &#123;&#125; print LaterWords(sample_memo,&quot;ahead&quot;,2) Which Words Meditation Joint Distribution Analysis Domain Knowledge Quiz Domain Knowledge Fill In ENSEMBLE OF LEARNERSEnsemble B&amp;BEnsemble Learning Boostinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/w75WyRjRpAg.mp4 Back to Boostinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PHBd2glewzM.mp4 Boosting Tends to Overfithttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UHxYXwvjH5c.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Hp4gJjSFSYc.mp4 SUPERVISED LEARNING PROJECTFinding donors for CharityMLOverviewProject OverviewIn this project, you will apply supervised learning techniques and an analytical mind on data collected for the U.S. census to help CharityML (a fictitious charity organization) identify people most likely to donate to their cause. You will first explore the data to learn how the census data is recorded. Next, you will apply a series of transformations and preprocessing techniques to manipulate the data into a workable format. You will then evaluate several supervised learners of your choice on the data, and consider which is best suited for the solution. Afterwards, you will optimize the model you’ve selected and present it as your solution to CharityML. Finally, you will explore the chosen model and its predictions under the hood, to see just how well it’s performing when considering the data it’s given. Project HighlightsThis project is designed to get you acquainted with the many supervised learning algorithms available in sklearn, and to also provide for a method of evaluating just how each model works and performs on a certain type of data. It is important in machine learning to understand exactly when and where a certain algorithm should be used, and when one should be avoided. Things you will learn by completing this project: How to identify when preprocessing is needed, and how to apply it. How to establish a benchmark for a solution to the problem. What each of several supervised learning algorithms accomplishes given a specific dataset. How to investigate whether a candidate solution model is adequate for the problem. Software RequirementsDescriptionCharityML is a fictitious charity organization located in the heart of Silicon Valley that was established to provide financial support for people eager to learn machine learning. After nearly 32,000 letters sent to people in the community, CharityML determined that every donation they received came from someone that was making more than $50,000 annually. To expand their potential donor base, CharityML has decided to send letters to residents of California, but to only those most likely to donate to the charity. With nearly 15 million working Californians, CharityML has brought you on board to help build an algorithm to best identify potential donors and reduce overhead cost of sending mail. Your goal will be evaluate and optimize several different supervised learners to determine which algorithm will provide the highest donation yield while also reducing the total number of letters being sent. Software and LibrariesThis project uses the following software and Python libraries: Python 2.7NumPypandasscikit-learn (v0.17)matplotlibYou will also need to have software installed to run and execute a Jupyter Notebook. If you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. Starting the ProjectFor this assignment, you can find the finding_donors folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three files: finding_donors.ipynb: This is the main file where you will be performing your work on the project. census.csv: The project dataset. You?ll load this data in the notebook. visuals.py: This Python script provides supplementary visualizations for the project. Do not modify. In the Terminal or Command Prompt, navigate to the folder containing the project files, and then use the command jupyter notebook finding_donors.ipynb to open up a browser window or tab to work with your notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the project. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Finding Donors for CharityML project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named finding_donors for ease of access: The finding_donors.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionFinding Donors for CharityMLCharityML is a fictitious charity organization located in the heart of Silicon Valley that was established to provide financial support for people eager to learn machine learning. After nearly 32,000 letters sent to people in the community, CharityML determined that every donation they received came from someone that was making more than $50,000 annually. To expand their potential donor base, CharityML has decided to send letters to residents of California, but to only those most likely to donate to the charity. With nearly 15 million working Californians, CharityML has brought you on board to help build an algorithm to best identify potential donors and reduce overhead cost of sending mail. Your goal will be evaluate and optimize several different supervised learners to determine which algorithm will provide the highest donation yield while also reducing the total number of letters being sent. Project FilesFor this assignment, you can find the finding_donors folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Finding Donors for CharityML project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named finding_donors for ease of access: The finding_donors.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated.I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! PROJECT windows + r cd + &lt;path&gt; my path is G:\Udacity\MLND\machine-learning-master\projects\finding_donors jupyter notebook finding_donors.ipynb Finding Donors for CharityML project rubric pandas.get_dummies() reviews submit second submit in review second review third review Unsupervised LearningLearn how to find patterns and structures in unlabeled data, perform feature transformations and improve the predictive performance of your models.Project: Creating Customer SegmentsFor most students, this project takes approximately 10 - 15 hours to complete (about 1 - 2 weeks).P3 Creating Customer Segments CLUSTERING play with k-means clustering sklearn.cluster.KMeans Expectation Maximization The Enron dataset sklearn.preprocessing.MinMaxScaler Introduction to Unsupervised LearningUnsupervised Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8oZpT6Hekhk.mp4 What You’ll Watch and Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1a68kAJAgIU.mp4 ClusteringUnsupervised Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Mx9f99bRB3Q.mp4 Clustering Movieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/g8PKffm8IRY.mp4 More ClusteringSingle Linkage Clusteringhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HfikjFVM3dg.mp4Quiz: Single Linkage ClusteringPlease use a comma to separate the two objects that will be linked in your answer. For instance, to describe a link from a to b, write “a,b” as your answer in the box. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vytc9CsjjAs.mp4 Single Linkage Clustering Twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aojgUed9M0w.mp4 Clustering Mini-ProjectClustering Mini-Project Videohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/68EGMItJiNM.mp4 K-Means Clustering Mini-ProjectIn this project, we’ll apply k-means clustering to our Enron financial data. Our final goal, of course, is to identify persons of interest; since we have labeled data, this is not a question that particularly calls for an unsupervised approach like k-means clustering. Nonetheless, you’ll get some hands-on practice with k-means in this project, and play around with feature scaling, which will give you a sneak preview of the next lesson’s material.The Enron dataset can be found here. Clustering FeaturesThe starter code can be found in k_means/k_means_cluster.py, which reads in the email + financial (E+F) dataset and gets us ready for clustering. You’ll start with performing k-means based on just two financial features–take a look at the code, and determine which features the code uses for clustering. Run the code, which will create a scatterplot of the data. Think a little bit about what clusters you would expect to arise if 2 clusters are created. Deploying ClusteringDeploy k-means clustering on the financial_features data, with 2 clusters specified as a parameter. Store your cluster predictions to a list called pred, so that the Draw() command at the bottom of the script works properly. In the scatterplot that pops up, are the clusters what you expected? FEATURE ENGINEERINGFeature ScalingChris’s T-Shirt Size (Intuition)https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oaqjLyiKOIA.mp4 A Metric for Chrishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/O0bvLU4l0is.mp4 Feature SelectionIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UAMwTr3cnok.mp4 Feature Selectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8CpRLplmdqE.mp4 DIMENSIONALITY REDUCTION sklearn.decomposition.PCA This paper gives a fairly in-depth look at how the ICA algorithm works. It’s long, but comprehensive Cocktail Party Demo PCAData Dimensionalityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gg7SAMMl4kM.mp4 Trickier Data Dimensionalityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-dcNhrSPmoY.mp4 PCA Mini-ProjectPCA Mini-Project Introhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rR68JXwKBxE.mp4 PCA Mini-ProjectOur discussion of PCA spent a lot of time on theoretical issues, so in this mini-project we’ll ask you to play around with some sklearn code. The eigenfaces code is interesting and rich enough to serve as the testbed for this entire mini-project. The starter code can be found in pca/eigenfaces.py. This was mostly taken from the example found here, on the sklearn documentation.Take note when running the code, that there are changes in one of the parameters for the SVC function called on line 94 of pca/eigenfaces.py. For the ‘class_weight’ parameter, the argument string “auto” is a valid value for sklearn version 0.16 and prior, but will be depreciated by 0.19. If you are running sklearn version 0.17 or later, the expected argument string should be “balanced”. If you get an error or warning when running pca/eigenfaces.py, make sure that you have the correct argument on line 98 that matches your installed version of sklearn. Feature TransformationIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/J9JsMNownYM.mp4 Feature Transformationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B6mPphwAXZk.mp4 SummaryWhat we have learnedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/74oyGTdFp0Y.mp4 UNSUPERVISED LEARNING PROJECTThings you will learn by completing this project: How to apply preprocessing techniques such as feature scaling and outlier detection. How to interpret data points that have been scaled, transformed, or reduced from PCA. How to analyze PCA dimensions and construct a new feature space. How to optimally cluster a set of data to find hidden patterns in a dataset. How to assess information given by cluster data and use it in a meaningful way. jupyter notebook customer_segments.ipynbCreating Customer Segments project rubricsubmit projectreviewsecond reviewseaborn.heatmap() git project: opengit bashand tpyecd &lt;path&gt;with\before space key and/between directory git init git status git add &lt;&gt; git commmit -m &quot;description&quot; create a repo in github git remote add origin URL git push origin master git push origin master git add &lt;&gt;``git commit -m &quot;second submit&quot;``git push origin master Identifying customers by clustering them.OverviewProject OverviewIn this project you will apply unsupervised learning techniques on product spending data collected for customers of a wholesale distributor in Lisbon, Portugal to identify customer segments hidden in the data. You will first explore the data by selecting a small subset to sample and determine if any product categories highly correlate with one another. Afterwards, you will preprocess the data by scaling each product category and then identifying (and removing) unwanted outliers. With the good, clean customer spending data, you will apply PCA transformations to the data and implement clustering algorithms to segment the transformed customer data. Finally, you will compare the segmentation found with an additional labeling and consider ways this information could assist the wholesale distributor with future service changes. Project HighlightsThis project is designed to give you a hands-on experience with unsupervised learning and work towards developing conclusions for a potential client on a real-world dataset. Many companies today collect vast amounts of data on customers and clientele, and have a strong desire to understand the meaningful relationships hidden in their customer base. Being equipped with this information can assist a company engineer future products and services that best satisfy the demands or needs of their customers. Things you will learn by completing this project: How to apply preprocessing techniques such as feature scaling and outlier detection. How to interpret data points that have been scaled, transformed, or reduced from PCA. How to analyze PCA dimensions and construct a new feature space. How to optimally cluster a set of data to find hidden patterns in a dataset. How to assess information given by cluster data and use it in a meaningful way. Software RequirementsDescriptionA wholesale distributor recently tested a change to their delivery method for some customers, by moving from a morning delivery service five days a week to a cheaper evening delivery service three days a week. Initial testing did not discover any significant unsatisfactory results, so they implemented the cheaper option for all customers. Almost immediately, the distributor began getting complaints about the delivery service change and customers were canceling deliveries — losing the distributor more money than what was being saved. You’ve been hired by the wholesale distributor to find what types of customers they have to help them make better, more informed business decisions in the future. Your task is to use unsupervised learning techniques to see if any similarities exist between customers, and how to best segment customers into distinct categories. Software and LibrariesThis project uses the following software and Python libraries: Python 2.7 NumPy pandas scikit-learn (v0.17) matplotlib You will also need to have software installed to run and execute a Jupyter Notebook. If you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. Starting the ProjectFor this assignment, you can find the customer_segments folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three files: customer_segments.ipynb: This is the main file where you will be performing your work on the project. customers.csv: The project dataset. You’ll load this data in the notebook. visuals.py: This Python script provides supplementary visualizations for the project. Do not modify. In the Terminal or Command Prompt, navigate to the folder containing the project files, and then use the command jupyter notebook customer_segments.ipynb to open up a browser window or tab to work with your notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the project. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Creating Customer Segments project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named customer_segments for ease of access: The customer_segments.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionCreating Customer SegmentsA wholesale distributor recently tested a change to their delivery method for some customers, by moving from a morning delivery service five days a week to a cheaper evening delivery service three days a week.Initial testing did not discover any significant unsatisfactory results, so they implemented the cheaper option for all customers. Almost immediately, the distributor began getting complaints about the delivery service change and customers were canceling deliveries — losing the distributor more money than what was being saved. You’ve been hired by the wholesale distributor to find what types of customers they have to help them make better, more informed business decisions in the future. Your task is to use unsupervised learning techniques to see if any similarities exist between customers, and how to best segment customers into distinct categories. Project FilesFor this assignment, you can find the customer_segments folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Creating Customer Segments project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named customer_segments for ease of access: The customer_segments.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! Supporting MaterialsVideos Zip FileCareer: NetworkingIn the following lesson, you will learn how tell your unique story to recruiters in a succinct and professional but relatable way. After completing these lessons, be sure to complete the online profile review projects, such as LinkedIn Profile Review. If you are a Nanodegree Plus student, Career Content and Career Development Projects are required for graduation. If you are enrolled in a standard Nanodegree program, Career Content and Career Development Projects are optional and do not affect your graduation. NETWORKINGDevelop Your Personal BrandWhy Network?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/exjEm9Paszk.mp4 Elevator Pitchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/S-nAHPrkQrQ.mp4 Personal BrandingHow to Stand OutImagine you’re a hiring manager for a company, and you need to pick 5 people to interview for a role. But you get 50 applications, and everyone seems pretty qualified. How do you compare job candidates? You’ll probably pick the candidates that stand out the most to you. Personal StoriesThe thing that always makes a job candidate unique is their personal story - their passion and how they got there. Employers aren’t just looking for someone with the skills, but they’re looking for someone who can drive the company’s mission and will be a part of innovation. That’s why they need to know your work ethic and what drives you. As someone wanting to impress an employer, you need to tell your personal story. You want employers to know how you solve problems, overcome challenges, achieve results. You want employers to know what excites you, what motivates you, what drives you forward. All of this can be achieved through effective storytelling, and effective branding. I’ll let you know I’ve branded and rebranded myself many times. That’s okay - people are complex and have multiple interests that change over time. In this next video, we’ll meet my coworker Chris who will show us how he used personal branding to help him in his recent career change.ResourcesBlog post: Storytelling, Personal Branding, and Getting Hired Meet Chrishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ccflD9x5WU.mp4 ResourcesBlog post: Overcome Imposter Syndrome Elevator Pitchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0QtgTG49E9I.mp4 Pitching to a Recruiterhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LxAdWaA-qTQ.mp4 Use Your Elevator Pitchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e-v60ieggSs.mp4 Optimize Your LinkedIn ProfileWhy LinkedInLinkedIn is the most popular professional networking platform out there, so most recruiters use it to find job seekers. It’s so common for hiring teams to use LinkedIn to find and look at candidates, that it’s almost a red flag if they’re unable to find a LinkedIn profile for you. It’s also a great platform for you to connect with other people in your field. Udacity for example has an Alumni LinkedIn group where graduates can collaborate on projects, practice job interviews, or discuss new trends in the industry together. Connecting with a fellow alum and asking for a referral would increase your chances of getting an interview. Find ConnectionsThe best way to use your LinkedIn effectively, however, is to have over 500 connections. This may seem like a lot, but once you get rolling, you’ll get to that number fast. After you actively start using it it, by joining groups and going to networking events, your number of connections will climb. You are more likely to show up in search results on LinkedIn if you have more connections, which means you’ll be more visible to recruiters. Join GroupsIncreasing the group of people you’re connected with also exposes you to what they’re working on or have done. For example, if you move to a new city, you can search your network to see who lives in the area, and ask for recommendations on apartment hunting, job leads, or other advice on adjusting to life in another city. Also, if you’re active in a LinkedIn group or if you frequently write LinkedIn blog posts, you’ll increase your visibility on the platform and likelihood that a recruiter will find your profile. How to Build Your LinkedIn ProfileLinkedIn guides you well when filling out your profile. It tells you if your profile is strong and offers recommendations on how to improve it. We recommend you follow LinkedIn’s advice because it’ll increase your visibility on the network, thus increasing the number of opportunities you may come across. Tips for an Awesome LinkedIn ProfileIn the lessons on conducting a successful job search and resume writing, we talk about how you can describe your work experiences in a way that targets a specific job. Use what you learn to describe your experiences in LinkedIn’s projects and work sections. You can even copy and paste over the bullet points in your resume to the work or project sections of LinkedIn. Making sure your resume and LinkedIn are consistent helps build your personal brand. Find Other Networking PlatformsRemember that LinkedIn isn’t the only professional networking platform out there. If you do have a great LinkedIn profile, that means you can also build an amazing profile on other platforms. Find some recommendations for online profiles on the Career Resource Center. Up NextBy now, you know how to target your job profile to your dream job. You know how to market yourself effectively through building off your elevator pitch. Being confident in this will help you network naturally, whether on LinkedIn or at an event in-person. Move on to the LinkedIn Profile Review and get personalized feedback on your online presence. Networking Your Way to a New JobCareer and Job Fairs Do’s and Don’tsWhat are career mixers? GitHub Profile Review Rubrics submit LinkedIn Profile Review rubrics submit Udacity Professional Profile ReviewReinforcement LearningDUE OCT 19Use Reinforcement Learning algorithms like Q-Learning to train artificial agents to take optimal actions in an environment. Project: Train a Smartcab to Drive For most students, this project takes approximately 15 - 21 hours to complete (about 2 - 3 weeks).P4 Train a Smartcab to Drive Markov Decision Processes Further details on this quiz can be found in Chapter 17 of Artificial Intelligence: A Modern Approach REINFORCEMENT LEARNING Andrew Moore’s slides on Zero-Sum Games Andrew Moore’s slides on Non-Zero-Sum Games This paper offers a summary and an investigation of the field of reinforcement learning. It’s long, but chock-full of information! PROJECTSoftware Requirementspygame123Mac: conda install -c https://conda.anaconda.org/quasiben pygameLinux: conda install -c https://conda.anaconda.org/tlatorre pygameWindows: conda install -c https://conda.anaconda.org/prkrekel pygame Common Problems with PyGame Getting Started PyGame Information Google Group PyGame subreddit use the discussion forums MLND Student Slack Community Train a Smartcab to Drive project rubricsubmitwindows + rtypepip install pygame reviewREINFORCEMENT LEARNINGIntroduction to Reinforcement LearningReinforcement Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PeAHckcWFS0.mp4 What You’ll Watch and Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Z6ATPu4b9nc.mp4 Reinforcement Learning What You’ll Dohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1vQQphPLnkM.mp4 Markov Decision processesIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_ocNerSvh5Y.mp4 Reinforcement LearningReinforcement Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HeYSFWPX_4k.mp4 Rat Dinosaurshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h7ExhVneBDU.mp4 GAME THEORYGame TheoryGame Theoryhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vYHk1SPpnmQ.mp4 What Is Game Theory?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jwlteKFyiHU.mp4 PROJECTTrain a cab to drive itself.Overview Software RequirementsDescriptionIn the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents, known as smartcabs, to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to depend on smartcabs to get to where they need to go as safely and reliably as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or reliable as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, your task as an employee for a national taxicab company is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and reliability can be achieved. Software RequirementsThis project uses the following software and Python libraries: Python 2.7 NumPy pandas matplotlib PyGameIf you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. pygame can then be installed using one of the following commands: Mac: conda install -c https://conda.anaconda.org/quasiben pygameLinux: conda install -c https://conda.anaconda.org/tlatorre pygameWindows: conda install -c https://conda.anaconda.org/prkrekel pygame Please note that installing pygame can be done using pip as well. You can run an example to make sure pygame is working before actually performing the project by running: python -m pygame.examples.aliens Common Problems with PyGameFixing Common PyGame ProblemsThe PyGame library can in some cases require a bit of troubleshooting to work correctly for this project. While the PyGame aspect of the project is not required for a successful submission (you can complete the project without a visual simulation, although it is more difficult), it is very helpful to have it working! If you encounter an issue with PyGame, first see these helpful links below that are developed by communities of users working with the library: Getting Started PyGame Information Google Group PyGame subreddit Problems most often reported by students“PyGame won’t install on my machine; there was an issue with the installation.”Solution: As has been recommended for previous projects, Udacity suggests that you are using the Anaconda distribution of Python, which can then allow you to install PyGame through the conda-specific command. “I’m seeing a black screen when running the code; output says that it can’t load car images.”Solution: The code will not operate correctly unless it is run from the top-level directory for smartcab. The top-level directory is the one that contains the README and the project notebook. If you continue to have problems with the project code in regards to PyGame, you can also use the discussion forums to find posts from students that encountered issues that you may be experiencing. Additionally, you can seek help from a swath of students in the MLND Student Slack Community. Starting the ProjectFor this assignment, you can find the smartcab folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three directories: /logs/: This folder will contain all log files that are given from the simulation when specific prerequisites are met. /images/: This folder contains various images of cars to be used in the graphical user interface. You will not need to modify or create any files in this directory. /smartcab/: This folder contains the Python scripts that create the environment, graphical user interface, the simulation, and the agents. You will not need to modify or create any files in this directory except for agent.py. It also contains two files: smartcab.ipynb: This is the main file where you will answer questions and provide an analysis for your work. -visuals.py: This Python script provides supplementary visualizations for the analysis. Do not modify.Finally, in /smartcab/ are the following four files: Modify: agent.py: This is the main Python file where you will be performing your work on the project. Do not modify: environment.py: This Python file will create the smartcab environment. planner.py: This Python file creates a high-level planner for the agent to follow towards a set goal. simulator.py: This Python file creates the simulation and graphical user interface. Running the CodeIn a terminal or command window, navigate to the top-level project directory smartcab/ (that contains the three project directories) and run one of the following commands: python smartcab/agent.py orpython -m smartcab.agent This will run the agent.py file and execute your implemented agent code into the environment. Additionally, use the command jupyter notebook smartcab.ipynbfrom this same directory to open up a browser window or tab to work with your analysis notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the implementation necessary for your agent.py agent file. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. DefinitionsEnvironmentThe smartcab operates in an ideal, grid-like city (similar to New York City), with roads going in the North-South and East-West directions. Other vehicles will certainly be present on the road, but there will be no pedestrians to be concerned with. At each intersection there is a traffic light that either allows traffic in the North-South direction or the East-West direction. U.S. Right-of-Way rules apply: On a green light, a left turn is permitted if there is no oncoming traffic making a right turn or coming straight through the intersection. On a red light, a right turn is permitted if no oncoming traffic is approaching from your left through the intersection. To understand how to correctly yield to oncoming traffic when turning left, you may refer to this official drivers’ education video, or this passionate exposition. Inputs and OutputsAssume that the smartcab is assigned a route plan based on the passengers’ starting location and destination. The route is split at each intersection into waypoints, and you may assume that the smartcab, at any instant, is at some intersection in the world. Therefore, the next waypoint to the destination, assuming the destination has not already been reached, is one intersection away in one direction (North, South, East, or West). The smartcab has only an egocentric view of the intersection it is at: It can determine the state of the traffic light for its direction of movement, and whether there is a vehicle at the intersection for each of the oncoming directions. For each action, the smartcab may either idle at the intersection, or drive to the next intersection to the left, right, or ahead of it. Finally, each trip has a time to reach the destination which decreases for each action taken (the passengers want to get there quickly). If the allotted time becomes zero before reaching the destination, the trip has failed. Rewards and GoalThe smartcab will receive positive or negative rewards based on the action it as taken. Expectedly, the smartcab will receive a small positive reward when making a good action, and a varying amount of negative reward dependent on the severity of the traffic violation it would have committed. Based on the rewards and penalties the smartcab receives, the self-driving agent implementation should learn an optimal policy for driving on the city roads while obeying traffic rules, avoiding accidents, and reaching passengers’ destinations in the allotted time. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Train a Smartcab to Drive project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named smartcab for ease of access: The agent.py Python file with all code implemented as required in the instructed tasks. The /logs/ folder which should contain five log files that were produced from your simulation and used in the analysis. The smartcab.ipynb notebook file with all questions answered and all visualization cells executed and displaying results. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionTrain a Smartcab to DriveIn the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents — known as smartcabs — to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to rely on smartcabs to get to where they need to go as safely and efficiently as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or efficient as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, your task as an employee for a national taxicab company is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and efficiency can be achieved. Project FilesFor this assignment, you can find the smartcab folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Train a Smartcab to Drive project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named smartcab for ease of access: Theagent.py Python file with all code implemented as required in the instructed tasks. The /logs/ folder which should contain five log files that were produced from your simulation and used in the analysis. The smartcab.ipynb notebook file with all questions answered and all visualization cells executed and displaying results. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of this page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! Supporting MaterialsVideos Zip File View Submissionsubmission Deep LearningP5 Build a Digit Recognition Program FROM MACHINE LEARNING TO DEEP LEARNINGSOFTWARE AND TOOLSTensorFlow Download and SetupMethod 1: Pre-built Docker container with TensorFlow and all assignmentsTo get started with TensorFlow quickly and work on your assignments, follow the instructions in this README.Note: If you are on a Windows machine, Method 1 is your only option due to lack of native TensorFlow support. (not needed) Check your GPUright click computer-&gt;property-&gt;设备管理器-&gt;显示适配器I use the CPU only method (failed) First try from discussion at Udacity Install Docker Toolbox (you can get it here). I recommend installing every optional package. -&gt;failed Create a virtual machine for your udacity tensorflow work:docker-machine create -d virtualbox --virtualbox-memory 2048 tensorflow In a cmd.exe prompt, runFOR /f &quot;tokens=*&quot; %i IN (&#39;docker-machine env --shell cmd tensorflow&#39;) DO %i Next, rundocker run -p 8888:8888 --name tensorflow-udacity -it b.gcr.io/tensorflow-udacity/assignments:0.5.0 In a browser, go tohttp://192.168.99.100:8888/tree (failed) Second tryI have 2 versions in python, so I will not use this one. Click here and follow the instructions. Download Python 3.5.3 and choose Windows x86-64 executable installer -&gt; install Python3.5.x and add path. windows + r-&gt;pip3 install --upgrade tensorflow (failed) Third try from discussion at Udacitywindows + rohe = preprocessing.OneHotEncoder() # creating OneHotEncoder object label_encoded_data = label_encoder.fit_transform(data[&apos;health&apos;]) ohe.fit_transform(label_encoded_data.reshape(-1,1)) After executing the above steps, I can use tensorflow by selecting the following option in Jupyter notebook: Kernel =&gt; Change kernel =&gt; python [conda env:py35] Note: I used python 2.7 and jupyter notebook for the earlier assignments. (Useful) Forth methodFollow this video and install Ubuntu in Virtualbox. 虚拟硬盘文件保存位置C:\Users\SSQ\VirtualBox VMs\Deep Learning Ubuntu\Deep Learning Ubuntu.vdi location of shared file C:\Users\SSQ\virtualbox share Follow this blog to copy files between host OS and guest OS.for me I usesudo mount -t vboxsf virtualbox_share /mnt/ Follow this TensorFlow for mac ox, follow this videoregister mega https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows (success) Fifth try with pip installFollow this website When I type pip install tensorflow in Virtualbox (OS:Linux),it always shows ReadTimeoutError: HTTPSConnectionPool(host=&#39;pypi.python.org&#39;, port=443): Read timed out.,so I choosesudo pip install --upgrade https://pypi.tuna.tsinghua.edu.cn/packages/7b/c5/a97ed48fcc878e36bb05a3ea700c077360853c0994473a8f6b0ab4c2ddd2/tensorflow-1.0.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=a7483a4da4d70cc628e9e207238f77c0to install tensorflow Collecting numpy&gt;=1.11.0 (from tensorflow==1.0.0) Downloading numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl (16.5MB) sudo pip install --upgrade https://pypi.python.org/packages/cb/47/19e96945ee6012459e85f87728633f05b1e8791677ae64370d16ac4c849e/numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=9f9bc53d2e281831e1a75be0c09a9548 From this mirror sudo pip install --upgrade https://mirrors.ustc.edu.cn/pypi/web/packages/cb/47/19e96945ee6012459e85f87728633f05b1e8791677ae64370d16ac4c849e/numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=9f9bc53d2e281831e1a75be0c09a9548 Try again successpip install --index https://pypi.mirrors.ustc.edu.cn/simple/ tensorflow Validate your installation$ python import tensorflow as tfhello = tf.constant(‘Hello, TensorFlow!’)sess = tf.Session()print(sess.run(hello)) Hello, TensorFlow! (success) Sixth try with anaconda inatallFollow this website Download anaconda in the VirtualBoxfor me it shows readtimeouterror So I decide to download it in my host OS and copy it to my share file C:\Users\SSQ\virtualbox share and I can find it in the /mnt from my Linux system.type bash /mnt/Anaconda2-4.3.0-Linux-x86_64.shtype yesAnaconda2 will now be installed into this location:/home/ssq/anaconda2 Press ENTER to confirm the locationPress CTRL-C to abort the installationOr specify a different location below click Enter Do you wish the installer to prepend the Anaconda2 install locationto PATH in your /home/ssq/.bashrc ? [yes|no] yes Open new terminal and type conda create -n tensorflow Fetching package metadata … CondaHTTPError: HTTP None None for url Elapsed: None An HTTP error occurred when trying to retrieve this URL.ConnectionError(ReadTimeoutError(“HTTPSConnectionPool(host=’repo.continuum.io’, port=443): Read timed out.”,),) Try again conda create -n tensorflow source activate tensorflow From ssq@ssq-VirtualBox:~$ to (tensorflow) ssq@ssq-VirtualBox:~$ Successy pip install --index https://pypi.mirrors.ustc.edu.cn/simple/ tensorflow Validate your installation$ python import tensorflow as tfhello = tf.constant(‘Hello, TensorFlow!’)sess = tf.Session()print(sess.run(hello)) Hello, TensorFlow! source deactivate tensorflow From (tensorflow) ssq@ssq-VirtualBox:~$ to ssq@ssq-VirtualBox:~$ (failed)docker installInstall docker with sudo apt install docker.io AssignmentsAssignmentsNote: If you installed TensorFlow using the pre-built Docker container, you do not have to fetch assignment code separately. Just run the container and access the notebooks as mentioned here. Get Starter CodeStarter code packages (Jupyter notebooks) are available from the main TensorFlow repository. Clone it and navigate to the tensorflow/examples/udacity/ directory. This contains all the Jupyter notebooks (.ipynb files) as well as a Docker spec (Dockerfile). RunDepending on how you installed TensorFlow, do one of the following to run assignment code: **Pip/virtualenv**: Run `ipython notebook` and open http://localhost:8888 in a browser. **Docker**: As mentioned in README.md: First build a local Docker container: docker build -t $USER/assignments . Run the container: docker run -p 8888:8888 -it --rm $USER/assignments Now find your VM&apos;s IP using docker-machine ip default (say, 192.168.99.100) and open http://192.168.99.100:8888 You should be able to see a list of notebooks, one for each assignment. Click on the appropriate one to open it, and follow the inline instructions. And you’re ready to start exploring! To get further help on each assignment, navigate to the appropriate node. If you want to learn more about iPython (or Jupyter) notebooks, visit jupyter.org. Assignment 1: notMNISTAssignment 1: notMNISTPreprocess notMNIST data and train a simple logistic regression model on it notMNIST dataset samples Starter CodeOpen the iPython notebook for this assignment (1_notmnist.ipynb), and follow the instructions to implement and run each indicated step. Some of the early steps that preprocess the data have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer the posed questions (save your responses as markdown in the notebook). In the end, you should have a model trained on the notMNIST dataset, which is able to recognize a subset of English letters in different fonts. How accurately does your model predict the correct labels on the test dataset? Problem 2: Verify normalized imagesNote how imshow() displays an image using a color map. You can change this using the cmap parameter. Check out more options in the API reference. DEEP NEURAL NETWORKSDeep Neural NetworksAssignment 2: SGDAssignment 2: Stochastic Gradient DescentTrain a fully-connected network using Gradient Descent and Stochastic Gradient Descent Note: The assignments in this course build on each other, so please finish Assignment 1 before attempting this. Starter CodeOpen the iPython notebook for this assignment (2_fullyconnected.ipynb), and follow the instructions to implement and/or run each indicated step. Some steps have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). Your new model should perform better than the one you developed for Assignment 1. Also, the time required to train using Stochastic Gradient Descent (SGD) should be considerably less than simple Gradient Descent (GD). ErrorsError: valueError: Only call softmax_cross_entropy_with_logits with named arguments (labels=…, logits=…, …) Fix: loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) Assignment 3: RegularizationAssignment 3: RegularizationUse regularization techniques to improve a deep learning model Note: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (3_regularization.ipynb), and follow the instructions to implement and run each indicated step. Some steps have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). Try to apply the different regularization techniques you have learnt, and compare their results. Which seems to work better? Is one clearly better than the others? Error in VirtualBoxError:1234567Unable to allocate and lock memory. The virtual machine will be paused. Please close applications to free up memory or close the VM.错误 ID: HostMemoryLow严重: 非致命性错误 How to fix:Close all the process in the host OS and free up memory.Restart VM CONVOLUTIONAL NEURAL NETWORKSReadingsReadingsFor a closer look at the arithmetic behind convolution, and how it is affected by your choice of padding scheme, stride and other parameters, please refer to this illustrated guide: V. Dumoulin and F. Visin, A guide to convolution arithmetic for deep learning. Assignment 4: Convolutional ModelsDesign and train a Convolutional Neural NetworkNote: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (4_convolutions.ipynb), and follow the instructions to implement and run each indicated step. Some steps have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). Improve the model by experimenting with its structure - how many layers, how they are connected, stride, pooling, etc. For more efficient training, try applying techniques such as dropout and learning rate decay. What does your final architecture look like? DEEP MODELS FOR TEXT AND SEQUENCEStSNELaurens van der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE. Journal of Machine Learning Research, 2008. Vol. 9, pp. 2579-2605. Assignment 5: Word2Vec and CBOWAssignment 5: Word2Vec and CBOWTrain a skip-gram model on Text8 data and visualize the output Note: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (5_word2vec.ipynb), and follow the instructions to implement and run each indicated step. The first model (Word2Vec) has been implemented for you. Using that as a reference, train a CBOW (Continuous Bag of Words) model. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). How does your CBOW model perform compared to the given Word2Vec model? Opensudo mount -t vboxsf virtualbox_share /mnt/jupyter notebook runTypeError:Input &#39;y&#39; of &#39;Mul&#39; Op has type float32 that does not match type int32 of argument &#39;x&#39;. 12python -c &apos;import tensorflow as tf; print(tf.__version__)&apos;1.0.0 Method:tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, train_labels, embed,num_sampled, vocabulary_size)) Reference:https://github.com/nlintz/TensorFlow-Tutorials/issues/80 Assignment 6: LSTMsAssignment 6: LSTMsTrain a Long Short-Term Memory network to predict character sequences Note: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (6_lstm.ipynb), and follow the instructions to implement and run each indicated step. A basic LSTM model has been provided; improve it by solving the given problems. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). What changes did you make to use bigrams as input instead of individual characters? Were you able to implement the sequence-to-sequence LSTM? If so, what additional challenges did you have to solve? RunAttributeError:&#39;module&#39; object has no attribute &#39;concat_v2&#39; 12# Classifier.logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits, tf.concat_v2(train_labels, 0))) ValueError:Only call softmax_cross_entropy_with_logits with named arguments (labels=…, logits=…, …) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits, tf.concat(train_labels, 0))) Method12# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits, tf.concat(train_labels, 0))) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits=logits, labels=tf.concat(train_labels, 0))) Have a trypip uninstall tensorflow pip install --ignore-installed --upgrade https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb sudo pip install --index https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb sudo pip install --upgrade https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb sudo pip install --index https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb PROJECT submit (new) Deep LearningMACHINE LEARNING TO DEEP LEARNINGDeep LearningDeep LearningUp to this point you’ve been introduced to a number of different learning schemes that take place in machine learning. You’ve seen supervised learning, where we try to extrapolate labels for new data given labelled data we already have. You’ve seen unsupervised learning, where we try to classify data into groups and extract new information hidden in the data. Lastly, you’ve seen reinforcement learning, where we try to create a model that learns the rules of an environment to best maximize its return or reward. In this lesson, you’ll learn about a relatively new branch of machine learning called deep learning, which attempts to model high-level abstractions about data using networks of graphs. Deep learning, much like the other branches of machine learning you’ve seen, is similarly focused on learning representations in data. Additionally, modeling high-level abstractions about data is very similar to artificial intelligence — the idea that knowledge can be represented and acted upon intelligently. What You’ll Watch and LearnFor this lesson, you’ll want to learn about algorithms that help you to construct the deep network graphs necessary to model high-level abstractions about data. In addition, you’ll also want to learn how to construct deep models that can interpret and identify words and letters in text — just like how a human reads! To do that, you’ll work on Udacity’s Deep Learning course, co-authored by Google. Vincent Vanhoucke, Principle Scientist at Google Brain, will be your instructor for this lesson. With Vincent as your guide, you’ll learn the ins and outs of Deep Learning and TensorFlow, which is Google’s Deep Learning framework. Deep Learning What You’ll DoIn this lesson, you’ll learn how you can develop algorithms that are suitable to model high-level abstractions of data and create a type of “intelligence” that is able to use this abstraction for processing new information. First, you’ll learn about deep neural networks — artificial neural networks that have multiple hidden layers of information between its input and output. Next, you’ll learn about convolutional neural networks — a different flavor of neural networks that are modeled after biological processes like visual and aural feedback. Finally, you’ll learn about deep models for sequence learning — models that can “understand” written and spoken language and text. The underlying lesson from these concepts is that, with enough data and time to learn, we can develop intelligent agents that think and act in many of the same ways we as humans do. Being able to model complex human behaviors and tasks like driving a car, processing spoken language, or even building a winning strategy for the game of Go, is a task that could not be done without use of deep learning. Software and ToolsTensorFlowTensorFlowWe will be using TensorFlow™, an open-source library developed by Google, to build deep learning models throughout the course. Coding will be in Python 2.7 using iPython notebooks, which you should be familiar with. Download and SetupMethod 1: Pre-built Docker container with TensorFlow and all assignmentsTo get started with TensorFlow quickly and work on your assignments, follow the instructions in this README. Note: If you are on a Windows machine, Method 1 is your only option due to lack of native TensorFlow support. – OR – Method 2: Install TensorFlow on your computer (Linux or Mac OS X only), then fetch assignment code separatelyFollow the instructions to download and setup TensorFlow. Choose one of the three ways to install: Pip: Install TensorFlow directly on your computer. You need to have Python 2.7 and pip installed; and this may impact other Python packages that you may have.Virtualenv: Install TensorFlow in an isolated (virtual) Python environment. You need to have Python 2.7 and virtualenv installed; this will not affect Python packages in any other environment.Docker: Run TensorFlow in an isolated Docker container (virtual machine) on your computer. You need to have Vagrant, Docker and virtualization software like VirtualBox installed; this will keep TensorFlow completely isolated from the rest of your computer, but may require more memory to run.Links: Tutorials, How-Tos, Resources, Source code, Stack Overflow INTRO TO TENSORFLOWIntro to TensorFlowWhat is Deep Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/INt1nULYPak.mp4 Solving Problems - Big and Smallhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WHcRQMGSbqg.mp4 Let’s Get Startedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ySIDqaXLhHw.mp4 Installing TensorFlowThroughout this lesson, you’ll apply your knowledge of neural networks on real datasets using TensorFlow (link for China), an open source Deep Learning library created by Google. You’ll use TensorFlow to classify images from the notMNIST dataset - a dataset of images of English letters from A to J. You can see a few example images below.Your goal is to automatically detect the letter based on the image in the dataset. You’ll be working on your own computer for this lab, so, first things first, install TensorFlow! InstallAs usual, we’ll be using Conda to install TensorFlow. You might already have a TensorFlow environment, but check to make sure you have all the necessary packages. OS X or LinuxRun the following commands to setup your environment: conda create -n tensorflow python=3.5 source activate tensorflow conda install pandas matplotlib jupyter notebook scipy scikit-learn pip install tensorflow WindowsAnd installing on Windows. In your console or Anaconda shell, conda create -n tensorflow python=3.5 activate tensorflow conda install pandas matplotlib jupyter notebook scipy scikit-learn pip install tensorflow Hello, world!Try running the following code in your Python console to make sure you have TensorFlow properly installed. The console will print “Hello, world!” if TensorFlow is installed. Don’t worry about understanding what it does. You’ll learn about it in the next section. import tensorflow as tf # Create TensorFlow object called tensor hello_constant = tf.constant(&apos;Hello World!&apos;) with tf.Session() as sess: # Run the tf.constant operation in the session output = sess.run(hello_constant) print(output) Tryopen cmd with admin conda create -n tensorflow python=3.5 C:\windows\system32&gt;conda create -n tensorflow python=3.5 Fetching package metadata ........... Solving package specifications: . Package plan for installation in environment C:\Program Files\Anaconda2\envs\ten sorflow: The following NEW packages will be INSTALLED: pip: 9.0.1-py35_1 python: 3.5.3-0 setuptools: 27.2.0-py35_1 vs2015_runtime: 14.0.25123-0 wheel: 0.29.0-py35_0 Proceed ([y]/n)? y vs2015_runtime 100% |###############################| Time: 0:00:02 776.58 kB/s python-3.5.3-0 100% |###############################| Time: 0:01:29 361.95 kB/s setuptools-27. 100% |###############################| Time: 0:00:00 1.09 MB/s wheel-0.29.0-p 100% |###############################| Time: 0:00:00 1.55 MB/s pip-9.0.1-py35 100% |###############################| Time: 0:00:01 997.36 kB/s # # To activate this environment, use: # &gt; activate tensorflow # # To deactivate this environment, use: # &gt; deactivate tensorflow # # * for power-users using bash, you must source # activate tensorflow (tensorflow) C:\windows\system32&gt; conda install pandas matplotlib jupyter notebook scipy scikit-learn Fetching package metadata ………..Solving package specifications: . Package plan for installation in environment C:\Program Files\Anaconda2\envs\tensorflow: The following NEW packages will be INSTALLED: bleach: 1.5.0-py35_0 colorama: 0.3.7-py35_0 cycler: 0.10.0-py35_0 decorator: 4.0.11-py35_0 entrypoints: 0.2.2-py35_1 html5lib: 0.999-py35_0 icu: 57.1-vc14_0 [vc14] ipykernel: 4.5.2-py35_0 ipython: 5.3.0-py35_0 ipython_genutils: 0.1.0-py35_0 ipywidgets: 6.0.0-py35_0 jinja2: 2.9.5-py35_0 jpeg: 9b-vc14_0 [vc14] jsonschema: 2.5.1-py35_0 jupyter: 1.0.0-py35_3 jupyter_client: 5.0.0-py35_0 jupyter_console: 5.1.0-py35_0 jupyter_core: 4.3.0-py35_0 libpng: 1.6.27-vc14_0 [vc14] markupsafe: 0.23-py35_2 matplotlib: 2.0.0-np112py35_0 mistune: 0.7.4-py35_0 mkl: 2017.0.1-0 nbconvert: 5.1.1-py35_0 nbformat: 4.3.0-py35_0 notebook: 4.4.1-py35_0 numpy: 1.12.0-py35_0 openssl: 1.0.2k-vc14_0 [vc14] pandas: 0.19.2-np112py35_1 pandocfilters: 1.4.1-py35_0 path.py: 10.1-py35_0 pickleshare: 0.7.4-py35_0 prompt_toolkit: 1.0.13-py35_0 pygments: 2.2.0-py35_0 pyparsing: 2.1.4-py35_0 pyqt: 5.6.0-py35_2 python-dateutil: 2.6.0-py35_0 pytz: 2016.10-py35_0 pyzmq: 16.0.2-py35_0 qt: 5.6.2-vc14_3 [vc14] qtconsole: 4.2.1-py35_2 scikit-learn: 0.18.1-np112py35_1 scipy: 0.19.0-np112py35_0 simplegeneric: 0.8.1-py35_1 sip: 4.18-py35_0 six: 1.10.0-py35_0 testpath: 0.3-py35_0 tk: 8.5.18-vc14_0 [vc14] tornado: 4.4.2-py35_0 traitlets: 4.3.2-py35_0 wcwidth: 0.1.7-py35_0 widgetsnbextension: 2.0.0-py35_0 win_unicode_console: 0.5-py35_0 zlib: 1.2.8-vc14_3 [vc14] Proceed ([y]/n)? y mkl-2017.0.1-0 100% |###############################| Time: 0:04:46 470.85 kB/s icu-57.1-vc14_ 100% |###############################| Time: 0:01:28 403.91 kB/s jpeg-9b-vc14_0 100% |###############################| Time: 0:00:00 379.04 kB/s openssl-1.0.2k 100% |###############################| Time: 0:00:13 393.72 kB/s tk-8.5.18-vc14 100% |###############################| Time: 0:00:04 473.45 kB/s zlib-1.2.8-vc1 100% |###############################| Time: 0:00:00 503.24 kB/s colorama-0.3.7 100% |###############################| Time: 0:00:00 622.07 kB/s decorator-4.0. 100% |###############################| Time: 0:00:00 690.00 kB/s entrypoints-0. 100% |###############################| Time: 0:00:00 625.06 kB/s ipython_genuti 100% |###############################| Time: 0:00:00 597.35 kB/s jsonschema-2.5 100% |###############################| Time: 0:00:00 503.91 kB/s libpng-1.6.27- 100% |###############################| Time: 0:00:01 432.48 kB/s markupsafe-0.2 100% |###############################| Time: 0:00:00 520.82 kB/s mistune-0.7.4- 100% |###############################| Time: 0:00:00 441.53 kB/s numpy-1.12.0-p 100% |###############################| Time: 0:00:10 354.48 kB/s pandocfilters- 100% |###############################| Time: 0:00:00 363.00 kB/s path.py-10.1-p 100% |###############################| Time: 0:00:00 293.57 kB/s pygments-2.2.0 100% |###############################| Time: 0:00:04 302.43 kB/s pyparsing-2.1. 100% |###############################| Time: 0:00:00 270.85 kB/s pytz-2016.10-p 100% |###############################| Time: 0:00:00 233.38 kB/s pyzmq-16.0.2-p 100% |###############################| Time: 0:00:02 266.24 kB/s simplegeneric- 100% |###############################| Time: 0:00:00 373.89 kB/s sip-4.18-py35_ 100% |###############################| Time: 0:00:00 268.95 kB/s six-1.10.0-py3 100% |###############################| Time: 0:00:00 409.00 kB/s testpath-0.3-p 100% |###############################| Time: 0:00:00 329.72 kB/s tornado-4.4.2- 100% |###############################| Time: 0:00:02 253.88 kB/s wcwidth-0.1.7- 100% |###############################| Time: 0:00:00 329.53 kB/s win_unicode_co 100% |###############################| Time: 0:00:00 302.28 kB/s cycler-0.10.0- 100% |###############################| Time: 0:00:00 393.21 kB/s html5lib-0.999 100% |###############################| Time: 0:00:00 260.77 kB/s jinja2-2.9.5-p 100% |###############################| Time: 0:00:01 250.23 kB/s pickleshare-0. 100% |###############################| Time: 0:00:00 326.15 kB/s prompt_toolkit 100% |###############################| Time: 0:00:01 281.79 kB/s python-dateuti 100% |###############################| Time: 0:00:00 280.81 kB/s qt-5.6.2-vc14_ 100% |###############################| Time: 0:02:03 469.10 kB/s scipy-0.19.0-n 100% |###############################| Time: 0:00:20 656.15 kB/s traitlets-4.3. 100% |###############################| Time: 0:00:00 418.63 kB/s bleach-1.5.0-p 100% |###############################| Time: 0:00:00 508.29 kB/s ipython-5.3.0- 100% |###############################| Time: 0:00:02 406.32 kB/s jupyter_core-4 100% |###############################| Time: 0:00:00 365.87 kB/s pandas-0.19.2- 100% |###############################| Time: 0:00:13 548.51 kB/s pyqt-5.6.0-py3 100% |###############################| Time: 0:00:08 586.14 kB/s scikit-learn-0 100% |###############################| Time: 0:00:16 282.73 kB/s jupyter_client 100% |###############################| Time: 0:00:00 250.90 kB/s matplotlib-2.0 100% |###############################| Time: 0:00:17 508.36 kB/s nbformat-4.3.0 100% |###############################| Time: 0:00:00 1.41 MB/s ipykernel-4.5. 100% |###############################| Time: 0:00:00 1.39 MB/s nbconvert-5.1. 100% |###############################| Time: 0:00:00 1.42 MB/s jupyter_consol 100% |###############################| Time: 0:00:00 397.64 kB/s notebook-4.4.1 100% |###############################| Time: 0:00:06 890.12 kB/s qtconsole-4.2. 100% |###############################| Time: 0:00:00 705.98 kB/s widgetsnbexten 100% |###############################| Time: 0:00:01 727.40 kB/s ipywidgets-6.0 100% |###############################| Time: 0:00:00 632.13 kB/s jupyter-1.0.0- 100% |###############################| Time: 0:00:00 665.76 kB/s ERROR conda.core.link:_execute_actions(330): An error occurred while installing package &apos;defaults::qt-5.6.2-vc14_3&apos;. UnicodeDecodeError(&apos;utf8&apos;, &apos;\xd2\xd1\xb8\xb4\xd6\xc6 1 \xb8\xf6\xce\xc4\ xbc\xfe\xa1\xa3\r\n&apos;, 0, 1, &apos;invalid continuation byte&apos;) Attempting to roll back. UnicodeDecodeError(&apos;utf8&apos;, &apos;\xd2\xd1\xb8\xb4\xd6\xc6 1 \xb8\xf6\xce\xc4\ xbc\xfe\xa1\xa3\r\n&apos;, 0, 1, &apos;invalid continuation byte&apos;) (tensorflow) C:\windows\system32&gt;pip install tensorflow Hello, Tensor World!Hello, Tensor World!Let’s analyze the Hello World script you ran. For reference, I’ve added the code below. import tensorflow as tf # Create TensorFlow object called hello_constant hello_constant = tf.constant(&apos;Hello World!&apos;) with tf.Session() as sess: # Run the tf.constant operation in the session output = sess.run(hello_constant) print(output) TensorIn TensorFlow, data isn’t stored as integers, floats, or strings. These values are encapsulated(封装) in an object called a tensor. In the case of hello_constant = tf.constant(&#39;Hello World!&#39;), hello_constant is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below: # A is a 0-dimensional int32 tensor A = tf.constant(1234) # B is a 1-dimensional int32 tensor B = tf.constant([123,456,789]) # C is a 2-dimensional int32 tensor C = tf.constant([ [123,456,789], [222,333,444] ]) tf.constant() is one of many TensorFlow operations you will use in this lesson. The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes. SessionTensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process which you learned about in the MiniFlow lesson. Let’s take the TensorFlow code you ran and turn that into a graph:A “TensorFlow Session”, as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let’s see how you use it. with tf.Session() as sess: output = sess.run(hello_constant) The code has already created the tensor, hello_constant, from the previous lines. The next step is to evaluate the tensor in a session. The code creates a session instance, sess, using tf.Session. The sess.run() function then evaluates the tensor and returns the results. Quiz: TensorFlow InputInputIn the last section, you passed a tensor into a session and it returned the result. What if you want to use a non-constant? This is where tf.placeholder() and feed_dict come into place. In this section, you’ll go over the basics of feeding data into TensorFlow. tf.placeholder()Sadly you can’t just set x to your dataset and put it in TensorFlow, because over time you’ll want your TensorFlow model to take in different datasets with different parameters. You need tf.placeholder()! tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs. Session’s feed_dictx = tf.placeholder(tf.string) with tf.Session() as sess: output = sess.run(x, feed_dict={x: &apos;Hello World&apos;}) Use the feed_dict parameter in tf.session.run() to set the placeholder tensor. The above example shows the tensor x being set to the string &quot;Hello, world&quot;. It’s also possible to set more than one tensor using feed_dict as shown below. x = tf.placeholder(tf.string) y = tf.placeholder(tf.int32) z = tf.placeholder(tf.float32) with tf.Session() as sess: output = sess.run(x, feed_dict={x: &apos;Test String&apos;, y: 123, z: 45.67}) Note: If the data passed to the feed_dict doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error “ValueError: invalid literal for...”. QuizLet’s see how well you understand tf.placeholder() and feed_dict. The code below throws an error, but I want you to make it return the number 123. Change line 11, so that the code returns the number 123. Note: The quizzes are running TensorFlow version 0.12.1. However, all the code used in this course is compatible with version 1.0. We’ll be upgrading our in class quizzes to the newest version in the near future. # Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf def run(): output = None x = tf.placeholder(tf.int32) with tf.Session() as sess: # TODO: Feed the x tensor 123 output = sess.run(x,feed_dict={x:123}) return output Quiz: TensorFlow MathTensorFlow MathGetting the input is great, but now you need to use it. You’re going to use basic math functions that everyone knows and loves - add, subtract, multiply, and divide - with tensors. (There’s many more math functions you can check out in the documentation.) Additionx = tf.add(5, 2) # 7 You’ll start with the add function. The tf.add() function does exactly what you expect it to do. It takes in two numbers, two tensors, or one of each, and returns their sum as a tensor. Subtraction and MultiplicationHere’s an example with subtraction and multiplication. x = tf.subtract(10, 4) # 6 y = tf.multiply(2, 5) # 10 The x tensor will evaluate to 6, because 10 - 4 = 6. The y tensor will evaluate to 10, because 2 * 5 = 10. That was easy! Converting typesIt may be necessary to convert between types to make certain operators work together. For example, if you tried the following, it would fail with an exception: tf.subtract(tf.constant(2.0),tf.constant(1)) # Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: That’s because the constant 1 is an integer but the constant 2.0 is a floating point value and subtract expects them to match. In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the 2.0 to an integer before subtracting, like so, will give the correct result: tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1)) # 1 QuizLet’s apply what you learned to convert an algorithm to TensorFlow. The code below is a simple algorithm using division and subtraction. Convert the following algorithm in regular Python to TensorFlow and print the results of the session. You can use tf.constant() for the values 10, 2, and 1. # Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf # TODO: Convert the following to TensorFlow: x = 10 y = 2 z = x/y - 1 x=tf.constant(x) y=tf.constant(y) z=tf.constant(z) #z=tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1),tf.float64)) # TODO: Print z from a session with tf.Session() as sess: output = sess.run(z) print(output) Transition to ClassificationGood job! You’ve accomplished a lot. In particular, you did the following: Ran operations in tf.Session. Created a constant tensor with tf.constant(). Used tf.placeholder() and feed_dict to get input. Applied the tf.add(), tf.subtract(), tf.multiply(), and tf.divide() functions using numeric data. Learned about casting between types with tf.cast()You know the basics of TensorFlow, so let’s take a break and get back to the theory of neural networks. In the next few videos, you’re going to learn about one of the most popular applications of neural networks - classification. Supervised Classificationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XTGsutypAPE.mp4 Training Your Logistic Classifierhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WQsdr1EJgz8.mp4 Quiz: TensorFlow Linear FunctionLinear functions in TensorFlowThe most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation asHere, W is a matrix of the weights connecting two layers. The output y, the input x, and the biases b are all vectors. Weights and Bias in TensorFlowThe goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you’ll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can’t be modified. This is where tf.Variable class comes in. tf.Variable()x = tf.Variable(5) The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You’ll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors. Initialization init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init)The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen. Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You’ll learn more about this in the next lesson, when you study gradient descent. Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You’ll use the tf.truncated_normal() function to generate random numbers from a normal distribution. tf.truncated_normal()n_features = 120 n_labels = 5 weights = tf.Variable(tf.truncated_normal((n_features, n_labels))) The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean. Since the weights are already helping prevent the model from getting stuck, you don’t need to randomize the bias. Let’s use the simplest solution, setting the bias to 0. tf.zeros()n_labels = 5 bias = tf.Variable(tf.zeros(n_labels)) The tf.zeros() function returns a tensor with all zeros. Linear Classifier QuizYou’ll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. The above is a small sample of the data you’ll be training on. Notice how some of the 1s are written with a serif at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model. The images above are trained weights for each label (0, 1, and 2). The weights display the unique properties of each digit they have found. Complete this quiz to train your own weights using the MNIST dataset. Instructions Open quiz.py. Implement get_weights to return a tf.Variable of weights Implement get_biases to return a tf.Variable of biases Implement xW + b in the linear function Open sandbox.py Initialize all weightsSince xW in xW + b is matrix multiplication, you have to use the tf.matmul() function instead of tf.multiply(). Don’t forget that order matters in matrix multiplication, so tf.matmul(a,b) is not the same as tf.matmul(b,a). quiz.py# Solution is available in the other &quot;quiz_solution.py&quot; tab import tensorflow as tf def get_weights(n_features, n_labels): &quot;&quot;&quot; Return TensorFlow weights :param n_features: Number of features :param n_labels: Number of labels :return: TensorFlow weights &quot;&quot;&quot; # TODO: Return weights return tf.Variable(tf.truncated_normal((n_features, n_labels))) def get_biases(n_labels): &quot;&quot;&quot; Return TensorFlow bias :param n_labels: Number of labels :return: TensorFlow bias &quot;&quot;&quot; # TODO: Return biases return tf.Variable(tf.zeros(n_labels)) def linear(input, w, b): &quot;&quot;&quot; Return linear function in TensorFlow :param input: TensorFlow input :param w: TensorFlow weights :param b: TensorFlow biases :return: TensorFlow linear function &quot;&quot;&quot; # TODO: Linear Function (xW + b) return tf.add(tf.matmul(input,w),b) sandbox.py# Solution is available in the other &quot;sandbox_solution.py&quot; tab import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data from quiz import get_weights, get_biases, linear def mnist_features_labels(n_labels): &quot;&quot;&quot; Gets the first &lt;n&gt; labels from the MNIST dataset :param n_labels: Number of labels to use :return: Tuple of feature list and label list &quot;&quot;&quot; mnist_features = [] mnist_labels = [] mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # In order to make quizzes run faster, we&apos;re only looking at 10000 images for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)): # Add features and labels if it&apos;s for the first &lt;n&gt;th labels if mnist_label[:n_labels].any(): mnist_features.append(mnist_feature) mnist_labels.append(mnist_label[:n_labels]) return mnist_features, mnist_labels # Number of features (28*28 image is 784 features) n_features = 784 # Number of labels n_labels = 3 # Features and Labels features = tf.placeholder(tf.float32) labels = tf.placeholder(tf.float32) # Weights and Biases w = get_weights(n_features, n_labels) b = get_biases(n_labels) # Linear Function xW + b logits = linear(features, w, b) # Training data train_features, train_labels = mnist_features_labels(n_labels) with tf.Session() as session: # TODO: Initialize session variables session.run(tf.global_variables_initializer()) # Softmax prediction = tf.nn.softmax(logits) # Cross entropy # This quantifies how far off the predictions were. # You&apos;ll learn more about this in future lessons. cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1) # Training loss # You&apos;ll learn more about this in future lessons. loss = tf.reduce_mean(cross_entropy) # Rate at which the weights are changed # You&apos;ll learn more about this in future lessons. learning_rate = 0.08 # Gradient Descent # This is the method used to train the model # You&apos;ll learn more about this in future lessons. optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # Run optimizer and get loss _, l = session.run( [optimizer, loss], feed_dict={features: train_features, labels: train_labels}) # Print loss print(&apos;Loss: {}&apos;.format(l)) Quiz: TensorFlow SoftmaxTensorFlow SoftmaxYou might remember in the Intro to TFLearn lesson we used the softmax function to calculate class probabilities as output from the network. The softmax function squashes it’s inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It’s the perfect function to use as the output activation for a network predicting multiple classes. TensorFlow SoftmaxWe’re using TensorFlow to build neural networks and, appropriately, there’s a function for calculating softmax. x = tf.nn.softmax([2.0, 1.0, 0.2]) Easy as that! tf.nn.softmax() implements the softmax function for you. It takes in logits and returns softmax activations. QuizUse the softmax function in the quiz below to return the softmax of the logits. quiz.py# Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf def run(): output = None logit_data = [2.0, 1.0, 0.1] logits = tf.placeholder(tf.float32) # TODO: Calculate the softmax of the logits # softmax = softmax = tf.nn.softmax([2.0, 1.0, 0.1]) with tf.Session() as sess: # TODO: Feed in the logit data # output = sess.run(softmax, ) output = sess.run(softmax,feed_dict={logits:logit_data} ) return output One-Hot Encoding https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/phYsxqlilUk.mp4 13 L One Hot EncodingOne-Hot Encoding With Scikit-LearnTransforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using LabelBinarizer. Check it out below! import numpy as np from sklearn import preprocessing # Example labels labels = np.array([1,5,3,2,1,4,2,1,3]) # Create the encoder lb = preprocessing.LabelBinarizer() # Here the encoder finds the classes and assigns one-hot vectors lb.fit(labels) # And finally, transform the labels into one-hot encoded vectors lb.transform(labels) &gt;&gt;&gt; array([[1, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0]]) Quiz: TensorFlow Cross EntropyCross Entropy in TensorFlowIn the Intro to TFLearn lesson we discussed using cross entropy as the cost function for classification with one-hot encoded labels. Again, TensorFlow has a function to do the cross entropy calculations for us. Let’s take what you learned from the video and create a cross entropy function in TensorFlow. To create a cross entropy function in TensorFlow, you’ll need to use two new functions: tf.reduce_sum() tf.log() Reduce Sumx = tf.reduce_sum([1, 2, 3, 4, 5]) # 15 The tf.reduce_sum() function takes an array of numbers and sums them together. Natural Logx = tf.log(100) # 4.60517 This function does exactly what you would expect it to do. tf.log() takes the natural log of a number. QuizPrint the cross entropy using softmax_data and one_hot_encod_label.(Alternative link for users in China.) quiz.py# Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf softmax_data = [0.7, 0.2, 0.1] one_hot_data = [1.0, 0.0, 0.0] softmax = tf.placeholder(tf.float32) one_hot = tf.placeholder(tf.float32) # TODO: Print cross entropy from session cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax))) with tf.Session() as sess: print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})) 0.356675 Minimizing Cross Entropy https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YrDMXFhvh9E.mp4 Transition into Practical Aspects of Learning https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bKqkRFOOKoA.mp4 Quiz: Numerical Stability https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_SbGcOS-jcQ.mp4 a = 1000000000 for i in range(1000000): a = a + 1e-6 print(a - 1000000000) 0.953674316406 Normalized Inputs and Initial Weights Measuring Performance Optimizing a Logistic Classifier Stochastic Gradient Descent Momentum and Learning Rate Decay Parameter Hyperspace Quiz: Mini-batchMini-batchingIn this section, you’ll go over what mini-batching is and how to apply it in TensorFlow. Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset. Mini-batching is computationally inefficient, since you can’t calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all. It’s also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you’re performing SGD with each batch. Let’s look at the MNIST dataset with weights and a bias to see if your machine can handle it. from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # The features are already scaled and the data is shuffled train_features = mnist.train.images test_features = mnist.test.images train_labels = mnist.train.labels.astype(np.float32) test_labels = mnist.test.labels.astype(np.float32) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) Question 1Calculate the memory size of train_features, train_labels, weights, and bias in bytes. Ignore memory for overhead, just calculate the memory required for the stored data. You may have to look up how much memory a float32 requires, using this link. train_features Shape: (55000, 784) Type: float32 train_labels Shape: (55000, 10) Type: float32 weights Shape: (784, 10) Type: float32 bias Shape: (10,) Type: float32 How many bytes of memory does train_features need?550007844=172480000 How many bytes of memory does train_labels need?2200000 How many bytes of memory does weights need?31360 How many bytes of memory does bias need?40 The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn’t that much memory. You could train this whole dataset on most CPUs and GPUs. But larger datasets that you’ll use in the future measured in gigabytes or more. It’s possible to purchase more memory, but it’s expensive. A Titan X GPU with 12 GB of memory costs over $1,000. Instead, in order to run large models on your machine, you’ll learn how to use mini-batching. Let’s look at how you implement mini-batching in TensorFlow. TensorFlow Mini-batchingIn order to use mini-batching, you must first divide your data into batches. Unfortunately, it’s sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you’d like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you’d wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7128 + 1104 = 1000) In that case, the size of the batches would vary, so you need to take advantage of TensorFlow’s tf.placeholder() function to receive the varying batch sizes. Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes]. # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) What does None do here? The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0. Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples. Question 2Use the parameters below, how many batches are there, and what is the last batch size? features is (50000, 400) labels is (50000, 10) batch_size is 128 How many batches are there?50000/128+1=391 What is the last batch size?50000%128=80 Now that you know the basics, let’s learn how to implement mini-batching. Question 3Implement the batches function to batch features and labels. The function should return each batch with a maximum size of batch_size. To help you with the quiz, look at the following example output of a working batches function. # 4 Samples of features example_features = [ [&apos;F11&apos;,&apos;F12&apos;,&apos;F13&apos;,&apos;F14&apos;], [&apos;F21&apos;,&apos;F22&apos;,&apos;F23&apos;,&apos;F24&apos;], [&apos;F31&apos;,&apos;F32&apos;,&apos;F33&apos;,&apos;F34&apos;], [&apos;F41&apos;,&apos;F42&apos;,&apos;F43&apos;,&apos;F44&apos;]] # 4 Samples of labels example_labels = [ [&apos;L11&apos;,&apos;L12&apos;], [&apos;L21&apos;,&apos;L22&apos;], [&apos;L31&apos;,&apos;L32&apos;], [&apos;L41&apos;,&apos;L42&apos;]] example_batches = batches(3, example_features, example_labels) The example_batches variable would be the following: [ # 2 batches: # First is a batch of size 3. # Second is a batch of size 1 [ # First Batch is size 3 [ # 3 samples of features. # There are 4 features per sample. [&apos;F11&apos;, &apos;F12&apos;, &apos;F13&apos;, &apos;F14&apos;], [&apos;F21&apos;, &apos;F22&apos;, &apos;F23&apos;, &apos;F24&apos;], [&apos;F31&apos;, &apos;F32&apos;, &apos;F33&apos;, &apos;F34&apos;] ], [ # 3 samples of labels. # There are 2 labels per sample. [&apos;L11&apos;, &apos;L12&apos;], [&apos;L21&apos;, &apos;L22&apos;], [&apos;L31&apos;, &apos;L32&apos;] ] ], [ # Second Batch is size 1. # Since batch size is 3, there is only one sample left from the 4 samples. [ # 1 sample of features. [&apos;F41&apos;, &apos;F42&apos;, &apos;F43&apos;, &apos;F44&apos;] ], [ # 1 sample of labels. [&apos;L41&apos;, &apos;L42&apos;] ] ] ] Implement the batches function in the “quiz.py” file below. “quiz.py”import math def batches(batch_size, features, labels): &quot;&quot;&quot; Create batches of features and labels :param batch_size: The batch size :param features: List of features :param labels: List of labels :return: Batches of (Features, Labels) &quot;&quot;&quot; assert len(features) == len(labels) # TODO: Implement batching output_batches = [] sample_size = len(features) for start_i in range(0, sample_size, batch_size): end_i = start_i + batch_size batch = [features[start_i:end_i], labels[start_i:end_i]] output_batches.append(batch) return output_batches “sandbox.py”from quiz import batches from pprint import pprint # 4 Samples of features example_features = [ [&apos;F11&apos;,&apos;F12&apos;,&apos;F13&apos;,&apos;F14&apos;], [&apos;F21&apos;,&apos;F22&apos;,&apos;F23&apos;,&apos;F24&apos;], [&apos;F31&apos;,&apos;F32&apos;,&apos;F33&apos;,&apos;F34&apos;], [&apos;F41&apos;,&apos;F42&apos;,&apos;F43&apos;,&apos;F44&apos;]] # 4 Samples of labels example_labels = [ [&apos;L11&apos;,&apos;L12&apos;], [&apos;L21&apos;,&apos;L22&apos;], [&apos;L31&apos;,&apos;L32&apos;], [&apos;L41&apos;,&apos;L42&apos;]] # PPrint prints data structures like 2d arrays, so they are easier to read pprint(batches(3, example_features, example_labels)) Let’s use mini-batching to feed batches of MNIST features and labels into a linear model. Set the batch size and run the optimizer over all the batches with the batches function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller. “quiz.py”from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf import numpy as np from helper import batches learning_rate = 0.001 n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # The features are already scaled and the data is shuffled train_features = mnist.train.images test_features = mnist.test.images train_labels = mnist.train.labels.astype(np.float32) test_labels = mnist.test.labels.astype(np.float32) # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) # Logits - xW + b logits = tf.add(tf.matmul(features, weights), bias) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost) # Calculate accuracy correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # TODO: Set batch size batch_size = 128 assert batch_size is not None, &apos;You must set the batch size&apos; init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) # TODO: Train optimizer on all batches # for batch_features, batch_labels in ______ for batch_features, batch_labels in batches(batch_size, train_features, train_labels): sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels}) # Calculate accuracy for test dataset test_accuracy = sess.run( accuracy, feed_dict={features: test_features, labels: test_labels}) print(&apos;Test Accuracy: {}&apos;.format(test_accuracy)) The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times. You’ll go over this subject in the next section where we talk about “epochs”. EpochsEpochsAn epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs. The following TensorFlow code trains a model using 10 epochs. from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf import numpy as np from helper import batches # Helper function created in Mini-batching section def print_epoch_stats(epoch_i, sess, last_features, last_labels): &quot;&quot;&quot; Print cost and validation accuracy of an epoch &quot;&quot;&quot; current_cost = sess.run( cost, feed_dict={features: last_features, labels: last_labels}) valid_accuracy = sess.run( accuracy, feed_dict={features: valid_features, labels: valid_labels}) print(&apos;Epoch: {:&lt;4} - Cost: {:&lt;8.3} Valid Accuracy: {:&lt;5.3}&apos;.format( epoch_i, current_cost, valid_accuracy)) n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # The features are already scaled and the data is shuffled train_features = mnist.train.images valid_features = mnist.validation.images test_features = mnist.test.images train_labels = mnist.train.labels.astype(np.float32) valid_labels = mnist.validation.labels.astype(np.float32) test_labels = mnist.test.labels.astype(np.float32) # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) # Logits - xW + b logits = tf.add(tf.matmul(features, weights), bias) # Define loss and optimizer learning_rate = tf.placeholder(tf.float32) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost) # Calculate accuracy correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) init = tf.global_variables_initializer() batch_size = 128 epochs = 10 learn_rate = 0.001 train_batches = batches(batch_size, train_features, train_labels) with tf.Session() as sess: sess.run(init) # Training cycle for epoch_i in range(epochs): # Loop over all batches for batch_features, batch_labels in train_batches: train_feed_dict = { features: batch_features, labels: batch_labels, learning_rate: learn_rate} sess.run(optimizer, feed_dict=train_feed_dict) # Print cost and validation accuracy of an epoch print_epoch_stats(epoch_i, sess, batch_features, batch_labels) # Calculate accuracy for test dataset test_accuracy = sess.run( accuracy, feed_dict={features: test_features, labels: test_labels}) print(&apos;Test Accuracy: {}&apos;.format(test_accuracy)) Running the code will output the following: Epoch: 0 - Cost: 11.0 Valid Accuracy: 0.204 Epoch: 1 - Cost: 9.95 Valid Accuracy: 0.229 Epoch: 2 - Cost: 9.18 Valid Accuracy: 0.246 Epoch: 3 - Cost: 8.59 Valid Accuracy: 0.264 Epoch: 4 - Cost: 8.13 Valid Accuracy: 0.283 Epoch: 5 - Cost: 7.77 Valid Accuracy: 0.301 Epoch: 6 - Cost: 7.47 Valid Accuracy: 0.316 Epoch: 7 - Cost: 7.2 Valid Accuracy: 0.328 Epoch: 8 - Cost: 6.96 Valid Accuracy: 0.342 Epoch: 9 - Cost: 6.73 Valid Accuracy: 0.36 Test Accuracy: 0.3801000118255615 Each epoch attempts to move to a lower cost, leading to better accuracy. This model continues to improve accuracy up to Epoch 9. Let’s increase the number of epochs to 100. ... Epoch: 79 - Cost: 0.111 Valid Accuracy: 0.86 Epoch: 80 - Cost: 0.11 Valid Accuracy: 0.869 Epoch: 81 - Cost: 0.109 Valid Accuracy: 0.869 .... Epoch: 85 - Cost: 0.107 Valid Accuracy: 0.869 Epoch: 86 - Cost: 0.107 Valid Accuracy: 0.869 Epoch: 87 - Cost: 0.106 Valid Accuracy: 0.869 Epoch: 88 - Cost: 0.106 Valid Accuracy: 0.869 Epoch: 89 - Cost: 0.105 Valid Accuracy: 0.869 Epoch: 90 - Cost: 0.105 Valid Accuracy: 0.869 Epoch: 91 - Cost: 0.104 Valid Accuracy: 0.869 Epoch: 92 - Cost: 0.103 Valid Accuracy: 0.869 Epoch: 93 - Cost: 0.103 Valid Accuracy: 0.869 Epoch: 94 - Cost: 0.102 Valid Accuracy: 0.869 Epoch: 95 - Cost: 0.102 Valid Accuracy: 0.869 Epoch: 96 - Cost: 0.101 Valid Accuracy: 0.869 Epoch: 97 - Cost: 0.101 Valid Accuracy: 0.869 Epoch: 98 - Cost: 0.1 Valid Accuracy: 0.869 Epoch: 99 - Cost: 0.1 Valid Accuracy: 0.869 Test Accuracy: 0.8696000006198883 From looking at the output above, you can see the model doesn’t increase the validation accuracy after epoch 80. Let’s see what happens when we increase the learning rate. learn_rate = 0.1 Epoch: 76 - Cost: 0.214 Valid Accuracy: 0.752 Epoch: 77 - Cost: 0.21 Valid Accuracy: 0.756 Epoch: 78 - Cost: 0.21 Valid Accuracy: 0.756 ... Epoch: 85 - Cost: 0.207 Valid Accuracy: 0.756 Epoch: 86 - Cost: 0.209 Valid Accuracy: 0.756 Epoch: 87 - Cost: 0.205 Valid Accuracy: 0.756 Epoch: 88 - Cost: 0.208 Valid Accuracy: 0.756 Epoch: 89 - Cost: 0.205 Valid Accuracy: 0.756 Epoch: 90 - Cost: 0.202 Valid Accuracy: 0.756 Epoch: 91 - Cost: 0.207 Valid Accuracy: 0.756 Epoch: 92 - Cost: 0.204 Valid Accuracy: 0.756 Epoch: 93 - Cost: 0.206 Valid Accuracy: 0.756 Epoch: 94 - Cost: 0.202 Valid Accuracy: 0.756 Epoch: 95 - Cost: 0.2974 Valid Accuracy: 0.756 Epoch: 96 - Cost: 0.202 Valid Accuracy: 0.756 Epoch: 97 - Cost: 0.2996 Valid Accuracy: 0.756 Epoch: 98 - Cost: 0.203 Valid Accuracy: 0.756 Epoch: 99 - Cost: 0.2987 Valid Accuracy: 0.756 Test Accuracy: 0.7556000053882599 Looks like the learning rate was increased too much. The final accuracy was lower, and it stopped improving earlier. Let’s stick with the previous learning rate, but change the number of epochs to 80. Epoch: 65 - Cost: 0.122 Valid Accuracy: 0.868 Epoch: 66 - Cost: 0.121 Valid Accuracy: 0.868 Epoch: 67 - Cost: 0.12 Valid Accuracy: 0.868 Epoch: 68 - Cost: 0.119 Valid Accuracy: 0.868 Epoch: 69 - Cost: 0.118 Valid Accuracy: 0.868 Epoch: 70 - Cost: 0.118 Valid Accuracy: 0.868 Epoch: 71 - Cost: 0.117 Valid Accuracy: 0.868 Epoch: 72 - Cost: 0.116 Valid Accuracy: 0.868 Epoch: 73 - Cost: 0.115 Valid Accuracy: 0.868 Epoch: 74 - Cost: 0.115 Valid Accuracy: 0.868 Epoch: 75 - Cost: 0.114 Valid Accuracy: 0.868 Epoch: 76 - Cost: 0.113 Valid Accuracy: 0.868 Epoch: 77 - Cost: 0.113 Valid Accuracy: 0.868 Epoch: 78 - Cost: 0.112 Valid Accuracy: 0.868 Epoch: 79 - Cost: 0.111 Valid Accuracy: 0.868 Epoch: 80 - Cost: 0.111 Valid Accuracy: 0.869 Test Accuracy: 0.86909999418258667 The accuracy only reached 0.86, but that could be because the learning rate was too high. Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy. In the upcoming TensorFLow Lab, you’ll get the opportunity to choose your own learning rate, epoch count, and batch size to improve the model’s accuracy. More about epoch in Quora. INTRO TO NEURAL NETWORKSIntro to Neural NetworksIntroducing Luishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nto-stLuN6M.mp4 Logistic Regression Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kSs6O3R7JUI.mp4 Logistic Regression Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1iNylA3fJDs.mp4 Neural Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Mqogpnp1lrU.mp4 Perceptron PerceptronNow you’ve seen how a simple neural network makes decisions: by taking in input data, processing that information, and finally, producing an output in the form of a decision! Let’s take a deeper dive into the university admission example and learn more about how this input data is processed. Data, like test scores and grades, is fed into a network of interconnected nodes. These individual nodes are called perceptrons or neurons, and they are the basic unit of a neural network. Each one looks at input data and decides how to categorize that data. In the example above, the input either passes a threshold for grades and test scores or doesn’t, and so the two categories are: yes (passed the threshold) and no (didn’t pass the threshold). These categories then combine to form a decision – for example, if both nodes produce a “yes” output, then this student gains admission into the university.Let’s zoom in even further and look at how a single perceptron processes input data. The perceptron above is one of the two perceptrons from the video that help determine whether or not a student is accepted to a university. It decides whether a student’s grades are high enough to be accepted to the university. You might be wondering: “How does it know whether grades or test scores are more important in making this acceptance decision?” Well, when we initialize a neural network, we don’t know what information will be most important in making a decision. It’s up to the neural network to learn for itself which data is most important and adjust how it considers that data. It does this with something called weights. WeightsWhen input data comes into a perceptron, it gets multiplied by a weight value that is assigned to this particular input. For example, the perceptron above have two inputs, tests for test scores and grades, so it has two associated weights that can be adjusted individually. These weights start out as random values, and as the neural network learns more about what kind of input data leads to a student being accepted into a university, the network adjusts the weights based on any errors in categorization that the previous weights resulted in. This is called training the neural network. A higher weight means the neural network considers that input more important than other inputs, and lower weight means that the data is considered less important. An extreme example would be if test scores had no affect at all on university acceptance; then the weight of the test score input data would be zero and it would have no affect on the output of the perceptron. Summing the Input DataSo, each input to a perceptron has an associated weight that represents its importance and these weights are determined during the learning process of a neural network, called training. In the next step, the weighted input data is summed up to produce a single value, that will help determine the final output - whether a student is accepted to a university or not. Let’s see a concrete example of this.When writing equations related to neural networks, the weights will always be represented by some type of the letter w. It will usually look like a W when it represents a matrix of weights or a w when it represents an individual weight, and it may include some additional information in the form of a subscript to specify which weights (you’ll see more on that next). But remember, when you see the letter w, think weights. In this example, we’ll use $w{​grades}$ for the weight of grades and $w{​test}$ for the weight of test. For the image above, let’s say that the weights are: $w{​grades}=-1$,$w{​test}=−0.2$. You don’t have to be concerned with the actual values, but their relative values are important. $w{​grades}$ is 5 times larger than ​​$w{​test}$, which means the neural network considers grades input 5 times more important than test in determining whether a student will be accepted into a university. The perceptron applies these weights to the inputs and sums them in a process known as linear combination. In our case, this looks like $$w{​grades}x{​grades}+w{​test}x{​test}=-1x{​grades}-0.2x{​test}$$​. Now, to make our equation less wordy, let’s replace the explicit names with numbers. Let’s use 1 for grades and 2 for tests. So now our equation becomes $$w{1}*x{1}+w{1}*x{1}$$​. In this example, we just have 2 simple inputs: grades and tests. Let’s imagine we instead had m different inputs and we labeled them $x{1},x{2}…x{m}$. Let’s also say that the weight corresponding to $x{1}$​ is $w_{1}$ and so on. In that case, we would express the linear combination succintly as: $$\Sigma1^mw{i}*x{i}$$Here, the Greek letter Sigma $\Sigma$ is used to represent summation. It simply means to evaluate the equation to the right multiple times and add up the results. In this case, the equation it will sum is $w{i}*x_{i}$ But where do we get $w{i}$ and $x{i}$ ? $\Sigma_1^m$ means to iterate over all i values, from 1 to m. So to put it all together, $\Sigma1^mw{i}*x_{i}$ means the following: Start at i=1 Evaluate $w{1}*x{1}$ and remember the results Move to i=2 Evaluate $w{2}*x{2}$ and add these results to $w{1}*x{1}$ Continue repeating that process until i=m, where m is the number of inputs. One last thing: you’ll see equations written many different ways, both here and when reading on your own. For example, you will often just see $\Sigmai$ instead of $\Sigma{i=1}^m$. The first is simply a shorter way of writing the second. That is, if you see a summation without a starting number or a defined end value, it just means perform the sum for all of the them. And sometimes, if the value to iterate over can be inferred, you’ll see it as just $\Sigma$. Just remember they’re all the same thing: $\Sigma{i=1}^m w{i}*x_{i} = \Sigmai w{i}*x{i} = \Sigma w{i}*x_{i}$. Calculating the Output with an Activation FunctionFinally, the result of the perceptron’s summation is turned into an output signal! This is done by feeding the linear combination into an activation function. Activation functions are functions that decide, given the inputs into the node, what should be the node’s output? Because it’s the activation function that decides the actual output, we often refer to the outputs of a layer as its “activations”. One of the simplest activation functions is the Heaviside step function. This function returns a 0 if the linear combination is less than 0. It returns a 1 if the linear combination is positive or equal to zero. The Heaviside step function is shown below, where h is the calculated linear combination:In the university acceptance example above, we used the weights $w{grades} = -1$, $w{​test} = −0.2$. Since​​ $w{grades}$ and $w{​test}$ are negative values, the activation function will only return a 1 if grades and test are 0! This is because the range of values from the linear combination using these weights and inputs are (−∞,0] (i.e. negative infinity to 0, including 0 itself). It’s easiest to see this with an example in two dimensions. In the following graph, imagine any points along the line or in the shaded area represent all the possible inputs to our node. Also imagine that the value along the y-axis is the result of performing the linear combination on these inputs and the appropriate weights. It’s this result that gets passed to the activation function. Now remember that the step activation function returns 1 for any inputs greater than or equal to zero. As you can see in the image, only one point has a y-value greater than or equal to zero – the point right at the origin, (0,0):Now, we certainly want more than one possible grade/test combination to result in acceptance, so we need to adjust the results passed to our activation function so it activates – that is, returns 1 – for more inputs. Specifically, we need to find a way so all the scores we’d like to consider acceptable for admissions produce values greater than or equal to zero when linearly combined with the weights into our node. One way to get our function to return 1 for more inputs is to add a value to the results of our linear combination, called a bias. A bias, represented in equations as b, lets us move values in one direction or another. For example, the following diagram shows the previous hypothetical function with an added bias of +3. The blue shaded area shows all the values that now activate the function. But notice that these are produced with the same inputs as the values shown shaded in grey – just adjusted higher by adding the bias term:Of course, with neural networks we won’t know in advance what values to pick for biases. That’s ok, because just like the weights, the bias can also be updated and changed by the neural network during training. So after adding a bias, we now have a complete perceptron formula:This formula returns 1 if the input $x{1},x{2}…x_{m}$ belongs to the accepted-to-university category or returns 0 if it doesn’t. The input is made up of one or more real numbers, each one represented by $x_{i}$, where m is the number of inputs. Then the neural network starts to learn! Initially, the weights $w_{i}$ and bias (b) are assigned a random value, and then they are updated using a learning algorithm like gradient descent. The weights and biases change so that the next training example is more accurately categorized, and patterns in data are “learned” by the neural network. Now that you have a good understanding of perceptions, let’s put that knowledge to use. In the next section, you’ll create the AND perceptron from the Neural Networks video by setting the values for weights and bias. AND Perceptron Quiz What are the weights and bias for the AND perceptron?Set the weights (weight1, weight2) and bias bias to the correct values that calculate AND operation as shown above.In this case, there are two inputs as seen in the table above (let’s call the first column input1 and the second column input2), and based on the perceptron formula, we can calculate the output. First, the linear combination will be the sum of the weighted inputs: linear_combination = weight1*input1 + weight2*input2 then we can put this value into the biased Heaviside step function, which will give us our output (0 or 1): If you still need a hint, think of a concrete example like so: Consider input1 and input2 both = 1, for an AND perceptron, we want the output to also equal 1! The output is determined by the weights and Heaviside step function such that output = 1, if weight1*input1 + weight2*input2 + bias &gt;= 0 or output = 0, if weight1*input1 + weight2*input2 + bias &lt; 0 So, how can you choose the values for weights and bias so that if both inputs = 1, the output = 1? Gradient Descenthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/29PmNG7fuuM.mp4Gradient is another term for rate of change or slope. If you need to brush up on this concept, check out Khan Academy’s great lectures on the topic. Gradient Descent: The Mathhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7sxA5Ap8AWM.mp4 Gradient Descent: The CodeImplementing Gradient DescentMultilayer Perceptronshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Rs9petvTBLk.mp4Khan Academy’s introduction to vectors.Khan Academy’s introduction to matrices. Backpropagationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/MZL97-2joxQ.mp4 Implementing BackpropagationFurther ReadingFrom Andrej Karpathy: Yes, you should understand backpropAlso from Andrej Karpathy, a lecture from Stanford’s CS231n course DEEP NEURAL NETWORKSTwo-Layer Neural Network Multilayer Neural NetworksIn this lesson, you’ll learn how to build multilayer neural networks with TensorFlow. Adding a hidden layer to a network allows it to model more complex functions. Also, using a non-linear activation function on the hidden layer lets it model non-linear functions. We shall learn about ReLU, a non-linear function, or rectified linear unit. The ReLU function is 0 for negative inputs and x for all inputs x&gt;0. Next, you’ll see how a ReLU hidden layer is implemented in TensorFlow. Quiz: TensorFlow ReLUs1234567891011121314151617181920212223242526272829303132333435# Quiz Solution# Note: You can&apos;t run code in this tabimport tensorflow as tfoutput = Nonehidden_layer_weights = [ [0.1, 0.2, 0.4], [0.4, 0.6, 0.6], [0.5, 0.9, 0.1], [0.8, 0.2, 0.8]]out_weights = [ [0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]# Weights and biasesweights = [ tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]biases = [ tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]# Inputfeatures = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])# TODO: Create Modelhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])hidden_layer = tf.nn.relu(hidden_layer)logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])# TODO: Print session resultswith tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(logits)) Deep Neural Network in TensorFlowDeep Neural Network in TensorFlowYou’ve seen how to build a logistic classifier using TensorFlow. Now you’re going to see how to use the logistic classifier to build a deep neural network. Step by StepIn the following walkthrough, we’ll step through TensorFlow code written to classify the letters in the MNIST database. If you would like to run the network on your computer, the file is provided here. You can find this and many more examples of TensorFlow at Aymeric Damien’s GitHub repository. CodeTensorFlow MNIST from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;.&quot;, one_hot=True, reshape=False) You’ll use the MNIST dataset provided by TensorFlow, which batches and One-Hot encodes the data for you. Learning Parametersimport tensorflow as tf # Parameters learning_rate = 0.001 training_epochs = 20 batch_size = 128 # Decrease batch size if you don&apos;t have enough memory display_step = 1 n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) The focus here is on the architecture of multilayer neural networks, not parameter tuning, so here we’ll just give you the learning parameters. Hidden Layer Parametersn_hidden_layer = 256 # layer number of features The variable n_hidden_layer determines the size of the hidden layer in the neural network. This is also known as the width of a layer. Weights and Biases# Store layers weight &amp; bias weights = { &apos;hidden_layer&apos;: tf.Variable(tf.random_normal([n_input, n_hidden_layer])), &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_layer, n_classes])) } biases = { &apos;hidden_layer&apos;: tf.Variable(tf.random_normal([n_hidden_layer])), &apos;out&apos;: tf.Variable(tf.random_normal([n_classes])) } Deep neural networks use multiple layers with each layer requiring it’s own weight and bias. The &#39;hidden_layer&#39; weight and bias is for the hidden layer. The &#39;out&#39; weight and bias is for the output layer. If the neural network were deeper, there would be weights and biases for each additional layer. Input# tf Graph input x = tf.placeholder(&quot;float&quot;, [None, 28, 28, 1]) y = tf.placeholder(&quot;float&quot;, [None, n_classes]) x_flat = tf.reshape(x, [-1, n_input]) The MNIST data is made up of 28px by 28px images with a single channel. The tf.reshape() function above reshapes the 28px by 28px matrices in x into row vectors of 784px. Multilayer Perceptron # Hidden layer with RELU activation layer_1 = tf.add(tf.matmul(x_flat, weights[&apos;hidden_layer&apos;]),\ biases[&apos;hidden_layer&apos;]) layer_1 = tf.nn.relu(layer_1) # Output layer with linear activation logits = tf.add(tf.matmul(layer_1, weights[&apos;out&apos;]), biases[&apos;out&apos;]) You’ve seen the linear function tf.add(tf.matmul(x_flat, weights[&#39;hidden_layer&#39;]), biases[&#39;hidden_layer&#39;])before, also known as xw + b. Combining linear functions together using a ReLU will give you a two layer network. Optimizer# Define loss and optimizer cost = tf.reduce_mean(\ tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ .minimize(cost) This is the same optimization technique used in the Intro to TensorFLow lab. Session# Initializing the variables init = tf.global_variables_initializer() # Launch the graph with tf.Session() as sess: sess.run(init) # Training cycle for epoch in range(training_epochs): total_batch = int(mnist.train.num_examples/batch_size) # Loop over all batches for i in range(total_batch): batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) and cost op (to get loss value) sess.run(optimizer, feed_dict={x: batch_x, y: batch_y}) The MNIST library in TensorFlow provides the ability to receive the dataset in batches. Calling the mnist.train.next_batch() function returns a subset of the training data. Deeper Neural NetworkThat’s it! Going from one layer to two is easy. Adding more layers to the network allows you to solve more complicated problems. In the next video, you’ll see how changing the number of layers can affect your network. Training a Deep Learning Networkhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CsB7yUtMJyk.mp4 Save and Restore TensorFlow ModelsSave and Restore TensorFlow ModelsTraining a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again! Fortunately, TensorFlow gives you the ability to save your progress using a class called tf.train.Saver. This class provides the functionality to save any tf.Variable to your file system. Saving VariablesLet’s start with a simple example of saving weights and bias Tensors. For the first example you’ll just save two variables. Later examples will save all the weights in a practical model. import tensorflow as tf # The file path to save the data save_file = &apos;./model.ckpt&apos; # Two Tensor Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3])) bias = tf.Variable(tf.truncated_normal([3])) # Class used to save and/or restore Tensor Variables saver = tf.train.Saver() with tf.Session() as sess: # Initialize all the Variables sess.run(tf.global_variables_initializer()) # Show the values of weights and bias print(&apos;Weights:&apos;) print(sess.run(weights)) print(&apos;Bias:&apos;) print(sess.run(bias)) # Save the model saver.save(sess, save_file) Weights: [[-0.97990924 1.03016174 0.74119264] [-0.82581609 -0.07361362 -0.86653847]] Bias: [ 1.62978125 -0.37812829 0.64723819] The Tensors weights and bias are set to random values using the tf.truncated_normal() function. The values are then saved to the save_file location, “model.ckpt”, using the tf.train.Saver.save() function. (The “.ckpt” extension stands for “checkpoint”.) If you’re using TensorFlow 0.11.0RC1 or newer, a file called “model.ckpt.meta” will also be created. This file contains the TensorFlow graph. Loading VariablesNow that the Tensor Variables are saved, let’s load them back into a new model. # Remove the previous weights and bias tf.reset_default_graph() # Two Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3])) bias = tf.Variable(tf.truncated_normal([3])) # Class used to save and/or restore Tensor Variables saver = tf.train.Saver() with tf.Session() as sess: # Load the weights and bias saver.restore(sess, save_file) # Show the values of weights and bias print(&apos;Weight:&apos;) print(sess.run(weights)) print(&apos;Bias:&apos;) print(sess.run(bias)) Weights: [[-0.97990924 1.03016174 0.74119264] [-0.82581609 -0.07361362 -0.86653847]] Bias: [ 1.62978125 -0.37812829 0.64723819] You’ll notice you still need to create the weights and bias Tensors in Python. The tf.train.Saver.restore() function loads the saved data into weights and bias. Since tf.train.Saver.restore() sets all the TensorFlow Variables, you don’t need to call tf.global_variables_initializer(). Save a Trained ModelLet’s see how to train a model and save its weights. First start with a model: # Remove previous Tensors and Operations tf.reset_default_graph() from tensorflow.examples.tutorials.mnist import input_data import numpy as np learning_rate = 0.001 n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;.&apos;, one_hot=True) # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) # Logits - xW + b logits = tf.add(tf.matmul(features, weights), bias) # Define loss and optimizer cost = tf.reduce_mean(\ tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ .minimize(cost) # Calculate accuracy correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) Let’s train that model, then save the weights: import math save_file = &apos;./train_model.ckpt&apos; batch_size = 128 n_epochs = 100 saver = tf.train.Saver() # Launch the graph with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # Training cycle for epoch in range(n_epochs): total_batch = math.ceil(mnist.train.num_examples / batch_size) # Loop over all batches for i in range(total_batch): batch_features, batch_labels = mnist.train.next_batch(batch_size) sess.run( optimizer, feed_dict={features: batch_features, labels: batch_labels}) # Print status for every 10 epochs if epoch % 10 == 0: valid_accuracy = sess.run( accuracy, feed_dict={ features: mnist.validation.images, labels: mnist.validation.labels}) print(&apos;Epoch {:&lt;3} - Validation Accuracy: {}&apos;.format( epoch, valid_accuracy)) # Save the model saver.save(sess, save_file) print(&apos;Trained Model Saved.&apos;) Epoch 0 - Validation Accuracy: 0.06859999895095825 Epoch 10 - Validation Accuracy: 0.20239999890327454 Epoch 20 - Validation Accuracy: 0.36980000138282776 Epoch 30 - Validation Accuracy: 0.48820000886917114 Epoch 40 - Validation Accuracy: 0.5601999759674072 Epoch 50 - Validation Accuracy: 0.6097999811172485 Epoch 60 - Validation Accuracy: 0.6425999999046326 Epoch 70 - Validation Accuracy: 0.6733999848365784 Epoch 80 - Validation Accuracy: 0.6916000247001648 Epoch 90 - Validation Accuracy: 0.7113999724388123 Trained Model Saved. Load a Trained ModelLet’s load the weights and bias from memory, then check the test accuracy. saver = tf.train.Saver() # Launch the graph with tf.Session() as sess: saver.restore(sess, save_file) test_accuracy = sess.run( accuracy, feed_dict={features: mnist.test.images, labels: mnist.test.labels}) print(&apos;Test Accuracy: {}&apos;.format(test_accuracy)) Test Accuracy: 0.7229999899864197 That’s it! You now know how to save and load a trained model in TensorFlow. Let’s look at loading weights and biases into modified models in the next section. FinetuningLoading the Weights and Biases into a New ModelSometimes you might want to adjust, or “finetune” a model that you have already trained and saved. However, loading saved Variables directly into a modified model can generate errors. Let’s go over how to avoid these problems. Naming ErrorTensorFlow uses a string identifier for Tensors and Operations called name. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name , and then give the name &lt;Type&gt;_&lt;number&gt; for the subsequent nodes. Let’s see how this can affect loading a model with a different order of weights and bias: import tensorflow as tf # Remove the previous weights and bias tf.reset_default_graph() save_file = &apos;model.ckpt&apos; # Two Tensor Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3])) bias = tf.Variable(tf.truncated_normal([3])) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Save Weights: {}&apos;.format(weights.name)) print(&apos;Save Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) saver.save(sess, save_file) # Remove the previous weights and bias tf.reset_default_graph() # Two Variables: weights and bias bias = tf.Variable(tf.truncated_normal([3])) weights = tf.Variable(tf.truncated_normal([2, 3])) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Load Weights: {}&apos;.format(weights.name)) print(&apos;Load Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: # Load the weights and bias - ERROR saver.restore(sess, save_file) The code above prints out the following: Save Weights: Variable:0 Save Bias: Variable_1:0 Load Weights: Variable_1:0 Load Bias: Variable:0 … InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. … You’ll notice that the name properties for weights and bias are different than when you saved the model. This is why the code produces the “Assign requires shapes of both tensors to match” error. The code saver.restore(sess, save_file) is trying to load weight data into bias and bias data into weights. Instead of letting TensorFlow set the name property, let’s set it manually: import tensorflow as tf tf.reset_default_graph() save_file = &apos;model.ckpt&apos; # Two Tensor Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3]), name=&apos;weights_0&apos;) bias = tf.Variable(tf.truncated_normal([3]), name=&apos;bias_0&apos;) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Save Weights: {}&apos;.format(weights.name)) print(&apos;Save Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) saver.save(sess, save_file) # Remove the previous weights and bias tf.reset_default_graph() # Two Variables: weights and bias bias = tf.Variable(tf.truncated_normal([3]), name=&apos;bias_0&apos;) weights = tf.Variable(tf.truncated_normal([2, 3]) ,name=&apos;weights_0&apos;) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Load Weights: {}&apos;.format(weights.name)) print(&apos;Load Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: # Load the weights and bias - No Error saver.restore(sess, save_file) print(&apos;Loaded Weights and Bias successfully.&apos;) Save Weights: weights_0:0 Save Bias: bias_0:0 Load Weights: weights_0:0 Load Bias: bias_0:0 Loaded Weights and Bias successfully. That worked! The Tensor names match and the data loaded correctly. Regularization Introhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pECnr-5F3_Q.mp4 Regularizationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QcJBhbuCl5g.mp4 Regularization Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E0eEW6V0_sA.mp4 Dropouthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6DcImJS8uV8.mp4 Dropout Pt. 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8nG8zzJMbZw.mp4 Quiz: TensorFlow DropoutTensorFlow Dropouthttps://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdfDropout is a regularization technique for reducing overfitting. The technique temporarily drops units (artificial neurons) from the network, along with all of those units’ incoming and outgoing connections. Figure 1 illustrates how dropout works. TensorFlow provides the tf.nn.dropout() function, which you can use to implement dropout. Let’s look at an example of how to use tf.nn.dropout(). keep_prob = tf.placeholder(tf.float32) # probability to keep units hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0]) hidden_layer = tf.nn.relu(hidden_layer) hidden_layer = tf.nn.dropout(hidden_layer, keep_prob) logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) The code above illustrates how to apply dropout to a neural network. The tf.nn.dropout() function takes in two parameters: hidden_layer: the tensor to which you would like to apply dropout keep_prob: the probability of keeping (i.e. not dropping) any given unit keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob. During training, a good starting value for keep_prob is 0.5. During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model. Quiz 1Take a look at the code snippet below. Do you see what’s wrong? There’s nothing wrong with the syntax, however the test accuracy is extremely low. ... keep_prob = tf.placeholder(tf.float32) # probability to keep units hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0]) hidden_layer = tf.nn.relu(hidden_layer) hidden_layer = tf.nn.dropout(hidden_layer, keep_prob) logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) ... with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch_i in range(epochs): for batch_i in range(batches): .... sess.run(optimizer, feed_dict={ features: batch_features, labels: batch_labels, keep_prob: 0.5}) validation_accuracy = sess.run(accuracy, feed_dict={ features: test_features, labels: test_labels, keep_prob: 0.5}) QUESTION 1 OF 2 What’s wrong with the above code? Dropout doesn’t work with batching. The keep_prob value of 0.5 is too low. (correct)There shouldn’t be a value passed to keep_prob when testing for accuracy.keep_prob should be set to 1.0 when evaluating validation accuracy. Quiz 2This quiz will be starting with the code from the ReLU Quiz and applying a dropout layer. Build a model with a ReLU layer and dropout layer using the keep_prob placeholder to pass in a probability of 0.5. Print the logits from the model. Note: Output will be different every time the code is run. This is caused by dropout randomizing the units it drops. “solution.py”# Quiz Solution # Note: You can&apos;t run code in this tab import tensorflow as tf hidden_layer_weights = [ [0.1, 0.2, 0.4], [0.4, 0.6, 0.6], [0.5, 0.9, 0.1], [0.8, 0.2, 0.8]] out_weights = [ [0.1, 0.6], [0.2, 0.1], [0.7, 0.9]] # Weights and biases weights = [ tf.Variable(hidden_layer_weights), tf.Variable(out_weights)] biases = [ tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))] # Input features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]]) # TODO: Create Model with Dropout keep_prob = tf.placeholder(tf.float32) hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0]) hidden_layer = tf.nn.relu(hidden_layer) hidden_layer = tf.nn.dropout(hidden_layer, keep_prob) logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) # TODO: Print logits from a session with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(logits, feed_dict={keep_prob: 0.5})) [[ 1.10000002 6.60000038] [ 0.30800003 0.7700001 ] [ 9.56000042 4.78000021]] CONVOLUTIONAL NEURAL NETWORKSIntro To CNNshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B61jxZ4rkMs.mp4 Colorhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BdQccpMwk80.mp4QUIZ QUESTION What would be easier for your classifier to learn? R, G, B(correct)(R + G + B) / 3 Statistical Invariancehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0Hr5YwUUhr0.mp4 Convolutional Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ISHGyvsT0QY.mp4 IntuitionIntuitionLet’s develop better intuition for how Convolutional Neural Networks (CNN) work. We’ll examine how humans classify images, and then see how CNNs use similar approaches. Let’s say we wanted to classify the following image of a dog as a Golden Retriever. As humans, how do we do this? One thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur. We essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog. In this case, we might break down the image into a combination of the following: A nose Two eyes Golden fur These pieces can be seen below: Going One Step FurtherBut let’s take this one step further. How do we determine what exactly a nose is? A Golden Retriever nose can be seen as an oval with two black holes inside it. Thus, one way of classifying a Retriever’s nose is to to break it up into smaller pieces and look for black holes (nostrils) and curves that define an oval as shown below. Broadly speaking, this is what a CNN learns to do. It learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image. Finally, the CNN classifies the image by combining the larger, more complex objects. In our case, the levels in the hierarchy are: Simple shapes, like ovals and dark circles Complex objects (combinations of simple shapes), like eyes, nose, and fur The dog as a whole (a combination of complex objects) With deep learning, we don’t actually program the CNN to recognize these specific features. Rather, the CNN learns on its own to recognize such objects through forward propagation and backpropagation! It’s amazing how well a CNN can learn to classify images, even though we never program the CNN with information about specific features to look for. A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects. The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors. The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs. Once again, the CNN learns all of this on its own. We don’t ever have to tell the CNN to go looking for lines or curves or noses or fur. The CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for. That’s a good start! Hopefully you’ve developed some intuition about how CNNs work. Next, let’s look at some implementation details. FiltersBreaking up an ImageThe first step for a CNN is to break up the image into smaller pieces. We do this by selecting a width and height that defines a filter. The filter looks at small pieces, or patches, of the image. These patches are the same size as the filter. We then simply slide this filter horizontally or vertically to focus on a different piece of the image. The amount by which the filter slides is referred to as the ‘stride’. The stride is a hyperparameter which you, the engineer, can tune. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy. Let’s look at an example. In this zoomed in image of the dog, we first start with the patch outlined in red. The width and height of our filter define the size of this square. We then move the square over to the right by a given stride (2 in this case) to get another patch. What’s important here is that we are grouping together adjacent pixels and treating them as a collective. In a normal, non-convolutional neural network, we would have ignored this adjacency. In a normal network, we would have connected every pixel in the input image to a neuron in the next layer. In doing so, we would not have taken advantage of the fact that pixels in an image are close together for a reason and have special meaning. By taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image. Filter DepthIt’s common to have more than one filter. Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. The amount of filters in a convolutional layer is called the filter depth. How many neurons does each patch connect to? That’s dependent on our filter depth. If we have a depth of k, we connect each patch of pixels to k neurons in the next layer. This gives us the height of k in the next layer, as shown below. In practice, k is a hyperparameter we tune, and most CNNs tend to pick the same starting values. But why connect a single patch to multiple neurons in the next layer? Isn’t one neuron good enough? Multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture. For example, one patch might include some white teeth, some blonde whiskers, and part of a red tongue. In that case, we might want a filter depth of at least three - one for each of teeth, whiskers, and tongue. Having multiple neurons for a given patch ensures that our CNN can learn to capture whatever characteristics the CNN learns are important. Remember that the CNN isn’t “programmed” to look for certain characteristics. Rather, it learns on its own which characteristics to notice. Feature Map Sizeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lp1NrLZnCUM.mp4What are the width, height and depth for padding = ‘same’, stride = 1? Enter your answers in the format “width, height, depth”28,28,8 What are the width, height and depth for padding = ‘valid’, stride = 1? Enter your answers in the format “width, height, depth”26,26,8 What are the width, height and depth for padding = ‘valid’, stride = 2? Enter your answers in the format “width, height, depth”13,13,8 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/W4xtf8LTz1c.mp4 dropped Convolutions continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/utOv-BKI_vo.mp4 Convolutions Cont.Note, a “Fully Connected” layer is a standard, non convolutional layer, where all inputs are connected to all output neurons. This is also referred to as a “dense” layer, and is what we used in the previous two lessons. ParametersParameter SharingThe weights, w, are shared across patches for a given layer in a CNN to detect the cat above regardless of where in the image it is located. When we are trying to classify a picture of a cat, we don’t care where in the image a cat is. If it’s in the top left or the bottom right, it’s still a cat in our eyes. We would like our CNNs to also possess this ability known as translation invariance. How can we achieve this? As we saw earlier, the classification of a given patch in an image is determined by the weights and biases corresponding to that patch. If we want a cat that’s in the top left patch to be classified in the same way as a cat in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way. This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren’t shared across the output channels. There’s an additional benefit to sharing our parameters. If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images. Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model. PaddingA 5x5 grid with a 3x3 filter. Source: Andrej Karpathy. Let’s say we have a 5x5 grid (as shown above) and a filter of size 3x3 with a stride of 1. What’s the width and height of the next layer? We see that we can fit at most three patches in each direction, giving us a dimension of 3x3 in our next layer. As we can see, the width and height of each subsequent layer decreases in such a scheme. In an ideal world, we’d be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency. How might we achieve this? One way is to simply add a border of 0s to our original 5x5 image. You can see what this looks like in the below image.The same grid with 0 padding. Source: Andrej Karpathy. This would expand our original image to a 7x7. With this, we now see how our next layer’s size is again a 5x5, keeping our dimensionality consistent. DimensionalityFrom what we’ve learned so far, how can we calculate the number of neurons of each layer in our CNN? Given: our input layer has a width of W and a height of H our convolutional layer has a filter size F we have a stride of S a padding of P and a filter depth of K, the following formula gives us the width of the next layer: W_out = (W−F+2P)/S+1. The output height would be H_out = (H-F+2P)/S + 1. And the output depth would be equal to the filter depth D_out = K. The output volume would be W_out * H_out * D_out. Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network. Quiz: Convolution Output ShapIntroductionFor the next few quizzes we’ll test your understanding of the dimensions in CNNs. Understanding dimensions will help you make accurate tradeoffs between model size and performance. As you’ll see, some parameters have a much bigger impact on model size than others. SetupH = height, W = width, D = depth We have an input of shape 32x32x3 (HxWxD) 20 filters of shape 8x8x3 (HxWxD) A stride of 2 for both the height and width (S) Valid padding of size 1 ( P ) Recall the formula for calculating the new height or width: new_height = (input_height - filter_height + 2 * P)/S + 1 new_width = (input_width - filter_width + 2 * P)/S + 1 Convolutional Layer Output ShapeWhat’s the shape of the output? The answer format is HxWxD, so if you think the new height is 9, new width is 9, and new depth is 5, then type 9x9x5. 14x14x20 Solution: Convolution OutputSolutionThe answer is 14x14x20. We can get the new height and width with the formula resulting in: (32 - 8 + 2 * 1)/2 + 1 = 14 (32 - 8 + 2 * 1)/2 + 1 = 14 The new depth is equal to the number of filters, which is 20.This would correspond to the following code: input = tf.placeholder(tf.float32, (None, 32, 32, 3)) filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth) filter_bias = tf.Variable(tf.zeros(20)) strides = [1, 2, 2, 1] # (batch, height, width, depth) padding = &apos;VALID&apos; conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias Note the output shape of conv will be [1, 13, 13, 20]. It’s 4D to account for batch size, but more importantly, it’s not [1, 14, 14, 20]. This is because the padding algorithm TensorFlow uses is not exactly the same as the one above. An alternative algorithm is to switch padding from &#39;VALID&#39; to SAME which would result in an output shape of [1, 16, 16, 20]. If you’re curious how padding works in TensorFlow, read this document. Quiz: Number of ParametersWe’re now going to calculate the number of parameters of the convolutional layer. The answer from the last quiz will come into play here! Being able to calculate the number of parameters in a neural network is useful since we want to have control over how much memory a neural network uses. SetupH = height, W = width, D = depth We have an input of shape 32x32x3 (HxWxD) 20 filters of shape 8x8x3 (HxWxD) A stride of 2 for both the height and width (S) Valid padding of size 1 ( P ) Output Layer 14x14x20 (HxWxD) HintWithout parameter sharing, each neuron in the output layer must connect to each neuron in the filter. In addition, each neuron in the output layer must also connect to a single bias neuron. Solution: Number of ParametersSolutionThere are 756560 total parameters. That’s a HUGE amount! Here’s how we calculate it: (8 * 8 * 3 + 1) * (14 * 14 * 20) = 756560 8 * 8 * 3 is the number of weights, we add 1 for the bias. Remember, each weight is assigned to every single part of the output (14 * 14 * 20). So we multiply these two numbers together and we get the final answer. Quiz: Parameter SharingNow we’d like you to calculate the number of parameters in the convolutional layer, if every neuron in the output layer shares its parameters with every other neuron in its same channel. This is the number of parameters actually used in a convolution layer (tf.nn.conv2d()). SetupH = height, W = width, D = depth We have an input of shape 32x32x3 (HxWxD) 20 filters of shape 8x8x3 (HxWxD) A stride of 2 for both the height and width (S) Zero padding of size 1 (P) Output Layer 14x14x20 (HxWxD) HintWith parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel. So the number of parameters is equal to the number of neurons in the filter, plus a bias neuron, all multiplied by the number of channels in the output layer. Convolution Layer Parameters 2How many parameters does the convolution layer have (with parameter sharing)?3860 Solution: Parameter SharingSolutionThere are 3860 total parameters. That’s 196 times fewer parameters! Here’s how the answer is calculated: (8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860 That’s 3840 weights and 20 biases. This should look similar to the answer from the previous quiz. The difference being it’s just 20 instead of (14 * 14 * 20). Remember, with weight sharing we use the same filter for an entire depth slice. Because of this we can get rid of 14 * 14 and be left with only 20. Visualizing CNNsVisualizing CNNsLet’s look at an example CNN to see how it works in action. The CNN we will look at is trained on ImageNet as described in this paper by Zeiler and Fergus. In the images below (from the same paper), we’ll see what each layer in this network detects and see how each layer detects more and more complex ideas. Layer 1Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle). The images above are from Matthew Zeiler and Rob Fergus’ deep visualization toolbox, which lets us visualize what each layer in a CNN focuses on. Each image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. These squares are shown below again for reference.As visualized here, the first layer of the CNN can recognize -45 degree lines. The first layer of the CNN is also able to recognize +45 degree lines, like the one above. Let’s now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.Example patches that activate the -45 degree line detector in the first layer. So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs(斑点). Layer 2A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or “what it sees”) based on the corresponding images from the grid on the right. The second layer of the CNN captures complex ideas. As you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right). The CNN learns to do this on its own. There is no special instruction for the CNN to focus on more complex objects in deeper layers. That’s just how it normally works out when you feed training data into a CNN. Layer 3A visualization of the third layer in the CNN. The gray grid on the left represents how this layer of the CNN activates (or “what it sees”) based on the corresponding images from the grid on the right. The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column). Layer 5A visualization of the fifth and final layer of the CNN. The gray grid on the left represents how this layer of the CNN activates (or “what it sees”) based on the corresponding images from the grid on the right. We’ll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN. The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles. On to TensorFlowThis concludes our high-level discussion of Convolutional Neural Networks. Next you’ll practice actually building these networks in TensorFlow. TensorFlow Convolution LayerTensorFlow Convolution LayerLet’s examine how to implement a CNN in TensorFlow. TensorFlow provides the tf.nn.conv2d() and tf.nn.bias_add() functions to create your own convolutional layers. # Output depth k_output = 64 # Image Properties image_width = 10 image_height = 10 color_channels = 3 # Convolution filter filter_size_width = 5 filter_size_height = 5 # Input/Image input = tf.placeholder( tf.float32, shape=[None, image_height, image_width, color_channels]) # Weight and bias weight = tf.Variable(tf.truncated_normal( [filter_size_height, filter_size_width, color_channels, k_output])) bias = tf.Variable(tf.zeros(k_output)) # Apply Convolution conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) # Add bias conv_layer = tf.nn.bias_add(conv_layer, bias) # Apply activation function conv_layer = tf.nn.relu(conv_layer) The code above uses the tf.nn.conv2d() function to compute the convolution with weight as the filter and [1, 2, 2, 1] for the strides. TensorFlow uses a stride for each input dimension,[batch, input_height, input_width, input_channels]. We are generally always going to set the stride for batch and input_channels (i.e. the first and fourth element in the strides array) to be 1. You’ll focus on changing input_height and input_width while setting batch and input_channels to 1. The input_height and input_width strides are for striding the filter over input. This example code uses a stride of 2 with 5x5 filter over input. The tf.nn.bias_add() function adds a 1-d bias to the last dimension in a matrix. Explore The Design Spacehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FG7M9tWH2nQ.mp4 TensorFlow Max PoolingTensorFlow Max PoolingBy Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons The image above is an example of max pooling with a 2x2 filter and stride of 2. The four 2x2 colors represent each time the filter was applied to find the maximum value. For example, [[1, 0], [4, 6]] becomes 6, because 6 is the maximum value in this set. Similarly, [[2, 3], [6, 8]] becomes 8. Conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values. TensorFlow provides the tf.nn.max_pool() function to apply max pooling to your convolutional layers. ... conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) conv_layer = tf.nn.bias_add(conv_layer, bias) conv_layer = tf.nn.relu(conv_layer) # Apply Max Pooling conv_layer = tf.nn.max_pool( conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) The tf.nn.max_pool() function performs max pooling with the ksize parameter as the size of the filter and the strides parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice. The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). For both ksize and strides, the batch and channel dimensions are typically set to 1. Quiz: Pooling IntuitionThe next few quizzes will test your understanding of pooling layers.QUIZ QUESTION A pooling layer is generally used to … Increase the size of the output(correct)Decrease the size of the output(correct)Prevent overfitting Gain information Solution: Pooling IntuitionSolutionThe correct answer is decrease the size of the output and prevent overfitting. Preventing overfitting is a consequence of reducing the output size, which in turn, reduces the number of parameters in future layers.Recently, pooling layers have fallen out of favor. Some reasons are: Recent datasets are so big and complex we’re more concerned about underfitting. Dropout is a much better regularizer. Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely. Quiz: Pooling MechanicsSetupH = height, W = width, D = depth We have an input of shape 4x4x5 (HxWxD) Filter of shape 2x2 (HxW) A stride of 2 for both the height and width (S) Recall the formula for calculating the new height or width: new_height = (input_height - filter_height)/S + 1 new_width = (input_width - filter_width)/S + 1 NOTE: For a pooling layer the output depth is the same as the input depth. Additionally, the pooling operation is applied individually for each depth slice. The image below gives an example of how a max pooling layer works. In this case, the max pooling filter has a shape of 2x2. As the max pooling filter slides across the input layer, the filter will output the maximum value of the 2x2 square. Pooling Layer Output Shape What’s the shape of the output? Format is HxWxD.2x2x5 Solution: Pooling MechanicsSolutionThe answer is 2x2x5. Here’s how it’s calculated using the formula: (4 - 2)/2 + 1 = 2 (4 - 2)/2 + 1 = 2 The depth stays the same.Here’s the corresponding code: input = tf.placeholder(tf.float32, (None, 4, 4, 5)) filter_shape = [1, 2, 2, 1] strides = [1, 2, 2, 1] padding = &apos;VALID&apos; pool = tf.nn.max_pool(input, filter_shape, strides, padding) The output shape of pool will be [1, 2, 2, 5], even if padding is changed to &#39;SAME&#39;. Quiz: Pooling PracticeGreat, now let’s practice doing some pooling operations manually.Max PoolingWhat’s the result of a max pooling operation on the input: [[[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]]]Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1. The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4. Work from the top left to the bottom right Enter your response hereSUBMIT NEXT Solution: Pooling PracticeSolutionThe correct answer is 2.5,10,15,6. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time. max(0, 1, 2, 2.5) = 2.5 max(0.5, 10, 1, -8) = 10 max(4, 0, 15, 1) = 15 max(5, 6, 2, 3) = 6 Quiz: Average PoolingMean PoolingWhat’s the result of a average (or mean) pooling? [[[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]]]Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1. The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4. Answer to 3 decimal places. Work from the top left to the bottom right Solution: Average PoolingSolutionThe correct answer is 1.375,0.875,5,4. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time. mean(0, 1, 2, 2.5) = 1.375 mean(0.5, 10, 1, -8) = 0.875 mean(4, 0, 15, 1) = 5 mean(5, 6, 2, 3) = 4 1x1 Convolutionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Zmzgerm6SjA.mp4 Inception Modulehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SlTm03bEOxA.mp4 Convolutional Network in TensorFlowConvolutional Network in TensorFlowIt’s time to walk through an example Convolutional Neural Network (CNN) in TensorFlow. The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers. The code you’ll be looking at is similar to what you saw in the segment on Deep Neural Network in TensorFlow, except we restructured the architecture of this network as a CNN. Just like in that segment, here you’ll study the line-by-line breakdown of the code. If you want, you can even download the code and run it yourself. Thanks to Aymeric Damien for providing the original TensorFlow model on which this segment is based. Time to dive in! DatasetYou’ve seen this section of code from previous lessons. Here we’re importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data. from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;.&quot;, one_hot=True, reshape=False) import tensorflow as tf # Parameters learning_rate = 0.00001 epochs = 10 batch_size = 128 # Number of samples to calculate validation and accuracy # Decrease this if you&apos;re running out of memory to calculate accuracy test_valid_size = 256 # Network Parameters n_classes = 10 # MNIST total classes (0-9 digits) dropout = 0.75 # Dropout, probability to keep units Weights and Biases# Store layers weight &amp; bias weights = { &apos;wc1&apos;: tf.Variable(tf.random_normal([5, 5, 1, 32])), &apos;wc2&apos;: tf.Variable(tf.random_normal([5, 5, 32, 64])), &apos;wd1&apos;: tf.Variable(tf.random_normal([7*7*64, 1024])), &apos;out&apos;: tf.Variable(tf.random_normal([1024, n_classes]))} biases = { &apos;bc1&apos;: tf.Variable(tf.random_normal([32])), &apos;bc2&apos;: tf.Variable(tf.random_normal([64])), &apos;bd1&apos;: tf.Variable(tf.random_normal([1024])), &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))} ConvolutionsConvolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution The above is an example of a convolution with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, [[1, 0, 1], [0, 1, 0], [1, 0, 1]], then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using tf.nn.conv2d() and tf.nn.bias_add(). def conv2d(x, W, b, strides=1): x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=’SAME’) x = tf.nn.bias_add(x, b) return tf.nn.relu(x)The tf.nn.conv2d() function computes the convolution against weight W as shown above. In TensorFlow, strides is an array of 4 elements; the first element in this array indicates the stride for batch and last element indicates stride for features. It’s good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them. You can always set the first and last element to 1 in strides in order to use all batches and features. The middle two elements are the strides for height and width respectively. I’ve mentioned stride as one number because you usually have a square stride where height = width. When someone says they are using a stride of 3, they usually mean tf.nn.conv2d(x, W, strides=[1, 3, 3, 1]). To make life easier, the code is using tf.nn.bias_add() to add the bias. Using tf.add() doesn’t work when the tensors aren’t the same shape. Max PoolingMax Pooling with 2x2 filter and stride of 2. Source: http://cs231n.github.io/convolutional-networks/ The above is an example of max pooling with a 2x2 filter and stride of 2. The left square is the input and the right square is the output. The four 2x2 colors in input represents each time the filter was applied to create the max on the right side. For example, [[1, 1], [5, 6]] becomes 6 and [[3, 2], [1, 2]] becomes 3. def maxpool2d(x, k=2): return tf.nn.max_pool( x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=’SAME’)The tf.nn.max_pool() function does exactly what you would expect, it performs max pooling with the ksize parameter as the size of the filter. ModelImage from Explore The Design Space video In the code below, we’re creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step. Then next step applies max pooling, turning each sample into 14x14x32. All the layers are applied from conv1 to output, producing 10 class predictions. def conv_net(x, weights, biases, dropout): # Layer 1 - 28*28*1 to 14*14*32 conv1 = conv2d(x, weights[&apos;wc1&apos;], biases[&apos;bc1&apos;]) conv1 = maxpool2d(conv1, k=2) # Layer 2 - 14*14*32 to 7*7*64 conv2 = conv2d(conv1, weights[&apos;wc2&apos;], biases[&apos;bc2&apos;]) conv2 = maxpool2d(conv2, k=2) # Fully connected layer - 7*7*64 to 1024 fc1 = tf.reshape(conv2, [-1, weights[&apos;wd1&apos;].get_shape().as_list()[0]]) fc1 = tf.add(tf.matmul(fc1, weights[&apos;wd1&apos;]), biases[&apos;bd1&apos;]) fc1 = tf.nn.relu(fc1) fc1 = tf.nn.dropout(fc1, dropout) # Output Layer - class prediction - 1024 to 10 out = tf.add(tf.matmul(fc1, weights[&apos;out&apos;]), biases[&apos;out&apos;]) return out SessionNow let’s run it! # tf Graph input x = tf.placeholder(tf.float32, [None, 28, 28, 1]) y = tf.placeholder(tf.float32, [None, n_classes]) keep_prob = tf.placeholder(tf.float32) # Model logits = conv_net(x, weights, biases, keep_prob) # Define loss and optimizer cost = tf.reduce_mean(\ tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ .minimize(cost) # Accuracy correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # Initializing the variables init = tf. global_variables_initializer() # Launch the graph with tf.Session() as sess: sess.run(init) for epoch in range(epochs): for batch in range(mnist.train.num_examples//batch_size): batch_x, batch_y = mnist.train.next_batch(batch_size) sess.run(optimizer, feed_dict={ x: batch_x, y: batch_y, keep_prob: dropout}) # Calculate batch loss and accuracy loss = sess.run(cost, feed_dict={ x: batch_x, y: batch_y, keep_prob: 1.}) valid_acc = sess.run(accuracy, feed_dict={ x: mnist.validation.images[:test_valid_size], y: mnist.validation.labels[:test_valid_size], keep_prob: 1.}) print(&apos;Epoch {:&gt;2}, Batch {:&gt;3} -&apos; &apos;Loss: {:&gt;10.4f} Validation Accuracy: {:.6f}&apos;.format( epoch + 1, batch + 1, loss, valid_acc)) # Calculate Test Accuracy test_acc = sess.run(accuracy, feed_dict={ x: mnist.test.images[:test_valid_size], y: mnist.test.labels[:test_valid_size], keep_prob: 1.}) print(&apos;Testing Accuracy: {}&apos;.format(test_acc)) That’s it! That is a CNN in TensorFlow. Now that you’ve seen a CNN in TensorFlow, let’s see if you can apply it on your own! TensorFlow Convolution LayerUsing Convolution Layers in TensorFlowLet’s now apply what we’ve learned to build real CNNs in TensorFlow. In the below exercise, you’ll be asked to set up the dimensions of the Convolution filters, the weights, the biases. This is in many ways the trickiest part to using CNNs in TensorFlow. Once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straight forward. ReviewYou should go over the TensorFlow documentation for 2D convolutions. Most of the documentation is straightforward, except perhaps the padding argument. The padding might differ depending on whether you pass &#39;VALID&#39; or &#39;SAME&#39;. Here are a few more things worth reviewing: Introduction to TensorFlow -&gt; TensorFlow Variables.How to determine the dimensions of the output based on the input size and the filter size (shown below). You’ll use this to determine what the size of your filter should be. new_height = (input_height - filter_height + 2 P)/S + 1 new_width = (input_width - filter_width + 2 P)/S + 1 Instructions Finish off each TODO in the conv2d function. Setup the strides, padding and filter weight/bias (F_w and F_b) such that the output shape is (1, 2, 2, 3). Note that all of these except strides should be TensorFlow variables. 12345678910111213141516171819202122232425262728293031323334&quot;&quot;&quot;Setup the strides, padding and filter weight/bias such thatthe output shape is (1, 2, 2, 3).&quot;&quot;&quot;import tensorflow as tfimport numpy as np# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)# (1, 4, 4, 1)x = np.array([ [0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))X = tf.constant(x)def conv2d(input): # Filter (weights and bias) # The shape of the filter weight is (height, width, input_depth, output_depth) # The shape of the filter bias is (output_depth,) # TODO: Define the filter weights `F_W` and filter bias `F_b`. # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all. F_W = ? F_b = ? # TODO: Set the stride for each dimension (batch_size, height, width, depth) strides = [?, ?, ?, ?] # TODO: set the padding, either &apos;VALID&apos; or &apos;SAME&apos;. padding = ? # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after. return tf.nn.conv2d(input, F_W, strides, padding) + F_bout = conv2d(X) Solution: TensorFlow Convolution LayerSolutionHere’s how I did it. NOTE: there’s more than 1 way to get the correct output shape. Your answer might differ from mine. def conv2d(input): # Filter (weights and bias) F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) F_b = tf.Variable(tf.zeros(3)) strides = [1, 2, 2, 1] padding = &apos;VALID&apos; return tf.nn.conv2d(input, F_W, strides, padding) + F_b I want to transform the input shape (1, 4, 4, 1) to (1, 2, 2, 3). I choose ‘VALID’ for the padding algorithm. I find it simpler to understand and it achieves the result I’m looking for. out_height = ceil(float(in_height - filter_height + 1) / float(strides[1])) out_width = ceil(float(in_width - filter_width + 1) / float(strides[2])) Plugging in the values: out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 out_width = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 In order to change the depth from 1 to 3, I have to set the output depth of my filter appropriately: F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) # (height, width, input_depth, output_depth) F_b = tf.Variable(tf.zeros(3)) # (output_depth) The input has a depth of 1, so I set that as the input_depth of the filter. TensorFlow Pooling LayerUsing Pooling Layers in TensorFlowIn the below exercise, you’ll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding. You should go over the TensorFlow documentation for tf.nn.max_pool(). Padding works the same as it does for a convolution. InstructionsFinish off each TODO in the maxpool function.Setup the strides, padding and ksize such that the output shape after pooling is (1, 2, 2, 1). 123456789101112131415161718192021222324252627&quot;&quot;&quot;Set the values to `strides` and `ksize` such thatthe output shape after pooling is (1, 2, 2, 1).&quot;&quot;&quot;import tensorflow as tfimport numpy as np# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)# (1, 4, 4, 1)x = np.array([ [0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))X = tf.constant(x)def maxpool(input): # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth) ksize = [?, ?, ?, ?] # TODO: Set the stride for each dimension (batch_size, height, width, depth) strides = [?, ?, ?, ?] # TODO: set the padding, either &apos;VALID&apos; or &apos;SAME&apos;. padding = ? # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool return tf.nn.max_pool(input, ksize, strides, padding) out = maxpool(X) Solution: TensorFlow Pooling LayerSolutionHere’s how I did it. NOTE: there’s more than 1 way to get the correct output shape. Your answer might differ from mine. def maxpool(input): ksize = [1, 2, 2, 1] strides = [1, 2, 2, 1] padding = ‘VALID’ return tf.nn.max_pool(input, ksize, strides, padding)I want to transform the input shape (1, 4, 4, 1) to (1, 2, 2, 1). I choose &#39;VALID&#39; for the padding algorithm. I find it simpler to understand and it achieves the result I’m looking for. out_height = ceil(float(in_height - filter_height + 1) / float(strides[1])) out_width = ceil(float(in_width - filter_width + 1) / float(strides[2])) Plugging in the values: out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 out_width = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 The depth doesn’t change during a pooling operation so I don’t have to worry about that. CNNs - Additional ResourcesAdditional ResourcesThere are many wonderful free resources that allow you to go into more depth around Convolutional Neural Networks. In this course, our goal is to give you just enough intuition to start applying this concept on real world problems so you have enough of an exposure to explore more on your own. We strongly encourage you to explore some of these resources more to reinforce your intuition and explore different ideas. These are the resources we recommend in particular: Andrej Karpathy’s CS231n Stanford course on Convolutional Neural Networks. Michael Nielsen’s free book on Deep Learning. Goodfellow, Bengio, and Courville’s more advanced free book on Deep Learning. DEEP LEARNING PROJECTProject DetailsIntroduction to the Projecthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/awEYy2Df3hg.mp4 Starting the ProjectStarting the ProjectFor this assignment, you can find the image_classification folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains 3 files: image_classification.ipynb: This is the main file where you will be performing your work on the project. Two helper files, problem_unittests.py and helper.py Submitting the ProjectSubmitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Object Classification Program project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named image_recognition for ease of access: The image_classification.ipynb notebook file with all questions answered and all code cells executed and displaying output along with the .html version of the notebook. All helper files. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. PROJECTImplement this project download this file from github open virtualbox copy downloaded file to my shared file C:\Users\SSQ\virtualbox share type sudo mount -t vboxsf virtualbox_share /mnt/ in ubuntu terminal type jupyter notebook image_classification.ipynb in the right directoryerror ImportError: No module named request (failed)try with anaconda3 in ubuntu download anaconda3 from this web type ./Anaconda3-4.3.1-Linux-x86_64.sh in terminal to run sh file Anaconda3 will now be installed into this location:/home/ssq/anaconda3installation finished.Do you wish the installer to prepend the Anaconda3 install locationto PATH in your /home/ssq/.bashrc ? [yes|no][no] &gt;&gt;&gt;You may wish to edit your .bashrc or prepend the Anaconda3 install location: $ export PATH=/home/ssq/anaconda3/bin:$PATH Thank you for installing Anaconda3! Share your notebooks and packages on Anaconda Cloud!Sign up for free: https://anaconda.org export PATH=/home/ssq/anaconda3/bin:$PATH in your .ipynb location conda create -n tensorflow error:ModuleNotFoundError: No module named &#39;tqdm&#39;method:conda install -c conda-forge tqdm Package plan for installation in environment /home/ssq/anaconda3: The following NEW packages will be INSTALLED: tqdm: 4.11.2-py36_0 conda-forge The following packages will be SUPERCEDED by a higher-priority channel: conda: 4.3.14-py36_0 --&gt; 4.2.13-py36_0 conda-forge conda-env: 2.6.0-0 --&gt; 2.6.0-0 conda-forge Proceed ([y]/n)? y Anaconda installation export PATH=/home/ssq/anaconda3/bin:$PATH source activate tensorflow 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yesconda install tensorflow conda install -c conda-forge tensorflow 12conda install pandas matplotlib jupyter notebook scipy scikit-learnpip install tensorflow pip3 install --upgrade pip (failed)pip3 install tensorflow in ubuntu from this web to create a new vb 安装增强 剪切板双向 set the shared file from this blog and type sudo mount -t vboxsf virtualbox_share /mnt/ type sudo apt install python3-pip in terminal to install python3 pip3 install tensorflow python3 and test (success)anaconda3 install in Win7 tensorflow download anaconda3 from this web 1234conda create -n tensorflow python=3.5activate tensorflowconda install pandas matplotlib jupyter notebook scipy scikit-learnpip install tensorflow (tensorflow) C:\Users\SSQ&gt;cd C:\Users\SSQ\virtualbox share\image-classification (tensorflow) C:\Users\SSQ\virtualbox share\image-classification&gt;jupyter notebook image_classification.ipynb ModuleNotFoundError: No module named &#39;tqdm&#39;method:(tensorflow) C:\Users\SSQ\virtualbox share\image-classification&gt;conda install tqdm anaconda3 install in win7 tensorflow-gpu view this page and this blog cuda_8.0.61_windows in win7 cudnn-8.0-windows7-x64-v6.0 1234conda create -n tensorflow-gpu python=3.5activate tensorflow-gpuconda install pandas matplotlib jupyter notebook scipy scikit-learnpip install tensorflow-gpu SubmissionImage ClassificationProject Submission Image Classification IntroductionIn this project, you’ll classify images from the CIFAR-10 dataset. The dataset consists of airplanes, dogs, cats, and other objects. The dataset will need to be preprocessed, then train a convolutional neural network on all the samples. You’ll normalize the images, one-hot encode the labels, build a convolutional layer, max pool layer, and fully connected layer. At then end, you’ll see their predictions on the sample images. Getting the project filesThe project files can be found in our public GitHub repo, in the image-classification folder. You can download the files from there, but it’s better to clone the repository to your computer This way you can stay up to date with any changes we make by pulling the changes to your local repository with git pull. Submission Ensure you’ve passed all the unit tests in the notebook. Ensure you pass all points on the rubric. When you’re done with the project, please save the notebook as an HTML file. You can do this by going to the File menu in the notebook and choosing “Download as” &gt; HTML. Ensure you submit both the Jupyter Notebook and it’s HTML version together. Package the “dlnd_image_classification.ipynb”, “helper.py”, “problem_unittests.py”, and the HTML file into a zip archive, or push the files from your GitHub repo. Hit Submit Project below! Submit Your Projectsubmitview submissionreference Career: Interview Practice INTERVIEW PRACTICE ML Interview Practice Technical Interview Practice Machine Learning SpecializationsCapstone ProposalPROJECTWriting up a Capstone proposalOverviewCapstone Proposal OverviewPlease note that once your Capstone Proposal has been submitted and you have passed the evaluation, you have to submit your Capstone project using the same proposal that you submitted. We do not allow the Capstone Proposal and the Capstone project to differ in terms of dataset and approach. In this capstone project proposal, prior to completing the following Capstone Project, you you will leverage what you’ve learned throughout the Nanodegree program to author a proposal for solving a problem of your choice by applying machine learning algorithms and techniques. A project proposal encompasses seven key points: The project’s domain background — the field of research where the project is derived; A problem statement — a problem being investigated for which a solution will be defined; The datasets and inputs — data or inputs being used for the problem; A solution statement — a the solution proposed for the problem given; A benchmark model — some simple or historical model or result to compare the defined solution to; A set of evaluation metrics — functional representations for how the solution can be measured; An outline of the project design — how the solution will be developed and results obtained. Capstone Proposal HighlightsThe capstone project proposal is designed to introduce you to writing proposals for major projects. Typically, before you begin working on a solution to a problem, a proposal is written to your peers, advisor, manager, etc., to outline the details of the problem, your research, and your approach to a solution. Things you will learn by completing this project proposal: How to research a real-world problem of interest. How to author a technical proposal document. How to organize a proposed workflow for designing a solution. DescriptionCapstone Proposal DescriptionThink about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as datasets, inputs, and research) to complete this project, and make the appropriate citations wherever necessary in your proposal. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. To determine whether your project and the problem you want to solve fits Udacity’s vision of a Machine Learning Capstone Project , please refer to the capstone proposal rubric and the capstone project rubric and make a note of each rubric criteria you will be evaluated on. A satisfactory project will have a proposal that clearly satisfies these requirements. Software and Data RequirementsSoftware RequirementsYour proposed project must be written in Python 2.7. Given the free-form nature of the machine learning capstone, the software and libraries you will need to successfully complete your work will vary depending on the chosen application area and problem definition. Because of this, it is imperative that all necessary software and libraries you consider using in your capstone project are accessible clearly documented. Please note that proprietary software, software that requires private licenses, or software behind a paywall or login account should be avoided. Data RequirementsEvery machine learning capstone project will most certainly require some form of dataset or input data structure (input text files, images, etc.). Similar to the software requirements above, the data you are considering must either be publicly accessible or provided by you during the submission process, and private or proprietary data should not be used without expressed permission. Please take into consideration the file size of your data — while there is no strict upper limit, input files that are excessively large may require reviewers longer than an acceptable amount of time to acquire all of your project files. This can take away from the reviewer’s time that could be put towards evaluating your proposal. If the data you are considering fits the criteria of being too large, consider whether you could work with a subset of the data instead, or provide a representative sample of the data. EthicsUdacity’s A/B Testing course, as part of the Data Analyst Nanodegree, has a segment that discusses the sensitivity of data and the expectation of privacy from those whose information has been collected. While most data you find available to the public will not have any ethical complications, it is extremely important that you are considering where the data you are using came from, and whether that data contains any sensitive information. For example, if you worked for a bank and wanted to use customers’ bank statements as part of your project, this would most likely be an unethical choice of data and should be avoided. If you have any questions regarding the nature of a dataset or software you intend to use for the capstone project, please send an email to machine-support@udacity.com with the subject “Capstone Project Dataset/Software Inquiry”. Proposal GuidelinesReport GuidelinesYour project submission will be evaluated on the written proposal that is submitted. Additionally, depending on the project you are proposing, other materials such as the data being used will be evaluated. It is expected that the proposal contains enough detail, documentation, analysis, and discussion to adequately reflect the work you intend to complete for the project. Because of this, it is extremely important that the proposal is written in a professional, standardized way, so those who review your project’s proposal are able to clearly identify each component of your project in the report. Without a properly written proposal, your project cannot be sufficiently evaluated. A project proposal template is provided for you to understand how a project proposal should be structured. We strongly encourage students to have a proposal that is approximately two to three pages in length. The Machine Learning Capstone Project proposal should be treated no different than a written research paper for academics. Your goal is to ultimately present the research you’ve discovered into the respective problem domain you’ve chosen, and then clearly articulate your intended project to your peers. The narrative found in the project proposal template provides for a “proposal checklist” that will aid you in fully completing a documented proposal. Please make use of this resource! Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Capstone Project Proposal rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include A project proposal, in PDF format only, with the name proposal.pdf, addressing each of the seven key points of a proposal. The recommended page length for a proposal is approximately two to three pages. Any additional supporting material such as datasets, images, or input files that are necessary for your project and proposal. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in an included README.md file.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionCapstone ProposalProject SubmissionIn this capstone project proposal, prior to completing the following Capstone Project, you you will leverage what you’ve learned throughout the Nanodegree program to author a proposal for solving a problem of your choice by applying machine learning algorithms and techniques. A project proposal encompasses seven key points: The project’s domain background — the field of research where the project is derived; A problem statement — a problem being investigated for which a solution will be defined; The datasets and inputs — data or inputs being used for the problem; A solution statement — a the solution proposed for the problem given; A benchmark model — some simple or historical model or result to compare the defined solution to; A set of evaluation metrics — functional representations for how the solution can be measured; An outline of the project design — how the solution will be developed and results obtained. Think about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as datasets, inputs, and research) to complete this project, and make the appropriate citations wherever necessary in your proposal. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. EvaluationYour project will be reviewed by a Udacity reviewer against the Capstone Project Proposal rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include A project proposal, in PDF format only, with the name proposal.pdf, addressing each of the seven key points of a proposal. The recommended page length for a proposal is approximately two to three pages. Any additional supporting material such as datasets, images, or input files that are necessary for your project and proposal. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in an included README.md file.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! submit Capstone ProjectPROJECTMachine Learning Capstone ProjectOverviewCapstone Project OverviewIn this capstone project, you will leverage what you’ve learned throughout the Nanodegree program to solve a problem of your choice by applying machine learning algorithms and techniques. You will first define the problem you want to solve and investigate potential solutions and performance metrics. Next, you will analyze the problem through visualizations and data exploration to have a better understanding of what algorithms and features are appropriate for solving it. You will then implement your algorithms and metrics of choice, documenting the preprocessing, refinement, and postprocessing steps along the way. Afterwards, you will collect results about the performance of the models used, visualize significant quantities, and validate/justify these values. Finally, you will construct conclusions about your results, and discuss whether your implementation adequately solves the problem. Capstone Project HighlightsThis project is designed to prepare you for delivering a polished, end-to-end solution report of a real-world problem in a field of interest. When developing new technology, or deriving adaptations of previous technology, properly documenting your process is critical for both validating and replicating your results. Things you will learn by completing this project: How to research and investigate a real-world problem of interest. How to accurately apply specific machine learning algorithms and techniques. How to properly analyze and visualize your data and results for validity. How to document and write a report of your work. DescriptionCapstone DescriptionThink about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as data sets) to complete this project, and make the appropriate citations wherever necessary in your report. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. Note: For students who have enrolled before October 17th, we strongly encourage that you look at the Capstone Proposal project that is available as an elective before this project. If you have an idea for your capstone project but aren’t ready to begin working on the implementation, or even if you want to get feedback on how you will approach a solution to your problem, you can use the Capstone Proposal project to have a peer-review from one of our Capstone Project reviewers! For whichever application area or problem you ultimately investigate, there are five major stages to this capstone project which you will move through and subsequently document. Each stage plays a significant role in the development life cycle of beginning with a problem definition and finishing with a polished, working solution. As you make your way through developing your project, be sure that you are also working on a rough draft of your project report, as it is the most important aspect to your submission! To determine whether your project and the problem you want to solve fits Udacity’s vision of a Machine Learning Capstone Project , please refer to the capstone project rubric and make a note of each rubric criteria you will be evaluated on. A satisfactory project will have a report that encompasses each stage and component of the rubric. Software and Data RequirementsSoftware RequirementsYour project must be written in Python 2.7. Given the free-form nature of the machine learning capstone, the software and libraries you will need to successfully complete your work will vary depending on the chosen application area and problem definition. Because of this, it is imperative that all necessary software and libraries used in your capstone project are accessible to the reviewer and clearly documented. Information regarding the software and libraries your project makes use of should be included in the README along with your submission. Please note that proprietary software, software that requires private licenses, or software behind a paywall or login account should be avoided. Data RequirementsEvery machine learning capstone project will most certainly require some form of dataset or input data structure (input text files, images, etc.). Similar to the software requirements above, the data you use must either be publicly accessible or provided by you during the submission process, and private or proprietary data should not be used without expressed permission. Please take into consideration the file size of your data — while there is no strict upper limit, input files that are excessively large may require reviewers longer than an acceptable amount of time to acquire all of your project files and/or execute the provided development code. This can take away from the reviewer’s time that could be put towards evaluating your submission. If the data you are working with fits the criteria of being too large, consider whether you can work with a subset of the data instead, or provide a representative sample of the data which the reviewer may use to verify the solution explored in the project. EthicsUdacity’s A/B Testing course, as part of the Data Analyst Nanodegree, has a segment that discusses the sensitivity of data and the expectation of privacy from those whose information has been collected. While most data you find available to the public will not have any ethical complications, it is extremely important that you are considering where the data you are using came from, and whether that data contains any sensitive information. For example, if you worked for a bank and wanted to use customers’ bank statements as part of your project, this would most likely be an unethical choice of data and should be avoided. Report GuidelinesReport GuidelinesYour project submission will be evaluated primarily on the report that is submitted. It is expected that the project report contains enough detail, documentation, analysis, and discussion to adequately reflect the work you completed for your project. Because of this, it is extremely important that the report is written in a professional, standardized way, so those who review your project submission are able to clearly identify each component of your project in the report. Without a properly written report, your project cannot be sufficiently evaluated. A project report template is provided for you to understand how a project report should be structured. We strongly encourage students to have a report that is approximately nine to fifteen pages in length. The Machine Learning Capstone Project report should be treated no different than a written research paper for academics. Your goal is to ultimately present the research you’ve discovered into the respective problem domain you’ve chosen, and then discuss each stage of the project as they are completed. The narrative found in the A project report template provides for a “report checklist” that will aid you in staying on track for both your project and the documentation in your report. Each stage can be found as a section that will guide you through each component of the project development life cycle. Please make use of this resource! Example ReportsExample Machine Learning Capstone ReportsIncluded in the project files for the Capstone are three example reports that were written by students just like yourselves. Because the written report for your project will be how you are evaluated, it is absolutely critical that you are producing a clear, detailed, well-written report that adequately reflects the work that you’ve completed for your Capstone. Following along with the Capstone Guidelines will be very helpful as you begin writing your report. Our first example report comes from graduate Martin Bede, whose project design in the field of computer vision, named “Second Sight”, was to create an Android application that would extract text from the device’s camera and read it aloud. Martin’s project cites the growing concern of vision loss as motivation for developing software that can aid those unable to see or read certain print. Our second example report comes from an anonymous graduate whose project design in the field of image recognition was to implement a Convolutional Neural Network (CNN) to train on the Cifar-10 dataset and successfully identify different objects in new images. This student describes with thorough detail how a CNN can be used quite effectively as a descriptor-learning image recognition algorithm. Our third example report comes from graduate Naoki Shibuya, who took advantage of the pre-curated robot motion planning “Plot and Navigate a Virtual Maze” project. Pay special attention to the emphasis Naoki places on discussing the methodology and results: Projects relying on technical implementations require valuable observations and visualizations of how the solution performs under various circumstances and constraints. Each example report given has many desirable qualities we expect from students when completing the Machine Learning Capstone project. Once you begin writing your project report for which ever problem domain you choose, be sure to reference these examples whenever necessary! Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Machine Learning Capstone project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include Your capstone proposal document as proposal.pdf if you have completed the pre-requisite Capstone Proposal project. Please also include your review link in the student submission notes. A project report (in PDF format only) addressing the five major project development stages. The recommended page length for a project report is approximately nine to fifteen pages. Please do not export an iPython Notebook as PDF for your project report. All development Python code used for your project that is required to reproduce your implemented solution and result. Your code should be in a neat and well-documented format. Using iPython Notebooks is strongly encouraged for development. A README documentation file which briefly describes the software and libraries used in your project, including any necessary references to supporting material. If your project requires setup/startup, ensure that your README includes the necessary instructions. Any additional supporting material such as datasets, images, or input files that are necessary for your project’s development and implementation. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in your included README.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionCapstone ProjectProject SubmissionIn this capstone project, you will leverage what you’ve learned throughout the Nanodegree program to solve a problem of your choice by applying machine learning algorithms and techniques. You will first define the problem you want to solve and investigate potential solutions and performance metrics. Next, you will analyze the problem through visualizations and data exploration to have a better understanding of what algorithms and features are appropriate for solving it. You will then implement your algorithms and metrics of choice, documenting the preprocessing, refinement, and postprocessing steps along the way. Afterwards, you will collect results about the performance of the models used, visualize significant quantities, and validate/justify these values. Finally, you will construct conclusions about your results, and discuss whether your implementation adequately solves the problem. Think about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as datasets, inputs, and research) to complete this project, and make the appropriate citations wherever necessary in your proposal. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. Note: For students who have enrolled before October 17th, we strongly encourage that you look at the Capstone Proposal project that is available as an elective before this project. If you have an idea for your capstone project but aren’t ready to begin working on the implementation, or even if you want to get feedback on how you will approach a solution to your problem, you can use the Capstone Proposal project to have a peer-review from one of our Capstone Project reviewers! For whichever application area or problem you ultimately investigate, there are five major stages to this capstone project which you will move through and subsequently document. Each stage plays a significant role in the development life cycle of beginning with a problem definition and finishing with a polished, working solution. As you make your way through developing your project, be sure that you are also working on a rough draft of your project report, as it is the most important aspect to your submission! EvaluationYour project will be reviewed by a Udacity reviewer against the Machine Learning Capstone project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include Your capstone proposal document as proposal.pdf if you have completed the pre-requisite Capstone Proposal project. Please also include your review link in the student submission notes. A project report (in PDF format only) addressing the five major project development stages. The recommended page length for a project report is approximately nine to fifteen pages. Please do not export an iPython Notebook as PDF for your project report. All development Python code used for your project that is required to reproduce your implemented solution and result. Your code should be in a neat and well-documented format. Using iPython Notebooks is strongly encouraged for development. A README documentation file which briefly describes the software and libraries used in your project, including any necessary references to supporting material. If your project requires setup/startup, ensure that your README includes the necessary instructions. Any additional supporting material such as datasets, images, or input files that are necessary for your project’s development and implementation. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in your included README. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! Supporting MaterialsVideos Zip FileTHE MNIST DATABASE of handwritten digitsMachine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networksselfdrivingcars (2,2),25,500,128,200,success Testing Accuracy: 0.8081980186934564 First result1 convnet1 fully_con12345678conv2d_maxpool(x, 25, (2,2), (1,1), (2,2), (2,2))flatten(x)fully_conn(x, 500)tf.nn.dropout(x, keep_prob)output(x,10)epochs = 200batch_size = 128keep_probability = 0.5 save as capstone_model.meta Testing Accuracy: 0.808136261212445 Second result3 convnets1 fully_con123456789101112x = conv2d_maxpool(x, 10, (2,2), (1,1), (2,2), (2,2))x = conv2d_maxpool(x, 10, (2,2), (1,1), (2,2), (2,2))x = conv2d_maxpool(x, 10, (2,2), (1,1), (2,2), (2,2))flatten(x)fully_conn(x, 500)tf.nn.dropout(x, keep_prob)output(x,10)epochs = 200batch_size = 128keep_probability = 0.5validation_accuracy: 0.7073670029640198 Testing Accuracy: 0.8427798201640447with fully datasetsTesting Accuracy: 0.8851721937559089 Submit Your ProjectML Stanford]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Udacity</tag>
        <tag>NanoDegree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity cs101 Intro to CS Notebook]]></title>
    <url>%2F2017%2F02%2F05%2FUdacity%20cs101%20Intro%20to%20CS%20Notebook%2F</url>
    <content type="text"><![CDATA[Course can be found hereclassroomDone in 2017-05-01 LESSONSHow to manage dataMeasure Udacity123456789101112131415161718192021222324# Define a procedure, measure_udacity,# that takes as its input a list of strings,# and returns a number that is a count# of the number of elements in the input# list that start with the uppercase # letter &apos;U&apos;.def measure_udacity(word): count = 0 for i in word: if i.find(&apos;U&apos;)&gt;=0: print &apos;i: &apos;,i count+=1 print &apos;count: &apos;,count return count print measure_udacity([&apos;Dave&apos;,&apos;Sebastian&apos;,&apos;Katy&apos;])#&gt;&gt;&gt; 0print measure_udacity([&apos;Umika&apos;,&apos;Umberto&apos;])#&gt;&gt;&gt; 2 Find Element123456789101112131415161718192021# Define a procedure, find_element,# that takes as its inputs a list# and a value of any type, and# returns the index of the first# element in the input list that# matches the value.# If there is no matching element,# return -1.def find_element(U,u): if u in U: return U.index(u) else: return -1print find_element([1,2,3],3)#&gt;&gt;&gt; 2print find_element([&apos;alpha&apos;,&apos;beta&apos;],&apos;gamma&apos;)#&gt;&gt;&gt; -1 Union1234567891011121314151617181920212223# Define a procedure, union,# that takes as inputs two lists.# It should modify the first input# list to be the set union of the two# lists. You may assume the first list# is a set, that is, it contains no # repeated elements.def union(a,b): for e in b: if e not in a: a.append(e)# To test, uncomment all lines # below except those beginning with &gt;&gt;&gt;.a = [1,2,3]b = [2,4,6]union(a,b)print a #&gt;&gt;&gt; [1,2,3,4,6]print b#&gt;&gt;&gt; [2,4,6] Pop Quiz31.1 Get All LinksPrint All Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Inr_DYUqxk8.mp4 Quiz: Print All Linksprint_all_links must keep going until there are no more links to print. Think about looping forever (set while loop condition) until there are no more links (i.e. else:). What do you do when there are no more links (body of else: condition)? At 1:26, Dave uses a procedure get_page(). The code for this procedure is given later in the course, in Lesson 4. This is the code: def get_page(url): try: import urllib return urllib.urlopen(url).read() except: return &apos;&apos; Include this code above your get_next_target() procedure in your answer. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BFYeJzcejxM.mp4 Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xssLR71EuUw.mp4 Starting Get All Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/m3oEwba-yxU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/f4L30M_25AI.mp4 Updating Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8krkKyimMUA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4-5169UHZTM.mp4 Finishing Get All Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E3_IlnR_j44.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LUnnw_TBxPI.mp4 Finishing the Web Crawlerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BWKxjRDadkI.mp4 Crawling Processhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/l_ulQLpFQJQ.mp4 Quiz: Crawling ProcessPseudo code from the video: start with tocrawl = [seed] crawled = [] while there are more pages tocrawl: pick a page from tocrawl add that page to crawled add all the link targets on this page to tocrawl return crawled The seed page where crawling begins. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_DESjvmuSsA.mp4 Crawl Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bI3rP7tAGdA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XRkqyIvx39w.mp4 Crawl Web Loophttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h4pJFmz7l1g.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jRm4rYw1w6c.mp4 Crawl Ifhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lZhKW6QTmX0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sopj7b5XEfk.mp4 Finishing Crawl Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nQl4F9uMvGU.mp4 Quiz: Finishing Crawl WebHint: at some point, you will have to call get_page on page. It seems counterintuitive, but we use the word page to refer to both the url and the html of a webpage. The get_page procedure takes a url and returns html. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#Finish crawl webdef get_page(url): # This is a simulated get_page procedure so that you can test your # code on two pages &quot;http://xkcd.com/353&quot; and &quot;http://xkcd.com/554&quot;. # A procedure which actually grabs a page from the web will be # introduced in unit 4. try: if url == &quot;http://xkcd.com/353&quot;: return &apos;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;&lt;?xml-stylesheet href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; type=&quot;text/css&quot; media=&quot;screen&quot; ?&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.1//EN&quot; &quot;http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd&quot;&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt; &lt;head&gt; &lt;title&gt;xkcd: Python&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt; &lt;!--[if IE]&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/ecbbecc.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt;&lt;![endif]--&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/atom+xml&quot; title=&quot;Atom 1.0&quot; href=&quot;/atom.xml&quot; /&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/rss+xml&quot; title=&quot;RSS 2.0&quot; href=&quot;/rss.xml&quot; /&gt; &lt;link rel=&quot;icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;link rel=&quot;shortcut icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;div id=&quot;topContainer&quot;&gt; &lt;div id=&quot;topLeft&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt;\t&lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://xkcd.com/554&quot;&quot;&gt;Archive&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;\t &lt;li&gt;&lt;a href=&quot;http://blag.xkcd.com/&quot;&gt;News/Blag&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://store.xkcd.com/&quot;&gt;Store&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/about/&quot;&gt;About&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://forums.xkcd.com/&quot;&gt;Forums&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;topRight&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;div id=&quot;topRightContainer&quot;&gt; &lt;div id=&quot;logo&quot;&gt; &lt;a href=&quot;/&quot;&gt;&lt;img src=&quot;http://imgs.xkcd.com/s/9be30a7.png&quot; alt=&quot;xkcd.com logo&quot; height=&quot;83&quot; width=&quot;185&quot;/&gt;&lt;/a&gt; &lt;h2&gt;&lt;br /&gt;A webcomic of romance,&lt;br/&gt; sarcasm, math, and language.&lt;/h2&gt; &lt;div class=&quot;clearleft&quot;&gt;&lt;/div&gt; &lt;br /&gt;XKCD updates every Monday, Wednesday, and Friday. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;contentContainer&quot;&gt; &lt;div id=&quot;middleContent&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt;&lt;br/&gt;&lt;br /&gt;&lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/352/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_t&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/354/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;br/&gt;&lt;br/&gt;&lt;img src=&quot;http://imgs.xkcd.com/comics/python.png&quot; title=&quot;I wrote 20 short programs in Python yesterday. It was wonderful. Perl, Im leaving you.&quot; alt=&quot;Python&quot; /&gt;&lt;br/&gt;&lt;br/&gt;&lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/352/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_b&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/354/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;h3&gt;Permanent link to this comic: http://xkcd.com/353/&lt;/h3&gt;&lt;h3&gt;Image URL (for hotlinking/embedding): http://imgs.xkcd.com/comics/python.png&lt;/h3&gt;&lt;div id=&quot;transcript&quot; style=&quot;display: none&quot;&gt;[[ Guy 1 is talking to Guy 2, who is floating in the sky ]]Guy 1: You39;re flying! How?Guy 2: Python!Guy 2: I learned it last night! Everything is so simple!Guy 2: Hello world is just 39;print &amp;quot;Hello, World!&amp;quot; 39;Guy 1: I dunno... Dynamic typing? Whitespace?Guy 2: Come join us! Programming is fun again! It39;s a whole new world up here!Guy 1: But how are you flying?Guy 2: I just typed 39;import antigravity39;Guy 1: That39;s it?Guy 2: ...I also sampled everything in the medicine cabinet for comparison.Guy 2: But i think this is the python.&#123;&#123; I wrote 20 short programs in Python yesterday. It was wonderful. Perl, I39;m leaving you. &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;middleFooter&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;img src=&quot;http://imgs.xkcd.com/s/a899e84.jpg&quot; width=&quot;520&quot; height=&quot;100&quot; alt=&quot;Selected Comics&quot; usemap=&quot; comicmap&quot; /&gt; &lt;map name=&quot;comicmap&quot;&gt; &lt;area shape=&quot;rect&quot; coords=&quot;0,0,100,100&quot; href=&quot;/150/&quot; alt=&quot;Grownups&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;104,0,204,100&quot; href=&quot;/730/&quot; alt=&quot;Circuit Diagram&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;208,0,308,100&quot; href=&quot;/162/&quot; alt=&quot;Angular Momentum&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;312,0,412,100&quot; href=&quot;/688/&quot; alt=&quot;Self-Description&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;416,0,520,100&quot; href=&quot;/556/&quot; alt=&quot;Alternative Energy Revolution&quot; /&gt; &lt;/map&gt;&lt;br/&gt;&lt;br /&gt;Search comic titles and transcripts:&lt;br /&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/jsapi&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; google.load(\&quot;search\&quot;, \&quot;1\&quot;); google.setOnLoadCallback(function() &#123; google.search.CustomSearchControl.attachAutoCompletion( \&quot;012652707207066138651:zudjtuwe28q\&quot;, document.getElementById(\&quot;q\&quot;), \&quot;cse-search-box\&quot;); &#125;);&lt;/script&gt;&lt;form action=&quot;//www.google.com/cse&quot; id=&quot;cse-search-box&quot;&gt; &lt;div&gt; &lt;input type=&quot;hidden&quot; name=&quot;cx&quot; value=&quot;012652707207066138651:zudjtuwe28q&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;ie&quot; value=&quot;UTF-8&quot; /&gt; &lt;input type=&quot;text&quot; name=&quot;q&quot; id=&quot;q&quot; autocomplete=&quot;off&quot; size=&quot;31&quot; /&gt; &lt;input type=&quot;submit&quot; name=&quot;sa&quot; value=&quot;Search&quot; /&gt; &lt;/div&gt;&lt;/form&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/cse/brand?form=cse-search-box&amp;lang=en&quot;&gt;&lt;/script&gt;&lt;a href=&quot;/rss.xml&quot;&gt;RSS Feed&lt;/a&gt; - &lt;a href=&quot;/atom.xml&quot;&gt;Atom Feed&lt;/a&gt;&lt;br /&gt; &lt;br/&gt; &lt;div id=&quot;comicLinks&quot;&gt; Comics I enjoy:&lt;br/&gt; &lt;a href=&quot;http://www.qwantz.com&quot;&gt;Dinosaur Comics&lt;/a&gt;, &lt;a href=&quot;http://www.asofterworld.com&quot;&gt;A Softer World&lt;/a&gt;, &lt;a href=&quot;http://pbfcomics.com/&quot;&gt;Perry Bible Fellowship&lt;/a&gt;, &lt;a href=&quot;http://www.boltcity.com/copper/&quot;&gt;Copper&lt;/a&gt;, &lt;a href=&quot;http://questionablecontent.net/&quot;&gt;Questionable Content&lt;/a&gt;, &lt;a href=&quot;http://achewood.com/&quot;&gt;Achewood&lt;/a&gt;, &lt;a href=&quot;http://wondermark.com/&quot;&gt;Wondermark&lt;/a&gt;, &lt;a href=&quot;http://thisisindexed.com/&quot;&gt;Indexed&lt;/a&gt;, &lt;a href=&quot;http://www.buttercupfestival.com/buttercupfestival.htm&quot;&gt;Buttercup Festival&lt;/a&gt; &lt;/div&gt; &lt;br/&gt; Warning: this comic occasionally contains strong language (which may be unsuitable for children), unusual humor (which may be unsuitable for adults), and advanced mathematics (which may be unsuitable for liberal-arts majors).&lt;br/&gt; &lt;br/&gt; &lt;h4&gt;We did not invent the algorithm. The algorithm consistently finds Jesus. The algorithm killed Jeeves. &lt;br /&gt;The algorithm is banned in China. The algorithm is from Jersey. The algorithm constantly finds Jesus.&lt;br /&gt;This is not the algorithm. This is close.&lt;/h4&gt;&lt;br/&gt; &lt;div class=&quot;line&quot;&gt;&lt;/div&gt; &lt;br/&gt; &lt;div id=&quot;licenseText&quot;&gt; &lt;!-- &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border:none&quot; src=&quot;http://imgs.xkcd.com/static/somerights20.png&quot; /&gt;&lt;/a&gt;&lt;br/&gt; --&gt; This work is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;Creative Commons Attribution-NonCommercial 2.5 License&lt;/a&gt;.&lt;!-- &lt;rdf:RDF xmlns=&quot;http://web.resource.org/cc/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns &quot;&gt;&lt;Work rdf:about=&quot;&quot;&gt;&lt;dc:creator&gt;Randall Munroe&lt;/dc:creator&gt;&lt;dcterms:rightsHolder&gt;Randall Munroe&lt;/dcterms:rightsHolder&gt;&lt;dc:type rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;dc:source rdf:resource=&quot;http://www.xkcd.com/&quot;/&gt;&lt;license rdf:resource=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot; /&gt;&lt;/Work&gt;&lt;License rdf:about=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Reproduction&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Distribution&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Notice&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Attribution&quot; /&gt;&lt;prohibits rdf:resource=&quot;http://web.resource.org/cc/CommercialUse&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/DerivativeWorks&quot; /&gt;&lt;/License&gt;&lt;/rdf:RDF&gt; --&gt; &lt;br/&gt; This means you\&quot;re free to copy and share these comics (but not to sell them). &lt;a href=&quot;/license.html&quot;&gt;More details&lt;/a&gt;.&lt;br/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; &apos; elif url == &quot;http://xkcd.com/554&quot;: return &apos;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt; &lt;?xml-stylesheet href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; type=&quot;text/css&quot; media=&quot;screen&quot; ?&gt; &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.1//EN&quot; &quot;http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd&quot;&gt; &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt; &lt;head&gt; &lt;title&gt;xkcd: Not Enough Work&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt; &lt;!--[if IE]&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/ecbbecc.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt;&lt;![endif]--&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/atom+xml&quot; title=&quot;Atom 1.0&quot; href=&quot;/atom.xml&quot; /&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/rss+xml&quot; title=&quot;RSS 2.0&quot; href=&quot;/rss.xml&quot; /&gt; &lt;link rel=&quot;icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;link rel=&quot;shortcut icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;div id=&quot;topContainer&quot;&gt; &lt;div id=&quot;topLeft&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/archive/&quot;&gt;Archive&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blag.xkcd.com/&quot;&gt;News/Blag&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://store.xkcd.com/&quot;&gt;Store&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/about/&quot;&gt;About&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://forums.xkcd.com/&quot;&gt;Forums&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;topRight&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;div id=&quot;topRightContainer&quot;&gt; &lt;div id=&quot;logo&quot;&gt; &lt;a href=&quot;/&quot;&gt;&lt;img src=&quot;http://imgs.xkcd.com/s/9be30a7.png&quot; alt=&quot;xkcd.com logo&quot; height=&quot;83&quot; width=&quot;185&quot;/&gt;&lt;/a&gt; &lt;h2&gt;&lt;br /&gt;A webcomic of romance,&lt;br/&gt; sarcasm, math, and language.&lt;/h2&gt; &lt;div class=&quot;clearleft&quot;&gt;&lt;/div&gt; XKCD updates every Monday, Wednesday, and Friday. &lt;br /&gt; Blag: Remember geohashing? &lt;a href=&quot;http://blog.xkcd.com/2012/02/27/geohashing-2/&quot;&gt;Something pretty cool&lt;/a&gt; happened Sunday. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;contentContainer&quot;&gt; &lt;div id=&quot;middleContent&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;h1&gt;Not Enough Work&lt;/h1&gt;&lt;br/&gt; &lt;br /&gt; &lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/553/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_t&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/555/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;br/&gt; &lt;br/&gt; &lt;img src=&quot;http://imgs.xkcd.com/comics/not_enough_work.png&quot; title=&quot;It39;s even harder if you39;re an asshole who pronounces &amp;lt;&amp;gt; brackets.&quot; alt=&quot;Not Enough Work&quot; /&gt;&lt;br/&gt; &lt;br/&gt; &lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/553/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_b&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/555/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;h3&gt;Permanent link to this comic: http://xkcd.com/554/&lt;/h3&gt; &lt;h3&gt;Image URL (for hotlinking/embedding): http://imgs.xkcd.com/comics/not_enough_work.png&lt;/h3&gt; &lt;div id=&quot;transcript&quot; style=&quot;display: none&quot;&gt;Narration: Signs your coders don39;t have enough work to do: [[A man sitting at his workstation; a female co-worker behind him]] Man: I39;m almost up to my old typing speed in dvorak [[Two men standing by a server rack]] Man 1: Our servers now support gopher. Man 1: Just in case. [[A woman standing near her workstation speaking to a male co-worker]] Woman: Our pages are now HTML, XHTML-STRICT, and haiku-compliant Man: Haiku? Woman: &amp;lt;div class=&amp;quot;main&amp;quot;&amp;gt; Woman: &amp;lt;span id=&amp;quot;marquee&amp;quot;&amp;gt; Woman: Blog!&amp;lt; span&amp;gt;&amp;lt; div&amp;gt; [[A woman sitting at her workstation]] Woman: Hey! Have you guys seen this webcomic? &#123;&#123;title text: It39;s even harder if you39;re an asshole who pronounces &amp;lt;&amp;gt; brackets.&#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;middleFooter&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;img src=&quot;http://imgs.xkcd.com/s/a899e84.jpg&quot; width=&quot;520&quot; height=&quot;100&quot; alt=&quot;Selected Comics&quot; usemap=&quot; comicmap&quot; /&gt; &lt;map name=&quot;comicmap&quot;&gt; &lt;area shape=&quot;rect&quot; coords=&quot;0,0,100,100&quot; href=&quot;/150/&quot; alt=&quot;Grownups&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;104,0,204,100&quot; href=&quot;/730/&quot; alt=&quot;Circuit Diagram&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;208,0,308,100&quot; href=&quot;/162/&quot; alt=&quot;Angular Momentum&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;312,0,412,100&quot; href=&quot;/688/&quot; alt=&quot;Self-Description&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;416,0,520,100&quot; href=&quot;/556/&quot; alt=&quot;Alternative Energy Revolution&quot; /&gt; &lt;/map&gt;&lt;br/&gt;&lt;br /&gt; Search comic titles and transcripts:&lt;br /&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/jsapi&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; google.load(&quot;search&quot;, &quot;1&quot;); google.search.CustomSearchControl.attachAutoCompletion( &quot;012652707207066138651:zudjtuwe28q&quot;, document.getElementById(&quot;q&quot;), &quot;cse-search-box&quot;); &#125;); &lt;/script&gt; &lt;form action=&quot;//www.google.com/cse&quot; id=&quot;cse-search-box&quot;&gt; &lt;div&gt; &lt;input type=&quot;hidden&quot; name=&quot;cx&quot; value=&quot;012652707207066138651:zudjtuwe28q&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;ie&quot; value=&quot;UTF-8&quot; /&gt; &lt;input type=&quot;text&quot; name=&quot;q&quot; id=&quot;q&quot; autocomplete=&quot;off&quot; size=&quot;31&quot; /&gt; &lt;input type=&quot;submit&quot; name=&quot;sa&quot; value=&quot;Search&quot; /&gt; &lt;/div&gt; &lt;/form&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/cse/brand?form=cse-search-box&amp;lang=en&quot;&gt;&lt;/script&gt; &lt;a href=&quot;/rss.xml&quot;&gt;RSS Feed&lt;/a&gt; - &lt;a href=&quot;/atom.xml&quot;&gt;Atom Feed&lt;/a&gt; &lt;br /&gt; &lt;br/&gt; &lt;div id=&quot;comicLinks&quot;&gt; Comics I enjoy:&lt;br/&gt; &lt;a href=&quot;http://threewordphrase.com/&quot;&gt;Three Word Phrase&lt;/a&gt;, &lt;a href=&quot;http://oglaf.com/&quot;&gt;Oglaf&lt;/a&gt; (nsfw), &lt;a href=&quot;http://www.smbc-comics.com/&quot;&gt;SMBC&lt;/a&gt;, &lt;a href=&quot;http://www.qwantz.com&quot;&gt;Dinosaur Comics&lt;/a&gt;, &lt;a href=&quot;http://www.asofterworld.com&quot;&gt;A Softer World&lt;/a&gt;, &lt;a href=&quot;http://buttersafe.com/&quot;&gt;Buttersafe&lt;/a&gt;, &lt;a href=&quot;http://pbfcomics.com/&quot;&gt;Perry Bible Fellowship&lt;/a&gt;, &lt;a href=&quot;http://questionablecontent.net/&quot;&gt;Questionable Content&lt;/a&gt;, &lt;a href=&quot;http://www.buttercupfestival.com/buttercupfestival.htm&quot;&gt;Buttercup Festival&lt;/a&gt; &lt;/div&gt; &lt;br/&gt; Warning: this comic occasionally contains strong language (which may be unsuitable for children), unusual humor (which may be unsuitable for adults), and advanced mathematics (which may be unsuitable for liberal-arts majors).&lt;br/&gt; &lt;br/&gt; &lt;h4&gt;We did not invent the algorithm. The algorithm consistently finds Jesus. The algorithm killed Jeeves. &lt;br /&gt;The algorithm is banned in China. The algorithm is from Jersey. The algorithm constantly finds Jesus.&lt;br /&gt;This is not the algorithm. This is close.&lt;/h4&gt;&lt;br/&gt; &lt;div class=&quot;line&quot;&gt;&lt;/div&gt; &lt;br/&gt; &lt;div id=&quot;licenseText&quot;&gt; &lt;!-- &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border:none&quot; src=&quot;http://imgs.xkcd.com/static/somerights20.png&quot; /&gt;&lt;/a&gt;&lt;br/&gt; --&gt; This work is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;Creative Commons Attribution-NonCommercial 2.5 License&lt;/a&gt;. &lt;!-- &lt;rdf:RDF xmlns=&quot;http://web.resource.org/cc/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns &quot;&gt;&lt;Work rdf:about=&quot;&quot;&gt;&lt;dc:creator&gt;Randall Munroe&lt;/dc:creator&gt;&lt;dcterms:rightsHolder&gt;Randall Munroe&lt;/dcterms:rightsHolder&gt;&lt;dc:type rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;dc:source rdf:resource=&quot;http://www.xkcd.com/&quot;/&gt;&lt;license rdf:resource=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot; /&gt;&lt;/Work&gt;&lt;License rdf:about=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Reproduction&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Distribution&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Notice&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Attribution&quot; /&gt;&lt;prohibits rdf:resource=&quot;http://web.resource.org/cc/CommercialUse&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/DerivativeWorks&quot; /&gt;&lt;/License&gt;&lt;/rdf:RDF&gt; --&gt; &lt;br/&gt; This means you&quot;re free to copy and share these comics (but not to sell them). &lt;a href=&quot;/license.html&quot;&gt;More details&lt;/a&gt;.&lt;br/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; &apos; except: return &quot;&quot; return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef union(p,q): for e in q: if e not in p: p.append(e)def get_all_links(page): links = [] while True: url,endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] while tocrawl: page = tocrawl.pop() if page not in crawled: union(tocrawl,get_all_links(get_page(page))) crawled.append(page) return crawled https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sII5zYOFywM.mp4 Conclusionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Qm4wJi2Me6Y.mp4 Problem SetListshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fzaaNzGDcCg.mp4 Mutating Listshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kFEMVfPAP-A.mp4 Product List12345678910111213141516171819202122232425# Define a procedure, product_list,# that takes as input a list of numbers,# and returns a number that is# the result of multiplying all# those numbers together.def product_list(list_of_numbers): tem = 1 for i in list_of_numbers: tem = tem * i return temprint product_list([9])#&gt;&gt;&gt; 9print product_list([1,2,3,4])#&gt;&gt;&gt; 24print product_list([])#&gt;&gt;&gt; 1 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RTPL87SBv6o.mp4 Greatest123456789101112131415161718192021# Define a procedure, greatest,# that takes as input a list# of positive numbers, and# returns the greatest number# in that list. If the input# list is empty, the output# should be 0.def greatest(list_of_numbers): tem = 0 for i in list_of_numbers: if i&gt;tem: tem = i return temprint greatest([4,23,1])#&gt;&gt;&gt; 23print greatest([])#&gt;&gt;&gt; 0 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/okAtgJROqgs.mp4 Lists of Lists1234567891011121314151617181920212223242526272829303132333435363738394041424344# Define a procedure, total_enrollment,# that takes as an input a list of elements,# where each element is a list containing# three elements: a university name,# the total number of students enrolled,# and the annual tuition fees.# The procedure should return two numbers,# not a string, # giving the total number of students# enrolled at all of the universities# in the list, and the total tuition fees# (which is the sum of the number# of students enrolled times the# tuition fees for each university).udacious_univs = [[&apos;Udacity&apos;,90000,0]]usa_univs = [ [&apos;California Institute of Technology&apos;,2175,37704], [&apos;Harvard&apos;,19627,39849], [&apos;Massachusetts Institute of Technology&apos;,10566,40732], [&apos;Princeton&apos;,7802,37000], [&apos;Rice&apos;,5879,35551], [&apos;Stanford&apos;,19535,40569], [&apos;Yale&apos;,11701,40500] ]def total_enrollment(p): total_stu = 0 total_fee = 0 for a,b,c in p: total_stu = total_stu + b total_fee = total_fee + b* c return total_stu, total_fee#print total_enrollment(udacious_univs)#&gt;&gt;&gt; (90000,0)# The L is automatically added by Python to indicate a long# number. If you are trying the question in an outside # interpreter you might not see it.#print total_enrollment(usa_univs)#&gt;&gt;&gt; (77285,3058581079) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xk4fB0yfq58.mp4 Max Pages12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# The web crawler we built at the end of Unit 3 has some serious# flaws if we were going to use it in a real crawler. One# problem is if we start with a good seed page, it might# run for an extremely long time (even forever, since the# number of URLS on the web is not actually finite). This# question and the following one explore two different ways# to limit the pages that it can crawl.# Modify the crawl_web procedure to take a second parameter,# max_pages, that limits the number of pages to crawl.# Your procedure should terminate the crawl after# max_pages different pages have been crawled, or when# there are no more pages to crawl.# The following definition of get_page provides an interface# to the website found at http://www.udacity.com/cs101x/index.html# The function output order does not affect grading.def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl! &apos; &apos;&lt;p&gt; It is a good idea to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to &apos; &apos;crawl&lt;/a&gt; before you try to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; &apos; &apos;or &lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;. &apos; &apos;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &apos;) elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I &apos; &apos;am quite good at &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I cant get enough &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; The magic words are Squeamish Ossifrage! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) except: return &quot;&quot; return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef union(p,q): for e in q: if e not in p: p.append(e)def get_all_links(page): links = [] while True: url,endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed, max_pages): tocrawl = [seed] crawled = [] while tocrawl: page = tocrawl.pop() if page not in crawled and len(crawled) &lt; max_pages : union(tocrawl, get_all_links(get_page(page))) crawled.append(page) return crawledprint crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,1) #&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,3) #&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,500) #&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/crawling.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/kicking.html&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Mh0Rw9fV9UU.mp4 Max Depth123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148# # This question explores a different way (from the previous question)# to limit the pages that it can crawl.######### THREE GOLD STARS ## Yes, we really mean it! This is really tough (but doable) unless # you have some previous experience before this course.# Modify the crawl_web procedure to take a second parameter,# max_depth, that limits the depth of the search. We can # define the depth of a page as the number of links that must# be followed to reach that page starting from the seed page,# that is, the length of the shortest path from the seed to# the page. No pages whose depth exceeds max_depth should be# included in the crawl. # # For example, if max_depth is 0, the only page that should# be crawled is the seed page. If max_depth is 1, the pages# that should be crawled are the seed page and every page that # it links to directly. If max_depth is 2, the crawl should # also include all pages that are linked to by these pages.## Note that the pages in the crawl may be in any order.## The following definition of get_page provides an interface# to the website found at http://www.udacity.com/cs101x/index.html# The function output order does not affect grading.def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl! &apos; &apos;&lt;p&gt; It is a good idea to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to &apos; &apos;crawl&lt;/a&gt; before you try to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; &apos; &apos;or &lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;. &apos; &apos;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &apos;) elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I &apos; &apos;am quite good at &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I cant get enough &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; The magic words are Squeamish Ossifrage! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://top.contributors/velak.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/jesyspa.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/forbiddenvoid.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/jesyspa.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/elssar.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/kilaws.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/forbiddenvoid.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/charlzz.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/johang.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/graemeblake.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/kilaws.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/tomvandenbosch.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/mathprof.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/graemeblake.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/dreyescat.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/angel.html&quot;&gt;&apos;) elif url == &quot;A1&quot;: return &apos;&lt;a href=&quot;B1&quot;&gt; &lt;a href=&quot;C1&quot;&gt; &apos; elif url == &quot;B1&quot;: return &apos;&lt;a href=&quot;E1&quot;&gt;&apos; elif url == &quot;C1&quot;: return &apos;&lt;a href=&quot;D1&quot;&gt;&apos; elif url == &quot;D1&quot;: return &apos;&lt;a href=&quot;E1&quot;&gt; &apos; elif url == &quot;E1&quot;: return &apos;&lt;a href=&quot;F1&quot;&gt; &apos; except: return &quot;&quot; return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef union(p,q): for e in q: if e not in p: p.append(e)def get_all_links(page): links = [] while True: url,endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed,max_depth): tocrawl = [seed] crawled = [] count = 0 while tocrawl: page = tocrawl.pop() if page not in crawled and count &lt; max_depth: union(tocrawl, get_all_links(get_page(page))) crawled.append(page) count +=1 return crawledprint crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,0)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,1)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/crawling.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,50)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/crawling.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/kicking.html&apos;]print crawl_web(&quot;http://top.contributors/forbiddenvoid.html&quot;,2)#&gt;&gt;&gt; [&apos;http://top.contributors/forbiddenvoid.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/graemeblake.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/angel.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/dreyescat.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/johang.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/charlzz.html&apos;]print crawl_web(&quot;A1&quot;,3)#&gt;&gt;&gt; [&apos;A1&apos;, &apos;C1&apos;, &apos;B1&apos;, &apos;E1&apos;, &apos;D1&apos;, &apos;F1&apos;]# (May be in any order) 1234567891011121314def crawl_web(seed,max_depth): tocrawl = [seed] crawled = [] next_depth = [] depth = 0 while tocrawl and depth &lt;= max_depth: page = tocrawl.pop() if page not in crawled: union(next_depth, get_all_links(get_page(page))) crawled.append(page) if not tocrawl: tocrawl, next_depth = next_depth, [] depth = depth + 1 return crawled https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TRNyIIrB73Q.mp4 Sudoku12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# THREE GOLD STARS# Sudoku [http://en.wikipedia.org/wiki/Sudoku]# is a logic puzzle where a game# is defined by a partially filled# 9 x 9 square of digits where each square# contains one of the digits 1,2,3,4,5,6,7,8,9.# For this question we will generalize# and simplify the game.# Define a procedure, check_sudoku,# that takes as input a square list# of lists representing an n x n# sudoku puzzle solution and returns the boolean# True if the input is a valid# sudoku square and returns the boolean False# otherwise.# A valid sudoku square satisfies these# two properties:# 1. Each column of the square contains# each of the whole numbers from 1 to n exactly once.# 2. Each row of the square contains each# of the whole numbers from 1 to n exactly once.# You may assume the the input is square and contains at# least one row and column.correct = [[1,2,3], [2,3,1], [3,1,2]]incorrect = [[1,2,3,4], [2,3,1,3], [3,1,2,3], [4,4,4,4]]incorrect2 = [[1,2,3,4], [2,3,1,4], [4,1,2,3], [3,4,1,2]]incorrect3 = [[1,2,3,4,5], [2,3,1,5,6], [4,5,2,1,3], [3,4,5,2,1], [5,6,4,3,2]]incorrect4 = [[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;], [&apos;b&apos;,&apos;c&apos;,&apos;a&apos;], [&apos;c&apos;,&apos;a&apos;,&apos;b&apos;]]incorrect5 = [ [1, 1.5], [1.5, 1]] def check_sudoku(): #print check_sudoku(incorrect)#&gt;&gt;&gt; False#print check_sudoku(correct)#&gt;&gt;&gt; True#print check_sudoku(incorrect2)#&gt;&gt;&gt; False#print check_sudoku(incorrect3)#&gt;&gt;&gt; False#print check_sudoku(incorrect4)#&gt;&gt;&gt; False#print check_sudoku(incorrect5)#&gt;&gt;&gt; False https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/60wESSZRSp0.mp41234567891011121314151617181920def check_sudoku(p): n = len(p) digit = 1 while digit &lt;=n: i = 0 while i &lt; n: row_count = 0 col_count = 0 j = 0 while j &lt; n: if p[i][j] == digit: row_count = row_count + 1 if p[j][i] == digit: col_count = col_count + 1 j = j + 1 if row_count != 1 or col_count != 1: return False i = i+1 digit = digit + 1 return True Problem Set(Optional)Exploring List Properties1234567891011121314# Investigating adding and appending to lists# If you run the following four lines of codes, what are list1 and list2?list1 = [1,2,3,4]list2 = [1,2,3,4]list1 = list1 + [5, 6]list2.append([5, 6])# to check, you can print them out using the print statements below.print &quot;showing list1 and list2:&quot;print list1print list2 showing list1 and list2: [1, 2, 3, 4, 5, 6] [1, 2, 3, 4, [5, 6]] Symmetric Square1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# A list is symmetric if the first row is the same as the first column,# the second row is the same as the second column and so on. Write a# procedure, symmetric, which takes a list as input, and returns the# boolean True if the list is symmetric and False if it is not.def symmetric(p): # Your code here n = len(p) if n == 0: return True if len(p[0]) != n: return False i = 0 while i &lt; n: j = 0 while j &lt; n: if p[i][j] != p[j][i]: return False j = j +1 i = i +1 return True print symmetric([[1, 2, 3], [2, 3, 4], [3, 4, 1]])#&gt;&gt;&gt; Trueprint symmetric([[&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;], [&quot;dog&quot;, &quot;dog&quot;, &quot;fish&quot;], [&quot;fish&quot;, &quot;fish&quot;, &quot;cat&quot;]])#&gt;&gt;&gt; Trueprint symmetric([[&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;], [&quot;dog&quot;, &quot;dog&quot;, &quot;dog&quot;], [&quot;fish&quot;,&quot;fish&quot;,&quot;cat&quot;]])#&gt;&gt;&gt; Falseprint symmetric([[1, 2], [2, 1]])#&gt;&gt;&gt; Trueprint symmetric([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])#&gt;&gt;&gt; Falseprint symmetric([[1,2,3], [2,3,1]])#&gt;&gt;&gt; False Mean of a List123456789101112131415161718192021222324252627# The mean of a set of numbers is the sum of the numbers divided by the# number of numbers. Write a procedure, list_mean, which takes a list of numbers# as its input and return the mean of the numbers in the list.# Hint: You will need to work out how to make your division into decimal# division instead of integer division. You get decimal division if any of# the numbers involved are decimals.def list_mean(p): n = len(p) if n ==0: return -1 sum = 0 for i in p: sum = sum+i return sum*1.0 / n print list_mean([1,2,3,4])#&gt;&gt;&gt; 2.5print list_mean([1,3,4,5,2])#&gt;&gt;&gt; 3.0print list_mean([])#&gt;&gt;&gt; ??? You decide. If you decide it should give an error, comment# out the print line above to prevent your code from being graded as# incorrect.print list_mean([2])#&gt;&gt;&gt; 2.0 Notes on lists Problem Set(Optional 2)Antisymmetric Square12345678910111213141516171819202122232425262728293031# By Dimitris_GR from forums# Modify Problem Set 31&apos;s (Optional) Symmetric Square to return True # if the given square is antisymmetric and False otherwise. # An nxn square is called antisymmetric if A[i][j]=-A[j][i] # for each i=0,1,...,n-1 and for each j=0,1,...,n-1.def antisymmetric(A): #Write your code here# Test Cases:print antisymmetric([[0, 1, 2], [-1, 0, 3], [-2, -3, 0]]) #&gt;&gt;&gt; Trueprint antisymmetric([[0, 0, 0], [0, 0, 0], [0, 0, 0]])#&gt;&gt;&gt; Trueprint antisymmetric([[0, 1, 2], [-1, 0, -2], [2, 2, 3]])#&gt;&gt;&gt; Falseprint antisymmetric([[1, 2, 5], [0, 1, -9], [0, 0, 1]])#&gt;&gt;&gt; False Recognize Identity Matrix12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# By Ashwath from forums# Given a list of lists representing a n * n matrix as input, # define a procedure that returns True if the input is an identity matrix # and False otherwise.# An IDENTITY matrix is a square matrix in which all the elements # on the principal/main diagonal are 1 and all the elements outside # the principal diagonal are 0. # (A square matrix is a matrix in which the number of rows # is equal to the number of columns)def is_identity_matrix(matrix): #Write your code here# Test Cases:matrix1 = [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]print is_identity_matrix(matrix1)#&gt;&gt;&gt;Truematrix2 = [[1,0,0], [0,1,0], [0,0,0]]print is_identity_matrix(matrix2)#&gt;&gt;&gt;Falsematrix3 = [[2,0,0], [0,2,0], [0,0,2]]print is_identity_matrix(matrix3)#&gt;&gt;&gt;Falsematrix4 = [[1,0,0,0], [0,1,1,0], [0,0,0,1]]print is_identity_matrix(matrix4)#&gt;&gt;&gt;Falsematrix5 = [[1,0,0,0,0,0,0,0,0]]print is_identity_matrix(matrix5)#&gt;&gt;&gt;Falsematrix6 = [[1,0,0,0], [0,1,0,1], [0,0,1,0], [0,0,0,1]]print is_identity_matrix(matrix6)#&gt;&gt;&gt;Falsematrix7 = [[1, -1, 1], [0, 1, 0], [0, 0, 1]]print is_identity_matrix(matrix7)#&gt;&gt;&gt;False Numbers in Lists1234567891011121314151617181920212223242526272829# Numbers in lists by SeanMc from forums# define a procedure that takes in a string of numbers from 1-9 and# outputs a list with the following parameters:# Every number in the string should be inserted into the list.# If a number x in the string is less than or equal # to the preceding number y, the number x should be inserted # into a sublist. Continue adding the following numbers to the # sublist until reaching a number z that# is greater than the number y. # Then add this number z to the normal list and continue.#Hint - &quot;int()&quot; turns a string&apos;s element into a numberdef numbers_in_lists(string): # YOUR CODE#testcasesstring = &apos;543987&apos;result = [5,[4,3],9,[8,7]]print repr(string), numbers_in_lists(string) == resultstring= &apos;987654321&apos;result = [9,[8,7,6,5,4,3,2,1]]print repr(string), numbers_in_lists(string) == resultstring = &apos;455532123266&apos;result = [4, 5, [5, 5, 3, 2, 1, 2, 3, 2], 6, [6]]print repr(string), numbers_in_lists(string) == resultstring = &apos;123456789&apos;result = [1, 2, 3, 4, 5, 6, 7, 8, 9]print repr(string), numbers_in_lists(string) == result Frequency Analysis12345678910111213141516171819202122232425262728293031323334# Crypto Analysis: Frequency Analysis## To analyze encrypted messages, to find out information about the possible # algorithm or even language of the clear text message, one could perform # frequency analysis. This process could be described as simply counting # the number of times a certain symbol occurs in the given text. # For example:# For the text &quot;test&quot; the frequency of &apos;e&apos; is 1, &apos;s&apos; is 1 and &apos;t&apos; is 2.## The input to the function will be an encrypted body of text that only contains # the lowercase letters a-z. # As output you should return a list of the normalized frequency # for each of the letters a-z. # The normalized frequency is simply the number of occurrences, i, # divided by the total number of characters in the message, n.def freq_analysis(message): ## # Your code here ## return freq_list#Testsprint freq_analysis(&quot;abcd&quot;)#&gt;&gt;&gt; [0.25, 0.25, 0.25, 0.25, 0.0, ..., 0.0]print freq_analysis(&quot;adca&quot;)#&gt;&gt;&gt; [0.5, 0.0, 0.25, 0.25, 0.0, ..., 0.0]print freq_analysis(&apos;bewarethebunnies&apos;)#&gt;&gt;&gt; [0.0625, 0.125, 0.0, 0.0, ..., 0.0] Responding to QueriesIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gXkELecZYlk.mp4 Welcome to Unit 4! The notes for Unit 4 are here: PDF and web. By the end of this unit, we’ll have a working search engine that can crawl and build an index of set of web pages, and respond to keyword queries! You’ll also learn about designing and using complex data structures that build on the list structure we introduced in the previous unit. Data Structureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pv5-RgG1pdk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nNEXCEH0dEw.mp4 Add to Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B2J-bDQ4M1o.mp4123456789101112131415161718192021222324252627282930# Define a procedure, add_to_index,# that takes 3 inputs:# - an index: [[&lt;keyword&gt;,[&lt;url&gt;,...]],...]# - a keyword: String# - a url: String# If the keyword is already# in the index, add the url# to the list of urls associated# with that keyword.# If the keyword is not in the index,# add an entry to the index: [keyword,[url]]index = []def add_to_index(index,keyword,url):#add_to_index(index,&apos;udacity&apos;,&apos;http://udacity.com&apos;)#add_to_index(index,&apos;computing&apos;,&apos;http://acm.org&apos;)#add_to_index(index,&apos;udacity&apos;,&apos;http://npr.org&apos;)#print index#&gt;&gt;&gt; [[&apos;udacity&apos;, [&apos;http://udacity.com&apos;, &apos;http://npr.org&apos;]], #&gt;&gt;&gt; [&apos;computing&apos;, [&apos;http://acm.org&apos;]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SGkb6vqS7zA.mp4123456def add_to_index(index,keyword,url): for entry in index: if entry[0]== keyword: entry[1].append(url) return index.append([keyword,[url]]) Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hzDJhLS4yCo.mp412345678910111213141516171819202122232425262728# Define a procedure, lookup,# that takes two inputs:# - an index# - keyword# The procedure should return a list# of the urls associated# with the keyword. If the keyword# is not in the index, the procedure# should return an empty list.index = [[&apos;udacity&apos;, [&apos;http://udacity.com&apos;, &apos;http://npr.org&apos;]], [&apos;computing&apos;, [&apos;http://acm.org&apos;]]]def lookup(index,keyword): for entry in index: if entry[0]==keyword: return entry[1] return []print lookup(index,&apos;udacity&apos;)#&gt;&gt;&gt; [&apos;http://udacity.com&apos;,&apos;http://npr.org&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bVjECgrnKj4.mp4 Building the Web Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aRteT5uKqfg.mp4 Add Page to Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_5rpzWzFnJM.mp41234567891011121314151617181920212223242526272829303132333435# Define a procedure, add_page_to_index,# that takes three inputs:# - index# - url (String)# - content (String)# It should update the index to include# all of the word occurences found in the# page content by adding the url to the# word&apos;s associated url list.index = []def add_to_index(index,keyword,url): for entry in index: if entry[0] == keyword: entry[1].append(url) return index.append([keyword,[url]])def add_page_to_index(index,url,content): contents = content.split() for word in contents: add_to_index(index,word,url) add_page_to_index(index,&apos;fake.text&apos;,&quot;This is a test&quot;)print index#&gt;&gt;&gt; [[&apos;This&apos;, [&apos;fake.text&apos;]], [&apos;is&apos;, [&apos;fake.text&apos;]], [&apos;a&apos;, [&apos;fake.text&apos;]],#&gt;&gt;&gt; [&apos;test&apos;,[&apos;fake.text&apos;]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i3V-Aw4y-hg.mp4 Finishing the Web Crawlerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dQjsf-4cWo0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cPKnNmFTS80.mp4 Startuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1XElSoLZfKQ.mp4 The Internethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ePw5eGJXuw8.mp4 Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dy4KsLNw1lU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_8Xgtd4j7j8.mp4 Smoke Signalshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8B6WSjA7DG8.mp4 Latencyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6_1akTCAnt4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5NoF37dKsAI.mp4 Bandwidthhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/P83jTqcQ10A.mp4 Bitshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6HCFOyZI9tA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4OxrAgA30T8.mp4 Buckets of Bitshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IS7TO_lLXFE.mp4 What Is Your Bandwidth?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jG252FaodkA.mp4 Traceroutehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kU30juVBCBg.mp4123456789101112131415161718192021222324252627282930313233343536373839C:\Users\SSQ&gt;tracert www.udacity.com通过最多 30 个跃点跟踪到 apollo-mesos-elb-berlioz2-prod-885022263.us-west-2.elb.amazonaws.com [52.32.68.151] 的路由: 1 2 ms 60 ms 1 ms 192.168.1.1 2 14 ms 2 ms 4 ms 222.199.225.1 3 1 ms 1 ms 1 ms 202.4.128.193 4 342 ms 415 ms 227 ms 202.4.128.213 5 631 ms 413 ms 727 ms 172.30.33.5 6 683 ms 390 ms 638 ms 10.255.100.161 7 635 ms 532 ms 680 ms 124.205.98.145 8 * * * 请求超时。 9 * * * 请求超时。 10 * 707 ms * 14.197.246.209 11 483 ms 709 ms * 221.4.0.134 12 784 ms 701 ms 700 ms 221.4.0.133 13 736 ms 698 ms 721 ms 120.80.3.37 14 * 719 ms 768 ms 120.81.0.101 15 * * 793 ms 219.158.111.253 16 886 ms 646 ms 825 ms 219.158.13.98 17 886 ms 605 ms 601 ms 219.158.103.94 18 1135 ms 855 ms 1167 ms 219.158.116.234 19 956 ms 910 ms 1132 ms sjp-brdr-04.inet.qwest.net [63.146.27.85] 20 1157 ms 1001 ms * tuk-edge-13.inet.qwest.net [67.14.4.206] 21 1181 ms 962 ms 1157 ms 65-122-235-170.dia.static.qwest.net [65.122.235.170] 22 * * * 请求超时。 23 * * * 请求超时。 24 * * * 请求超时。 25 * * * 请求超时。 26 * * * 请求超时。 27 1034 ms 995 ms 1037 ms 52.93.14.71 28 1007 ms * 1067 ms 52.93.14.70 29 * * * 请求超时。 30 955 ms 915 ms 926 ms 205.251.232.222跟踪完成。 Traveling Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bQqvRI8NSFo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8csDnLICd4w.mp4 Making a Networkhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jFElXIkFEhc.mp4 Protocolshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0U31-O4oEPc.mp4 Conclusionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1wKlRFhn4zg.mp4 Lesson 16 Problem SetData Structureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6rE8vvdYn2c.mp4 Ben BitdiddleBen Bitdiddle suggests changing the index code by replacing the add_to_index and lookup procedures with the ones shown below the question. def add_to_index(index, keyword, url): index.append([keyword, url]) def lookup(index, keyword): result = [] for entry in index: if entry[0] == keyword: result.append(entry[1]) return result This changes the structure of index, but suppose the only way we use index is by calling add_to_index and lookup. How would this affect the search engine? **It would produce the wrong results for some lookup queries. It would produce the same results for all queries, but lookup would sometimes be faster than the original code. It would produce the same results for all queries, but add_to_index would be faster and lookup would usually be slower than the original code. It would produce the same results and take the same amount of time for all queries** Old Code def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return [] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h5KA5t8yo3I.mp4 Networkinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rl7zOmndGLY.mp4 Better Splitting123456789101112131415161718192021222324252627282930313233# 1 Gold Star# The built-in &lt;string&gt;.split() procedure works# okay, but fails to find all the words on a page# because it only uses whitespace to split the# string. To do better, we should also use punctuation# marks to split the page into words.# Define a procedure, split_string, that takes two# inputs: the string to split and a string containing# all of the characters considered separators. The# procedure should return a list of strings that break# the source string up by the characters in the# splitlist.def split_string(source,splitlist):#out = split_string(&quot;This is a test-of the,string separation-code!&quot;,&quot; ,!-&quot;)#print out#&gt;&gt;&gt; [&apos;This&apos;, &apos;is&apos;, &apos;a&apos;, &apos;test&apos;, &apos;of&apos;, &apos;the&apos;, &apos;string&apos;, &apos;separation&apos;, &apos;code&apos;]#out = split_string(&quot;After the flood ... all the colors came out.&quot;, &quot; .&quot;)#print out#&gt;&gt;&gt; [&apos;After&apos;, &apos;the&apos;, &apos;flood&apos;, &apos;all&apos;, &apos;the&apos;, &apos;colors&apos;, &apos;came&apos;, &apos;out&apos;]#out = split_string(&quot;First Name,Last Name,Street Address,City,State,Zip Code&quot;,&quot;,&quot;)#print out#&gt;&gt;&gt;[&apos;First Name&apos;, &apos;Last Name&apos;, &apos;Street Address&apos;, &apos;City&apos;, &apos;State&apos;, &apos;Zip Code&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/alpdXaaSfGI.mp412345678910111213def split_string(source,splitlist): output=[] atsplit=True for char in source: if char in splitlist: atsplit =True else: if atsplit: output.append(char) atsplit=False else: output[-1] =output[-1]+char return output Improving the Index123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# The current index includes a url in the list of urls# for a keyword multiple times if the keyword appears# on that page more than once.# It might be better to only include the same url# once in the url list for a keyword, even if it appears# many times.# Modify add_to_index so that a given url is only# included once in the url list for a keyword,# no matter how many times that keyword appears.def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]])def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl!&lt;p&gt; It is a good idea to&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to crawl&lt;/a&gt; before you try to&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; or&lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I amquite good at &lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&lt;/body&gt; &lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I cant get enough&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return &apos;&apos;&apos;&lt;html&gt;&lt;body&gt;The magic words are Squeamish Ossifrage!&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; except: return &quot;&quot; return &quot;&quot;def union(a, b): for e in b: if e not in a: a.append(e)def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return indexdef add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url)def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None#index = crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;)#print lookup(index,&quot;is&quot;)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GD98Z_3cANU.mp412345678def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: if not url in entry[1]: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) Counting Clicks123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# 2 Gold Stars# One way search engines rank pages# is to count the number of times a# searcher clicks on a returned link.# This indicates that the person doing# the query thought this was a useful# link for the query, so it should be# higher in the rankings next time.# (In Unit 6, we will look at a different# way of ranking pages that does not depend# on user clicks.)# Modify the index such that for each url in a# list for a keyword, there is also a number# that counts the number of times a user# clicks on that link for this keyword.# The result of lookup(index,keyword) should# now be a list of url entries, where each url# entry is a list of a url and a number# indicating the number of times that url# was clicked for this query keyword.# You should define a new procedure to simulate# user clicks for a given link:# record_user_click(index,word,url)# that modifies the entry in the index for# the input word by increasing the count associated# with the url by 1.# You also will have to modify add_to_index# in order to correctly create the new data# structure, and to prevent the repetition of# entries as in homework 4-5.def record_user_click(index,keyword,url):def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]])def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl!&lt;p&gt; It is a good idea to&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to crawl&lt;/a&gt; before you try to&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; or&lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I amquite good at &lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&lt;/body&gt; &lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I cant get enough&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;!&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return &apos;&lt;html&gt;&lt;body&gt;The magic words are Squeamish Ossifrage!&lt;/body&gt;&lt;/html&gt;&apos; except: return &quot;&quot; return &quot;&quot;def union(a, b): for e in b: if e not in a: a.append(e)def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return indexdef add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url)def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None#Here is an example showing a sequence of interactions:index = crawl_web(&apos;http://www.udacity.com/cs101x/index.html&apos;)print lookup(index, &apos;good&apos;)#&gt;&gt;&gt; [[&apos;http://www.udacity.com/cs101x/index.html&apos;, 0],#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/crawling.html&apos;, 0]]record_user_click(index, &apos;good&apos;, &apos;http://www.udacity.com/cs101x/crawling.html&apos;)print lookup(index, &apos;good&apos;)#&gt;&gt;&gt; [[&apos;http://www.udacity.com/cs101x/index.html&apos;, 0],#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/crawling.html&apos;, 1]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XAb3iFZfOl0.mp4123456789101112131415161718def record_user_click(index, keyword, url): urls = lookup(index, keyword) if urls: for entry in urls: if entry[0] == url: entry[1] = entry[1]+1def add_to_index(index, keyword, url): # format of index: [[keyword, [[url, count], [url, count],..]],...] for entry in index: if entry[0] == keyword: for urls in entry[1]: if urls[0] == url: return entry[1].append([url,0]) return # not found, add new keyword to index index.append([keyword, [[url,0]]]) Time Spent at Routershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jqNiHcMsS_Y.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TWEE2f1w55U.mp4 Problem Set(Optional)Word CountLatencyConverting SecondsDownload CalculatorHow Programs RunIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XJfrUOoQSOI.mp4Welcome to Unit 5! The notes for Unit 5 are here: PDF and web. The main goal of Unit 5 is to learn about how computer scientists measure cost, which is mostly about understanding how the resources needed to run a program scale with the size of its input. We’ll also learn about implementing and using a hash table, a data structure that will massively improve the performance of our search engine. It was a privilege to meet with Gabriel Weinberg, the founder of DuckDuckGo, to film this introduction. DuckDuckGo protects the privacy of its users and gets around 3 million searches per day. Gabriel’s blog is full of interesting articles about computing and startups. Making Things Fasthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JIeuI6mknUk.mp4 Measuring Speedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5vnXm71KECU.mp4 Stopwatchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ms0iENK29jA.mp4 Spin Loophttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hBw8qWGHrEs.mp4 Predicting Run Timehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HBwT29hWXrs.mp4 Make Big Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zxfXpB6U_0w.mp4 Index Size Vs. Timehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yYm5t1wLarM.mp4Sample timings: &gt;&gt;&gt; time_execution(&apos;lookup(index10000, &quot;udacity&quot;)&apos;) (None, 0.000968000000000302) &gt;&gt;&gt; time_execution(&apos;lookup(index10000, &quot;udacity&quot;)&apos;) (None, 0.000905999999863066) &gt;&gt;&gt; time_execution(&apos;lookup(index100000, &quot;udacity&quot;)&apos;) (None, 0.008590000000002652) &gt;&gt;&gt; time_execution(&apos;lookup(index100000, &quot;udacity&quot;)&apos;) (None, 0.008517999999998093) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5DHrCwtBuGU.mp4 Lookup Timehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PsCqA6fJ1hk.mp4This quiz depends on the code for make_big_index(size) from a few segments before: def make_big_index(size): index = [] letters = [&apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;] while len(index) &lt; size: word = make_string(letters) add_to_index(index, word, &apos;fake&apos;) for i in range(len(letters) - 1, 0, -1): if letters[i] &lt; &apos;z&apos;: letters[i] = chr(ord(letters[i]) + 1) break else: letters[i] = &apos;a&apos; return index This quiz depends on the code for make_big_index(size) from a few segments before (as well as the code for lookup and add_to_index):https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WtfufPxl8Mw.mp4 Worst Casehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TBylO5VopA4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/26jeGBtszyk.mp4 Fast Enoughhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lSakl4WtFiE.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rbYT97miQMY.mp4 Making Lookup Fasterhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AArXvYMTCOM.mp4 Hash Tablehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KxGQbWGPeak.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fdddZ5zcHyI.mp4 Hash Functionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xzQy09kBswM.mp4ord()ord(&#39;a&#39;)-&gt;97chr() Modulus Operatorhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/b2J5RyLdNy8.mp4 Modulus Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/l8cjHI9UbW4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nmV96-OcGi4.mp4 Equivalent Expressionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yoUU_QDJv4o.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TRuWp6uRBKI.mp4 Bad Hashhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gGSY4yAusdk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qn99D3acUnA.mp4 Better Hash Functionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SKbp6T6C-0Q.mp41234567891011121314151617181920212223242526# Define a function, hash_string,# that takes as inputs a keyword# (string) and a number of buckets,# and returns a number representing# the bucket for that keyword.def hash_string(keyword,buckets):#print hash_string(&apos;a&apos;,12)#&gt;&gt;&gt; 1#print hash_string(&apos;b&apos;,12)#&gt;&gt;&gt; 2#print hash_string(&apos;a&apos;,13)#&gt;&gt;&gt; 6#print hash_string(&apos;au&apos;,12)#&gt;&gt;&gt; 10#print hash_string(&apos;udacity&apos;,12)#&gt;&gt;&gt; 11 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9vGXclxo8Kc.mp4My claim about the performance being better with the % buckets inside the loop is (often, and possibly always?) incorrect. Some enterprising students have done experiments showing this, and there is more discussion in the forum. Testing Hash Functionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TjWwI-MvEhI.mp4 Keywords and Bucketshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2cL69wIOpVk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0k-hAMfA5uY.mp4 Implementing Hash Tableshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sRfcPW1Rj_4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3AN9tyu_w-I.mp4 Empty Hash Tablehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/a_NjE-wJQGc.mp41234567# Creating an Empty Hash Table# Define a procedure, make_hashtable,# that takes as input a number, nbuckets,# and returns an empty hash table with# nbuckets empty buckets.def make_hashtable(nbuckets): https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e3gDr_MWqDA.mp4 The Hard Wayhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cpAkNOOdzLw.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GY8OtZj6LZA.mp4 Finding Bucketshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ZOo7GAm2qU.mp4123456789101112131415161718192021222324252627282930313233# Define a procedure, hashtable_get_bucket,# that takes two inputs - a hashtable, and# a keyword, and returns the bucket where the# keyword could occur.def hashtable_get_bucket(htable,keyword):def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return table#table = [[[&apos;Francis&apos;, 13], [&apos;Ellis&apos;, 11]], [], [[&apos;Bill&apos;, 17],#[&apos;Zoe&apos;, 14]], [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Rochelle&apos;, 4], [&apos;Nick&apos;, 2]]]#print hashtable_get_bucket(table, &quot;Zoe&quot;)#&gt;&gt;&gt; [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]]#print hashtable_get_bucket(table, &quot;Brick&quot;)#&gt;&gt;&gt; []#print hashtable_get_bucket(table, &quot;Lilith&quot;)#&gt;&gt;&gt; [[&apos;Louis&apos;, 29], [&apos;Rochelle&apos;, 4], [&apos;Nick&apos;, 2]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e_ZLxgElqks.mp4 Adding Keywordshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VFulPFO-OS0.mp4123456789101112131415161718192021222324252627282930313233343536373839404142434445# Define a procedure,## hashtable_add(htable,key,value)## that adds the key to the hashtable (in # the correct bucket), with the correct # value and returns the new hashtable.## (Note that the video question and answer# do not return the hashtable, but your code# should do this to pass the test cases.)def hashtable_add(htable,key,value): # your code here return htable def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return table#table = make_hashtable(5)#hashtable_add(table,&apos;Bill&apos;, 17)#hashtable_add(table,&apos;Coach&apos;, 4)#hashtable_add(table,&apos;Ellis&apos;, 11)#hashtable_add(table,&apos;Francis&apos;, 13)#hashtable_add(table,&apos;Louis&apos;, 29)#hashtable_add(table,&apos;Nick&apos;, 2)#hashtable_add(table,&apos;Rochelle&apos;, 4)#hashtable_add(table,&apos;Zoe&apos;, 14)#print table#&gt;&gt;&gt; [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]], #&gt;&gt;&gt; [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ge6HmR7EuDI.mp412345678910111213141516171819202122232425262728293031323334353637383940414243444546# Define a procedure,## hashtable_add(htable,key,value)## that adds the key to the hashtable (in # the correct bucket), with the correct # value and returns the new hashtable.## (Note that the video question and answer# do not return the hashtable, but your code# should do this to pass the test cases.)def hashtable_add(htable,key,value): # your code here bucket = hashtable_get_bucket(htable,key) bucket.append([key,value]) return htable def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return tabletable = make_hashtable(5)hashtable_add(table,&apos;Bill&apos;, 17)hashtable_add(table,&apos;Coach&apos;, 4)hashtable_add(table,&apos;Ellis&apos;, 11)hashtable_add(table,&apos;Francis&apos;, 13)hashtable_add(table,&apos;Louis&apos;, 29)hashtable_add(table,&apos;Nick&apos;, 2)hashtable_add(table,&apos;Rochelle&apos;, 4)hashtable_add(table,&apos;Zoe&apos;, 14)print table#&gt;&gt;&gt; [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]], #&gt;&gt;&gt; [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]] Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vJJ0wakAu7s.mp41234567891011121314151617181920212223242526272829303132333435363738394041424344# Define a procedure,# hashtable_lookup(htable,key)# that takes two inputs, a hashtable# and a key (string),# and returns the value associated# with that key.def hashtable_lookup(htable,key):def hashtable_add(htable,key,value): bucket = hashtable_get_bucket(htable,key) bucket.append([key,value])def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return table#table = [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]],#[[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]]#print hashtable_lookup(table, &apos;Francis&apos;)#&gt;&gt;&gt; 13#print hashtable_lookup(table, &apos;Louis&apos;)#&gt;&gt;&gt; 29#print hashtable_lookup(table, &apos;Zoe&apos;)#&gt;&gt;&gt; 14 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cKkaGzt9pwk.mp4 Update1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Define a procedure,# hashtable_update(htable,key,value)# that updates the value associated with key. If key is already in the# table, change the value to the new value. Otherwise, add a new entry# for the key and value.# Hint: Use hashtable_lookup as a starting point.# Make sure that you return the new htabledef hashtable_update(htable,key,value): # Your code here return htabledef hashtable_lookup(htable,key): bucket = hashtable_get_bucket(htable,key) for entry in bucket: if entry[0] == key: return entry[1] return Nonedef hashtable_add(htable,key,value): bucket = hashtable_get_bucket(htable,key) bucket.append([key,value])def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return tabletable = [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]],[[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]]#hashtable_update(table, &apos;Bill&apos;, 42)#hashtable_update(table, &apos;Rochelle&apos;, 94)#hashtable_update(table, &apos;Zed&apos;, 68)#print table#&gt;&gt;&gt; [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [[&apos;Zed&apos;, 68]], [[&apos;Bill&apos;, 42], #&gt;&gt;&gt; [&apos;Zoe&apos;, 14]], [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], #&gt;&gt;&gt; [&apos;Rochelle&apos;, 94]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UPiqKaXshfw.mp4 Dictionarieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Tne9hgBqCUY.mp4 Using Dictionarieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5wTxBLzR5aM.mp4For further information on Hash Tables in Python, please refer to this article here Populationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/em3CWlSaEKY.mp412345678910111213# Define a Dictionary, population,# that provides information# on the world&apos;s largest cities.# The key is the name of a city# (a string), and the associated# value is its population in# millions.# Key | Value# Shanghai | 17.8# Istanbul | 13.3# Karachi | 13.0# Mumbai | 12.5 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/H-eGAkgjg_s.mp4 A Noble Gashttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qJNsfRfFw-c.mp4 Modifying the Search Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ncC1XboU_lo.mp4Here’s the code in a more readable format:12345678910111213141516171819202122232425262728293031323334353637383940def get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return index def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None Here’s the code in a more readable way: (thanks to Christina-49) def get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return links12345678910111213141516171819202122232425262728293031def crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return index def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None Here’s the code in a more readable format: (thanks to Christina-49)https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NBJx7q8XNpE.mp4 Changing Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OdToP6LQRoc.mp412345678# Change the lookup procedure# to now work with dictionaries.def lookup(index, keyword): if keyword in index: return index[keyword] else: return None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/avNhSME0qxQ.mp4 Coming Up Nexthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-qYyswP4FqI.mp4 Problem SetGrowth Measuring CostFor which of these procedures does the worst-case running time scale linearly in the number of elements in the input list p? (You may assume that the elements in the list are all small numbers) Sum_listdef sum_list(p): sum = 0 for e in p: sum = sum + e return sum Has_duplicate_element def has_duplicate_element(p): res = [] for i in range(0, len(p)): for j in range(0, len(p)): if i != j and p[i] == p[j]: return True return False Mystery def mystery(p): i = 0 while True: if i &gt;= len(p): break if p[i] % 2: i = i + 2 else: i = i + 1 return i Peter muddles up odd and even in the last question. The statement p[i] % 2 is True whenp[i] is odd and False when it is even, so the worst case is when all the elements in the list are even.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/tlFdhxXJzaw.mp4 Hash String Hash StringSuppose we have a hash table implemented as described in Unit 5 using the hash_string function. def hash_string(keyword, buckets): h = 0 for c in keyword: h = (h + ord(c)) % buckets return h Which of the following are true statements? Statement 1The number of string comparisons done to lookup a keyword that is not a key in the hash table may be less than the number needed to lookup a keyword that is a key in the hash table. Statement 2We should expect the time to lookup most keywords in the hash table will decrease as we increase the number of buckets. Statement 3It is always better to have more buckets in a hash table. Statement 4The time to lookup a keyword in the hash table is always less than the time it would take in a linear time list (as used in Unit 4). Is Offered12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# Dictionaries of Dictionaries (of Dictionaries)# The next several questions concern the data structure below for keeping# track of Udacity&apos;s courses (where all of the values are strings):# &#123; &lt;hexamester&gt;, &#123; &lt;class&gt;: &#123; &lt;property&gt;: &lt;value&gt;, ... &#125;,# ... &#125;,# ... &#125;#For example,courses = &#123; &apos;feb2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;, &apos;assistant&apos;: &apos;Andy&apos;&#125;&#125;, &apos;apr2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Sarah&apos;&#125;, &apos;cs212&apos;: &#123;&apos;name&apos;: &apos;The Design of Computer Programs&apos;, &apos;teacher&apos;: &apos;Peter N.&apos;, &apos;assistant&apos;: &apos;Andy&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs253&apos;: &#123;&apos;name&apos;: &apos;Web Application Engineering - Building a Blog&apos;, &apos;teacher&apos;: &apos;Steve&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs262&apos;: &#123;&apos;name&apos;: &apos;Programming Languages - Building a Web Browser&apos;, &apos;teacher&apos;: &apos;Wes&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;&#125;, &apos;cs387&apos;: &#123;&apos;name&apos;: &apos;Applied Cryptography&apos;, &apos;teacher&apos;: &apos;Dave&apos;&#125;&#125;, &apos;jan2044&apos;: &#123; &apos;cs001&apos;: &#123;&apos;name&apos;: &apos;Building a Quantum Holodeck&apos;, &apos;teacher&apos;: &apos;Dorina&apos;&#125;, &apos;cs003&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Robotics Teacher&apos;, &apos;teacher&apos;: &apos;Jasper&apos;&#125;, &#125; &#125;# If you want to loop through the keys in the dictionary,# you can use the construct below.# for &lt;key&gt; in &lt;dictionary&gt;:# &lt;block&gt; # For example, this procedure returns a list of all the courses offered # in the given hexamester:def courses_offered(courses, hexamester): res = [] for c in courses[hexamester]: res.append(c) return res# You do not need to use this code if you do not want to and may find another, # simpler method to answer this question, although later ones may require this.# Define a procedure, is_offered(courses, course, hexamester), that returns # True if the input course is offered in the input hexamester, and returns # False otherwise. For example,#print is_offered(courses, &apos;cs101&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; True#print is_offered(courses, &apos;cs003&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; False# (Note: it is okay if your procedure produces an error if the input # hexamester is not included in courses.# For example, is_offered(courses, &apos;cs101&apos;, &apos;dec2011&apos;) can produce an error.)# However, do not leave any uncommented statements in your code which lead # to an error as your code will be graded as incorrect.def is_offered(courses, course, hexamester):#print is_offered(courses, &apos;cs101&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; True#print is_offered(courses, &apos;cs003&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; False#print is_offered(courses, &apos;cs001&apos;, &apos;jan2044&apos;)#&gt;&gt;&gt; True#print is_offered(courses, &apos;cs253&apos;, &apos;feb2012&apos;)#&gt;&gt;&gt; False https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Qq8Hd290n5c.mp4 When Offered123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# Dictionaries of Dictionaries (of Dictionaries)# The next several questions concern the data structure below for keeping# track of Udacity&apos;s courses (where all of the values are strings):# &#123; &lt;hexamester&gt;, &#123; &lt;class&gt;: &#123; &lt;property&gt;: &lt;value&gt;, ... &#125;,# ... &#125;,# ... &#125;# For example,courses = &#123; &apos;feb2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;, &apos;assistant&apos;: &apos;Andy&apos;&#125;&#125;, &apos;apr2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Sarah&apos;&#125;, &apos;cs212&apos;: &#123;&apos;name&apos;: &apos;The Design of Computer Programs&apos;, &apos;teacher&apos;: &apos;Peter N.&apos;, &apos;assistant&apos;: &apos;Andy&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs253&apos;: &#123;&apos;name&apos;: &apos;Web Application Engineering - Building a Blog&apos;, &apos;teacher&apos;: &apos;Steve&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs262&apos;: &#123;&apos;name&apos;: &apos;Programming Languages - Building a Web Browser&apos;, &apos;teacher&apos;: &apos;Wes&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;&#125;, &apos;cs387&apos;: &#123;&apos;name&apos;: &apos;Applied Cryptography&apos;, &apos;teacher&apos;: &apos;Dave&apos;&#125;&#125;, &apos;jan2044&apos;: &#123; &apos;cs001&apos;: &#123;&apos;name&apos;: &apos;Building a Quantum Holodeck&apos;, &apos;teacher&apos;: &apos;Dorina&apos;&#125;, &apos;cs003&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Robotics Teacher&apos;, &apos;teacher&apos;: &apos;Jasper&apos;&#125;, &#125; &#125;# For the following questions, you will find the# for &lt;key&gt; in &lt;dictionary&gt;:# &lt;block&gt;# construct useful. This loops through the key values in the Dictionary. For# example, this procedure returns a list of all the courses offered in the given# hexamester:def courses_offered(courses, hexamester): res = [] for c in courses[hexamester]: res.append(c) return res# Define a procedure, when_offered(courses, course), that takes a courses data# structure and a string representing a class, and returns a list of strings# representing the hexamesters when the input course is offered.def when_offered(courses,course):#print when_offered (courses, &apos;cs101&apos;)#&gt;&gt;&gt; [&apos;apr2012&apos;, &apos;feb2012&apos;]#print when_offered(courses, &apos;bio893&apos;)#&gt;&gt;&gt; [] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hftOGwEW4qY.mp4 Involved12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# Dictionaries of Dictionaries (of Dictionaries)# The next several questions concern the data structure below for keeping# track of Udacity&apos;s courses (where all of the values are strings):# &#123; &lt;hexamester&gt;, &#123; &lt;class&gt;: &#123; &lt;property&gt;: &lt;value&gt;, ... &#125;,# ... &#125;,# ... &#125;# For example,courses = &#123; &apos;feb2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;, &apos;assistant&apos;: &apos;Andy&apos;&#125;&#125;, &apos;apr2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Sarah&apos;&#125;, &apos;cs212&apos;: &#123;&apos;name&apos;: &apos;The Design of Computer Programs&apos;, &apos;teacher&apos;: &apos;Peter N.&apos;, &apos;assistant&apos;: &apos;Andy&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs253&apos;: &#123;&apos;name&apos;: &apos;Web Application Engineering - Building a Blog&apos;, &apos;teacher&apos;: &apos;Steve&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs262&apos;: &#123;&apos;name&apos;: &apos;Programming Languages - Building a Web Browser&apos;, &apos;teacher&apos;: &apos;Wes&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;&#125;, &apos;cs387&apos;: &#123;&apos;name&apos;: &apos;Applied Cryptography&apos;, &apos;teacher&apos;: &apos;Dave&apos;&#125;&#125;, &apos;jan2044&apos;: &#123; &apos;cs001&apos;: &#123;&apos;name&apos;: &apos;Building a Quantum Holodeck&apos;, &apos;teacher&apos;: &apos;Dorina&apos;&#125;, &apos;cs003&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Robotics Teacher&apos;, &apos;teacher&apos;: &apos;Jasper&apos;&#125;, &#125; &#125;# For the following questions, you will find the# for &lt;key&gt; in &lt;dictionary&gt;:# &lt;block&gt;# construct useful. This loops through the key values in the Dictionary. For# example, this procedure returns a list of all the courses offered in the given# hexamester:def courses_offered(courses, hexamester): res = [] for c in courses[hexamester]: res.append(c) return res# [Double Gold Star] Define a procedure, involved(courses, person), that takes # as input a courses structure and a person and returns a Dictionary that # describes all the courses the person is involved in. A person is involved # in a course if they are a value for any property for the course. The output # Dictionary should have hexamesters as its keys, and each value should be a # list of courses that are offered that hexamester (the courses in the list # can be in any order).def involved(courses, person):# For example:#print involved(courses, &apos;Dave&apos;)#&gt;&gt;&gt; &#123;&apos;apr2012&apos;: [&apos;cs101&apos;, &apos;cs387&apos;], &apos;feb2012&apos;: [&apos;cs101&apos;]&#125;#print involved(courses, &apos;Peter C.&apos;)#&gt;&gt;&gt; &#123;&apos;apr2012&apos;: [&apos;cs262&apos;], &apos;feb2012&apos;: [&apos;cs101&apos;]&#125;#print involved(courses, &apos;Dorina&apos;)#&gt;&gt;&gt; &#123;&apos;jan2044&apos;: [&apos;cs001&apos;]&#125;#print involved(courses,&apos;Peter&apos;)#&gt;&gt;&gt; &#123;&#125;#print involved(courses,&apos;Robotic&apos;)#&gt;&gt;&gt; &#123;&#125;#print involved(courses, &apos;&apos;)#&gt;&gt;&gt; &#123;&#125; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ej9rXa13kr4.mp4 Refactoring1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 6. In video 28. Update, it was suggested that some of the duplicate code in# lookup and update could be avoided by a better design. We can do this by# defining a procedure that finds the entry corresponding to a given key, and# using that in both lookup and update.# Here are the original procedures:def hashtable_update(htable, key, value): bucket = hashtable_get_bucket(htable, key) for entry in bucket: if entry[0] == key: entry[1] = value return bucket.append([key, value])def hashtable_lookup(htable, key): bucket = hashtable_get_bucket(htable, key) for entry in bucket: if entry[0] == key: return entry[1] return Nonedef make_hashtable(size): table = [] for unused in range(0, size): table.append([]) return tabledef hash_string(s, size): h = 0 for c in s: h = h + ord(c) return h % sizedef hashtable_get_bucket(htable, key): return htable[hash_string(key, len(htable))]# Whenever we have duplicate code like the loop that finds the entry in# hashtable_update and hashtable_lookup, we should think if there is a better way# to write this that would avoid the duplication. We should be able to rewrite# these procedures to be shorter by defining a new procedure and rewriting both# hashtable_update and hashtable_lookup to use that procedure.# Modify the code for both hashtable_update and hashtable_lookup to have the same# behavior they have now, but using fewer lines of code in each procedure. You# should define a new procedure to help with this. Your new version should have# approximately the same running time as the original version, but neither# hashtable_update or hashtable_lookup should include any for or while loop, and# the block of each procedure should be no more than 6 lines long.# Your procedures should have the same behavior as the originals. For example,table = make_hashtable(10)hashtable_update(table, &apos;Python&apos;, &apos;Monty&apos;)hashtable_update(table, &apos;CLU&apos;, &apos;Barbara Liskov&apos;)hashtable_update(table, &apos;JavaScript&apos;, &apos;Brendan Eich&apos;)hashtable_update(table, &apos;Python&apos;, &apos;Guido van Rossum&apos;)print hashtable_lookup(table, &apos;Python&apos;)#&gt;&gt;&gt; Guido van Rossum https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EU9NvdGoAt4.mp4 Memoization1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# [Double Gold Star] Memoization is a way to make code run faster by saving# previously computed results. Instead of needing to recompute the value of an# expression, a memoized computation first looks for the value in a cache of# pre-computed values.# Define a procedure, cached_execution(cache, proc, proc_input), that takes in# three inputs: a cache, which is a Dictionary that maps inputs to proc to# their previously computed values, a procedure, proc, which can be called by# just writing proc(proc_input), and proc_input which is the input to proc.# Your procedure should return the value of the proc with input proc_input,# but should only evaluate it if it has not been previously called.def cached_execution(cache, proc, proc_input): # Your code here# Here is an example showing the desired behavior of cached_execution:def factorial(n): print &quot;Running factorial&quot; result = 1 for i in range(2, n + 1): result = result * i return resultcache = &#123;&#125; # start cache as an empty dictionary### first execution (should print out Running factorial and the result)print cached_execution(cache, factorial, 50)print &quot;Second time:&quot;### second execution (should only print out the result)print cached_execution(cache, factorial, 50)# Here is a more interesting example using cached_execution# (do not worry if you do not understand this, though,# it will be clearer after Unit 6):def cached_fibo(n): if n == 1 or n == 0: return n else: return (cached_execution(cache, cached_fibo, n - 1 ) + cached_execution(cache, cached_fibo, n - 2 )) cache = &#123;&#125; # new cache for this procedure# do not try this at home...at least without a cache!print cached_execution(cache, cached_fibo,100) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_aPiXzmiems.mp4 Problem Set(Optional)Shift a LetterShift n LetterRotateQ&amp;AHash Tableshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eiktSrhdrxs.mp4 Rehashinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UMsVMW2S53w.mp4 Importing Librarieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/G3ovp33txfc.mp4 Programming Literacyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0oXF2nOTX6I.mp4 How to Have Infinite PowerInfinite Powerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/emhiUKHuBXY.mp4The notes for Unit 6 are here: PDF and web. This unit introduces what I think is the most fascinating and powerful idea in all of computing - recursive definitions. Understanding them requires some mind-bending gymnastics, but once you do, you will find elegant and powerful new ways to think about nearly all problems you encounter. The course moves through this pretty quickly, but fortunately many students have contributed great additional resources that explain things very well and with more detail than I do in the course, and give you more practice with recursive programs. Here are a few that I think are especially good: Yet Another Attempt to Explain Recursion by GoldsongUnderstanding Recursion: The Stack Model by Charles LinStG’s Recursion Collection by Sam the Great Long Wordshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-PhZlJuDf_o.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XzJO5xc3QIk.mp4 Counterhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ap3okJ5jIUE.mp4 Counter Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6s-aT1rO3JQ.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YnPLnU9D3mQ.mp4 Expanding Our Grammarhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qYsl757ShjA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nU2DBYNw1jM.mp4 Recursive Definitionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LinhpqM4cCg.mp4 Ancestorshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_AQRlt9UA4o.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ip3vojOsIkI.mp4 Recursive Procedureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Wa6y0I_uojk.mp4 Recursive FactorialAround ~0:06, Dave says that factorial takes a positive whole number as its input, but factorial can also take 0 as an input as well. Instead, then, the input to factorial can be any positive integer or 0. (Side note: whole numbers are defined differently in different contexts, but they are often defined as all of the non-negative integers. This means the whole numbers are 0, 1, 2, 3, 4…, and if we use this terminology, factorial could take any whole number as its input.) Note: If you get a The server encountered an error. Please try running again. error, that may mean that your program is not terminating when tested. Make sure your recursion will eventually reach a base case.1234567891011121314151617# Define a procedure, factorial, that takes a natural number as its input, and# returns the number of ways to arrange the input number of items.def factorial(n):#print factorial(0)#&gt;&gt;&gt; 1#print factorial(5)#&gt;&gt;&gt; 120#print factorial(10)#&gt;&gt;&gt; 3628800 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gWoWZHonPdE.mp4123456789101112131415161718# Define a procedure, factorial, that takes a natural number as its input, and# returns the number of ways to arrange the input number of items.def factorial(n): if n==0: return 1 else: return n*factorial(n-1)print factorial(0)#&gt;&gt;&gt; 1print factorial(5)#&gt;&gt;&gt; 120print factorial(10)#&gt;&gt;&gt; 3628800 Palindromeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zWnI5eACaLM.mp41234567891011121314151617181920212223242526# Define a procedure is_palindrome, that takes as input a string, and returns a# Boolean indicating if the input string is a palindrome.# Base Case: &apos;&apos; =&gt; True# Recursive Case: if first and last characters don&apos;t match =&gt; False# if they do match, is the middle a palindrome?def is_palindrome(s): if len(s)==0: return True else: if s[0]==s[-1]: s=s[1:-1] return is_palindrome(s) else: return False print is_palindrome(&apos;&apos;)#&gt;&gt;&gt; Trueprint is_palindrome(&apos;abab&apos;)#&gt;&gt;&gt; Falseprint is_palindrome(&apos;abba&apos;)#&gt;&gt;&gt; True https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LTZXRoLZaJQ.mp4 Recursive Vs Iterativehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kirruxKHaqk.mp4 Bunnieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E6oGm_Z2aQk.mp4123456789101112131415161718192021222324# Define a procedure, fibonacci, that takes a natural number as its input, and# returns the value of that fibonacci number.# Two Base Cases:# fibonacci(0) =&gt; 0# fibonacci(1) =&gt; 1# Recursive Case:# n &gt; 1 : fibonacci(n) =&gt; fibonacci(n-1) + fibonacci(n-2)def fibonacci(n): if n&gt;1: return fibonacci(n-1) + fibonacci(n-2) else: if n==1: return 1 else: return 0print fibonacci(0)#&gt;&gt;&gt; 0print fibonacci(1)#&gt;&gt;&gt; 1print fibonacci(15)#&gt;&gt;&gt; 610 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/S6wCTLG8BJg.mp4 Divide and Be Conqueredhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eBu1Z_FBwb4.mp4Note that this is the standard mathematical definition of the Fibonacci sequence, which is a bit different from the counting rabbits motivation in the original problem. The mathematical sequence starts with 0, which is more elegant mathematically, but wouldn’t make as much sense for rabbits multiplying. Counting Callshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Cai4WuKg4SM.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bqF8QuYX8YA.mp4At 1:58 minutes onwards, the formula should be fibo(36 - (n - 1)) = fibo(36 - n + 1) and not fibo(36 - n - 1). Faster Fibonaccihttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oJAHMzgSTyA.mp41234567891011#Define a faster fibonacci procedure that will enable us to computer#fibonacci(36).def fibonacci(n): current = 0 # fibonacci(0) at the bginning after = 1 # fibonacci(1) at the beginning for i in range(0, n): current, after = after, current + after return currentprint fibonacci(36)#&gt;&gt;&gt; 14930352 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7k7tMKxH6Dg.mp4 Ranking Web Pageshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/k32gyEM5H3Y.mp4 Popularityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EP55W6keH7E.mp4 Good Definitionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FX8RlDKEEiU.mp4A note on the notation: friends(p) is a list of all friends of p.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HbLwTw6N-0s.mp4 Circular Definitionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Rxp6JuoNqL0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YUwZCZVtLaU.mp4 Relaxationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e7-gweWZ0io.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kpXVV8aiZFU.mp4 Page Rankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IKXvSKaI2Ko.mp4 Altavistahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/T6AeBtfaLco.mp4AltaVista was finally shut down in July 2013. Here’s an interesting article from the Washington Post: AltaVista is dead. Here’s why it’s so hard to compete with Google. (mostly an interview with Gabriel Weinberg).https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ZlFPQ2qQo0.mp4 Urankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/H8vrZMAllIY.mp4 Implementing Urankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B5lAVjLd76Q.mp4123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204# Modify the crawl_web procedure so that instead of just returning the # index, it returns an index and a graph. The graph should be a # Dictionary where the key:value entries are:# url: [list of pages url links to] def crawl_web(seed): # returns index, graph of outlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;:[list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) #Insert Code Here graph[page]=outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphcache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipes:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt; and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbanzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablespoons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &#125;def get_page(url): if url in cache: return cache[url] else: return None def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url]def lookup(index, keyword): if keyword in index: return index[keyword] else: return Noneindex , graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;) if &apos;http://udacity.com/cs101x/urank/index.html&apos; in graph: print graph[&apos;http://udacity.com/cs101x/urank/index.html&apos;]#&gt;&gt;&gt; [&apos;http://udacity.com/cs101x/urank/hummus.html&apos;,#&apos;http://udacity.com/cs101x/urank/arsenic.html&apos;,#&apos;http://udacity.com/cs101x/urank/kathleen.html&apos;,#&apos;http://udacity.com/cs101x/urank/nickel.html&apos;,#&apos;http://udacity.com/cs101x/urank/zinc.html&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sR8EJLpWwb4.mp4 Computing Page Rankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_ctzQdS3EfA.mp4 Formal Calculationshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YZ3kRWKL0DI.mp4 Computer Rankshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sPaVbELrmh0.mp4 Finishing Urankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GovJzUltdL8.mp4rank(page, time) is defined as: ∑​p∈inlinks​​​outlinks​​d⋅rank(t−1,p)​​ or: rank(page, 0) = 1/npages rank(page, t) = (1-d)/npages + sum (d * rank(p, t - 1) / number of outlinks from p) over all pages p that link to this page Thanks to Henry for suggesting to add this.The URLs have changed around a bit! Here’s a new index page you can start with to test out your search engine: https://www.udacity.com/cs101x/urank/index.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225#Finishing the page ranking algorithm.def compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages #Insert Code Here for p in graph: if page in graph[p]: newrank = newrank + d * ranks[p] / len(graph[p]) newranks[page] = newrank ranks = newranks return rankscache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipies:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt; and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbonzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablesppons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &#125;def crawl_web(seed): # returns index, graph of inlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;, [list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) graph[page] = outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphdef get_page(url): if url in cache: return cache[url] else: return None def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url]def lookup(index, keyword): if keyword in index: return index[keyword] else: return Noneindex, graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;)#print graphranks = compute_ranks(graph)print ranks#&gt;&gt;&gt; &#123;&apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: 0.11661866666666663,#&apos;http://udacity.com/cs101x/urank/zinc.html&apos;: 0.038666666666666655,#&apos;http://udacity.com/cs101x/urank/hummus.html&apos;: 0.038666666666666655,#&apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: 0.054133333333333325,#&apos;http://udacity.com/cs101x/urank/index.html&apos;: 0.033333333333333326,#&apos;http://udacity.com/cs101x/urank/nickel.html&apos;: 0.09743999999999997&#125; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/k861qM5OqvU.mp4 Search Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7IlDnp39b0U.mp4 Problem SetRecursive Grammarshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ej9obZ0QECY.mp4 Rabbits Multiplying123456789101112131415161718192021222324252627282930313233343536373839404142# Rabbits Multiplying# A (slightly) more realistic model of rabbit multiplication than the Fibonacci# model, would assume that rabbits eventually die. For this question, some# rabbits die from month 6 onwards.## Thus, we can model the number of rabbits as:## rabbits(1) = 1 # There is one pair of immature rabbits in Month 1# rabbits(2) = 1 # There is one pair of mature rabbits in Month 2## For months 3-5:# Same as Fibonacci model, no rabbits dying yet# rabbits(n) = rabbits(n - 1) + rabbits(n - 2)### For months &gt; 5:# All the rabbits that are over 5 months old die along with a few others# so that the number that die is equal to the number alive 5 months ago.# Before dying, the bunnies reproduce.# rabbits(n) = rabbits(n - 1) + rabbits(n - 2) - rabbits(n - 5)## This produces the rabbit sequence: 1, 1, 2, 3, 5, 7, 11, 16, 24, 35, 52, ...## Define a procedure rabbits that takes as input a number n, and returns a# number that is the value of the nth number in the rabbit sequence.# For example, rabbits(10) -&gt; 35. (It is okay if your procedure takes too# long to run on inputs above 30.)def rabbits(n):#print rabbits(10)#&gt;&gt;&gt; 35#s = &quot;&quot;#for i in range(1,12):# s = s + str(rabbits(i)) + &quot; &quot;#print s#&gt;&gt;&gt; 1 1 2 3 5 7 11 16 24 35 52 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pcGGCOPPtmE.mp4 Spreading Udaciousness12345678910111213141516171819202122232425262728293031323334353637383940414243# Spreading Udaciousness # One of our modest goals is to teach everyone in the world to program and# understand computer science. To estimate how long this will take we have# developed a (very flawed!) model:# Everyone answering this question will convince a number, spread, (input to # the model) of their friends to take the course next offering. This will # continue, so that all of the newly recruited students, as well as the original# students, will convince spread of their# friends to take the following offering of the course.# recruited friends are unique, so there is no duplication among the newly# recruited students. Define a procedure, hexes_to_udaciousness(n, spread,# target), that takes three inputs: the starting number of Udacians, the spread# rate (how many new friends each Udacian convinces to join each hexamester),# and the target number, and outputs the number of hexamesters needed to reach # (or exceed) the target.# For credit, your procedure must not use: while, for, or import math. def hexes_to_udaciousness(n, spread, target):# 0 more needed, since n already exceeds target#print hexes_to_udaciousness(100000, 2, 36230) #&gt;&gt;&gt; 0# after 1 hexamester, there will be 50000 + (50000 * 2) Udacians#print hexes_to_udaciousness(50000, 2, 150000) #&gt;&gt;&gt; 1 # need to match or exceed the target#print hexes_to_udaciousness(50000, 2, 150001)#&gt;&gt;&gt; 2 # only 12 hexamesters (2 years) to world domination!#print hexes_to_udaciousness(20000, 2, 7 * 10 ** 9) #&gt;&gt;&gt; 12 # more friends means faster world domination!#print hexes_to_udaciousness(15000, 3, 7 * 10 ** 9)#&gt;&gt;&gt; 10 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/q_atgGWy57Y.mp4 Deep Count1234567891011121314151617181920212223242526272829303132333435363738# Deep Count # The built-in len operator outputs the number of top-level elements in a List,# but not the total number of elements. For this question, your goal is to count# the total number of elements in a list, including all of the inner lists.# Define a procedure, deep_count, that takes as input a list, and outputs the# total number of elements in the list, including all elements in lists that it# contains.# For this procedure, you will need a way to test if a value is a list. We have# provided a procedure, is_list(p) that does this:def is_list(p): return isinstance(p, list)# It is not necessary to understand how is_list works. It returns True if the# input is a List, and returns False otherwise.def deep_count(p):#print deep_count([1, 2, 3])#&gt;&gt;&gt; 3# The empty list still counts as an element of the outer list#print deep_count([1, [], 3]) #&gt;&gt;&gt; 3 #print deep_count([1, [1, 2, [3, 4]]])#&gt;&gt;&gt; 7#print deep_count([[[[[[[[1, 2, 3]]]]]]]])#&gt;&gt;&gt; 10 Feeling Lucky123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231#Feeling Lucky #In Unit 6, we implemented a page ranking algorithm, but didn&apos;t finish the final#step of using it to improve our search results. For this question, you will use#the page rankings to produce the best output for a given query.#Define a procedure, lucky_search, that takes as input an index, a ranks#dictionary (the result of compute_ranks), and a keyword, and returns the one#URL most likely to be the best site for that keyword. If the keyword does not#appear in the index, lucky_search should return None.def lucky_search(index, ranks, keyword): cache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipies:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt; and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbonzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablesppons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;,&#125;def get_page(url): if url in cache: return cache[url] return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url] def lookup(index, keyword): if keyword in index: return index[keyword] else: return Nonedef crawl_web(seed): # returns index, graph of inlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;, [list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) graph[page] = outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphdef compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: newrank = newrank + d * (ranks[node] / len(graph[node])) newranks[page] = newrank ranks = newranks return ranks#Here&apos;s an example of how your procedure should work on the test site: #index, graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;)#ranks = compute_ranks(graph)#print lucky_search(index, ranks, &apos;Hummus&apos;)#&gt;&gt;&gt; http://udacity.com/cs101x/urank/kathleen.html#print lucky_search(index, ranks, &apos;the&apos;)#&gt;&gt;&gt; http://udacity.com/cs101x/urank/nickel.html#print lucky_search(index, ranks, &apos;babaganoush&apos;)#&gt;&gt;&gt; None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6qVB4lZmzMc.mp4123456789def lucky_search(index, ranks, keyword): pages=lookup(index,keyword) if not pages: return None best_page=pages[0] for candidate in pages: if ranks[candidate]&gt;ranks[best_page]: best_page=candidate return best_page Problem Set 6 StarredFamily Trees123456789101112131415161718192021222324252627282930313233343536373839404142434445# Single Gold Star# Family Trees# In the lecture, we showed a recursive definition for your ancestors. For this# question, your goal is to define a procedure that finds someone&apos;s ancestors,# given a Dictionary that provides the parent relationships.# Here&apos;s an example of an input Dictionary:ada_family = &#123; &apos;Judith Blunt-Lytton&apos;: [&apos;Anne Isabella Blunt&apos;, &apos;Wilfrid Scawen Blunt&apos;], &apos;Ada King-Milbanke&apos;: [&apos;Ralph King-Milbanke&apos;, &apos;Fanny Heriot&apos;], &apos;Ralph King-Milbanke&apos;: [&apos;Augusta Ada King&apos;, &apos;William King-Noel&apos;], &apos;Anne Isabella Blunt&apos;: [&apos;Augusta Ada King&apos;, &apos;William King-Noel&apos;], &apos;Byron King-Noel&apos;: [&apos;Augusta Ada King&apos;, &apos;William King-Noel&apos;], &apos;Augusta Ada King&apos;: [&apos;Anne Isabella Milbanke&apos;, &apos;George Gordon Byron&apos;], &apos;George Gordon Byron&apos;: [&apos;Catherine Gordon&apos;, &apos;Captain John Byron&apos;], &apos;John Byron&apos;: [&apos;Vice-Admiral John Byron&apos;, &apos;Sophia Trevannion&apos;] &#125;# Define a procedure, ancestors(genealogy, person), that takes as its first input# a Dictionary in the form given above, and as its second input the name of a# person. It should return a list giving all the known ancestors of the input# person (this should be the empty list if there are none). The order of the list# does not matter and duplicates will be ignored.def ancestors(genealogy, person):# Here are some examples:#print ancestors(ada_family, &apos;Augusta Ada King&apos;)#&gt;&gt;&gt; [&apos;Anne Isabella Milbanke&apos;, &apos;George Gordon Byron&apos;,# &apos;Catherine Gordon&apos;,&apos;Captain John Byron&apos;]#print ancestors(ada_family, &apos;Judith Blunt-Lytton&apos;)#&gt;&gt;&gt; [&apos;Anne Isabella Blunt&apos;, &apos;Wilfrid Scawen Blunt&apos;, &apos;Augusta Ada King&apos;,# &apos;William King-Noel&apos;, &apos;Anne Isabella Milbanke&apos;, &apos;George Gordon Byron&apos;,# &apos;Catherine Gordon&apos;, &apos;Captain John Byron&apos;]#print ancestors(ada_family, &apos;Dave&apos;)#&gt;&gt;&gt; [] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SQ6508of_ZA.mp4 Khayyam Triangle12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Double Gold Star# Khayyam Triangle# The French mathematician, Blaise Pascal, who built a mechanical computer in# the 17th century, studied a pattern of numbers now commonly known in parts of# the world as Pascal&apos;s Triangle (it was also previously studied by many Indian,# Chinese, and Persian mathematicians, and is known by different names in other# parts of the world).# The pattern is shown below:# 1# 1 1# 1 2 1# 1 3 3 1# 1 4 6 4 1# ...# Each number is the sum of the number above it to the left and the number above# it to the right (any missing numbers are counted as 0).# Define a procedure, triangle(n), that takes a number n as its input, and# returns a list of the first n rows in the triangle. Each element of the# returned list should be a list of the numbers at the corresponding row in the# triangle.def triangle(n):#For example:#print triangle(0)#&gt;&gt;&gt; []#print triangle(1)#&gt;&gt;&gt; [[1]]#print triangle(2)#&gt;&gt; [[1], [1, 1]]#print triangle(3)#&gt;&gt;&gt; [[1], [1, 1], [1, 2, 1]]#print triangle(6)#&gt;&gt;&gt; [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1], [1, 5, 10, 10, 5, 1]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i8X3KHanfXE.mp412345678910111213141516def make_next_row(row): prev=0 result=[] for i in row: result.append(i+prev) prev=i result.append(prev) return resultdef triangle(n): current=[1] result=[] for unuse in range(0,n): result.append(current) current=make_next_row(current) return result Only a Little Lucky123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269# Triple Gold Star# Only A Little Lucky# The Feeling Lucky question (from the regular homework) assumed it was enough# to find the best-ranked page for a given query. For most queries, though, we# don&apos;t just want the best page (according to the page ranking algorithm), we# want a list of many pages that match the query, ordered from the most likely# to be useful to the least likely.# Your goal for this question is to define a procedure, ordered_search(index,# ranks, keyword), that takes the same inputs as lucky_search from Question 5,# but returns an ordered list of all the URLs that match the query.# To order the pages, use the quicksort algorithm, invented by Sir Tony Hoare in# 1959. Quicksort provides a way to sort any list of data, using an expected# number of comparisons that scales as n log n where n is the number of elements# in the list.# The idea of quicksort is quite simple:# If the list has zero or one elements, it is already sorted.# Otherwise, pick a pivot element, and split the list into two partitions: one# contains all the elements equal to or lower than the value of the pivot# element, and the other contains all the elements that are greater than the# pivot element. Recursively sort each of the sub-lists, and then return the# result of concatenating the sorted left sub-list, the pivot element, and the# sorted right sub-list.# For simplicity, use the first element in the list as your pivot element (this# is not usually a good choice, since it means if the input list is already# nearly sorted, the actual work will be much worse than expected).def ordered_search(index, ranks, keyword):cache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipies:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the&lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from&lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try&lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbonzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablesppons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;,&#125;def get_page(url): if url in cache: return cache[url] return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url)def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url]def lookup(index, keyword): if keyword in index: return index[keyword] else: return Nonedef crawl_web(seed): # returns index, graph of inlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;, [list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) graph[page] = outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphdef compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: newrank = newrank + d * (ranks[node] / len(graph[node])) newranks[page] = newrank ranks = newranks return ranks# Here are some example showing what ordered_search should do:# Observe that the result list is sorted so the highest-ranking site is at the# beginning of the list.# Note: the intent of this question is for students to write their own sorting# code, not to use the built-in sort procedure.index, graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;)ranks = compute_ranks(graph)#print ordered_search(index, ranks, &apos;Hummus&apos;)#&gt;&gt;&gt; [&apos;http://udacity.com/cs101x/urank/kathleen.html&apos;,# &apos;http://udacity.com/cs101x/urank/nickel.html&apos;,# &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;,# &apos;http://udacity.com/cs101x/urank/hummus.html&apos;,# &apos;http://udacity.com/cs101x/urank/index.html&apos;]#print ordered_search(index, ranks, &apos;the&apos;)#&gt;&gt;&gt; [&apos;http://udacity.com/cs101x/urank/nickel.html&apos;,# &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;,# &apos;http://udacity.com/cs101x/urank/hummus.html&apos;,# &apos;http://udacity.com/cs101x/urank/index.html&apos;]#print ordered_search(index, ranks, &apos;babaganoush&apos;)#&gt;&gt;&gt; None The provided solution isn’t complete - it doesn’t actually include the ordered_search code, but only the code for sorting the pages. Atlas7-115196 provided a more complete solution to this problem! See the forum post: http://forums.udacity.com/questions/100371211/corrected-udacity-solution#cs101 1234567891011121314151617def quicksort_urls(ranks,urls): if not urls or len(urls)&lt;=1: return urls else: pivot=ranks[urls[0]] before=[] after=[] for url in urls[1:]: if ranks[url]&gt;=pivot: before.append(url) else: after.append(url) return quicksort_urls(ranks,before)+[urls[0]]+quicksort_urls(ranks,after)def ordered_search(index, ranks, keyword): urls=lookup(index, keyword) return quicksort_urls(ranks,urls) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lI9O8wUEDFc.mp4 Q&amp;APythonichttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2g6qtjwKkA0.mp4Correction - Peter Norvig teaches class CS212. Python Versionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/owH7bqKiR-g.mp4 Using Recursionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VWyHjEh0tfA.mp4 Recursion in Other Languageshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/siNYLJ1YaAc.mp4 Pagerankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rN-5K_q4JDc.mp4 Challenges in Searchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ulkWpQl6izE.mp4 International Charactershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9QbeX7LOl0g.mp4 Past, Present, and Future of ComputerPast, Present, and Futurehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4TChbk9pnxQ.mp4 Themeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-6OLwm10pqs.mp4 Overviewhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-atl1N1mvu0.mp4 Computer Sciencehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YBJk5Z5bAEA.mp4 Computer Sciencehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/DIbtX0GqIA8.mp4 Past of Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vvIj_PWFoyY.mp4 Computer History Museumhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ClTnWszPp3Q.mp4 Babbage Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5nYcND7WjCY.mp4 First Hard Drivehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oSCCFDZLRgY.mp4 Search Before Computershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/F2DTZoa-zPo.mp4 Search on the Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mgR9sInLwfc.mp4 Present of Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9fxDFZGwUiA.mp4 Slac and Big Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4_0sCB_csRI.mp4 Mozillahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_IfqKBbEqck.mp4 Open Sourcehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TUBAD93kCfA.mp4 Getting Involvedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/56KQGpGOwLM.mp4 Having an Impacthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lL36LxpsXBI.mp4 Benetechhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FqQFwXOJTeU.mp4 Future of Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EsTiQxNDQfo.mp4 Text Analysishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/679-n8LWaVo.mp4 Energy Aware Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wLyAANVyJQM.mp4 Computer Securityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/H87Yxc4p-C8.mp4 Theory of Computationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/W7nD3AMJDAI.mp4 Quantum Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XafsCK3yk4U.mp4 Stay Udacioushttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oDxqlHY6V1w.mp4 Cumulative Practice ProblemsPick One12345678910111213141516171819202122232425# Question 1: Pick One# Define a procedure, pick_one, that takes three inputs: a Boolean # and two other values. If the first input is True, it should return # the second input. If the first input is False, it should return the # third input.# For example, pick_one(True, 37, &apos;hello&apos;) should return 37, and# pick_one(False, 37, &apos;hello&apos;) should return &apos;hello&apos;.def pick_one():print pick_one(True, 37, &apos;hello&apos;)#&gt;&gt;&gt; 37print pick_one(False, 37, &apos;hello&apos;)#&gt;&gt;&gt; helloprint pick_one(True, &apos;red pill&apos;, &apos;blue pill&apos;)#&gt;&gt;&gt; red pillprint pick_one(False, &apos;sunny&apos;, &apos;rainy&apos;)#&gt;&gt;&gt; rainy https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K2s3b4XTaN0.mp412 Triangular Numbers1234567891011121314151617181920212223242526# Question 2. Triangular Numbers# The triangular numbers are the numbers 1, 3, 6, 10, 15, 21, ...# They are calculated as follows.# 1# 1 + 2 = 3# 1 + 2 + 3 = 6# 1 + 2 + 3 + 4 = 10# 1 + 2 + 3 + 4 + 5 = 15# Write a procedure, triangular, that takes as its input a positive # integer n and returns the nth triangular number.def triangular():print triangular(1)#&gt;&gt;&gt;1print triangular(3)#&gt;&gt;&gt; 6print triangular(10)#&gt;&gt;&gt; 55 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/tQplGM4DtTA.mp4 Linear TimeFor the procedures below, check the procedures whose running time scales linearly with the length of the input in the worst case. You may assume the elements in input_list are fairly small numbers. def proc1(input_list): maximum = None for element in input_list : if not maximum or maximum &lt; element: maximum = element return maximum def proc2(input_list): sum = 0 while len(input_list) &gt; 0: sum = sum + input_list[0] # Assume input_list[0] is constant time input_list = input_list[1:] # Assume input_list[1:] is constant time return sum def proc3(input_list): for i in range(0, len(input_list)): for j in range(0, len(input_list)): if input_list[i] == input_list[j] and i != j: return False return True https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nnNQTjmc3DY.mp4 Remove Tags12345678910111213141516171819202122232425262728# Question 4: Remove Tags# When we add our words to the index, we don&apos;t really want to include# html tags such as &lt;body&gt;, &lt;head&gt;, &lt;table&gt;, &lt;a href=&quot;...&quot;&gt; and so on.# Write a procedure, remove_tags, that takes as input a string and returns# a list of words, in order, with the tags removed. Tags are defined to be# strings surrounded by &lt; &gt;. Words are separated by whitespace or tags. # You may assume the input does not include any unclosed tags, that is, # there will be no &apos;&lt;&apos; without a following &apos;&gt;&apos;.def remove_tags():print remove_tags(&apos;&apos;&apos;&lt;h1&gt;Title&lt;/h1&gt;&lt;p&gt;This is a &lt;a href=&quot;http://www.udacity.com&quot;&gt;link&lt;/a&gt;.&lt;p&gt;&apos;&apos;&apos;)#&gt;&gt;&gt; [&apos;Title&apos;,&apos;This&apos;,&apos;is&apos;,&apos;a&apos;,&apos;link&apos;,&apos;.&apos;]print remove_tags(&apos;&apos;&apos;&lt;table cellpadding=&apos;3&apos;&gt; &lt;tr&gt;&lt;td&gt;Hello&lt;/td&gt;&lt;td&gt;World!&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt;&apos;&apos;&apos;)#&gt;&gt;&gt; [&apos;Hello&apos;,&apos;World!&apos;]print remove_tags(&quot;&lt;hello&gt;&lt;goodbye&gt;&quot;)#&gt;&gt;&gt; []print remove_tags(&quot;This is plain text.&quot;)#&gt;&gt;&gt; [&apos;This&apos;, &apos;is&apos;, &apos;plain&apos;, &apos;text.&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5LrTUeawyfI.mp41234567def remove_tags(content): start_pos = content.find(&apos;&lt;&apos;) while start_pos !=-1: end_pos = content.find(&apos;&gt;&apos;,start_pos) content=content[:start_pos]+&apos; &apos;+content[end_pos+1:] start_pos = content.find(&apos;&lt;&apos;) return content.split() Date Converter1234567891011121314151617181920212223242526272829303132333435363738# Question 5: Date Converter# Write a procedure date_converter which takes two inputs. The first is # a dictionary and the second a string. The string is a valid date in # the format month/day/year. The procedure should return# the date written in the form &lt;day&gt; &lt;name of month&gt; &lt;year&gt;.# For example , if the# dictionary is in English,english = &#123;1:&quot;January&quot;, 2:&quot;February&quot;, 3:&quot;March&quot;, 4:&quot;April&quot;, 5:&quot;May&quot;, 6:&quot;June&quot;, 7:&quot;July&quot;, 8:&quot;August&quot;, 9:&quot;September&quot;,10:&quot;October&quot;, 11:&quot;November&quot;, 12:&quot;December&quot;&#125;# then &quot;5/11/2012&quot; should be converted to &quot;11 May 2012&quot;. # If the dictionary is in Swedishswedish = &#123;1:&quot;januari&quot;, 2:&quot;februari&quot;, 3:&quot;mars&quot;, 4:&quot;april&quot;, 5:&quot;maj&quot;, 6:&quot;juni&quot;, 7:&quot;juli&quot;, 8:&quot;augusti&quot;, 9:&quot;september&quot;,10:&quot;oktober&quot;, 11:&quot;november&quot;, 12:&quot;december&quot;&#125;# then &quot;5/11/2012&quot; should be converted to &quot;11 maj 2012&quot;.# Hint: int(&apos;12&apos;) converts the string &apos;12&apos; to the integer 12.def date_converter():print date_converter(english, &apos;5/11/2012&apos;)#&gt;&gt;&gt; 11 May 2012print date_converter(english, &apos;5/11/12&apos;)#&gt;&gt;&gt; 11 May 12print date_converter(swedish, &apos;5/11/2012&apos;)#&gt;&gt;&gt; 11 maj 2012print date_converter(swedish, &apos;12/5/1791&apos;)#&gt;&gt;&gt; 5 december 1791 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YgW5c4mH19g.mp41234567def date_converter(dic,string_date): start_pos=string_date.find(&apos;/&apos;) end_pos=string_date.find(&apos;/&apos;,start_pos+1) month=int(string_date[:start_pos]) day = string_date[start_pos+1:end_pos] year=string_date[end_pos+1:] return day+&apos; &apos;+dic[month]+&apos; &apos;+year or123def date_converter(dic,string_date): month,day,year=string_date.split(&apos;/&apos;) return day+&apos; &apos;+dic[int(month)]+&apos; &apos;+year TerminationFor each of the procedures defined below, check the box if the procedure always terminates for all inputs that are natural numbers (1,2,3…). def proc1(n): while True: n = n - 1 if n == 0: break return 3 def proc2(n): if n == 0: return n return 1 + proc2(n - 2) def proc3(n): if n &lt;= 3: return 1 return proc3(n - 1) + proc3(n - 2) + proc3(n - 3) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RBc-2ZVuH9E.mp4 Find and Replace123456789101112131415161718192021222324252627282930313233343536# Question 7: Find and Replace# For this question you need to define two procedures:# make_converter(match, replacement)# Takes as input two strings and returns a converter. It doesn&apos;t have# to make a specific type of thing. It can # return anything you would find useful in apply_converter.# apply_converter(converter, string)# Takes as input a converter (produced by make_converter), and # a string, and returns the result of applying the converter to the # input string. This replaces all occurrences of the match used to # build the converter, with the replacement. It keeps doing # replacements until there are no more opportunities for replacements.def make_converter(match, replacement):def apply_converter(converter, string):# For example,c1 = make_converter(&apos;aa&apos;, &apos;a&apos;)print apply_converter(c1, &apos;aaaa&apos;)#&gt;&gt;&gt; ac = make_converter(&apos;aba&apos;, &apos;b&apos;)print apply_converter(c, &apos;aaaaaabaaaaa&apos;)#&gt;&gt;&gt; ab# Note that this process is not guaranteed to terminate for all inputs# (for example, apply_converter(make_converter(&apos;a&apos;, &apos;aa&apos;), &apos;a&apos;) would # run forever). https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-xUQEeXjC8g.mp41234567891011def make_converter(match, replacement): return [match, replacement]def apply_converter(converter, string): previous=None while previous!=string: previous=string position=string.find(converter[0]) if position!=-1: string=string[:position] + converter[1] + string[position+len(converter[0]):] return string Longest Repetition123456789101112131415161718192021222324252627# Question 8: Longest Repetition# Define a procedure, longest_repetition, that takes as input a # list, and returns the element in the list that has the most # consecutive repetitions. If there are multiple elements that # have the same number of longest repetitions, the result should # be the one that appears first. If the input list is empty, # it should return None.def longest_repetition():#For example,print longest_repetition([1, 2, 2, 3, 3, 3, 2, 2, 1])# 3print longest_repetition([&apos;a&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;d&apos;, &apos;d&apos;])# bprint longest_repetition([1,2,3,4,5])# 1print longest_repetition([])# None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LWSvPPfH9Cw.mp4123456789101112131415def longest_repetition(input_list): best=None length=0 current=None current_length=0 for element in input_list: if current != element: current_length=1 current=element else: current_length+=1 if current_length&gt;length: best=current length=current_length return best Deep Reverse1234567891011121314151617181920212223242526272829303132# Question 9: Deep Reverse# Define a procedure, deep_reverse, that takes as input a list, # and returns a new list that is the deep reverse of the input list. # This means it reverses all the elements in the list, and if any # of those elements are lists themselves, reverses all the elements # in the inner list, all the way down. # Note: The procedure must not change the input list.# The procedure is_list below is from Homework 6. It returns True if # p is a list and False if it is not.def is_list(p): return isinstance(p, list)def deep_reverse():#For example,p = [1, [2, 3, [4, [5, 6]]]]print deep_reverse(p)#&gt;&gt;&gt; [[[[6, 5], 4], 3, 2], 1]print p#&gt;&gt;&gt; [1, [2, 3, [4, [5, 6]]]]q = [1, [2,3], 4, [5,6]]print deep_reverse(q)#&gt;&gt;&gt; [ [6,5], 4, [3, 2], 1]print q#&gt;&gt;&gt; [1, [2,3], 4, [5,6]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aYLkoPSiiG0.mp412345678def deep_reverse(input_list): if is_list(input_list): new_list=[] for i in range(len(input_list)-1,-1,-1): new_list.append(deep_reverse(input_list[i])) return new_list else: return input_list Challenging Practice ProblemsStirling and Bell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# One Gold Star# Question 1-star: Stirling and Bell Numbers# The number of ways of splitting n items in k non-empty sets is called# the Stirling number, S(n,k), of the second kind. For example, the group # of people Dave, Sarah, Peter and Andy could be split into two groups in # the following ways.# 1. Dave, Sarah, Peter Andy# 2. Dave, Sarah, Andy Peter# 3. Dave, Andy, Peter Sarah# 4. Sarah, Andy, Peter Dave# 5. Dave, Sarah Andy, Peter# 6. Dave, Andy Sarah, Peter# 7. Dave, Peter Andy, Sarah# so S(4,2) = 7# If instead we split the group into one group, we have just one way to # do it.# 1. Dave, Sarah, Peter, Andy# so S(4,1) = 1# or into four groups, there is just one way to do it as well# 1. Dave Sarah Peter Andy# so S(4,4) = 1# If we try to split into more groups than we have people, there are no# ways to do it.# The formula for calculating the Stirling numbers is# S(n, k) = k*S(n-1, k) + S(n-1, k-1)# Furthermore, the Bell number B(n) is the number of ways of splitting n # into any number of parts, that is,# B(n) is the sum of S(n,k) for k =1,2, ... , n.# Write two procedures, stirling and bell. The first procedure, stirling # takes as its inputs two positive integers of which the first is the # number of items and the second is the number of sets into which those # items will be split. The second procedure, bell, takes as input a # positive integer n and returns the Bell number B(n).def stirling(): def bell(): #print stirling(1,1)#&gt;&gt;&gt; 1#print stirling(2,1)#&gt;&gt;&gt; 1#print stirling(2,2)#&gt;&gt;&gt; 1#print stirling(2,3)#&gt;&gt;&gt;0#print stirling(3,1)#&gt;&gt;&gt; 1#print stirling(3,2)#&gt;&gt;&gt; 3#print stirling(3,3)#&gt;&gt;&gt; 1#print stirling(4,1)#&gt;&gt;&gt; 1#print stirling(4,2)#&gt;&gt;&gt; 7#print stirling(4,3)#&gt;&gt;&gt; 6#print stirling(4,4)#&gt;&gt;&gt; 1#print stirling(5,1)#&gt;&gt;&gt; 1#print stirling(5,2)#&gt;&gt;&gt; 15#print stirling(5,3)#&gt;&gt;&gt; 25#print stirling(5,4)#&gt;&gt;&gt; 10#print stirling(5,5)#&gt;&gt;&gt; 1#print stirling(20,15)#&gt;&gt;&gt; 452329200#print bell(1)#&gt;&gt;&gt; 1#print bell(2)#&gt;&gt;&gt; 2#print bell(3)#&gt;&gt;&gt; 5#print bell(4)#&gt;&gt;&gt; 15#print bell(5)#&gt;&gt;&gt; 52#print bell(15)#&gt;&gt;&gt; 1382958545 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TJ4M9ZyAZ3I.mp4123456789101112def stirling(n,k): if n&lt;k: return 0 if n==k or k==1: return 1 return k*stirling(n-1, k) + stirling(n-1, k-1)def bell(n): result=0 for i in range(1,n+1): result+=stirling(n,i) return result Combating Link Spam123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Two Gold Stars# Question 2: Combatting Link Spam# One of the problems with our page ranking system is pages can # collude with each other to improve their page ranks. We consider # A-&gt;B a reciprocal link if there is a link path from B to A of length # equal to or below the collusion level, k. The length of a link path # is the number of links which are taken to travel from one page to the # other.# If k = 0, then a link from A to A is a reciprocal link for node A, # since no links needs to be taken to get from A to A.# If k=1, B-&gt;A would count as a reciprocal link if there is a link # A-&gt;B, which includes one link and so is of length 1. (it requires # two parties, A and B, to collude to increase each others page rank).# If k=2, B-&gt;A would count as a reciprocal link for node A if there is# a path A-&gt;C-&gt;B, for some page C, (link path of length 2),# or a direct link A-&gt; B (link path of length 1).# Modify the compute_ranks code to # - take an extra input k, which is a non-negative integer, and # - exclude reciprocal links of length up to and including k from # helping the page rank.def compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: newrank = newrank + d * (ranks[node]/len(graph[node])) newranks[page] = newrank ranks = newranks return ranks# For exampleg = &#123;&apos;a&apos;: [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], &apos;b&apos;:[&apos;a&apos;], &apos;c&apos;:[&apos;d&apos;], &apos;d&apos;:[&apos;a&apos;]&#125;#print compute_ranks(g, 0) # the a-&gt;a link is reciprocal#&gt;&gt;&gt; &#123;&apos;a&apos;: 0.26676872354238684, &apos;c&apos;: 0.1216391112164609,# &apos;b&apos;: 0.1216391112164609, &apos;d&apos;: 0.1476647842238683&#125;#print compute_ranks(g, 1) # a-&gt;a, a-&gt;b, b-&gt;a links are reciprocal#&gt;&gt;&gt; &#123;&apos;a&apos;: 0.14761759762962962, &apos;c&apos;: 0.08936469270123457,# &apos;b&apos;: 0.04999999999999999, &apos;d&apos;: 0.12202199703703702&#125;#print compute_ranks(g, 2)# a-&gt;a, a-&gt;b, b-&gt;a, a-&gt;c, c-&gt;d, d-&gt;a links are reciprocal# (so all pages end up with the same rank)#&gt;&gt;&gt; &#123;&apos;a&apos;: 0.04999999999999999, &apos;c&apos;: 0.04999999999999999,# &apos;b&apos;: 0.04999999999999999, &apos;d&apos;: 0.04999999999999999&#125; There was a typo in the last test example. # a-&gt;a, a-&gt;b, b-&gt;a, a-&gt;c, c-&gt;d, c-&gt;a links are reciprocal should read # a-&gt;a, a-&gt;b, b-&gt;a, a-&gt;c, c-&gt;d, d-&gt;a links are reciprocal https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hP-UDkuUL0o.mp412345678910111213141516171819202122232425262728def is_reciprocal_link(graph,source,destination,k): if k==0: return source==destination if source in graph[destination]: return True for node in graph[destination]: if is_reciprocal_link(graph,source,node,k-1): return True return False def compute_ranks(graph,k): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: # node links to page if not is_reciprocal_link(graph,node,page,k): newrank = newrank + d * (ranks[node]/len(graph[node])) newranks[page] = newrank ranks = newranks return ranks Elementary Cellular Automatonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/M_pkidxeGMY.mp412345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# THREE GOLD STARS# Question 3-star: Elementary Cellular Automaton# Please see the video for additional explanation.# A one-dimensional cellular automata takes in a string, which in our # case, consists of the characters &apos;.&apos; and &apos;x&apos;, and changes it according # to some predetermined rules. The rules consider three characters, which # are a character at position k and its two neighbours, and determine # what the character at the corresponding position k will be in the new # string.# For example, if the character at position k in the string is &apos;.&apos; and # its neighbours are &apos;.&apos; and &apos;x&apos;, then the pattern is &apos;..x&apos;. We look up # &apos;..x&apos; in the table below. In the table, &apos;..x&apos; corresponds to &apos;x&apos; which # means that in the new string, &apos;x&apos; will be at position k.# Rules:# pattern in position k in contribution to# Value current string new string pattern number# is 0 if replaced by &apos;.&apos;# and value if replaced# by &apos;x&apos;# 1 &apos;...&apos; &apos;.&apos; 1 * 0# 2 &apos;..x&apos; &apos;x&apos; 2 * 1# 4 &apos;.x.&apos; &apos;x&apos; 4 * 1# 8 &apos;.xx&apos; &apos;x&apos; 8 * 1# 16 &apos;x..&apos; &apos;.&apos; 16 * 0# 32 &apos;x.x&apos; &apos;.&apos; 32 * 0# 64 &apos;xx.&apos; &apos;.&apos; 64 * 0# 128 &apos;xxx&apos; &apos;x&apos; 128 * 1# ----------# 142# To calculate the patterns which will have the central character x, work # out the values required to sum to the pattern number. For example,# 32 = 32 so only pattern 32 which is x.x changes the central position to# an x. All the others have a . in the next line.# 23 = 16 + 4 + 2 + 1 which means that &apos;x..&apos;, &apos;.x.&apos;, &apos;..x&apos; and &apos;...&apos; all # lead to an &apos;x&apos; in the next line and the rest have a &apos;.&apos;# For pattern 142, and starting string# ...........x...........# the new strings created will be# ..........xx........... (generations = 1)# .........xx............ (generations = 2)# ........xx............. (generations = 3)# .......xx.............. (generations = 4)# ......xx............... (generations = 5)# .....xx................ (generations = 6)# ....xx................. (generations = 7)# ...xx.................. (generations = 8)# ..xx................... (generations = 9)# .xx.................... (generations = 10)# Note that the first position of the string is next to the last position # in the string.# Define a procedure, cellular_automaton, that takes three inputs: # a non-empty string, # a pattern number which is an integer between 0 and 255 that# represents a set of rules, and # a positive integer, n, which is the number of generations. # The procedure should return a string which is the result of# applying the rules generated by the pattern to the string n times.def cellular_automaton():print cellular_automaton(&apos;.x.x.x.x.&apos;, 17, 2)#&gt;&gt;&gt; xxxxxxx..print cellular_automaton(&apos;.x.x.x.x.&apos;, 249, 3)#&gt;&gt;&gt; .x..x.x.xprint cellular_automaton(&apos;...x....&apos;, 125, 1)#&gt;&gt;&gt; xx.xxxxxprint cellular_automaton(&apos;...x....&apos;, 125, 2)#&gt;&gt;&gt; .xxx....print cellular_automaton(&apos;...x....&apos;, 125, 3)#&gt;&gt;&gt; .x.xxxxxprint cellular_automaton(&apos;...x....&apos;, 125, 4)#&gt;&gt;&gt; xxxx...xprint cellular_automaton(&apos;...x....&apos;, 125, 5)#&gt;&gt;&gt; ...xxx.xprint cellular_automaton(&apos;...x....&apos;, 125, 6)#&gt;&gt;&gt; xx.x.xxxprint cellular_automaton(&apos;...x....&apos;, 125, 7)#&gt;&gt;&gt; .xxxxx..print cellular_automaton(&apos;...x....&apos;, 125, 8)#&gt;&gt;&gt; .x...xxxprint cellular_automaton(&apos;...x....&apos;, 125, 9)#&gt;&gt;&gt; xxxx.x.xprint cellular_automaton(&apos;...x....&apos;, 125, 10)#&gt;&gt;&gt; ...xxxxx Sorry about this. There is a mistake in the video in generation 3 for pattern 30, which makes all the following lines incorrect as well. The corrected output is: ...x.... (input) ..xxx... ( generations = 1) .xx..x.. ( generations = 2) xx.xxxx. ( generations = 3) x..x.... ( generations = 4) xxxxx..x ( generations = 5) .....xxx ( generations = 6) Additional information: Elementary Cellular Automaton at Wolfram’s Mathworld12345678910111213141516171819202122def cellular_automaton(input_string,pattern_number,generation): patterns=&#123;&#125; pattern_list=[&apos;...&apos;,&apos;..x&apos;,&apos;.x.&apos;,&apos;.xx&apos;,&apos;x..&apos;,&apos;x.x&apos; ,&apos;xx.&apos;,&apos;xxx&apos;] n=len(input_string) # build my patterns dictionary for i in range(7,-1,-1): if pattern_number/(2**i)==1: patterns[pattern_list[i]]=&apos;x&apos; pattern_number=pattern_number-2**i else: patterns[pattern_list[i]]=&apos;.&apos; # apply patterns to input_string # with generation times for unuse in range(generation): new_string=&apos;&apos; for i in range(n): pattern=input_string[i-1]+input_string[i]+input_string[(i+1)%n] new_string = new_string + patterns[pattern] input_string=new_string return new_string https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Jc1vOOWfQaA.mp4 Code EditorA Place to Try Things Out1234567# Use this to try out anything you like. Use print to display your answer# when you press the &quot;Test Run&quot; button.# Use the &quot;Reset&quot; button to reset the screen a = 1e+9for i in range(1000000): a+=1print a - 1e+9 Project PrepProject DescriptionFinal Project DescriptionCongratulations on making it to the final project! Your job is to take simple text strings like “Alex likes Carla, Ryan, and Priya” and turn them into a social network. To do this, you must complete a number of required procedures, as described on the next screen. You must also create a “make-your-own” procedure. Most of this project will take place inside the browser and most of it will be auto-graded. Feel free to share your final code with your peers in the Discussion Forum for additional feedback. If you have any questions, ask on the Discussion Forum! Gamer’s Network123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264# --------------------------- ## Intro to CS Final Project ## Gaming Social Network ## --------------------------- ### Background# ==========# You and your friend have decided to start a company that hosts a gaming# social network site. Your friend will handle the website creation (they know # what they are doing, having taken our web development class). However, it is # up to you to create a data structure that manages the game-network information # and to define several procedures that operate on the network. ## In a website, the data is stored in a database. In our case, however, all the # information comes in a big string of text. Each pair of sentences in the text # is formatted as follows: # # &lt;user&gt; is connected to &lt;user1&gt;, ..., &lt;userM&gt;.&lt;user&gt; likes to play &lt;game1&gt;, ..., &lt;gameN&gt;.## For example:# # John is connected to Bryant, Debra, Walter.John likes to play The Movie: The Game, The Legend of Corgi, Dinosaur Diner.# # Note that each sentence will be separated from the next by only a period. There will # not be whitespace or new lines between sentences.# # Your friend records the information in that string based on user activity on # the website and gives it to you to manage. You can think of every pair of# sentences as defining a user&apos;s profile.## Consider the data structures that we have used in class - lists, dictionaries,# and combinations of the two (e.g. lists of dictionaries). Pick one that# will allow you to manage the data above and implement the procedures below. # # You may assume that &lt;user&gt; is a unique identifier for a user. For example, there# can be at most one &apos;John&apos; in the network. Furthermore, connections are not # symmetric - if &apos;Bob&apos; is connected to &apos;Alice&apos;, it does not mean that &apos;Alice&apos; is# connected to &apos;Bob&apos;.## Project Description# ====================# Your task is to complete the procedures according to the specifications below# as well as to implement a Make-Your-Own procedure (MYOP). You are encouraged # to define any additional helper procedures that can assist you in accomplishing # a task. You are encouraged to test your code by using print statements and the # Test Run button. # ----------------------------------------------------------------------------- # Example string input. Use it to test your code.example_input=&quot;John is connected to Bryant, Debra, Walter.\John likes to play The Movie: The Game, The Legend of Corgi, Dinosaur Diner.\Bryant is connected to Olive, Ollie, Freda, Mercedes.\Bryant likes to play City Comptroller: The Fiscal Dilemma, Super Mushroom Man.\Mercedes is connected to Walter, Robin, Bryant.\Mercedes likes to play The Legend of Corgi, Pirates in Java Island, Seahorse Adventures.\Olive is connected to John, Ollie.\Olive likes to play The Legend of Corgi, Starfleet Commander.\Debra is connected to Walter, Levi, Jennie, Robin.\Debra likes to play Seven Schemers, Pirates in Java Island, Dwarves and Swords.\Walter is connected to John, Levi, Bryant.\Walter likes to play Seahorse Adventures, Ninja Hamsters, Super Mushroom Man.\Levi is connected to Ollie, John, Walter.\Levi likes to play The Legend of Corgi, Seven Schemers, City Comptroller: The Fiscal Dilemma.\Ollie is connected to Mercedes, Freda, Bryant.\Ollie likes to play Call of Arms, Dwarves and Swords, The Movie: The Game.\Jennie is connected to Levi, John, Freda, Robin.\Jennie likes to play Super Mushroom Man, Dinosaur Diner, Call of Arms.\Robin is connected to Ollie.\Robin likes to play Call of Arms, Dwarves and Swords.\Freda is connected to Olive, John, Debra.\Freda likes to play Starfleet Commander, Ninja Hamsters, Seahorse Adventures.&quot;# ----------------------------------------------------------------------------- # create_data_structure(string_input): # Parses a block of text (such as the one above) and stores relevant # information into a data structure. You are free to choose and design any # data structure you would like to use to manage the information.# # Arguments: # string_input: block of text containing the network information## You may assume that for all the test cases we will use, you will be given the # connections and games liked for all users listed on the right-hand side of an# &apos;is connected to&apos; statement. For example, we will not use the string # &quot;A is connected to B.A likes to play X, Y, Z.C is connected to A.C likes to play X.&quot;# as a test case for create_data_structure because the string does not # list B&apos;s connections or liked games.# # The procedure should be able to handle an empty string (the string &apos;&apos;) as input, in# which case it should return a network with no users.# # Return:# The newly created network data structuredef create_data_structure(string_input): return network# ----------------------------------------------------------------------------- # # Note that the first argument to all procedures below is &apos;network&apos; This is the ## data structure that you created with your create_data_structure procedure, ## though it may be modified as you add new users or new connections. Each ## procedure below will then modify or extract information from &apos;network&apos; # # ----------------------------------------------------------------------------- ## ----------------------------------------------------------------------------- # get_connections(network, user): # Returns a list of all the connections that user has## Arguments: # network: the gamer network data structure# user: a string containing the name of the user# # Return: # A list of all connections the user has.# - If the user has no connections, return an empty list.# - If the user is not in network, return None.def get_connections(network, user): return []# ----------------------------------------------------------------------------- # get_games_liked(network, user): # Returns a list of all the games a user likes## Arguments: # network: the gamer network data structure# user: a string containing the name of the user# # Return: # A list of all games the user likes.# - If the user likes no games, return an empty list.# - If the user is not in network, return None.def get_games_liked(network,user): return []# ----------------------------------------------------------------------------- # add_connection(network, user_A, user_B): # Adds a connection from user_A to user_B. Make sure to check that both users # exist in network.# # Arguments: # network: the gamer network data structure # user_A: a string with the name of the user the connection is from# user_B: a string with the name of the user the connection is to## Return: # The updated network with the new connection added.# - If a connection already exists from user_A to user_B, return network unchanged.# - If user_A or user_B is not in network, return False.def add_connection(network, user_A, user_B): return network# ----------------------------------------------------------------------------- # add_new_user(network, user, games): # Creates a new user profile and adds that user to the network, along with# any game preferences specified in games. Assume that the user has no # connections to begin with.# # Arguments:# network: the gamer network data structure# user: a string containing the name of the user to be added to the network# games: a list of strings containing the user&apos;s favorite games, e.g.:# [&apos;Ninja Hamsters&apos;, &apos;Super Mushroom Man&apos;, &apos;Dinosaur Diner&apos;]## Return: # The updated network with the new user and game preferences added. The new user # should have no connections.# - If the user already exists in network, return network *UNCHANGED* (do not change# the user&apos;s game preferences)def add_new_user(network, user, games): return network # ----------------------------------------------------------------------------- # get_secondary_connections(network, user): # Finds all the secondary connections (i.e. connections of connections) of a # given user.# # Arguments: # network: the gamer network data structure# user: a string containing the name of the user## Return: # A list containing the secondary connections (connections of connections).# - If the user is not in the network, return None.# - If a user has no primary connections to begin with, return an empty list.# # NOTE: # It is OK if a user&apos;s list of secondary connections includes the user # himself/herself. It is also OK if the list contains a user&apos;s primary # connection that is a secondary connection as well.def get_secondary_connections(network, user): return []# ----------------------------------------------------------------------------- # count_common_connections(network, user_A, user_B): # Finds the number of people that user_A and user_B have in common.# # Arguments: # network: the gamer network data structure# user_A: a string containing the name of user_A# user_B: a string containing the name of user_B## Return: # The number of connections in common (as an integer).# - If user_A or user_B is not in network, return False.def count_common_connections(network, user_A, user_B): return 0# ----------------------------------------------------------------------------- # find_path_to_friend(network, user_A, user_B): # Finds a connections path from user_A to user_B. It has to be an existing # path but it DOES NOT have to be the shortest path.# # Arguments:# network: The network you created with create_data_structure. # user_A: String holding the starting username (&quot;Abe&quot;)# user_B: String holding the ending username (&quot;Zed&quot;)# # Return:# A list showing the path from user_A to user_B.# - If such a path does not exist, return None.# - If user_A or user_B is not in network, return None.## Sample output:# &gt;&gt;&gt; print find_path_to_friend(network, &quot;Abe&quot;, &quot;Zed&quot;)# &gt;&gt;&gt; [&apos;Abe&apos;, &apos;Gel&apos;, &apos;Sam&apos;, &apos;Zed&apos;]# This implies that Abe is connected with Gel, who is connected with Sam, # who is connected with Zed.# # NOTE:# You must solve this problem using recursion!# # Hints: # - Be careful how you handle connection loops, for example, A is connected to B. # B is connected to C. C is connected to B. Make sure your code terminates in # that case.# - If you are comfortable with default parameters, you might consider using one # in this procedure to keep track of nodes already visited in your search. You # may safely add default parameters since all calls used in the grading script # will only include the arguments network, user_A, and user_B.def find_path_to_friend(network, user_A, user_B): # your RECURSIVE solution here! return None# Make-Your-Own-Procedure (MYOP)# ----------------------------------------------------------------------------- # Your MYOP should either perform some manipulation of your network data # structure (like add_new_user) or it should perform some valuable analysis of # your network (like path_to_friend). Don&apos;t forget to comment your MYOP. You # may give this procedure any name you want.# Replace this with your own procedure! You can also uncomment the lines below# to see how your code behaves. Have fun!#net = create_data_structure(example_input)#print net#print get_connections(net, &quot;Debra&quot;)#print get_connections(net, &quot;Mercedes&quot;)#print get_games_liked(net, &quot;John&quot;)#print add_connection(net, &quot;John&quot;, &quot;Freda&quot;)#print add_new_user(net, &quot;Debra&quot;, []) #print add_new_user(net, &quot;Nick&quot;, [&quot;Seven Schemers&quot;, &quot;The Movie: The Game&quot;]) # True#print get_secondary_connections(net, &quot;Mercedes&quot;)#print count_common_connections(net, &quot;Mercedes&quot;, &quot;John&quot;)#print find_path_to_friend(net, &quot;John&quot;, &quot;Ollie&quot;)]]></content>
      <categories>
        <category>Udacity</category>
        <category>Intro to CS</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Udacity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blog = Hexo + Github]]></title>
    <url>%2F2017%2F02%2F03%2Fhexo%20blog%2F</url>
    <content type="text"><![CDATA[stay tuned… update in 2017-07-25 comment donate module 2017-02-03 点击Hexo 官网，按要求配置 主题采用屠夫的Maupassant 在 themes\maupassant/_config.yml 中采用fancybox，disqus，self_search以及google_analytics 注释掉rss ,timeline以及 recent_comments self_search中jQuery-based Local Search Engine for Hexo的操作流程 2017-02-04 ipad竖屏响应问题，更改 hexo\themes\maupassant\source\css\style.scss 文件中的12/* middle*/@media print, screen and (max-width: 48em) 将其中的 48em(48*16=768) 改为 47.9em 图片云床采用 七牛云 ，想注册的话，我有邀请码。Google一下，采用吴小龙同學 的方法 美凡提到图片瘦身功能，很赞 采用Page Analytics Chrome Extension进行访问分析 Emoji 2017-02-05Fix post-nav bug, aim to put the previous post link on the left side and the next post on the right side.Firstly check these files. hexo\themes\maupassant\layout\index.jade controls the format of your posts showing in your home page. hexo\themes\maupassant\layout\post.jade determines the format of your post. hexo\themes\maupassant\source\css\style.scss. Through previous 3 files I find that there is something wrong with page.prev and page.next, for instance page.prev points to the post that I post after the current post.Find .post-nav in file style.scss and change the code in class post-nav as follows: 12345678&amp;.pre &#123; float: right; /* left*/ &amp;:after &#123; /* after*/ font-family: &quot;FontAwesome&quot;; content: &quot;\f0da&quot;; /* f0d9*/ padding-right: 0.3em; &#125;&#125; 2017-02-05 Find bug in categories2017-02-07 Find bug in contents Find bug in web tab, when clicking pageAbout 2017-02-21 Want to add Hits beside the dates to the post listed in the home page. 2017-02-22 Change the theme to Next Add LeanCloud for click counter. Theme: Using Next instead of maupassant. Configure sidebar.swiginG:\hexo\themes\next\layout\_macro for removing sidebar in the home page: Adding ifstatement before sidebar-toggle, do not forget the close tag 123456789&#123;% if is_post %&#125; &lt;div class=&quot;sidebar-toggle&quot;&gt; &lt;div class=&quot;sidebar-toggle-line-wrap&quot;&gt; &lt;span class=&quot;sidebar-toggle-line sidebar-toggle-line-first&quot;&gt;&lt;/span&gt; &lt;span class=&quot;sidebar-toggle-line sidebar-toggle-line-middle&quot;&gt;&lt;/span&gt; &lt;span class=&quot;sidebar-toggle-line sidebar-toggle-line-last&quot;&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt;&#123;% endif %&#125; Comment sidebar-nav-overview for not seeing the overview 123&#123;# &lt;li class=&quot;sidebar-nav-overview&quot; data-target=&quot;site-overview&quot;&gt; #&#125; &#123;# &#123;&#123; __(&apos;sidebar.overview&apos;) &#125;&#125; #&#125;&#123;# &lt;/li&gt; #&#125; 2017-03-03 Add new page with Menu.XXXcd g:hexohexo new page friendsFollow this instruction,add friends: friends in hexo\themes\next\languages\default.yml menu: home: Home archives: Archives categories: Categories tags: Tags about: About friends: friends search: Search 2017-3-09 Add emoji to Hexo cd g:hexo 12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --save 12cd node_modules/hexo-renderer-markdown-it/npm i markdown-it-emoji --save Open _config.yml in g:hexo, add these codes at the bottom1234567891011121314151617181920markdown: render: html: true xhtmlOut: false breaks: false linkify: true typographer: true quotes: &apos;“”‘’&apos; plugins: - markdown-it-footnote - markdown-it-sup - markdown-it-sub - markdown-it-abbr - markdown-it-emoji anchors: level: 1 collisionSuffix: &apos;v&apos; permalink: true permalinkClass: header-anchor permalinkSymbol: &apos; &apos; :smile: U+2764 \xE2\x9D\xA4 :smile: U+2764 \xE2\x9D\xA4 emoji-cheat-sheet Reference:Blog 2017-03-16 Add video to Next Need url 123&lt;video width=&quot;480&quot; height=&quot;320&quot; controls&gt;&lt;source src=&quot;&quot;&gt;&lt;/video&gt; Only YouTube 1&#123;% youtube ID %&#125; Only YouTubeRight click YouTube video, choose “copy embed code…” and paste it to your post. Need url 1&lt;iframe width=&quot;870&quot; height=&quot;489&quot; src=&quot;https://www.youtube.com/embed/WHcRQMGSbqg&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt; 2017-03-20 Add center quotebalabalaba balabalaab 2017-05-06 Markdown TableFrom this post and this website 左对齐标题 右对齐标题 居中对齐标题 短文本 中等文本 稍微长一点的文本 稍微长一点的文本 短文本 中等文本 -:表示内容和标题栏居右对齐，:-表示内容和标题栏居左对齐，:-:表示内容和标题栏居中对齐。 2017-05-07 Markdown checkbox(failed)first try cd g:hexo npm install markdown-it-checkbox --saveit appears 123456789hexo-site@0.0.0 G:\hexo`-- markdown-it-checkbox@1.1.0 `-- underscore@1.8.3npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) cd to directory of markdown-it and type npm install markdown-it-checkbox --save it appears1234markdown-it@5.1.0 G:\hexo\node_modules\markdown-it`-- markdown-it-checkbox@1.1.0 `-- underscore@1.8.3 `` checkbox does not appear [x] hello hello (failed)second try12cd node_modules/hexo-renderer-markdown-it/npm un markdown-it-emoji --save cd g:hexo 12npm un hexo-renderer-markdown-it --savenpm i hexo-renderer-marked --save it appears1234567891011121314151617181920npm WARN prefer global marked@0.3.6 should be installed with -ghexo-site@0.0.0 G:\hexo`-- hexo-renderer-marked@0.3.0 `-- marked@0.3.6npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)npm WARN Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN &#123; Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN errno: -4048,npm WARN code: &apos;EPERM&apos;,npm WARN syscall: &apos;lstat&apos;,npm WARN path: &apos;G:\\hexo\\node_modules\\markdown-it&apos; &#125; tpye npm i hexo-renderer-marked --save -g123456789101112131415161718192021222324252627282930&gt; hexo-util@0.6.0 postinstall C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-renderer-marked\node_modules\hexo-util&gt; npm run build:highlight&gt; hexo-util@0.6.0 build:highlight C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-renderer-marked\node_modules\hexo-util&gt; node scripts/build_highlight_alias.js &gt; highlight_alias.jsonC:\Users\SSQ\AppData\Roaming\npm`-- hexo-renderer-marked@0.3.0 +-- hexo-util@0.6.0 | +-- bluebird@3.5.0 | +-- camel-case@3.0.0 | | +-- no-case@2.3.1 | | | `-- lower-case@1.1.4 | | `-- upper-case@1.1.3 | +-- cross-spawn@4.0.2 | | +-- lru-cache@4.0.2 | | | +-- pseudomap@1.0.2 | | | `-- yallist@2.1.2 | | `-- which@1.2.14 | | `-- isexe@2.0.0 | +-- highlight.js@9.11.0 | +-- html-entities@1.2.1 | `-- striptags@2.2.1 +-- marked@0.3.6 +-- object-assign@4.1.1 `-- strip-indent@1.0.1 `-- get-stdin@4.0.1 add following code at the bottom of _config.yml12345678910marked: gfm: true pedantic: false sanitize: false tables: true breaks: true smartLists: true smartypants: true modifyAnchors: &apos;&apos; autolink: true failed (failed)third tryadd &quot;hexo-renderer-marked&quot;: &quot;^0.3.0&quot;, in the G:\hexo\package.jsoncd g:hexonpm i hexo-renderer-marked --savefailed forth trynpm i hexo-renderer-marked -f 2017-05-09 add passwords for blogsadd &quot;hexo-blog-encrypt&quot;: &quot;^1.1.12&quot;, in the G:\hexo\package.jsoncd g:hexonpm i hexo-blog-encrypt --saveit appears123456789101112131415161718hexo-site@0.0.0 G:\hexo`-- hexo-blog-encrypt@1.1.12npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)npm WARN Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN &#123; Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN errno: -4048,npm WARN code: &apos;EPERM&apos;,npm WARN syscall: &apos;lstat&apos;,npm WARN path: &apos;G:\\hexo\\node_modules\\markdown-it&apos; &#125; try npm i hexo-blog-encrypt -f 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141npm WARN using --force I sure hope you know what you are doing.&gt; fsevents@1.1.1 install G:\hexo\node_modules\fsevents&gt; node installhexo-site@0.0.0 G:\hexo+-- hexo@3.2.2| `-- hexo-fs@0.1.6| `-- chokidar@1.6.1| `-- fsevents@1.1.1| `-- node-pre-gyp@0.6.33| +-- mkdirp@0.5.1| | `-- minimist@0.0.8| +-- nopt@3.0.6| | `-- abbrev@1.1.0| +-- npmlog@4.0.2| | +-- are-we-there-yet@1.1.2| | | +-- delegates@1.0.0| | | `-- readable-stream@2.2.2| | +-- console-control-strings@1.1.0| | +-- gauge@2.7.3| | | +-- aproba@1.1.1| | | +-- has-unicode@2.0.1| | | +-- object-assign@4.1.1| | | +-- signal-exit@3.0.2| | | +-- string-width@1.0.2| | | | +-- code-point-at@1.1.0| | | | `-- is-fullwidth-code-point@1.0.0| | | | `-- number-is-nan@1.0.1| | | +-- strip-ansi@3.0.1| | | | `-- ansi-regex@2.1.1| | | `-- wide-align@1.1.0| | `-- set-blocking@2.0.0| +-- rc@1.1.7| | +-- deep-extend@0.4.1| | +-- ini@1.3.4| | +-- minimist@1.2.0| | `-- strip-json-comments@2.0.1| +-- request@2.79.0| | +-- aws-sign2@0.6.0| | +-- aws4@1.6.0| | +-- caseless@0.11.0| | +-- combined-stream@1.0.5| | | `-- delayed-stream@1.0.0| | +-- extend@3.0.0| | +-- forever-agent@0.6.1| | +-- form-data@2.1.2| | | `-- asynckit@0.4.0| | +-- har-validator@2.0.6| | | +-- chalk@1.1.3| | | | +-- ansi-styles@2.2.1| | | | +-- escape-string-regexp@1.0.5| | | | +-- has-ansi@2.0.0| | | | `-- supports-color@2.0.0| | | +-- commander@2.9.0| | | | `-- graceful-readlink@1.0.1| | | +-- is-my-json-valid@2.15.0| | | | +-- generate-function@2.0.0| | | | +-- generate-object-property@1.2.0| | | | | `-- is-property@1.0.2| | | | +-- jsonpointer@4.0.1| | | | `-- xtend@4.0.1| | | `-- pinkie-promise@2.0.1| | | `-- pinkie@2.0.4| | +-- hawk@3.1.3| | | +-- boom@2.10.1| | | +-- cryptiles@2.0.5| | | +-- hoek@2.16.3| | | `-- sntp@1.0.9| | +-- http-signature@1.1.1| | | +-- assert-plus@0.2.0| | | +-- jsprim@1.3.1| | | | +-- extsprintf@1.0.2| | | | +-- json-schema@0.2.3| | | | `-- verror@1.3.6| | | `-- sshpk@1.10.2| | | +-- asn1@0.2.3| | | +-- assert-plus@1.0.0| | | +-- bcrypt-pbkdf@1.0.1| | | +-- dashdash@1.14.1| | | | `-- assert-plus@1.0.0| | | +-- ecc-jsbn@0.1.1| | | +-- getpass@0.1.6| | | | `-- assert-plus@1.0.0| | | +-- jodid25519@1.0.2| | | +-- jsbn@0.1.1| | | `-- tweetnacl@0.14.5| | +-- is-typedarray@1.0.0| | +-- isstream@0.1.2| | +-- json-stringify-safe@5.0.1| | +-- mime-types@2.1.14| | | `-- mime-db@1.26.0| | +-- oauth-sign@0.8.2| | +-- qs@6.3.1| | +-- stringstream@0.0.5| | +-- tough-cookie@2.3.2| | | `-- punycode@1.4.1| | +-- tunnel-agent@0.4.3| | `-- uuid@3.0.1| +-- rimraf@2.5.4| | `-- glob@7.1.1| | +-- fs.realpath@1.0.0| | +-- inflight@1.0.6| | +-- minimatch@3.0.3| | | `-- brace-expansion@1.1.6| | | +-- balanced-match@0.4.2| | | `-- concat-map@0.0.1| | +-- once@1.4.0| | `-- path-is-absolute@1.0.1| +-- semver@5.3.0| +-- tar@2.2.1| | +-- block-stream@0.0.9| | +-- fstream@1.0.10| | | `-- graceful-fs@4.1.11| | `-- inherits@2.0.3| `-- tar-pack@3.3.0| +-- debug@2.2.0| | `-- ms@0.7.1| +-- fstream-ignore@1.0.5| +-- once@1.3.3| | `-- wrappy@1.0.2| +-- readable-stream@2.1.5| | +-- buffer-shims@1.0.0| | +-- core-util-is@1.0.2| | +-- isarray@1.0.0| | +-- process-nextick-args@1.0.7| | +-- string_decoder@0.10.31| | `-- util-deprecate@1.0.2| `-- uid-number@0.0.6`-- hexo-blog-encrypt@1.1.12npm WARN Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN &#123; Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN errno: -4048,npm WARN code: &apos;EPERM&apos;,npm WARN syscall: &apos;lstat&apos;,npm WARN path: &apos;G:\\hexo\\node_modules\\markdown-it&apos; &#125; 2017-05-09 restarthttps://xuanwo.org/2015/03/26/hexo-intor/Given the fact that I have installed Node.js and Git in website.So I open cmd with win+r and just type npm install -g hexo-cliIt appears123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124C:\Users\SSQ\AppData\Roaming\npm\hexo -&gt; C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\bin\hexo&gt; dtrace-provider@0.8.1 install C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\node_modules\dtrace-provider&gt; node scripts/install.js&gt; hexo-util@0.6.0 postinstall C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\node_modules\hexo-util&gt; npm run build:highlight&gt; hexo-util@0.6.0 build:highlight C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\node_modules\hexo-util&gt; node scripts/build_highlight_alias.js &gt; highlight_alias.jsonC:\Users\SSQ\AppData\Roaming\npm`-- hexo-cli@1.0.2 +-- abbrev@1.1.0 +-- bluebird@3.5.0 +-- chalk@1.1.3 | +-- ansi-styles@2.2.1 | +-- escape-string-regexp@1.0.5 | +-- has-ansi@2.0.0 | | `-- ansi-regex@2.1.1 | +-- strip-ansi@3.0.1 | `-- supports-color@2.0.0 +-- hexo-fs@0.1.6 | +-- chokidar@1.7.0 | | +-- anymatch@1.3.0 | | | +-- arrify@1.0.1 | | | `-- micromatch@2.3.11 | | | +-- arr-diff@2.0.0 | | | | `-- arr-flatten@1.0.3 | | | +-- array-unique@0.2.1 | | | +-- braces@1.8.5 | | | | +-- expand-range@1.8.2 | | | | | `-- fill-range@2.2.3 | | | | | +-- is-number@2.1.0 | | | | | +-- isobject@2.1.0 | | | | | +-- randomatic@1.1.6 | | | | | `-- repeat-string@1.6.1 | | | | +-- preserve@0.2.0 | | | | `-- repeat-element@1.1.2 | | | +-- expand-brackets@0.1.5 | | | | `-- is-posix-bracket@0.1.1 | | | +-- extglob@0.3.2 | | | +-- filename-regex@2.0.1 | | | +-- kind-of@3.2.0 | | | | `-- is-buffer@1.1.5 | | | +-- normalize-path@2.1.1 | | | | `-- remove-trailing-separator@1.0.1 | | | +-- object.omit@2.0.1 | | | | +-- for-own@0.1.5 | | | | | `-- for-in@1.0.2 | | | | `-- is-extendable@0.1.1 | | | +-- parse-glob@3.0.4 | | | | +-- glob-base@0.3.0 | | | | `-- is-dotfile@1.0.2 | | | `-- regex-cache@0.4.3 | | | +-- is-equal-shallow@0.1.3 | | | `-- is-primitive@2.0.0 | | +-- async-each@1.0.1 | | +-- glob-parent@2.0.0 | | +-- inherits@2.0.3 | | +-- is-binary-path@1.0.1 | | | `-- binary-extensions@1.8.0 | | +-- is-glob@2.0.1 | | | `-- is-extglob@1.0.0 | | +-- path-is-absolute@1.0.1 | | `-- readdirp@2.1.0 | | +-- minimatch@3.0.4 | | | `-- brace-expansion@1.1.7 | | | +-- balanced-match@0.4.2 | | | `-- concat-map@0.0.1 | | +-- readable-stream@2.2.9 | | | +-- buffer-shims@1.0.0 | | | +-- core-util-is@1.0.2 | | | +-- isarray@1.0.0 | | | +-- process-nextick-args@1.0.7 | | | +-- string_decoder@1.0.0 | | | `-- util-deprecate@1.0.2 | | `-- set-immediate-shim@1.0.1 | `-- graceful-fs@4.1.11 +-- hexo-log@0.1.2 | `-- bunyan@1.8.10 | +-- dtrace-provider@0.8.1 | | `-- nan@2.6.2 | +-- moment@2.18.1 | +-- mv@2.1.1 | | +-- mkdirp@0.5.1 | | | `-- minimist@0.0.8 | | +-- ncp@2.0.0 | | `-- rimraf@2.4.5 | | `-- glob@6.0.4 | | +-- inflight@1.0.6 | | | `-- wrappy@1.0.2 | | `-- once@1.4.0 | `-- safe-json-stringify@1.0.4 +-- hexo-util@0.6.0 | +-- camel-case@3.0.0 | | +-- no-case@2.3.1 | | | `-- lower-case@1.1.4 | | `-- upper-case@1.1.3 | +-- cross-spawn@4.0.2 | | +-- lru-cache@4.0.2 | | | +-- pseudomap@1.0.2 | | | `-- yallist@2.1.2 | | `-- which@1.2.14 | | `-- isexe@2.0.0 | +-- highlight.js@9.11.0 | +-- html-entities@1.2.1 | `-- striptags@2.2.1 +-- minimist@1.2.0 +-- object-assign@4.1.1 `-- tildify@1.2.0 `-- os-homedir@1.0.2npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\hexo-cli\node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) cd g:hexohexo initit appears1234567xxxxxxnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)INFO Start blogging with Hexo! 2017-05-10 add local searchdetails in this websiteopen git bash and type cd g:hexo npm install hexo-generator-searchdb --save 123456789hexo-site@0.0.0 G:\hexo`-- hexo-generator-searchdb@1.0.7 `-- striptags@3.0.1npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) 2017-05-10 fix tag bug from h1 to h2find tag.swig in G:\hexo_old\hexo\themes\next\layout replace following code123456&lt;div class=&quot;collection-title&quot;&gt; &lt;&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt;&#123;# #&#125;&#123;&#123; page.tag &#125;&#125;&#123;# #&#125;&lt;small&gt;&#123;&#123; __(&apos;title.tag&apos;) &#125;&#125;&lt;/small&gt; &lt;/&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt;&lt;/div&gt; with123456&lt;div class=&quot;collection-title&quot;&gt; &lt;h2 &gt; &#123;&#123; page.tag &#125;&#125; &lt;small&gt;&#123;&#123; __(&apos;title.tag&apos;) &#125;&#125;&lt;/small&gt; &lt;/h2&gt;&lt;/div&gt; 2017-05-12 note tagFollow this websiteContent (md partial supported) e.g. default primary success info warning danger 2017-05-22 input tag bugsIn the following code1&lt;input type=&quot;radio&quot; disabled&gt;&lt;label&gt;2&lt;/label&gt; &lt;label&gt; must follow the close tag &gt; of input closely, or it will appears bugs in content. 2017-05-25 space bug2017-06-09 Templateradio CONTENT chackbox CONTENT template table1---|--- Restart Hexo Blog open git bash npm install -g hexo-cli run 123hexo init g:hexocd g:hexonpm install run 12git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot; do in _config.yml 1234567# Sitetitle: SSQsubtitle:description:author: SSQlanguage: entimezone: 1234deploy: type: git repo: https://github.com/SSQ/SSQ.github.io.git branch: master run npm install hexo-deployer-git --save run hexo d check my blog in https://ssq.github.io/ in _config.yml change to theme: next copy file to g:hexo/source run npm install hexo-generator-searchdb --saveadd in _config.yml(hexo)12345search: path: search.xml field: post format: html limit: 10000 add in _config.yml(next)123# Local searchlocal_search: enable: true 2017-06-22 bug occurs in this circumstances use following code1&#123;% keyword %&#125; 2017-06-23 Cannot find second page of posts in my websiteproblem solved in this commit 2017-07-13 Export html file with markdownpreivewSublime Text 3 package MarkdownPreview can generate html file when typing ctrl + b.It can also generate content when typing [TOC] at the beginning of your md file. 2017-07-17 Course and Certificate TemplateCourse can be found hereMy certificate can be found hereLecture slides can be found here 2017-07-17 Change Blog Anchor Text Hover Color to BlueFind base.styl in G:\hexo\themes\next\source\css\_variablesChange variables $link-color to the following code1$link-color = $blue where blue is #0684bd 2017-07-17 Change read-more to BlueFind Mist.styl in G:\hexo\themes\next\source\css\_variablesChange variable $read-more-color to #0077b3(blue)123// $read-more-color = $link-color// red-&gt;#DE5233 blue-&gt;#0077b3$read-more-color = #0077b3 2017-07-17 Change code-foreground to redFind base.styl in G:\hexo\themes\next\source\css\_variableschange variable $code-foreground to #DE5233(red) in line 1511$code-foreground = #DE5233 2017-07-23 Add exam module primary 2017-07-25 comment donate modulecomment following code in hexo/themes/next1234# Donate 文章末尾显示打赏按钮# reward_comment: 打赏随意，感谢支持# wechatpay: /images/wechatpay.jpg# alipay: /images/alipay.jpg 2017-08-14 Set fonts in a larger size2017-08-20 Add PA readme model12345678910111213141516# Goal# File Description- `.rar` files is data file. - `amazon_baby.rar` (unzip `amazon_baby.csv`) consists of 183,531 customers with `name`, `review`, `rating`.- `.ipynb` file is the solution of Week 1 program assignment - `Predicting sentiment from product reviews with pandas.ipynb`- `.html` file is the html version of `.ipynb` file. - `Predicting+sentiment+from+product+reviews+with+pandas.html`# Snapshotopen `.html` file via brower for quick look.# Algorithm- Logistic Regression# Implementation# Implementation in detail]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Test blog]]></title>
    <url>%2F2017%2F02%2F02%2Ftest%20blog%2F</url>
    <content type="text"><![CDATA[This is a test blog $\omega_i$]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F02%2F01%2Fhello%20world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>