<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Coursera HSE Advanced Machine Learning Specialization]]></title>
    <url>%2F2017%2F11%2F19%2FCoursera%20HSE%20Advanced%20Machine%20Learning%20Specialization%2F</url>
    <content type="text"><![CDATA[For quick searchingCourse can be found hereVideo in YouTubeLecture Slides can be found in my Github About This SpecializationThis specialization gives an introduction to deep learning, reinforcement learning, natural language understanding, computer vision and Bayesian methods. Top Kaggle machine learning practitioners and CERN scientists will share their experience of solving real-world problems and help you to fill the gaps between theory and practice. Upon completion of 7 courses you will be able to apply modern machine learning methods in enterprise and understand the caveats of real-world data and settings. Projects OverviewYou will master your skills by solving a wide variety of real-world problems like image captioning and automatic game playing throughout the course projects. You will gain the hands-on experience of applying advanced machine learning techniques that provide the foundation to the current state-of-the art in AI. Introduction to Deep LearningCourse can be found hereLecture slides can be found here About this course: The goal of this course is to give learners basic understanding of modern neural networks and their applications in computer vision and natural language understanding. The course starts with a recap of linear models and discussion of stochastic optimization methods that are crucial for training deep neural networks. Learners will study all popular building blocks of neural networks including fully connected layers, convolutional and recurrent layers.Learners will use these building blocks to define complex modern architectures in TensorFlow and Keras frameworks. In the course project learner will implement deep neural network for the task of image captioning which solves the problem of giving a text description for an input image. The prerequisites for this course are:1) Basic knowledge of Python.2) Basic linear algebra and probability. Please note that this is an advanced course and we assume basic knowledge of machine learning. You should understand:1) Linear regression: mean squared error, analytical solution.2) Logistic regression: model, cross-entropy loss, class probability estimation.3) Gradient descent for linear models. Derivatives of MSE and cross-entropy loss functions.4) The problem of overfitting.5) Regularization for linear models. Who is this class for: Developers, analysts and researchers who are faced with tasks involving complex structure understanding such as image, sound and text analysis. Week 1 Introduction to optimizationWelcome to the “Introduction to Deep Learning” course! In the first week you’ll learn about linear models and stochatic optimization methods. Linear models are basic building blocks for many deep architectures, and stochastic optimization is used to learn every model that we’ll discuss in our course. Learning Objectives Train a linear model for classification or regression task using stochastic gradient descent Tune SGD optimization using different techniques Apply regularization to train better models Use linear models for classification and regression tasks Course introWelcome!5 minLinear model as the simplest neural networkLinear regression 9 minLinear classification 10 minGradient descent 5 minQuiz: Linear models 3 questionsQUIZLinear models3 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 26, 11:59 PM PST 1 point1.Consider a vector (1,−2,0.5). Apply a softmax transform to it and enter the first component (accurate to 2 decimal places). 1 point2.Suppose you are solving a 5-class classification problem with 10 features. How many parameters a linear model would have? Don’t forget bias terms! 1 point3.There is an analytical solution for linear regression parameters and MSE loss, but we usually prefer gradient descent optimization over it. What are the reasons? Gradient descent is more scalable and can be applied for problems with high number of features Gradient descent is a method developed especially for MSE loss Gradient descent can find parameter values that give lower MSE value than parameters from analytical solution Gradient descent doesn’t require to invert a matrix Regularization in machine learningOverfitting problem and model validation 6 minModel regularization 5 minQuiz: Overfitting and regularization 4 questionsQUIZOverfitting and regularization4 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 26, 11:59 PM PST 1 point1.Select correct statements about overfitting: Overfitting is a situation where a model gives lower quality for new data compared to quality on a training sample Overfitting happens when model is too simple for the problem Overfitting is a situation where a model gives comparable quality on new data and on a training sample Large model weights can indicate that model is overfitted1 point2.What disadvantages do model validation on holdout sample have? It requires multiple model fitting It is sensitive to the particular split of the sample into training and test parts It can give biased quality estimates for small samples123,1,13, 1 point3.Suppose you are using k-fold cross-validation to assess model quality. How many times should you train the model during this procedure? 1 k k(k−1)/2 k21 point4.Select correct statements about regularization: Weight penalty reduces the number of model parameters and leads to faster model training Reducing the training sample size makes data simpler and then leads to better quality Regularization restricts model complexity (namely the scale of the coefficients) to reduce overfitting Weight penalty drives model parameters closer to zero and prevents the model from being too sensitive to small changes in features Stochastic methods for optimizationStochastic gradient descent 5 minGradient descent extensions 9 minLinear models and optimizationProgramming Assignment: Linear models and optimization 3hprimary Weekprimary primary Weekprimary primary Weekprimary primary Weekprimary primary Weekprimary primary How to Win a Data Science Competition: Learn from Top KagglersCourse can be found hereLecture slides can be found here Weekprimary primary Weekprimary primary Weekprimary primary Bayesian Methods for Machine LearningCourse can be found hereLecture slides can be found here Weekprimary primary Weekprimary primary Weekprimary primary Introduction to Reinforcement LearningCourse can be found hereLecture slides can be found here Weekprimary primary Weekprimary primary Weekprimary primary Deep Learning in Computer VisionCourse can be found hereLecture slides can be found here Weekprimary primary Weekprimary primary Weekprimary primary Natural Language ProcessingCourse can be found hereLecture slides can be found here Weekprimary primary Weekprimary primary Weekprimary primary Addressing Large Hadron Collider Challenges by Machine LearningCourse can be found hereLecture slides can be found here Weekprimary primary Weekprimary primary Weekprimary primary]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera PU 程序设计与算法 Specialization]]></title>
    <url>%2F2017%2F09%2F26%2FCoursera%20PU%20%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E7%AE%97%E6%B3%95%20Specialization%2F</url>
    <content type="text"><![CDATA[For quick searchCoursera can be found here 本专项课程旨在系统培养你的程序设计与编写能力。系列课程从计算机的基础知识讲起，无论你来自任何学科和行业背景，都能快速理解；同时我们又系统性地介绍了C程序设计，C++程序设计，算法基础，数据结构与算法相关的内容，各门课之间联系紧密，循序渐进，能够帮你奠定坚实的程序开发基础；课程全部配套在线编程测试，将有效地训练和提升你编写程序的实际动手能力。并通过结业实践项目为你提供应用程序设计解决复杂现实问题的锻炼，从而积累实际开发的经验。因此，我们希望本专项课程能够帮助你完成从仅了解基本的计算机知识到能够利用高质量的程序解决实际问题的转变。 计算导论与C语言基础Lecture slides can be found [here]Coursera can be found here About this course: 你有没有好奇过：计算机为什么能够进行计算？计算机程序是怎样运行的？你是否想知道：计算机未来可能的发展趋势有哪些？程序是如何编写出来的？如何学习程序设计语言？程序设计语言的基本成分有哪些？《计算导论》这门课将帮助你解决这些疑惑。学完这门课，你将能够解释计算机和程序的基本运行原理以及它们的特性，向你的朋友讲述计算机的历史和发展趋势；同时，你也将充分“热身”，迎接“计算机程序设计语言”的学习！ Week 1 欢迎加入《计算导论与C语言基础》！欢迎大家来到《计算导论与C语言基础》！在这门课程当中，我们将敲开神秘的计算机世界之门，探索它的历史，解读它的基本原理，讨论它未来的发展趋势；同时我们还将学习C语言这一经典的编程语言，开启我们充满趣味与挑战的程序设计之旅。这个欢迎模块就让我们在出发之前读好“地图”，通过观看两段视频来了解一下这段奇妙的旅程都将涵盖哪些内容吧！PS：我们这门课程一直处在不断地建设与优化当中，吸取了很多以往课程的经典视频，所以如果你看到视频中出现了不同课程的名字，也不要惊讶哦，因为你正在集百家所长。 课程介绍专项课程介绍2 min欢迎加入《计算导论与C语言基础》2 minWeek 1 计算机的基本原理作为开篇的第一次课，我们先来了解一下现代计算机运行的基本原理。我们将从历史上的三次数学危机开始讲起，引出现代计算机的基本原型——图灵机的基本原理，进而解释支撑现代计算机技术的几个基础性理论（二进制、布尔代数、数字逻辑电路）及其解决的基本问题。本次课的主要目的：帮助同学们了解现代计算机的基本原理。本次课的焦点问题：计算机为什么能利用电路实现计算？ PS：我们这门课程一直处在不断地建设与优化当中，吸取了很多以往课程的经典视频，所以如果你看到视频中出现了不同课程的名字，也不要惊讶哦，因为你正在集百家所长：） 第一课从数学危机到图灵机 18 min图灵机的基本构成 6 min图灵机的运行机理14 minhttp://aturingmachine.com/ 第二课数的二进制表示10 min二进制数的布尔运算11 minWeek 2 计算机的历史与未来本次课将带领同学们了解计算机的演变历史，希望通过这个历史演变的过程帮助同学们了解“人类在计算科学方面是如何一步步积累成果的“。在此基础上，我们再来讨论一下未来计算机的发展趋势，并重点介绍了量子计算机的基本原理与研究现状。 本次课的主要目的：希望透过历史引发大家对计算机发展现状的思考。 本次课的焦点问题：未来计算机的发展趋势是什么？为什么不能把CPU造得更大些？什么是量子计算机？ 第一课历史上的计算设备18 min从电子管到云计算15 minhttps://www.top500.org/ 第二课摩尔定律下的计算危机10 min量子计算机的基本原理8 min量子计算新成果简介+鼓励9 minWeek 3 程序运行的基本原理本次课带大家走进计算机，了解计算机的几个基本构成成分及其作用，在此基础上，了解CPU指令的基本执行过程、了解计算机执行程序的过程。 本次课的主要目的：了解计算机是如何运行程序的。 本次课的焦点问题：为什么说现代计算机是冯诺依曼式计算机？电路为什么能存储数字？CPU是不是任意命令都能执行？ 第一课问题的提出4 min冯诺依曼式计算机8 min存储器的种类与特点8 min第二课存储器的原理与类型10 minCPU指令的执行8 min程序的执行9 min写在下一个部分之前的话写在下一个部分之前的话10 minWeek 4 感性认识计算机程序本次课也许是计算机程序设计部分“最重要”的一次课程，在这次课程中，我们将通过一个例子，感受一个结论——“计算机程序 其实是对 人们思维过程的一个描述”；在此基础上，我们将立刻把自己放置于一个“计算机程序设计语言”的设计者的角度，去思考“如果让我们设计一门程序设计语言，我们将如何设计？” 进而，在我们给出关于这个问题的“抽象回答”的基础上，我们迅速地带领大家“快步走进C程序”，迅速了解在C程序设计语言中，都有哪些成分。在这次课的最后，我们通过一个例子，以“感性的方式”让大家感受了一下“什么样的程序是好程序”。 下面就让我们开始这次“最重要”的课程吧—— 第一课说在前面的话5 min程序是你告诉计算机的话5 min如果你的大脑是台计算机10 min如果你来设计一门编程语言10 min第二课快步走进C程序之一11 min快步走进C程序之二9 min快步走进C程序之三11 min什么样的程序是好程序3 min编程作业Programming Assignment: 感性接触计算机程序3h实现冒泡排序1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;int main() &#123; int n, a[1000]; //一共n个数，n不超过1000。a用来保存这些数 cin &gt;&gt; n; // 输入n个数 for (int i = 0; i &lt; n; i++) &#123; cin &gt;&gt; a[i]; &#125; // 冒泡，不断比较相邻的两个数，如果顺序错了，那么就交换 for (int i = 0; i &lt; n; i++) &#123; for (int j = 1; j &lt; n - i; j++) &#123; if (a[j - 1] &gt; a[j]) &#123; int temp = a[j]; a[j] = a[j - 1]; a[j - 1] = temp; &#125; &#125; &#125; // 依次输出 for (int i = 0; i &lt; n; i++) &#123; cout &lt;&lt; a[i] &lt;&lt; endl; &#125; return 0;&#125; 奇偶排序（一）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;iostream&gt;using namespace std;int main() &#123; int a[10]; for (int i = 0; i &lt; 10; i++) &#123; cin &gt;&gt; a[i]; &#125; // 首先，我们把奇数放到数组左边，偶数放到数组右边 int l = 0, r = 9; //用左手和右手分别指向数组两端 while (l &lt;= r) &#123; bool leftIsOdd = a[l] % 2 == 1; bool rightIsEven = a[r] % 2 == 0; if (leftIsOdd) &#123; l++; &#125; else if (rightIsEven) &#123; r--; &#125; else if (!leftIsOdd &amp;&amp; !rightIsEven) &#123; int temp = a[l]; a[l] = a[r]; a[r] = temp; &#125; &#125; // 对l左边（奇数部分）冒泡，不断比较相邻的两个数，如果顺序错了，那么就交换 int start = 0, end = l; for (int i = start; i &lt; end - 1; i++) &#123; for (int j = start + 1; j &lt; start + end - i; j++) &#123; if (a[j - 1] &gt; a[j]) &#123; int temp = a[j]; a[j] = a[j - 1]; a[j - 1] = temp; &#125; &#125; &#125; // 对l右边（偶数部分）冒泡，不断比较相邻的两个数，如果顺序错了，那么就交换 start = l, end = 10; for (int i = start; i &lt; end - 1; i++) &#123; for (int j = start + 1; j &lt; start + end - i; j++) &#123; if (a[j - 1] &gt; a[j]) &#123; int temp = a[j]; a[j] = a[j - 1]; a[j - 1] = temp; &#125; &#125; &#125; for (int i = 0; i &lt; 10; i++) &#123; cout &lt;&lt; a[i] &lt;&lt; ' '; &#125; return 0;&#125; 奇偶排序（二）123456789101112131415161718192021222324252627#include&lt;iostream&gt;using namespace std;int main() &#123; int a[10]; for (int i = 0; i &lt; 10; i++) &#123; cin &gt;&gt; a[i]; &#125; // 冒泡，不断比较相邻的两个数，如果顺序错了，那么就交换 for (int i = 0; i &lt; 9; i++) &#123; for (int j = 1; j &lt; 10 - i; j++) &#123; // 与刚才的冒泡排序不同，我们不只是通过较数字的大小决定顺序 // 如果左边的为偶数，右边的为奇数，那么顺序也需要颠倒 bool leftIsEven = a[j - 1] % 2 == 0; bool rightIsEven = a[j] % 2 == 0; if ((leftIsEven &amp;&amp; !rightIsEven) || (leftIsEven == rightIsEven &amp;&amp; a[j - 1] &gt; a[j])) &#123; int temp = a[j]; a[j] = a[j - 1]; a[j - 1] = temp; &#125; &#125; &#125; for (int i = 0; i &lt; 10; i++) &#123; cout &lt;&lt; a[i] &lt;&lt; ' '; &#125; return 0;&#125; 配置编程环境（补充资料）下载、安装和使用IDE16 min使用IDE进行调试14 minWeek 5 从现实问题到计算机程序本次课程堪称计算机程序设计部分“第二重要”的课程。本次课将在大家感受过“什么是计算机程序”的基础上，来回答一个非常基本的问题——“如何设计计算机程序？” 我们将明确阐述“计算机程序是人们对自己头脑中构想的解决方案的描述”这一思想，并通过例子说明“要想写出计算机程序，必须先想出解决方案”的基本道理。 在此基础上，我们还希望通过一个简单的例子，让大家“感性地”了解一下，什么是“结构化的程序设计”（“结构化程序设计”是比“面向对象的程序设计”更基础的设计思想，因此，了解这种思想，非常重要！）。 第一课没有解决方案就没有程序10 min先有构想再写程序11 min第二课先有构想再写程序-示例12 min体验结构化的程序-示例23 min编程作业Programming Assignment:从现实问题到计算机程序3h写在下一个部分之前的话写在下一个部分之前的话10 minWeek 6 理性认识C程序 导论本次课帮助大家了解C语言的历史，了解C语言规范（Specification）的版本演进，了解C语言的规范是一个“宽松”的规范；在此基础上，我们将阐述一门程序设计语言所包含的四种基本成分（如上所述）。 焦点问题：为什么相同的C程序在不同的C程序编译器上，会编译出不同的结果？ 第一课明确学习进度2 minC语言的由来12 minC语言的标准9 minC语言的构成4 min编程作业Programming Assignment: 理性认识C程序 导论 抄写题3hProgramming Assignment: 理性认识C程序 导论 编程题3hWeek 7 C语言中的数据成分本节我们将介绍C语言中的“数据成分”。重点在于：掌握各种数据类型在内存中所占的空间大小，掌握各种数据类型的特点。 第一课再谈学习进度与安排5 min变量定义的含义10 min整数型的类别11 min第二课整数型的存储9 min整数的输入输出7 min最大与最小整数10 min第三课浮点型11 min字符型8 min布尔型3 min常数6 min变量命名8 min编程作业Programming Assignment: 数据成分应用练习3hProgramming Assignment: 综合练习（1）3hWeek 8 C语言中的运算成分本节我们将介绍C语言中的“运算成分”。重点在于：掌握各种运算符的基本含义，特别需要掌握“由各种运算符引起的数据类型转换规律”。 第一课说在前面的话3 min赋值运算18 min赋值运算的说明10 min第二课算术运算15 min自增自减运算22 min第三课关系运算6 min逻辑运算与混合运算19 min第四课逗号，条件，强转11 min位运算17 min编程作业Programming Assignment: 逻辑运算应用程序抄写练习3hProgramming Assignment: 综合练习（2）3hWeek 9 C语言中的控制成分本节我们将介绍C语言中的“控制成分”。重点在于：掌握各种控制语句的使用方式。顺便，了解一下历史上的Goto之争。 第一课再谈分支语句22 min第二课再谈循环语句18 minGoto之争13 minWeek 10 C程序中的数组在学习了C程序语言的几种重要的构成成分之后，为了帮助大家能够更好地使用已经学到的C语言成分编写程序，我们再介绍一种非常重要的数据结构——数组。 本部分的重点在于：掌握数组的定义、引用方法，并掌握数组的基本作用。特别的，需要大家掌握利用数组的下标来解决问题的“技巧”。 第一课再谈一维数组9 min二维数组12 min三维数组5 min第二课数组的作用之一9 min数组的作用之二 17 min编程作业Programming Assignment: 数组应用练习3h编程题＃1：求字母的个数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//编程题＃1：求字母的个数////来源 : POJ(Coursera声明：在POJ上完成的习题将不会计入Coursera的最后成绩。)//// 注意： 总时间限制 : 1000ms 内存限制 : 65536kB//// 描述//// 在一个字符串中找出元音字母a, e, i, o, u出现的次数。//// 输入//// 输入一行字符串（字符串中可能有空格，请用cin.getline(s, counts)方法把一行字符串输入到字符数组s中，其中counts是s的最大长度，这道题里面可以直接写80。），字符串长度小于80个字符。//// 输出//// 输出一行，依次输出a, e, i, o, u在输入字符串中出现的次数，整数之间用空格分隔。//// 样例输入//// If so, you already have a Google Account.You can sign in on the right.// 样例输出//// 5 4 3 7 3// 提示//// 注意，只统计小写元音字母a, e, i, o, u出现的次数。#include&lt;iostream&gt;using namespace std;int main()&#123; char s[80]; cin.getline(s, 80); int len; int a[6] = &#123; 0 &#125;; for (len = 0; s[len] != '\0'; len++)&#123; switch (s[len]) &#123; case 'a': a[1]++; break; case 'e': a[2]++; break; case 'i': a[3]++; break; case 'o': a[4]++; break; case 'u': a[5]++; break; default: break; &#125; &#125; for (int i = 1; i &lt; 6; i++) cout &lt;&lt; a[i] &lt;&lt; ' '; return 0;&#125; 编程题＃2：忽略大小写比较字符串大小123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990//编程题＃2：忽略大小写比较字符串大小////来源 : POJ(Coursera声明：在POJ上完成的习题将不会计入Coursera的最后成绩。)//// 注意： 总时间限制 : 1000ms 内存限制 : 65536kB//// 描述//// 一般我们用strcmp可比较两个字符串的大小，比较方法为对两个字符串从前往后逐个字符相比较（按ASCII码值大小比较），直到出现不同的字符或遇到'\0'为止。如果全部字符都相同，则认为相同；如果出现不相同的字符，则以第一个不相同的字符的比较结果为准。但在有些时候，我们比较字符串的大小时，希望忽略字母的大小，例如"Hello"和"hello"在忽略字母大小写时是相等的。请写一个程序，实现对两个字符串进行忽略字母大小写的大小比较。//// 输入//// 输入为两行，每行一个字符串，共两个字符串。（请用cin.getline(s, 80)录入每行字符串）（每个字符串长度都小于80）//// 输出//// 如果第一个字符串比第二个字符串小，输出一个字符"&lt;"//// 如果第一个字符串比第二个字符串大，输出一个字符"&gt;"//// 如果两个字符串相等，输出一个字符"="//// 样例输入//// 第一组// Hello// hello// 第二组// hello// HI// 第三组// hello// HELL// 样例输出//// 第一组// =// 第二组// &lt;// 第三组// &gt;// 提示//// strcmp的实现如下，结果用result保存。//// int i = 0;// char result;// while (s1[i] != '\0' &amp;&amp; (s1[i] == s2[i]))&#123;// i++;// &#125;// if (s1[i] &gt; s2[i]) &#123;// result = '&gt;';// &#125;// else if (s1[i] &lt; s2[i]) &#123;// result = '&lt;';// &#125;// else&#123;// result = '=';// &#125;// (((s1[i] &gt;= 65) &amp;&amp; (s1[i] &lt;= 90)) ? (s1[i] + 32) : s1[i])#include&lt;iostream&gt;using namespace std;int main()&#123; char s1[80], s2[80]; cin.getline(s1, 80); cin.getline(s2, 80); int i = 0; char result; char s1_tem = (((s1[0] &gt;= 65) &amp;&amp; (s1[0] &lt;= 90)) ? (s1[0] + 32) : s1[0]); char s2_tem = (((s2[0] &gt;= 65) &amp;&amp; (s2[0] &lt;= 90)) ? (s2[0] + 32) : s2[0]); while (s1[i] != '\0' &amp;&amp; (s1_tem == s2_tem))&#123; i++; s1_tem = (((s1[i] &gt;= 65) &amp;&amp; (s1[i] &lt;= 90)) ? (s1[i] + 32) : s1[i]); s2_tem = (((s2[i] &gt;= 65) &amp;&amp; (s2[i] &lt;= 90)) ? (s2[i] + 32) : s2[i]); &#125; if (s1_tem &gt; s2_tem) &#123; result = '&gt;'; &#125; else if (s1_tem &lt; s2_tem) &#123; result = '&lt;'; &#125; else&#123; result = '='; &#125; cout &lt;&lt; result &lt;&lt; endl; return 0;&#125; 编程题＃3：最长单词212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182//编程题＃3：最长单词2////来源 : POJ(Coursera声明：在POJ上完成的习题将不会计入Coursera的最后成绩。)//// 注意： 总时间限制 : 1000ms 内存限制 : 65536kB//// 描述//// 一个以'.'结尾的简单英文句子，单词之间用空格分隔，没有缩写形式和其它特殊形式//// 输入//// 一个以'.'结尾的简单英文句子（长度不超过500），单词之间用空格分隔，没有缩写形式和其它特殊形式//// 输出//// 该句子中最长的单词。如果多于一个，则输出第一个//// 样例输入//// 第一组// I am a student of Peking University.// 第二组// Hello world.// 样例输出//// 第一组// University// 第二组// Hello#include&lt;iostream&gt;using namespace std;int main()&#123; char s[500]; cin.getline(s, 500, '.'); int Flag = 0; int StartP = 0; int EndP = 0; int MaxNum = 0; int position[2] = &#123; 0 &#125;; for (int i = 0; s[i] != '\0'; i++)&#123; if (s[i] == ' ')&#123; //end of the word if (Flag == 1)&#123; int tem = EndP -StartP+1; if (tem &gt; MaxNum)&#123; MaxNum = tem; position[0] = StartP; position[1] = EndP; &#125; &#125; Flag = 0; &#125; //s[i]!=' ' &amp;&amp; Flag==0 //start of new word! else if (Flag == 0)&#123; Flag = 1; StartP = i; EndP = i; &#125; //s[i]!=' ' &amp;&amp; Flag==1 else&#123; EndP = i; &#125; &#125; int tem = EndP - StartP + 1; if (tem &gt; MaxNum)&#123; MaxNum = tem; position[0] = StartP; position[1] = EndP; &#125; for (int i = position[0]; i &lt;= position[1]; i++)&#123; cout &lt;&lt; s[i]; &#125; return 0;&#125; 编程题＃4：矩阵交换行1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192//编程题＃4：矩阵交换行////来源 : POJ(Coursera声明：在POJ上完成的习题将不会计入Coursera的最后成绩。)//// 注意： 总时间限制 : 1000ms 内存限制 : 65536kB//// 描述//// 在main函数中， 生成一个5 * 5的矩阵，输入矩阵数据，并输入n，m的值。判断n，m是否在数组范围内，如果不在，则输出error；如果在范围内，则将n行和m行交换，输出交换n，m后的新矩阵。//// 输入//// 5 * 5矩阵的数据，以及n和m的值。//// 输出//// 如果不可交换，则输出error//// 如果可交换，则输出新矩阵//// 样例输入//// 第一组// 1 2 2 1 2// 5 6 7 8 3// 9 3 0 5 3// 7 2 1 4 6// 3 0 8 2 4// 0 4// 第二组// 1 2 2 1 2// 5 6 7 8 3// 9 3 0 5 3// 7 2 1 4 6// 3 0 8 2 4// 5 1// 样例输出//// 第一组// 3 0 8 2 4// 5 6 7 8 3// 9 3 0 5 3// 7 2 1 4 6// 1 2 2 1 2// 第二组// error// 提示//// 输出error格式如下：//// cout &lt;&lt; "error" &lt;&lt; endl;////输出矩阵格式如下：////cout &lt;&lt; setw(4) &lt;&lt; num;////输出矩阵一行后要输出cout &lt;&lt; endl;////setw是iomanip库里定义的格式控制操作符，需要#include &lt;iomanip&gt; 包含这个头文件。//#include&lt;iostream&gt;#include&lt;iomanip&gt;using namespace std;int main()&#123; int a[5][5] = &#123; 0 &#125;; int n, m; for (int i = 0; i &lt; 5; i++) for (int j = 0; j &lt; 5; j++) cin &gt;&gt; a[i][j]; cin &gt;&gt; n &gt;&gt; m; if ((n &gt;= 0 &amp;&amp; n &lt;= 4) &amp;&amp; (m &gt;= 0 &amp;&amp; m &lt;= 4))&#123; int tem[5]; for (int j = 0; j &lt; 5; j++)&#123; tem[j] = a[n][j]; a[n][j] = a[m][j]; a[m][j] = tem[j]; &#125; for (int i = 0; i &lt; 5; i++)&#123; for (int j = 0; j &lt; 5; j++)&#123; cout &lt;&lt; setw(4) &lt;&lt; a[i][j]; &#125; cout &lt;&lt; endl; &#125; &#125; else cout &lt;&lt; "error" &lt;&lt; endl; return 0;&#125; 编程题＃5：异常细胞检测123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566//编程题＃5：异常细胞检测////来源 : POJ(Coursera声明：在POJ上完成的习题将不会计入Coursera的最后成绩。)//// 注意： 总时间限制 : 1000ms 内存限制 : 65536kB//// 描述//// 我们拍摄的一张CT照片用一个二维数组来存储，假设数组中的每个点代表一个细胞。每个细胞的颜色用0到255之间（包括0和255）的一个整数表示。我们定义一个细胞是异常细胞，如果这个细胞的颜色值比它上下左右4个细胞的颜色值都小50以上（包括50）。数组边缘上的细胞我们不检测。现在我们的任务是，给定一个存储CT照片的二维数组，写程序统计照片中异常细胞的数目。//// 输入//// 第一行包含一个整数N（100 &gt;= N&gt;2）.//// 下面有 N 行，每行有 N 个0~255之间的整数，整数之间用空格隔开。//// 输出//// 输出只有一行，包含一个整数，为异常细胞的数目。//// 样例输入//// 70 70 70 70// 70 10 70 70// 70 70 20 70// 70 70 70 70// 样例输出//// 2#include&lt;iostream&gt;#include&lt;iomanip&gt;using namespace std;int main()&#123; int a[100][100] = &#123; 0 &#125;; int N; int AbnormalNum = 0; cin &gt;&gt; N; for (int i = 0; i &lt; N; i++)&#123; for (int j = 0; j &lt; N; j++)&#123; cin &gt;&gt; a[i][j]; &#125; &#125; if (N&gt;2)&#123; for (int i = 1; i &lt; N - 1; i++)&#123; for (int j = 1; j &lt; N - 1; j++)&#123; if ((i&gt;0 &amp;&amp; i&lt;(N - 1)) &amp;&amp; (j&gt;0 &amp;&amp; j &lt; (N - 1))) &#123; if (a[i][j] + 50 &lt;= a[i - 1][j] &amp;&amp; a[i][j] + 50 &lt;= a[i + 1][j] &amp;&amp; a[i][j] + 50 &lt;= a[i][j - 1] &amp;&amp; a[i][j] + 50 &lt;= a[i][j + 1]) &#123; AbnormalNum += 1; &#125; &#125; &#125; &#125; cout &lt;&lt; AbnormalNum &lt;&lt; endl; &#125; else&#123; cout &lt;&lt; AbnormalNum &lt;&lt; endl; &#125; return 0;&#125; 编程题＃6：循环移动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//编程题＃6：循环移动////来源 : POJ(Coursera声明：在POJ上完成的习题将不会计入Coursera的最后成绩。)//// 注意： 总时间限制 : 1000ms 内存限制 : 65536kB//// 描述//// 给定一组整数，要求利用数组把这组数保存起来，再利用实现对数组中的数循环移动。假定共有n个整数，则要使前面各数顺序向后移m个位置，并使最后m各数变为最前面的m各数。//// 注意，不要用先输出后m个数，再输出前n - m个数的方法实现，也不要用两个数组的方式实现。//// 要求只用一个数组的方式实现，一定要保证在输出结果时，输出的顺序和数组中数的顺序是一致的。//// 输入//// 输入有两行：第一行包含一个正整数n和一个正整数m，第二行包含n个正整数。每两个正整数中间用一个空格分开。//// 输出//// 输出有一行：经过循环移动后数组中整数的顺序依次输出，每两个整数之间用空格分隔。//// 样例输入//// 11 4// 15 3 76 67 84 87 13 67 45 34 45// 样例输出//// 67 45 34 45 15 3 76 67 84 87 13// 提示//// 这是一道经典的算法问题，在企业面试里出现概率很高。除了循环m次每次移动一个数以外（这样需要对数组操作m*n次），你还能想到更高效的算法吗（只用操作3*n次）？依然要求不使用额外数组，在原数组上移位之后顺序输出。//#include&lt;iostream&gt;using namespace std;int main()&#123; int n, m; cin &gt;&gt; n &gt;&gt; m; int a[10000] = &#123; 0 &#125;; for (int i = 0; i &lt; n-m; i++)&#123; cin &gt;&gt; a[i+m]; &#125; for (int i = n - m; i &lt; n; i++)&#123; cin &gt;&gt; a[(i + m) % n]; &#125; for (int i = 0; i &lt; n; i++)&#123; cout &lt;&lt; a[i]&lt;&lt;" "; &#125; return 0;&#125; 编程题＃7：中位数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136//编程题＃7：中位数////来源 : POJ(Coursera声明：在POJ上完成的习题将不会计入Coursera的最后成绩。)//// 注意： 总时间限制 : 2000ms 内存限制 : 65536kB//// 描述//// 中位数定义：一组数据按从小到大的顺序依次排列，处在中间位置的一个数或最中间两个数据的平均值（如果这组数的个数为奇数，则中位数为位于中间位置的那个数；如果这组数的个数为偶数，则中位数是位于中间位置的两个数的平均值）.//// 给出一组无序整数，求出中位数，如果求最中间两个数的平均数，向下取整即可（不需要使用浮点数）//// 输入//// 该程序包含多组测试数据，每一组测试数据的第一行为N，代表该组测试数据包含的数据个数，1 &lt;= N &lt;= 15000.//// 接着N行为N个数据的输入，N = 0时结束输入//// 输出//// 输出中位数，每一组测试数据输出一行//// 样例输入//// 4// 10// 30// 20// 40// 3// 40// 30// 50// 4// 1// 2// 3// 4// 0// 样例输出//// 25// 40// 2// 提示//// 这是也一道经典的算法问题，在企业面试里出现概率很高，是“找到第K大的数”的变种。先排序再找中位数自然是很直接的做法，但排序本身很慢。我们只想找到第n / 2大的数，对于其他数的顺序我们并不关心。那么怎么在不排序的前提下找到第n / 2大的数呢？//#include&lt;iostream&gt;using namespace std;int main()&#123; int N; int a[15000] = &#123; 0 &#125;; int LessNumber = 0; int EqualNumber = 0; while (1)&#123; cin &gt;&gt; N; if (N != 0)&#123; for (int i = 0; i &lt; N; i++)&#123; cin &gt;&gt; a[i]; &#125; if (N % 2 == 0)&#123; int m1_index = N / 2 - 1; int m2_index = N / 2; int m1 = 0; int m2 = 0; for (int i = 0; i &lt; N; i++)&#123; int LessNumber = 0; int EqualNumber = 0; for (int j = 0; j &lt; N; j++)&#123; if (a[j] &lt; a[i])&#123; LessNumber += 1; &#125; else if (a[j] == a[i])&#123; EqualNumber += 1; &#125; &#125; EqualNumber -= 1; if (LessNumber &lt; m1_index)&#123; if (LessNumber + EqualNumber == m1_index)&#123; m1 = a[i]; &#125; if (LessNumber + EqualNumber &gt;= m2_index)&#123; cout &lt;&lt; a[i]; &#125; &#125; else if (LessNumber == m1_index)&#123; m1 = a[i]; if (LessNumber + EqualNumber &gt;= m2_index)&#123; cout &lt;&lt; a[i]; &#125; &#125; else if (LessNumber == m2_index)&#123; m2 = a[i]; &#125; &#125; cout &lt;&lt; (m1 + m2) / 2 &lt;&lt; endl; &#125; else&#123; int m_index = N / 2; int m = 0; for (int i = 0; i &lt; N - 1; i++)&#123; int LessNumber = 0; int EqualNumber = 0; for (int j = 0; j &lt; N; j++)&#123; if (a[j] &lt; a[i])&#123; LessNumber += 1; &#125; else if (a[j] == a[i])&#123; EqualNumber += 1; &#125; &#125; EqualNumber -= 1; if (LessNumber &lt; m_index)&#123; if (LessNumber + EqualNumber == m_index)&#123; m = a[i]; cout &lt;&lt; m &lt;&lt; endl; &#125; &#125; else if (LessNumber == m_index)&#123; m = a[i]; cout &lt;&lt; m &lt;&lt; endl; &#125; &#125; &#125; &#125; else break; &#125; return 0;&#125; Week 11 C程序中的字符串在能够运用“数组”来解决问题的基础上，再来学习一下“字符串”的特性，在此基础上，我们将讲授C语言的中的“第四种成分”——输入输出成分。 本部分的重点在于：掌握“数组”与“字符串”的区别，理解“输入缓冲区”的基本机理，掌握cin cout的使用技巧。 第一课字符数组与字符串9 min输入缓冲区8 min一个字符的输入11 min第二课一串字符的输入22 min字符串应用例题14 min写在下一个部分之前的话写在下一个部分之前的话10 minWeek C程序设计进阶Lecture slides can be found [here]Coursera can be found here About this course: 如果说学习过《计算导论与C语言基础》，你已经迈入了C语言的殿堂，那么《C程序设计进阶》将帮助你更上一层楼，理解“结构化程序设计的基本思想”，掌握“C程序设计的基本技巧”，养成“良好的编程习惯和编程风格”，编写出“真正具有生命力的计算机程序”。完成这门课的学习，你将能解释C程序设计语言的基本概念与知识，并且使用C语言编写计算机程序解决生活工作中的实际问题。 #Lecture slides can be found [here]Coursera can be found here primary #Lecture slides can be found [here]Coursera can be found here primary #Lecture slides can be found [here]Coursera can be found here primary]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>C</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera RU Fundamentals of Computing Specialization]]></title>
    <url>%2F2017%2F09%2F26%2FCoursera%20RU%20Fundamentals%20of%20Computing%20Specialization%2F</url>
    <content type="text"><![CDATA[For quick searchCoursera can be found here This Specialization covers much of the material that first-year Computer Science students take at Rice University. Students learn sophisticated programming skills in Python from the ground up and apply these skills in building more than 20 fun projects. The Specialization concludes with a Capstone exam that allows the students to demonstrate the range of knowledge that they have acquired in the Specialization. An Introduction to Interactive Programming in Python (Part 1)Lecture slides can be found hereCoursera can be found hereyoutube link About this course: This two-part course is designed to help students with very little or no computing background learn the basics of building simple interactive applications. Our language of choice, Python, is an easy-to learn, high-level computer language that is used in many of the computational courses offered on Coursera. To make learning Python easy, we have developed a new browser-based programming environment that makes developing interactive applications in Python simple. These applications will involve windows whose contents are graphical and respond to buttons, the keyboard and the mouse. In part 1 of this course, we will introduce the basic elements of programming (such as expressions, conditionals, and functions) and then use these elements to create simple interactive applications such as a digital stopwatch. Part 1 of this class will culminate in building a version of the classic arcade game “Pong”. Who is this class for: Recommended Background - A knowledge of high school mathematics is required. The class is designed for students with no prior programming experience. Week 1 Statements, expressions, variablesUnderstand the structure of this class, explore Python as a calculator Week 0a - ExpressionsIntroduction14 minCodeSkulptor11 minArithmetic Expressions13 minPractice Exercises for Expressions (optional)10 minWeek 0b - Variables and AssignmentsVariables11 minSaving in CodeSkulptor9 minPractice Exercises for Variables and Assignments (optional)10 minQuiz: Quiz 0 10 questionsQUIZQuiz 010 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineOctober 22, 11:59 PM PDT 1 point1.Which of the following are syntactically correct strings? Try each of them in CodeSkulptor. $$\color{red}{\verb|[Hello]|}$$ $$\color{red}{\verb|”‘ello”|}$$ $$\color{red}{\verb|”This course is great!”|}$$ $$\color{red}{\verb|”Goodbye’|}$$ $$\color{red}{\verb|”Hello, world.”|}$$1 point2.To display a value in the console, what Python keyword do you use? 1 point3.In the following code, there is one line starting with $$\color{red}{\verb|#|}$$. What does this line mean to Python? 123456tax_rate = 0.15income = 40000deduction = 10000# Calculate income taxestax = (income - deduction) * tax_rateprint tax This is a syntax error. This text is used as a file name for the code. This text is printed on the console. This is a comment aimed at the human reader. Python ignores such comments.1 point4.Which of the following arithmetic expressions are syntactically correct? Try each of them in CodeSkulptor. $$\color{red}{\verb|5 - 1 - 3 - 7 - 0|}$$ $$\color{red}{\verb|(7 - 2) / (3 ** 2)|}$$ $$\color{red}{\verb|3 * ((2 - 9) + 4)) * (2 + (1 - 3))|}$$ $$\color{red}{\verb|(8 + (1 + (2 * 4) - 3))|}$$ $$\color{red}{\verb|5 * 3 (7 - 2)|}$$1 point5.You would like to make it so that the variable $$\color{red}{\verb|ounces|}$$ has the value 16, thus representing one pound. What simple Python statement will accomplish this? $$\color{red}{\verb|ounces == 16|}$$ $$\color{red}{\verb|ounces := 16|}$$ $$\color{red}{\verb|16 = ounces|}$$ $$\color{red}{\verb|ounces = 16|}$$1 point6.A gram is equal to 0.035274 ounces. Assume that the variable $$\color{red}{\verb|mass_in_ounces|}$$ has a value representing a given mass in ounces. Which Python statement below uses the variable $$\color{red}{\verb|mass_in_ounces|}$$ to compute an equivalent mass $$\color{red}{\verb|mass_in_grams|}$$ expressed in grams? Think about it mathematically, but also test these expressions in CodeSkulptor3. If you are still confused, you might check out this student tutorial video by Kelly on unit conversions. $$\color{red}{\verb|mass_in_grams = mass_in_ounces / 0.035274|}$$ $$\color{red}{\verb|mass_in_grams = 0.035274 / mass_in_ounces|}$$ $$\color{red}{\verb|mass_in_grams = mass_in_ounces * 0.035274|}$$ $$\color{red}{\verb|mass_in_ounces = 0.035274 * mass_in_grams|}$$1 point7.Which of the following can be used as a variable name? Try using each in CodeSkulptor. $$\color{red}{\verb|my.number|}$$ $$\color{red}{\verb|16ounces|}$$ $$\color{red}{\verb|__number__|}$$ $$\color{red}{\verb|number123|}$$1 point8.Assume you have values in the variables $$\color{red}{\verb|x|}$$ and $$\color{red}{\verb|y|}$$. Which statement(s) would result in $$\color{red}{\verb|x|}$$ having the sum of the current values of $$\color{red}{\verb|x|}$$ and $$\color{red}{\verb|y|}$$? Test your answer in CodeSkulptor. $$\color{red}{\verb|y += x|}$$ $$\color{red}{\verb|x += y|}$$ $$\color{red}{\verb|x += y + x|}$$ $$\color{red}{\verb|x = x + y|}$$1 point9.Python file names traditionally end in what characters after a period? Don’t include the period. 1 point10.We encourage you to save your CodeSkulptor Python files where? On your computer only In “the cloud” only In “the cloud” and on your computer Nowhere — CodeSkulptor automatically saves your files for you Mini-project #0 - “We want… a shrubbery!” (optional)Mini-project Video10 minMini-project Description10 minMini-project development process Your task is simple: modify this program template to print1We want... a shrubbery! in the CodeSkulptor console. Your program should consist of a single line of Python. Before submitting your program, we suggest that you review the grading rubric given below. Code Clinic Tips10 minPractice Peer-graded Assignment: “We want… a shrubbery!”2hurl can be found here Week 2 Week 1 - Functions, logic, conditionalsLearn the basic constructs of Python programming, create a program that plays a variant of Rock-Paper-Scissors Week 1a - FunctionsFunctions15 minhttp://www.codeskulptor.org/#examples-functions.py 123456789101112131415161718192021222324252627282930313233343536373839# computes the area of a triangledef triangle_area(base, height): # header - ends in colon area = (1.0 / 2) * base * height # body - all of body is indented return area # body - return outputs valuea1 = triangle_area(3, 8)print a1a2 = triangle_area(14, 2)print a2# converts fahrenheit to celsiusdef fahrenheit2celsius(fahrenheit): celsius = (5.0 / 9) * (fahrenheit - 32) return celsius# test!!!c1 = fahrenheit2celsius(32)c2 = fahrenheit2celsius(212)print c1, c2# converts fahrenheit to kelvindef fahrenheit2kelvin(fahrenheit): celsius = fahrenheit2celsius(fahrenheit) kelvin = celsius + 273.15 return kelvin# test!!!k1 = fahrenheit2kelvin(32)k2 = fahrenheit2kelvin(212)print k1, k2# prints hello, world!def hello(): print "Hello, world!"# test!!!hello() # call to hello prints "Hello, world!"h = hello() # call to hello prints "Hello, world!" a second timeprint h # prints None since there was no return value 123456712.014.00.0 100.0273.15 373.15Hello, world!Hello, world!None Visualizing Functions12 minhttp://www.codeskulptor.org/#examples-functions.py 12345678def f(i,I=[]): for j in range(i): I.append(j) return I print f(3)print f(2,[4,5])print f(4) 123[0, 1, 2][4, 5, 0, 1][0, 1, 2, 0, 1, 2, 3] More Operations17 minhttp://www.codeskulptor.org/#examples-more_operations.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# Remainder - modular arithmetic# systematically restrict computation to a range# long division - divide by a number, we get a quotient plus a remainder# quotient is integer division //, the remainder is % (Docs)# problem - get the ones digit of a numbernum = 49tens = num // 10ones = num % 10print tens, onesprint 10 * tens + ones, num# application - 24 hour clock# http://en.wikipedia.org/wiki/24-hour_clockhour = 20shift = 8print (hour + shift) % 24# application - screen wraparound# Spaceship from week sevenwidth = 800position = 797move = 5position = (position + move) % widthprint position# Data conversion operations# convert an integer into string - str# convert an hour into 24-hour format "03:00", always print leading zerohour = 3ones = hour % 10tens = hour // 10print tens, ones, ":00"print str(tens), str(ones), ":00"print str(tens) + str(ones) + ":00"# convert a string into numbers using int and float# Python modules - extra functions implemented outside basic Pythonimport simplegui # access to drawing operations for interactive applicationsimport math # access to standard math functions, e.g; trigimport random # functions to generate random numbers# look in Docs for useful functionsprint math.pi 123456784 949 49420 3 :000 3 :0003:003.14159265359 Practice Exercises for Functions (optional)10 minWeek 1b - Logic and ConditionalsLogic and Comparisons10 minConditionals10 minhttp://www.codeskulptor.org/#examples-conditionals.py 12345678910111213141516171819202122232425def greet(friend, money): if friend and (money &gt; 20): print "Hi!" money = money - 20 elif friend: print "Hello" else: print "Ha ha" money = money + 10 return moneymoney = 15money = greet(True, money)print "Money:", moneyprint ""money = greet(False, money)print "Money:", moneyprint ""money = greet(True, money)print "Money:", moneyprint "" 12345678HelloMoney: 15Ha haMoney: 25Hi!Money: 5 http://www.codeskulptor.org/#examples-leap_year.py123456789101112131415161718192021# Conditionals Examples# Return True if year is a leap year, false otherwisedef is_leap_year(year): if (year % 400) == 0: return True elif (year % 100) == 0: return False elif (year % 4) == 0: return True else: return Falseyear = 2012leap_year = is_leap_year(year) if leap_year: print year, "is a leap year"else: print year, "is not a leap year" Programming Tips - 116 minhttp://www.codeskulptor.org/#examples_tips1.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103############# This is a compilation of the examples from Week 1's Programming Tips.# Many of these functions have errors, so this file won't run as is.############import math############# Has multiple NameErrorsdef volume_cube(side): return sidde ** 3s = 2print "Volume of cube with side", s, "is", volume(s), "."############# Has a NameErrordef random_dice(): die1 = random.randrange(1, 7) die2 = random.randrange(1, 7) return die1 + die2print "Sum of two random dice, rolled once:", random_dice()print "Sum of two random dice, rolled again:", random_dice()print "Sum of two random dice, rolled again:", random_dice()############# Has an AttributeErrordef volume_sphere(radius): return 4.0/3.0 * math.Pi * (radius ** 3)r = 2print "Volume of sphere of radius", r, "is", volume_sphere(r), "."############# Has multiple TypeErrorsdef area_triangle(base, height): return 0.5 * base * heightb = "5"h = "2 + 2"print "Area of triangle with base", b, "and height", h, "is", area_triangle(b), "."############# Has multiple SyntaxErrorsdef is_mary(x) if x = "Mary": print "Found Mary!" else: print "No Mary."is_mary(Mary)is_mary(Fred)############# Poor readabilitydef area(a,b,c): s = (a+b+c)/2.0 return math.sqrt(s*(s-a)*(s-b)*(s-c))############# Improved readabilitydef area_triangle_sss(side1, side2, side3): """ Returns the area of a triangle, given the lengths of its three sides. """ # Use Heron's formula semiperim = (side1 + side2 + side3) / 2.0 return math.sqrt(semiperim * (semiperim - side1) * (semiperim - side2) * (semiperim - side3))base = 3height = 4hyp = 5print "Area of triangle with sides", base, height, hyp, "is", area_triangle_sss(base, height, hyp), "."############# Could use error-checking of input valuedef favorites(instructor): """Return the favorite thing of the given instructor.""" if instructor == "Joe": return "games" elif instructor == "Scott": return "ties" elif instructor == "John": return "outdoors" print favorites("John")print favorites("Jeannie") Practice Exercises for Logic and Conditionals (optional)10 minQuiz: Quiz 110 questionsQUIZQuiz 110 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineOctober 29, 11:59 PM PDT 1 point1.An if statement can have at most how many else parts? Unlimited, i.e., 0 or more 1 01 point2.Consider the Boolean expression not (p or not q). Give the four following values in order, separated only by spaces: the value of the expression when p is True, and q is True, the value of the expression when p is True, and q is False, the value of the expression when p is False, and q is True, the value of the expression when p is False, and q is False, Remember, each of the four results you provide should be True or False with the proper capitalization. 1 point3.A common error for beginning programmers is to confuse the behavior of print statements and return statements. print statements can appear anywhere in your program and print a specified value(s) in the console. Note that execution of your Python program continues onward to the following statement. Remember that executing a print statement inside a function definition does not return a value from the function.return statements appear inside functions. The value associated with the return statement is substituted for the expression that called the function. Note that executing a return statement terminates execution of the function definition immediately. Any statements in the function definition following the return statement are ignored. Execution of your Python code resumes with the execution of the statement after the function call.As an example to illustrate these points, consider the following piece of code: 12345def do_stuff(): print "Hello world" return "Is it over yet?" print "Goodbye cruel world!"print do_stuff() Note that this code calls the function do_stuff in the last print statement. The definition of do_stuff includes two print statements and one return statement. Which of the following is the console output that results from executing this piece of code? While it is trivial to solve this question by cutting and pasting this code into CodeSkulptor, we suggest that you first attempt this problem by attempting to execute this code in your mind. 1234Hello worldIs it over yet?Goodbye cruel world!Is it over yet? 1Hello world 123Hello worldIs it over yet?Goodbye cruel world! 12Hello worldIs it over yet? 1 point4.Given a non-negative integer n, which of the following expressions computes the ten’s digit of n? For example, if n is 123, then we want the expression to evaluate to 2. Think about each expression mathematically, but also try each in CodeSkulptor. (n // 10) % 10 (n % 100 - n % 10) / 10 (n % 10) / 101 point5.The function calls random.randint(0, 10) and random.randrange(0, 10) generate random numbers in different ranges. What number can be generated by one of these functions, but not the other? (Refer to the CodeSkulptor documentation.) By the way, we (and most Python programmers) always prefer to use random.randrange() since it handles numerical ranges in a way that is more consistent with the rest of Python. 1 point6.Implement the mathematical function f(x)=−5x5+69x2−47 as a Python function. Then use Python to compute the function values f(0), f(1), f(2), and f(3). Enter the maximum of these four values calculated. 1 point7.When investing money, an important concept to know is compound interest. The equation FV=PV(1+rate)periods relates the following four quantities. The present value (PV) of your money is how much money you have now.The future value (FV) of your money is how much money you will have in the future.The nominal interest rate per period (rate) is how much interest you earn during a particular length of time, before accounting for compounding. This is typically expressed as a percentage.The number of periods (periods) is how many periods in the future this calculation is for.Finish the following code, run it, and submit the printed number. Provide at least four digits of precision after the decimal point. 12345678def future_value(present_value, annual_rate, periods_per_year, years): rate_per_period = annual_rate / periods_per_year periods = periods_per_year * years rate = rate_per_period # Put your code here. return present_value * (1 + rate)**periodsprint "$1000 at 2% compounded daily for 3 years yields $", future_value(1000,.02, 365, 3) Before submitting your answer, test your function on the following example. future_value(500, .04, 10, 10) should return 745.317442824 1 point8.There are several ways to calculate the area of a regular polygon. Given the number of sides, n, and the length of each side, s, the polygon’s area is ns24tan(πn)For example, a regular polygon with 5 sides, each of length 7 inches, has area 84.3033926289 square inches. Write a function that calculates the area of a regular polygon, given the number of sides and length of each side. Submit the area of a regular polygon with 7 sides each of length 3 inches. Enter a number (and not the units) with at least four digits of precision after the decimal point. Note that the use of inches as the unit of measurement in these examples is arbitrary. Python only keeps track of the numerical values, not the units. 1 point9.Running the following program results in the error SyntaxError: bad input on line 8 (‘return’). Which of the following describes the problem? 1234567def max_of_2(a, b): if a &gt; b: return a else: return bdef max_of_3(a, b, c):return max_of_2(a, max_of_2(b, c)) Extra parenthesis Missing colon Misspelled function name Incorrect indentation Misspelled variable name Misspelled keyword Wrong number of arguments in function call Missing parenthesis1 point10.The following code has a number of syntactic errors in it. The intended math calculations are correct, so the only errors are syntactic. Fix the syntactic errors. Once the code has been fully corrected, it should print out two numbers. The first should be 1.09888451159. Submit the second number printed in CodeSkulptor. Provide at least four digits of precision after the decimal point. 12345define project_to_distance(point_x point_y distance): dist_to_origin = math.square_root(pointx ** 2 + pointy ** 2) scale == distance / dist_to_origin print point_x * scale, point_y * scaleproject-to-distance(2, 7, 4) 12345def project_to_distance(point_x, point_y, distance): dist_to_origin = math.sqrt(point_x ** 2 + point_y ** 2) scale = distance / dist_to_origin print point_x * scale, point_y * scaleproject_to_distance(2, 7, 4) Mini-project #1 - Rock-paper-scissor-lizard-SpockMini-project Video15 minhttp://www.codeskulptor.org/#examples-rpsls_template.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# Rock-paper-scissors-lizard-Spock template# The key idea of this program is to equate the strings# "rock", "paper", "scissors", "lizard", "Spock" to numbers# as follows:## 0 - rock# 1 - Spock# 2 - paper# 3 - lizard# 4 - scissors# helper functionsdef name_to_number(name): # delete the following pass statement and fill in your code below pass # convert name to number using if/elif/else # don't forget to return the result!def number_to_name(number): # delete the following pass statement and fill in your code below pass # convert number to a name using if/elif/else # don't forget to return the result! def rpsls(player_choice): # delete the following pass statement and fill in your code below pass # print a blank line to separate consecutive games # print out the message for the player's choice # convert the player's choice to player_number using the function name_to_number() # compute random guess for comp_number using random.randrange() # convert comp_number to comp_choice using the function number_to_name() # print out the message for computer's choice # compute difference of comp_number and player_number modulo five # use if/elif/else to determine winner, print winner message # test your code - THESE CALLS MUST BE PRESENT IN YOUR SUBMITTED CODErpsls("rock")rpsls("Spock")rpsls("paper")rpsls("lizard")rpsls("scissors")# always remember to check your completed program against the grading rubric http://www.codeskulptor.org/#user43_2OiD0caumf_0.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# Rock-paper-scissors-lizard-Spock template# The key idea of this program is to equate the strings# "rock", "paper", "scissors", "lizard", "Spock" to numbers# as follows:## 0 - rock# 1 - Spock# 2 - paper# 3 - lizard# 4 - scissors# helper functionsdef name_to_number(name): # delete the following pass statement and fill in your code below #dic = &#123;"rock":0, "Spock":1, "paper":2, "lizard":3, "scissors":4] num = [x for x in range(5)] nam = ["rock", "Spock", "paper", "lizard", "scissors"] dic = dict(zip(nam,num)) # convert name to number using if/elif/else # don't forget to return the result! return dic[name]def number_to_name(number): # delete the following pass statement and fill in your code below num = [x for x in range(5)] nam = ["rock", "Spock", "paper", "lizard", "scissors"] dic = dict(zip(num,nam)) # convert number to a name using if/elif/else # don't forget to return the result! return dic[number]def rpsls(player_choice): # delete the following pass statement and fill in your code below # print a blank line to separate consecutive games print '\n' # print out the message for the player's choice print 'Player chooses ' + player_choice # convert the player's choice to player_number using the function name_to_number() player_number = name_to_number(player_choice) # compute random guess for comp_number using random.randrange() import random comp_number = random.randrange(0,5) # convert comp_number to comp_choice using the function number_to_name() comp_choice = number_to_name(comp_number) # print out the message for computer's choice print 'Computer chooses ' + comp_choice # compute difference of comp_number and player_number modulo five diff = (comp_number - player_number) % 5 # use if/elif/else to determine winner, print winner message if diff == 3 or diff == 4: print 'Player wins!' elif diff == 1 or diff == 2: print 'Computer wins!' else: print "Player and computer tie!" # test your code - THESE CALLS MUST BE PRESENT IN YOUR SUBMITTED CODErpsls("rock")rpsls("Spock")rpsls("paper")rpsls("lizard")rpsls("scissors")# always remember to check your completed program against the grading rubric Mini-project Description10 minPractice Mini-project: Mystical Octosphere (optional)10 minCode Clinic Tips10 min This item will be unlocked when the session begins.Peer-graded Assignment: Rock-paper-scissors-lizard-Spock2hhttp://www.codeskulptor.org/#user43_2OiD0caumf_0.py Review Your Peers: Rock-paper-scissors-lizard-SpockWeek 3 Week 2 - Event-driven programming, local/global variablesLearn the basics of event-driven programming, understand difference between local and global variables, create an interactive program that plays a simple guessing game Week 2a - Interactive Applications in PythonEvent-Driven Programming13 minhttp://www.codeskulptor.org/#examples-events.py1234567891011121314# Example of a simple event-driven program# CodeSkulptor GUI moduleimport simplegui# Event handlerdef tick(): print "tick!"# Register handlertimer = simplegui.create_timer(1000, tick)# Start timertimer.start() Local vs. Global Variables11 minhttp://www.codeskulptor.org/#examples-local_vs_global.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# global vs local examples# num1 is a global variablenum1 = 1print num1# num2 is a local variabledef fun(): num1 = 2 num2 = num1 + 1 print num2 fun()# the scope of global num1 is the whole program, num 1 remains definedprint num1# the scope of the variable num2 is fun(), num2 is now undefined# print num2# why use local variables?# give a descriptive name to a quantity# avoid computing something multiple timesdef fahren_to_kelvin(fahren): celsius = 5.0 / 9 * (fahren - 32) zero_celsius_in_kelvin = 273.15 return celsius + zero_celsius_in_kelvinprint fahren_to_kelvin(212)# the risk/reward of using global variables# risk - consider the software system for an airliner# critical piece - flight control system# non-critical piece - in-flight entertainment system# both systems might use a variable called "dial"# we don't want possibility that change the volume on your audio# causes the plane's flaps to change!# examplenum = 4def fun1(): global num num = 5 def fun2(): global num num = 6# note that num changes after each call with no obvious explanation print numfun1()print numfun2()print num# global variables are an easy way for event handlers# to communicate game information.# safer method - but they required more sophisticated# object-programming techniques 1234567131373.15456 SimpleGUI11 minhttp://www.codeskulptor.org/#examples-simplegui-0.py123456789101112131415161718192021222324252627# CodeSkulptor runs Python programs in your browser.# Click the upper left button to run this simple demo.# CodeSkulptor runs in Chrome 18+, Firefox 11+, and Safari 6+.# Some features may work in other browsers, but do not expect# full functionality. It does NOT run in Internet Explorer.import simpleguimessage = "Welcome!"# Handler for mouse clickdef click(): global message message = "Good job!"# Handler to draw on canvasdef draw(canvas): canvas.draw_text(message, [50,112], 36, "Red")# Create a frame and assign callbacks to event handlersframe = simplegui.create_frame("Home", 300, 200)frame.add_button("Click me", click)frame.set_draw_handler(draw)# Start the frame animationframe.start() http://www.codeskulptor.org/#examples-simplegui-1.py123456789101112131415161718192021222324252627282930# SimpleGUI program template# Import the moduleimport simplegui# Define global variables (program state)count = 0# Define "helper" functionsdef incre(): global count count += 1 # Define event handler functionsdef tick(): incre() print count def bpress(): global count count = 0# Create a frameframe = simplegui.create_frame("SimpleGUI Test",100,100)# Register event handlerstimer = simplegui.create_timer(1000, tick)frame.add_button("click me!", bpress)# Start frame and timersframe.start()timer.start() Practice Exercises for Interactive Applications (optional)10 minQuiz: Quiz 2a10 questionsQUIZQuiz 2a10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 5, 11:59 PM PST 1 point1.What typically calls an event handler? Some code that you didn’t write which generates the event. The code you write.1 point2.In CodeSkulptor, how many event handlers can be running at the same time? Unlimited, i.e., 0 or more 1 01 point3.What are the three parts of a frame? Refer to the video on SimpleGUI. Status Area Control Area Mouse Title Keyboard Background Area Canvas Border Options Area1 point4.For the SimpleGUI-based programs in this course, we recommended breaking down an interactive Python program into seven parts. Below, these parts are listed alphabetically. Create frameDefine classesDefine event handlersInitialize global variablesDefine helper functionsRegister event handlersStart frame and timersHowever, in lecture, we recommended a particular ordering of these parts. Enter 7 numbers in the range 1–7, separated only by spaces, to indicate the recommended ordering of the preceding elements of an interactive Python program. For example, if you think that the first action in your program should be to register your event handlers, enter 6 as the first number in the sequence. 1 point5.Assume the following global definition is part of your program.1x = 5 If each of the following function definitions are also part of your program, which of them needs a global x declaration? You can try each definition in CodeSkulptor. 123def d(y): y = x + y return y 12def c(y): return x + y 123def b(x,y): x = x + y return x 123def a(y): x = x + y return y 1 point6.Consider the following code.123456count = 0def square(x): global count count += 1 return x**2print square(square(square(square(3)))) What is the value of count at the end? Enter a number. (You can double check your answer in CodeSkulptor if you wish.) 1 point7.Consider the following code.12345a = 3b = 6def f(a): c = a + b return c Which names occur in the global scope? c f b a1 point8.Consider the following code.12345a = 3b = 6def f(a): c = a + b return c Which names occur in a local scope? f b a c1 point9.Which of the following are valid calls to create_frame? Look at the documentation for SimpleGUI frames, but also try the code in CodeSkulptor. 1f = simplegui.create_frame("My Frame", 100, 100) 1frame = simplegui.create_frame("Testing", 200, 200, 300) 1frame = simplegui.create_frame(100, 100, 100) 1frame = simplegui.create_frame("My Frame", 200, 200, 200, 200) 1 point10.Which of the following are valid ways of making a canvas with a red background? Look at the documentation for SimpleGUI constants, but also try the code in CodeSkulptor. 1234import simpleguiframe = simplegui.create_frame("My Frame", 100, 100)frame.set_canvas_background(Red)frame.start() 1234import simpleguiframe = simplegui.create_frame("My Frame", 100, 100)frame.set_canvas_background("Red")frame.start() 123import simpleguiframe = simplegui.create_frame("My Frame", 100, 100, "Red")frame.start() 12345import simpleguiframe = simplegui.create_frame("My Frame", 100, 100)frame.set_canvas_background("#FF0000")frame.start() Week 2b - Buttons and Input FieldsButtons10 minhttp://www.codeskulptor.org/#examples-buttons.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# calculator with all buttonsimport simplegui# intialize globalsstore = 12operand = 3# event handlers for calculator with a store and operanddef output(): """prints contents of store and operand""" print "Store = ", store print "Operand = ", operand print "" def swap(): """ swap contents of store and operand""" global store, operand store, operand = operand, store output() def add(): """ add operand to store""" global store store = store + operand output()def sub(): """ subtract operand from store""" global store store = store - operand output()def mult(): """ multiply store by operand""" global store store = store * operand output()def div(): """ divide store by operand""" global store store = store / operand output()# create framef = simplegui.create_frame("Calculator",300,300)# register event handlersf.add_button("Print", output, 100)f.add_button("Swap", swap, 100)f.add_button("Add", add, 100)f.add_button("Sub", sub, 100)f.add_button("Mult", mult, 100)f.add_button("Div", div, 100)# get frame rollingf.start() Input Fields9 minhttp://www.codeskulptor.org/#examples-input_fields.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# calculator with all buttonsimport simplegui# intialize globalsstore = 0operand = 0# event handlers for calculator with a store and operanddef output(): """prints contents of store and operand""" print "Store = ", store print "Operand = ", operand print "" def swap(): """ swap contents of store and operand""" global store, operand store, operand = operand, store output() def add(): """ add operand to store""" global store store = store + operand output()def sub(): """ subtract operand from store""" global store store = store - operand output()def mult(): """ multiply store by operand""" global store store = store * operand output()def div(): """ divide store by operand""" global store store = store / operand output()def enter(t): """ enter a new operand""" global operand operand = int(t) output() # create framef = simplegui.create_frame("Calculator",300,300)# register event handlers and create control elementsf.add_button("Print", output, 100)f.add_button("Swap", swap, 100)f.add_button("Add", add, 100)f.add_button("Sub", sub, 100)f.add_button("Mult", mult, 100)f.add_button("Div", div, 100)f.add_input("Enter", enter, 100)# get frame rollingf.start() Visualizing Events5 minhttp://www.codeskulptor.org/viz/#examples-input_fields.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# calculator with all buttonsimport simplegui# intialize globalsstore = 0operand = 0# event handlers for calculator with a store and operanddef output(): """prints contents of store and operand""" print "Store = ", store print "Operand = ", operand print "" def swap(): """ swap contents of store and operand""" global store, operand store, operand = operand, store output() def add(): """ add operand to store""" global store store = store + operand output()def sub(): """ subtract operand from store""" global store store = store - operand output()def mult(): """ multiply store by operand""" global store store = store * operand output()def div(): """ divide store by operand""" global store store = store / operand output()def enter(t): """ enter a new operand""" global operand operand = int(t) output() # create framef = simplegui.create_frame("Calculator",300,300)# register event handlers and create control elementsf.add_button("Print", output, 100)f.add_button("Swap", swap, 100)f.add_button("Add", add, 100)f.add_button("Sub", sub, 100)f.add_button("Mult", mult, 100)f.add_button("Div", div, 100)f.add_input("Enter", enter, 100)# get frame rollingf.start() Programming Tips - 213 minhttp://www.codeskulptor.org/#examples-tips2.pyhttps://docs.python.org/2/tutorial/controlflow.html#intermezzo-coding-style 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113############### Example of missing "global"n1 = 0def increment(): n1 = n1 + 1increment()increment()increment()print n1############### Example of missing "global"n2 = 0def assign(x): n2 = xassign(2)assign(15)assign(7)print n2############### Example of missing "return"n3 = 0def decrement(): global n3 n3 = n3 - 1x = decrement()print "x = ", xprint "n = ", n############### Example of print debuggingimport simpleguix = 0def f(n): print "f: n,x = ", n, x result = n ** x print "f: result = ",result return result def button_handler(): global x print "bh : x = ", x x += 1 print "bh : x = ", xdef input_handler(text): print "ih : text = ", text print f(float(text)) frame = simplegui.create_frame("Example", 200, 200)frame.add_button("Increment", button_handler)frame.add_input("Number:", input_handler, 100)frame.start()############### Examples of simplifying conditionalsdef f1(a, b): """Returns True exactly when a is False and b is True.""" if a == False and b == True: return True else: return Falsedef f2(a, b): """Returns True exactly when a is False and b is True.""" if not a and b: return True else: return False def f3(a, b): """Returns True exactly when a is False and b is True.""" return not a and bdef g1(a, b): """Returns False eactly when a and b are both True.""" if a == True and b == True: return False else: return True def g2(a, b): """Returns False eactly when a and b are both True.""" if a and b: return False else: return Truedef g3(a, b): """Returns False eactly when a and b are both True.""" return not (a and b) Practice Exercises for Button and Input Fields (optional)10 minQuiz: Quiz 2b10 questionsQUIZQuiz 2b10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 5, 11:59 PM PST 1 point1.In the following code, what does the number 100 represent?1my_button = frame.add_button("My Label", button_handler, 100) Use the CodeSkulptor documentation to look it up. Horizontal position of the button in pixels Height of the button in pixels Width of the button in pixels Vertical position of the button in pixels1 point2.How many control objects are allowed in a frame? 1 0 Unlimited, i.e., 0 or more1 point3.In SimpleGUI, one kind of object can be added to the control panel that doesn’t allow any handler. Thus, this object can’t respond to anything. What kind of object is that? Look at the documentation for SimpleGUI control objects. Label FrameThis should not be selectedThat isn’t a control object. Button Title Canvas Input field1 point4.When you enter text into an input field and press enter, the text is passed to the input field’s event handler. What is the data type of the text? A string A number A string or a number, depending on the text entered1 point5.Consider the following conditional statement.123456if p == False: return Falseelif q == False: return Falseelse: return True That is equivalent to which of the following simpler statements? Try to reason logically about each of the statements, but also try each in CodeSkulptor. 1return p and (not q) 1return q and p 1return (not p) or (not q) 1return not(p or q) 1 point6.Which of the following describes the mistake in the following code?123456def volume_cube(side): """ Returns the volume of a cube, given the length of its side. """ print side ** 3 s = 5print "The volume of a cube with sides", s, "long is", volume_cube(s), "." The call to volume_cube shouldn’t be within a print statement. More generally, function calls usually shouldn’t be within print statements. All of the printing should be done within the function. The function should return, not print, its result.1 point7.What kind of errors can happen if you are missing a needed global declaration in one of your function definitions? For this question, you need only consider the case where the problem is in the function that is missing the global declaration. If you are having trouble with this question, watch this week’s Programming Tips video again. NameError An incorrect computation that generates no error message AttributeError SyntaxError Error: local variable ‘…’ referenced before assignment1 point8.Which of the following function definitions are in the recommended code style? 123def f(x, y): """ Add the two inputs. """ return x + y 123def f (x, y): """ Add the two inputs. """ return x + y 123def f(x, y): """ Add the two inputs. """ return x+y 123def myFunction(x, y): """ Add the two inputs. """ return x + y 1 point9.Cut and paste the following code into CodeSkulptor. Run it and make an attempt to understand how it works.123456789101112131415161718192021# Simple interactive applicationimport simplegui# Define globals.message = "Welcome!"count = 0# Define event handlers.def button_handler(): """Count number of button presses.""" global count count += 1 print message," You have clicked", count, "times."def input_handler(text): """Get text to be displayed.""" global message message = text# Create frame and register event handlers.frame = simplegui.create_frame("Home", 100, 200)frame.add_button("Click me", button_handler)frame.add_input("New message:", input_handler, 100)# Start frame.frame.start() We’d like to modify the code so that the count is reset to zero whenever a new message is entered. Where would you need to modify this code to implement this change? Add an assignment to count at the end of this code. Add an assignment to count in the event handler for the input field. Also add a global count declaration there. Add an assignment to count in the event handler for the button. Add an assignment to count in the initialization of global variables.1 point10.In the game “Guess the number”, what is the minimum number of guesses necessary to guarantee that the guesser can always win if the secret number is chosen in range(0, 400)? Review the mini-project description for “Guess the number” if you are having trouble with this problem. 8 guesses 9 guesses 10 guessesThis should not be selected 12 guessesThis should not be selected It’s impossible to guarantee that you can always win at “Guess the number“.This should not be selectedThat’s clearly wrong. If nothing else, you could use the strategy of guessing each number from 0 to 399, thus needing 400 guesses. Mini-project #2 - “Guess the Number!”Mini-project Video6 minhttp://www.codeskulptor.org/#examples-guess_the_number_template.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344# template for "Guess the number" mini-project# input will come from buttons and an input field# all output for the game will be printed in the console# helper function to start and restart the gamedef new_game(): # initialize global variables used in your code here # remove this when you add your code pass# define event handlers for control paneldef range100(): # button that changes the range to [0,100) and starts a new game # remove this when you add your code passdef range1000(): # button that changes the range to [0,1000) and starts a new game pass def input_guess(guess): # main game logic goes here # remove this when you add your code pass # create frame# register event handlers for control elements and start frame# call new_game new_game()# always remember to check your completed program against the grading rubric Mini-project Description10 minhttp://www.codeskulptor.org/#examples-gtn_testing_template.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152# Testing template for "Guess the number"#################################################### Student should add code for "Guess the number" here# template for "Guess the number" mini-project# input will come from buttons and an input field# all output for the game will be printed in the consoleimport simpleguiimport randomimport math#################################################### Start our test #1 - assume global variable secret_number# is the the "secret number" - change name if necessarysecret_number = 74 input_guess("50")input_guess("75")input_guess("62")input_guess("68")input_guess("71")input_guess("73")input_guess("74")#################################################### Output from test #1#New game. Range is [0,100)#Number of remaining guesses is 7##Guess was 50#Number of remaining guesses is 6#Higher!##Guess was 75#Number of remaining guesses is 5#Lower!##Guess was 62#Number of remaining guesses is 4#Higher!##Guess was 68#Number of remaining guesses is 3#Higher!##Guess was 71#Number of remaining guesses is 2#Higher!##Guess was 73#Number of remaining guesses is 1#Higher!##Guess was 74#Number of remaining guesses is 0#Correct!##New game. Range is [0,100)#Number of remaining guesses is 7#################################################### Start our test #2 - assume global variable secret_number# is the the "secret number" - change name if necessary#range1000()#secret_number = 375 #input_guess("500")#input_guess("250")#input_guess("375")#################################################### Output from test #2#New game. Range is [0,100)#Number of remaining guesses is 7##New game. Range is [0,1000)#Number of remaining guesses is 10##Guess was 500#Number of remaining guesses is 9#Lower!##Guess was 250#Number of remaining guesses is 8#Higher!##Guess was 375#Number of remaining guesses is 7#Correct!##New game. Range is [0,1000)#Number of remaining guesses is 10#################################################### Start our test #3 - assume global variable secret_number# is the the "secret number" - change name if necessary#range100()#secret_number = 28 #input_guess("50")#input_guess("50")#input_guess("50")#input_guess("50")#input_guess("50")#input_guess("50")#input_guess("50")#################################################### Output from test #3#New game. Range is [0,100)#Number of remaining guesses is 7##Guess was 50#Number of remaining guesses is 6#Lower!##Guess was 50#Number of remaining guesses is 5#Lower!##Guess was 50#Number of remaining guesses is 4#Lower!##Guess was 50#Number of remaining guesses is 3#Lower!##Guess was 50#Number of remaining guesses is 2#Lower!##Guess was 50#Number of remaining guesses is 1#Lower!##Guess was 50#Number of remaining guesses is 0#You ran out of guesses. The number was 28##New game. Range is [0,100)#Number of remaining guesses is 7 Practice Mini-project: Magical Octosphere Reloaded (optional)10 minCode Clinic Tips10 minPeer-graded Assignment: “Guess the Number!”2hhttp://www.codeskulptor.org/#user43_h8ZiuoEmml_0.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# template for "Guess the number" mini-project# input will come from buttons and an input field# all output for the game will be printed in the consoleimport simpleguiimport randomimport math# helper function to start and restart the gamedef new_game(fl=0): # initialize global variables used in your code here # remove this when you add your code global secret_number global f f = fl global n if f == 0: secret_number = random.randrange(0, 100) n = math.ceil(math.log(101, 2)) print "range[0,100)" else: secret_number = random.randrange(0, 1000) n = math.ceil(math.log(1001, 2)) print "range[0,1000)" global c c = 0 print "remain:", n #print "s:", secret_number # define event handlers for control paneldef range100(): # button that changes the range to [0,100) and starts a new game print '\n' new_game() #print "remain:", n #print "s:", secret_numberdef range1000(): # button that changes the range to [0,1000) and starts a new game print '\n' new_game(1) #print "s:", secret_number def input_guess(guess): # main game logic goes here global n global c if c &lt; n: global secret_number c = c + 1 n_guess = int(guess) print "Guess was", n_guess if secret_number &lt; n_guess: print "Lower" print "remain:", n-c print '\n' if secret_number &gt; n_guess: print "Higher" print "remain:", n-c print '\n' if secret_number == n_guess: print "Correct" print "remain:", n-c print "You win! start new game!" print '\n' new_game(f) if c == n: print "You run out ot times! start new game! correct is", secret_number print '\n' new_game(f) # create frameframe = simplegui.create_frame('Guess the number!', 200, 200)# register event handlers for control elements and start frameinp = frame.add_input('My Guess', input_guess, 100)range100 = frame.add_button('range100', range100, 100)range1000 = frame.add_button('range1000', range1000, 100)# call new_game new_game()# always remember to check your completed program against the grading rubric Review Your Peers: “Guess the Number!”Week 4 Week 3 - Canvas, drawing, timersCreate a canvas in Python, learn how to draw on the canvas, create a digital stopwatch Week 3a - Drawing CanvasCanvas and Drawing12 minhttp://www.codeskulptor.org/#examples-canvas_and_drawing.py1234567891011121314151617# first example of drawing on the canvasimport simplegui# define draw handlerdef draw(canvas): canvas.draw_text("Hello!",[100, 100], 24, "White") canvas.draw_circle([100, 100], 2, 2, "Red")# create frameframe = simplegui.create_frame("Text drawing", 300, 200)# register draw handler frame.set_draw_handler(draw)# start frameframe.start() String Processing11 minhttp://www.codeskulptor.org/#examples-strings.py12345678910111213141516171819202122232425### String Processing# String literalss1 = "Rixner's funny"s2 = 'Warren wears nice ties!'s3 = " t-shirts!"#print s1, s2#print s3# Combining stringsa = ' and 's4 = "Warren" + a + "Rixner" + ' are nuts!'print s4# Characters and slicesprint s1[3]print len(s1)print s1[0:6] + s2[6:]print s2[:13] + s1[9:] + s3# Converting stringss5 = str(375)print s5[1:]i1 = int(s5[1:])print i1 + 38 http://www.codeskulptor.org/#examples-money-0.py1234567891011121314151617# convert xx.yy to xx dollars and yy centsdef convert(val): dollars = int(val) cents = int(100 * (val - dollars)) return str(dollars) + " dollars and " + str(cents) + " cents" # Testsprint convert(11.23)print convert(11.20)print convert(1.12)print convert(12.01)print convert(1.01)print convert(0.01)print convert(1.00)print convert(0)print convert(-1.40)print convert(12.55555) 1234567891011 dollars and 23 cents11 dollars and 19 cents1 dollars and 12 cents12 dollars and 0 cents1 dollars and 1 cents0 dollars and 1 cents1 dollars and 0 cents0 dollars and 0 cents-1 dollars and -39 cents12 dollars and 55 cents http://www.codeskulptor.org/#examples-money-1.py12345678910111213141516171819202122232425262728293031323334353637# Handle single quantitydef convert_units(val, name): result = str(val) + " " + name if val &gt; 1: result = result + "s" return result # convert xx.yy to xx dollars and yy centsdef convert(val): # Split into dollars and cents dollars = int(val) cents = int(round(100 * (val - dollars))) # Convert to strings dollars_string = convert_units(dollars, "dollar") cents_string = convert_units(cents, "cent") # return composite string if dollars == 0 and cents == 0: return "Broke!" elif dollars == 0: return cents_string elif cents == 0: return dollars_string else: return dollars_string + " and " + cents_string # Testsprint convert(11.23)print convert(11.20) print convert(1.12)print convert(12.01)print convert(1.01)print convert(0.01)print convert(1.00)print convert(0) 1234567811 dollars and 23 cents11 dollars and 20 cents1 dollar and 12 cents12 dollars and 1 cent1 dollar and 1 cent1 cent1 dollarBroke! Interactive Drawing12 minhttp://www.codeskulptor.org/#examples-money-1.py12345678910111213141516171819202122232425262728293031323334353637# Handle single quantitydef convert_units(val, name): result = str(val) + " " + name if val &gt; 1: result = result + "s" return result # convert xx.yy to xx dollars and yy centsdef convert(val): # Split into dollars and cents dollars = int(val) cents = int(round(100 * (val - dollars))) # Convert to strings dollars_string = convert_units(dollars, "dollar") cents_string = convert_units(cents, "cent") # return composite string if dollars == 0 and cents == 0: return "Broke!" elif dollars == 0: return cents_string elif cents == 0: return dollars_string else: return dollars_string + " and " + cents_string # Testsprint convert(11.23)print convert(11.20) print convert(1.12)print convert(12.01)print convert(1.01)print convert(0.01)print convert(1.00)print convert(0) 1234567811 dollars and 23 cents11 dollars and 20 cents1 dollar and 12 cents12 dollars and 1 cent1 dollar and 1 cent1 cent1 dollarBroke! http://www.codeskulptor.org/#examples-interactive_drawing.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# interactive application to convert a float in dollars and centsimport simplegui# define global valuevalue = 3.12# Handle single quantitydef convert_units(val, name): result = str(val) + " " + name if val &gt; 1: result = result + "s" return result # convert xx.yy to xx dollars and yy centsdef convert(val): # Split into dollars and cents dollars = int(val) cents = int(round(100 * (val - dollars))) # Convert to strings dollars_string = convert_units(dollars, "dollar") cents_string = convert_units(cents, "cent") # return composite string if dollars == 0 and cents == 0: return "Broke!" elif dollars == 0: return cents_string elif cents == 0: return dollars_string else: return dollars_string + " and " + cents_string # define draw handlerdef draw(canvas): canvas.draw_text(convert(value), [60, 110], 24, "White")# define an input field handlerdef input_handler(text): global value value = float(text)# create frameframe = simplegui.create_frame("Converter", 400, 200)# register event handlersframe.set_draw_handler(draw)frame.add_input("Enter value", input_handler, 100)# start frameframe.start() Practice Exercises for Drawing (optional)10 minQuiz: Quiz 3a10 questionsQUIZQuiz 3a10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 12, 11:59 PM PST 1 point What Python operator takes two strings (e.g., “sun” and “rise”) and forms the combination of these two strings, one followed by the other (e.g., “sunrise”)?+concatappendconcatenate*1 point2.What does the draw handler parameter represent? If you’ve forgotten, refer to the documentation. The canvas Nothing — it doesn’t have a parameter The object to be drawn The frame The location of the object to be drawn1 point3.What happens if you draw text outside the canvas coordinates? Try it in CodeSkulptor. The text coordinates are implicitly “wrapped” around using modular arithmetic, so that the text appears on the canvas Some or none of the text is shown. Conceptually, the text is drawn at whatever coordinates are given, but only whatever text fits within the canvas coordinates is shown. The text appears in the frame, but some or all of the text is shown outside the canvas area. Causes an error1 point4.Assume we have a canvas that is 200 pixels wide and 300 pixels high. We want to draw a green line between the upper left corner of the canvas and the lower right corner of the canvas. Which of the following calls will accomplish this? Try the code in CodeSkulptor. 1canvas.draw_line((200, 300), (0, 0), 10, "Green") 1canvas.draw_line((0, 0), (300, 200), 10, "Green") 1canvas.draw_line((200, 0), (0, 300), 10, "Green") 1 point5.Consider the following function definition.12345def date(month, day): """Given numbers month and day, returns a string of the form '2/12', with the month followed by the day.""" return month + "/" + dayprint date(2, 12) This definition leads to an error. To fix it, what Python expression should replace the question marks below?12345def date(month, day): """Given numbers month and day, returns a string of the form '2/12', with the month followed by the day.""" return ???print date(2, 12) string(month + “/“ + day) string(month) + “/“ + string(day) str(month / day) str(month + “/“ + day) str(month) + “/“ + str(day) string(month / day)1 point6.How many instances of the letter “l” are there in the following:121lll1l1l1l1ll1l111ll1l1ll1l1ll1ll111ll1ll1ll1l1ll1ll1ll1ll1lll1l1l1l1l1l1l1l1l1l 1l1l1ll1lll1l111ll1l1l1l1l1 Although it might be hard to tell, that string contains ones (1) and lower-case L’s (l). Create a small CodeSkulptor program, and use copy-and-paste to insert this string in your code. Your program should only need one function or method call. 1 point7.Where should your draw_text, draw_line, and similar drawing calls be? Anywhere in your code In a draw handler, or a helper function called from it In the handlers for the control objects that create or change the drawing, or their helper functions1 point8.Which of the following function calls are valid, i.e., don’t lead to an error? Read the documentation for these functions, and also try the code in CodeSkulptor. float(“5 five”) int(“5.4”) float(“5.4”) int(“5”)1 point9.Turn the following description into a CodeSkulptor program, and run it. Create a 300-by-300 canvas.Draw two circles with radius 20 and white lines of width 10. One is centered at (90,200) and one at (210,200).Draw a red line of width 40 from (50,180) to (250,180).Draw two red lines of width 5 from (55,170) to (90,120) and from (90,120) to (130,120).Draw a red line of width 140 from (180,108) to (180,160).The resulting picture is a simple diagram of what?123456789101112131415161718192021# first example of drawing on the canvasimport simplegui# define draw handlerdef draw(canvas): canvas.draw_circle([90,200], 20, 10, "White") canvas.draw_circle([210,200], 20, 10, "White") canvas.draw_line([50,180],[250,180], 40, "Red") canvas.draw_line([55,170],[90,120], 5, "Red") canvas.draw_line([90,120],[130,120], 5, "Red") canvas.draw_line([180,108],[180,160], 140, "Red")# create frameframe = simplegui.create_frame("Text drawing", 300, 300)# register draw handler frame.set_draw_handler(draw)# start frameframe.start() A person A house A computer A motorcycle An automobile1 point10.The following is a diagram of an archery target. To draw this in CodeSkulptor, we can put a small yellow circle with a black border on top of a slightly bigger yellow circle with a black border, … on top of a big white circle with a black border. In what order should your code draw these circles? Smallest first Largest first Week 3b - TimersTimers9 minhttp://www.codeskulptor.org/#examples-timers.py12345678910111213141516171819202122232425262728293031323334353637383940# Simple "screensaver" program.# Import modulesimport simpleguiimport random# Global statemessage = "Python is Fun!"position = [50, 50]width = 500height = 500interval = 2000# Handler for text boxdef update(text): global message message = text # Handler for timerdef tick(): x = random.randrange(0, width) y = random.randrange(0, height) position[0] = x position[1] = y# Handler to draw on canvasdef draw(canvas): canvas.draw_text(message, position, 36, "Red")# Create a frame frame = simplegui.create_frame("Home", width, height)# Register event handlerstext = frame.add_input("Message:", update, 150)frame.set_draw_handler(draw)timer = simplegui.create_timer(interval, tick)# Start the frame animationframe.start()timer.start() Visualizing Drawing and Timers6 minhttp://www.codeskulptor.org/viz/#examples-timers.py12345678910111213141516171819202122232425262728293031323334353637383940# Simple "screensaver" program.# Import modulesimport simpleguiimport random# Global statemessage = "Python is Fun!"position = [50, 50]width = 500height = 500interval = 2000# Handler for text boxdef update(text): global message message = text # Handler for timerdef tick(): x = random.randrange(0, width) y = random.randrange(0, height) position[0] = x position[1] = y# Handler to draw on canvasdef draw(canvas): canvas.draw_text(message, position, 36, "Red")# Create a frame frame = simplegui.create_frame("Home", width, height)# Register event handlerstext = frame.add_input("Message:", update, 150)frame.set_draw_handler(draw)timer = simplegui.create_timer(interval, tick)# Start the frame animationframe.start()timer.start() Programming Tips - 37 minhttp://www.codeskulptor.org/#examples-tips3-events.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748###################### Example of event-driven code, buggy versionimport simpleguisize = 10radius = 10# Define event handlers.def incr_button_handler(): """Increment the size.""" global size size += 1 label.set_text("Value: " + str(size)) def decr_button_handler(): """Decrement the size.""" global size # Insert check that size &gt; 1, to make sure it stays positive # NOTE that this restriction has changed from the video # since draw_circle now throws an error if radius is zero size -= 1 label.set_text("Value: " + str(size))def change_circle_handler(): """Change the circle radius.""" global radius radius = size # Insert code to make radius label change. def draw_handler(canvas): """Draw the circle.""" canvas.draw_circle((100, 100), radius, 5, "Red")# Create a frame and assign callbacks to event handlers.frame = simplegui.create_frame("Home", 200, 200)label = frame.add_label("Value: " + str(size))frame.add_button("Increase", incr_button_handler)frame.add_button("Decrease", decr_button_handler)frame.add_label("Radius: " + str(radius))frame.add_button("Change circle", change_circle_handler)frame.set_draw_handler(draw_handler)# Start the frame animationframe.start() 12345678910111213141516171819202122232425262728293031323334import simplegui###################### Buggy code -- doesn't start framemessage = "Welcome!"def click(): """Change message on mouse click.""" global message message = "Good job!"def draw(canvas): """Draw message.""" canvas.draw_text(message, [50,112], 36, "Red")# Create a frame and assign callbacks to event handlersframe = simplegui.create_frame("Home", 300, 200)frame.add_button("Click me", click)frame.set_draw_handler(draw)###################### Buggy code -- doesn't start timersdef timer1_handler(): print "1" def timer2_handler(): print "2"simplegui.create_timer(100, timer1_handler)simplegui.create_timer(300, timer2_handler) Practice Exercises for Timers (optional)10 minQuiz: Quiz 3b9 questionsQUIZQuiz 3b9 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 12, 11:59 PM PST 1 point1.When the following code is executed, how many times is timer_handler called?12345import simpleguidef timer_handler(): …timer = simplegui.create_timer(10, timer_handler)timer.start() The body of timer_handler isn’t given, as it is irrelevant for this question. You may want to finish the code and run it before submitting your answer. 0 — The code hasn’t been written correctly. 1 10 Unlimited — It is called repeatedly until you stop the program.1 point2.You want a timer to create exactly 1000 events. Which of the following solutions are possible? In the timer handler, have a local counter for the number of timer calls.In the timer handler, increment the counter. In the timer handler, check the count and possibly stop the timer. Have a global counter for the number of timer calls. In the timer handler, increment the counter. In the timer handler, check the count and possibly stop the timer. Specify the number of timer events when creating the timer. Have a global counter for the number of timer calls. Outside the timer handler, increment the counter. Outside the timer handler, check the count and possibly stop the timer.1 point3.How do you change the frequency of a running timer, either increasing or decreasing the frequency? E.g., in the code below, we want code at the question marks that changes the timer.12345…timer = simplegui.create_timer(1000, timer_handler)timer.start()…??? Just use set_timer_interval. 1timer.set_timer_interval(300) You can’t. But, you can stop this timer, and start a new one with a different frequency and same handler. 123timer.stop()timer = simplegui.create_timer(300, timer_handler)timer.start() Create and start the timer again. 12timer = simplegui.create_timer(300, timer_handler)timer.start() Just run create_timer. It will change the timer.1timer = simplegui.create_timer(300, timer_handler) 1 point4.How many timers can you have running at once? 0 1 Unlimited1 point5.The function time.time() is used in Python to keep track of time. What unit of time is associated with the value returned by time.time()? Hint: Look in the documentation. Milli-second Second Minute Hour1 point6.In Python, the time module can be used to determine the current time. This module includes the method time which returns the current system time in seconds since a date referred as the Epoch. The Epoch is fixed common date shared by all Python installations. Using the date of the Epoch and the current system time, an application such as a clock or calendar can compute the current time/date using basic arithmetic. Write a CodeSkulptor program that experiments with the method time.time() and determines what date and time corresponds to the Epoch. Enter the year of the Epoch as a four digit number. (Remember to import time.) 1 point7.The Python code below uses a timer to execute the function update() 10 times, computing a good approximation to a common mathematical function. Examine the code, and run it while varying the input value n. What is the common name for what this computes?1234567891011121314151617181920212223242526272829303132# Mystery computation in Python# Takes input n and computes output named resultimport simplegui# global stateresult = 1iteration = 0max_iterations = 10# helper functionsdef init(start): """Initializes n.""" global n n = start print "Input is", ndef get_next(current): """??? Part of mystery computation.""" return 0.5 * (current + n / current)# timer callbackdef update(): """??? Part of mystery computation.""" global iteration, result iteration += 1 # Stop iterating after max_iterations if iteration &gt;= max_iterations: timer.stop() print "Output is", result else: result = get_next(result)# register event handlerstimer = simplegui.create_timer(1, update)# start programinit(13)timer.start() Multiplication by 2: 2n Exponentiation: 2n Cosine of n Logarithm base 2 Square: n2 Multiplicative inverse: 1/n Square root of n1 point8.Given any initial natural number, consider the sequence of numbers generated by repeatedly following the rule: divide by two if the number is even ormultiply by 3 and add 1 if the number is odd.The Collatz conjecture states that this sequence always terminates at 1. For example, the sequence generated by 23 is: 23, 70, 35, 106, 53, 160, 80, 40, 20, 10, 5, 16, 8, 4, 2, 1 Write a Python program that takes a global variable n and uses a timer callback to repeatedly apply the rule above to n. Use the code from the previous question as a template. I suggest that your code prints out the sequence of numbers generated by this rule. Run this program for n = 217. What is the largest number in the sequence generated by this starting value? To test your code, starting at n = 23 generates a sequence with a maximum value of 160. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# Mystery computation in Python# Takes input n and computes output named resultimport simplegui# global stateresult = 1iteration = 0max_iterations = 50l = []# helper functionsdef init(start): """Initializes n.""" global n n = start l.append(n) print "Input is", ndef get_next(current): """??? Part of mystery computation.""" if current % 2 == 0: c = current / 2 elif current % 2 == 1: c = current * 3 + 1 global l l.append(c) #print c return c# timer callbackdef update(): """??? Part of mystery computation.""" global iteration, n iteration += 1 #print "result is",result # Stop iterating after max_iterations if (iteration &gt;= max_iterations) or (n==1): timer.stop() print "Output is", n print sorted(l, reverse=True) else: n = get_next(n)# register event handlerstimer = simplegui.create_timer(1, update)# start programinit(217)timer.start() 1 point9.CodeSkulptor runs your Python code by converting it into Javascript when you click the “Run” button and then executing this Javascript in your web browser. Open this example and run the provided code. If the SimpleGUI frame is spawned as a separate window, you should see an animation of an explosion in the canvas for this frame. If the SimpleGUI frame is spawned as a separate tab on top of the existing window containing the code (as happens in some browser configurations), the animation will “freeze” and a single static image is displayed. (If the SimpleGUI frame spawns as a separate window, you can also cause the animation to freeze by opening a new tab on top of the code window.) As explained in the FAQ (check the Resources tab on the left), what is the explanation for this behavior? Modern browser don’t support running Javascript in multiple windows simultaneously. This situation causes the animation to freeze. To save resources, modern browsers only execute the Javascript associated with the topmost tab of a window. The animation freezes since the code tab and its associated Javascript is no longer the topmost tab. Javascript and Python are incompatible languages. As a result, the Python in one tab can’t run at the same time as the Javascript in the SimpleGUI frame. Mini-project #3 - Stopwatch: The GameMini-project Video9 minhttp://www.codeskulptor.org/#examples-stopwatch_template.py1234567891011121314151617181920212223242526272829# template for "Stopwatch: The Game"# define global variables# define helper function format that converts time# in tenths of seconds into formatted string A:BC.Ddef format(t): pass # define event handlers for buttons; "Start", "Stop", "Reset"# define event handler for timer with 0.1 sec interval# define draw handler # create frame# register event handlers# start frame# Please remember to review the grading rubric Mini-project Description10 minCode Clinic Tips10 minThis item will be unlocked when the session begins.Peer-graded Assignment: Stopwatch: The Game2hhttp://www.codeskulptor.org/#user43_VIajy7NaN1_0.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# template for "Stopwatch: The Game"import simplegui# define global variablesc = 0message = "0:00.0"x = 0y = 0# define helper function format that converts time# in tenths of seconds into formatted string A:BC.Ddef format(t): abc = t / 10 d = t % 10 a = abc / 60 bc = abc % 60 if len(str(bc)) == 1: return str(a)+':0'+str(bc)+'.'+str(d) elif len(str(bc)) == 2: return str(a)+':'+str(bc)+'.'+str(d) # define event handlers for buttons; "Start", "Stop", "Reset"def startb(): timer.start() def stopb(): timer.stop() global x,y y = y + 1 if c % 10 == 0: x = x + 1 def resetb(): if timer.is_running(): timer.stop() global c, message,x,y c=0 message = "0:00.0" x=0 y=0 # define event handler for timer with 0.1 sec intervaldef tick(): global c, message c = c + 1 message = format(c) # define draw handlerdef draw(canvas): canvas.draw_text(message, [100,100], 36, "Red") canvas.draw_text(str(x)+'/'+str(y), [240,30], 36, "Red") # create frameframe = simplegui.create_frame("Stopwatch: The Game", 300, 200)# register event handlerstimer = simplegui.create_timer(100, tick)frame.set_draw_handler(draw)frame.add_button("Start", startb, 100)frame.add_button("Stop", stopb, 100)frame.add_button("Reset", resetb, 100)# start frameframe.start()# Please remember to review the grading rubric http://www.codeskulptor.org/#user43_VIajy7NaN1_1.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# template for "Stopwatch: The Game"import simplegui# define global variablesc = 0message = "0:00.0"x = 0y = 0# define helper function format that converts time# in tenths of seconds into formatted string A:BC.Ddef format(t): abc = t / 10 d = t % 10 a = abc / 60 bc = abc % 60 if len(str(bc)) == 1: return str(a)+':0'+str(bc)+'.'+str(d) elif len(str(bc)) == 2: return str(a)+':'+str(bc)+'.'+str(d) # define event handlers for buttons; "Start", "Stop", "Reset"def startb(): timer.start() def stopb(): if timer.is_running(): timer.stop() global x,y y = y + 1 if c % 10 == 0: x = x + 1 def resetb(): timer.stop() global c, message,x,y c=0 message = "0:00.0" x=0 y=0 # define event handler for timer with 0.1 sec intervaldef tick(): global c, message c = c + 1 message = format(c) # define draw handlerdef draw(canvas): canvas.draw_text(message, [100,100], 36, "Red") canvas.draw_text(str(x)+'/'+str(y), [240,30], 36, "Red") # create frameframe = simplegui.create_frame("Stopwatch: The Game", 300, 200)# register event handlerstimer = simplegui.create_timer(100, tick)frame.set_draw_handler(draw)frame.add_button("Start", startb, 100)frame.add_button("Stop", stopb, 100)frame.add_button("Reset", resetb, 100)# start frameframe.start()# Please remember to review the grading rubric Review Your Peers: Stopwatch: The GameWeek 5 Week 4 - Lists, keyboard input, the basics of modeling motionLearn the basics of lists in Python, model moving objects in Python, recreate the classic arcade game “Pong” Week 4a - Basics of ListsLists11 minKeyboard Input9 minhttp://www.codeskulptor.org/#examples-keyboard_echo.py1234567891011121314151617181920212223242526272829303132333435# Keyboard echoimport simplegui# initialize statecurrent_key = ' '# event handlersdef keydown(key): global current_key current_key = chr(key) def keyup(key): global current_key current_key = ' ' def draw(c): # NOTE draw_text now throws an error on some non-printable characters # Since keydown event key codes do not all map directly to # the printable character via ord(), this example now restricts # keys to alphanumerics if current_key in "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789": c.draw_text(current_key, [10, 25], 20, "Red") # create frame f = simplegui.create_frame("Echo", 35, 35)# register event handlersf.set_keydown_handler(keydown)f.set_keyup_handler(keyup)f.set_draw_handler(draw)# start framef.start() http://www.codeskulptor.org/#examples-position_control.py1234567891011121314151617181920212223242526272829303132333435# control the position of a ball using the arrow keysimport simplegui# Initialize globalsWIDTH = 600HEIGHT = 400BALL_RADIUS = 20ball_pos = [WIDTH / 2, HEIGHT / 2]# define event handlersdef draw(canvas): canvas.draw_circle(ball_pos, BALL_RADIUS, 2, "Red", "White")def keydown(key): vel = 4 if key == simplegui.KEY_MAP["left"]: ball_pos[0] -= vel elif key == simplegui.KEY_MAP["right"]: ball_pos[0] += vel elif key == simplegui.KEY_MAP["down"]: ball_pos[1] += vel elif key == simplegui.KEY_MAP["up"]: ball_pos[1] -= vel # create frame frame = simplegui.create_frame("Positional ball control", WIDTH, HEIGHT)# register event handlersframe.set_draw_handler(draw)frame.set_keydown_handler(keydown)# start frameframe.start() Motion14 minhttp://www.codeskulptor.org/#examples-motion_explicit.py12345678910111213141516171819202122232425262728293031323334353637383940# Ball motion with an explicit timerimport simplegui# Initialize globalsWIDTH = 600HEIGHT = 400BALL_RADIUS = 20init_pos = [WIDTH / 2, HEIGHT / 2]vel = [0, 3] # pixels per ticktime = 0# define event handlersdef tick(): global time time = time + 1def draw(canvas): # create a list to hold ball position ball_pos = [0, 0] # calculate ball position ball_pos[0] = init_pos[0] + time * vel[0] ball_pos[1] = init_pos[1] + time * vel[1] # draw ball canvas.draw_circle(ball_pos, BALL_RADIUS, 2, "Red", "White")# create frameframe = simplegui.create_frame("Motion", WIDTH, HEIGHT)# register event handlersframe.set_draw_handler(draw)timer = simplegui.create_timer(100, tick)# start frameframe.start()timer.start() http://www.codeskulptor.org/#examples-motion_implicit.py1234567891011121314151617181920212223242526272829# Ball motion with an implicit timerimport simplegui# Initialize globalsWIDTH = 600HEIGHT = 400BALL_RADIUS = 20ball_pos = [WIDTH / 2, HEIGHT / 2]vel = [0, 1] # pixels per update (1/60 seconds)# define event handlersdef draw(canvas): # Update ball position ball_pos[0] += vel[0] ball_pos[1] += vel[1] # Draw ball canvas.draw_circle(ball_pos, BALL_RADIUS, 2, "Red", "White")# create frameframe = simplegui.create_frame("Motion", WIDTH, HEIGHT)# register event handlersframe.set_draw_handler(draw)# start frameframe.start() Collisions and Reflections11 minhttp://www.codeskulptor.org/#examples-collisions_and_reflections.py1234567891011121314151617181920212223242526272829303132import simplegui# Initialize globalsWIDTH = 600HEIGHT = 400BALL_RADIUS = 20ball_pos = [WIDTH / 2, HEIGHT / 2]vel = [-40.0 / 60.0, 5.0 / 60.0]# define event handlersdef draw(canvas): # Update ball position ball_pos[0] += vel[0] ball_pos[1] += vel[1] # collide and reflect off of left hand side of canvas if ball_pos[0] &lt;= BALL_RADIUS: vel[0] = - vel[0] # Draw ball canvas.draw_circle(ball_pos, BALL_RADIUS, 2, "Red", "White")# create frameframe = simplegui.create_frame("Ball physics", WIDTH, HEIGHT)# register event handlersframe.set_draw_handler(draw)# start frameframe.start() Practice Exercises for Lists (optional)10 minQuiz: Quiz 4a9 questionsQUIZQuiz 4a9 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 19, 11:59 PM PST 1 point1.One of the tasks that you will engage in when learning a new programming language is locating the name of a built-in function that performs a common, simple operation. While you might be tempted to write your own code that performs this operation, locating a built-in function is usually preferable since the built-in version is automatically correct and others that read your code will immediately recognize what your code is doing. Python has a built-in function that adds up the numbers in a list. For example, given the list [1, 2, 5, 4], this function returns 1 + 2 + 5 + 4 = 12. Use your search skills to find the name of this built-in function. Enter the name of the built-in function below, without any parentheses or arguments. (Note that we could just tell you the name of this function. However, the point of this problem is for you to start learning how to locate useful language features on your own.) 1 point2.Let my_list be the list [“This”, “course”, “is”, “great”]. What is len(my_list)?What non-negative number is the index of “great”? I.e., how would you replace the question marks in my_list[???] so that the resulting value is “great”?Submit two numbers, one for each of these two questions, separated by spaces. 1 point3.Let my_list be the list [“This”, “course”, “is”, “great”]. We can use Python’s slice notation to get part of this list. What non-negative numbers can be used to get the slice [“course”, “is”]? I.e., what two non-negative numbers should we put in my_list[??? : ???] to get that result? Submit the two numbers in order, separated only by spaces. 1 point4.If we want to split a list my_list into two halves, which of the following uses slices to do so correctly? More precisely, if the length of my_list is 2n, i.e., even, then the two parts should each have length n. If its length is 2n+1, i.e., odd, then the two parts should have lengths n and n+1. my_list[: len(my_list) // 2] and my_list[len(my_list) // 2 :] my_list[0 : len(my_list) // 2] and my_list[len(my_list) // 2 + 1 : len(my_list)] my_list[: len(my_list) // 2 - 1] and my_list[len(my_list) // 2 :] my_list[0 : len(my_list) // 2] and my_list[len(my_list) // 2 : len(my_list)]1 point5.What is the distance between point [4, 7] and the nearest point on the circle centered at [2, 9] with radius 2? Provide at least 4 digits of accuracy. Hint: The distance between a point and a circle is the distance between the point and the center of the circle minus the radius of the circle. You can use the point-to-point distance code described in this week’s videos. 1 point6.A ball with velocity [4, 2] reflects off a vertical wall. What is its new velocity? [-4, 2] [4, -2] [4, 2] [-4, -2]1 point7.Which of the following illustrate how to properly structure a keydown or keyup event handler? (For more advanced Python programmers, assume that you have just imported simplegui and haven’t used from.) 123def keydown_handler(key): if "left" == simplegui.KEY_MAP[key]: … 123def keydown_handler(key): if key == simplegui.KEY_MAP["left"]: … 123def keydown_handler(key): if "left" == KEY_MAP[key]: … 123def keydown_handler(key): if key == KEY_MAP["left"]: … 1 point8.Assume you have a program with a keydown handler. You run it, and press a single key and hold it down continuously. How many times does the keydown handler get called? Experiment in CodeSkulptor to find out. 2 — once at the beginning and once when you release the key 1 Unlimited — i.e., repeatedly until you finally release the key1 point9.Several keys on the keyboard, such as Shift, CapsLock, and Ctrl, typically act to modify what happens when you press other keys, rather than doing anything on their own. When using the SimpleGUI keydown handler, how are such keys treated? Experiment in CodeSkulptor to find out. Independent key press events — e.g., pressing Shift by itself creates an event No effect — e.g., pressing the Shift key does not create or modify the behavior of any event. Modify other key presses — e.g., pressing the ‘a’ key creates an event with a different value than pressing Shift and ‘a’ together. Week 4b - Keyboard ControlVelocity Control9 minhttp://www.codeskulptor.org/#examples-position_control.py1234567891011121314151617181920212223242526272829303132333435# control the position of a ball using the arrow keysimport simplegui# Initialize globalsWIDTH = 600HEIGHT = 400BALL_RADIUS = 20ball_pos = [WIDTH / 2, HEIGHT / 2]# define event handlersdef draw(canvas): canvas.draw_circle(ball_pos, BALL_RADIUS, 2, "Red", "White")def keydown(key): vel = 4 if key == simplegui.KEY_MAP["left"]: ball_pos[0] -= vel elif key == simplegui.KEY_MAP["right"]: ball_pos[0] += vel elif key == simplegui.KEY_MAP["down"]: ball_pos[1] += vel elif key == simplegui.KEY_MAP["up"]: ball_pos[1] -= vel # create frame frame = simplegui.create_frame("Positional ball control", WIDTH, HEIGHT)# register event handlersframe.set_draw_handler(draw)frame.set_keydown_handler(keydown)# start frameframe.start() http://www.codeskulptor.org/#examples-velocity_control.py12345678910111213141516171819202122232425262728293031323334353637383940414243# control the velocity of a ball using the arrow keysimport simplegui# Initialize globalsWIDTH = 600HEIGHT = 400BALL_RADIUS = 20ball_pos = [WIDTH / 2, HEIGHT / 2]vel = [0, 0]# define event handlersdef draw(canvas): # Update ball position ball_pos[0] += vel[0] ball_pos[1] += vel[1] # Draw ball canvas.draw_circle(ball_pos, BALL_RADIUS, 2, "Red", "White")def keydown(key): acc = 1 if key==simplegui.KEY_MAP["left"]: vel[0] -= acc elif key==simplegui.KEY_MAP["right"]: vel[0] += acc elif key==simplegui.KEY_MAP["down"]: vel[1] += acc elif key==simplegui.KEY_MAP["up"]: vel[1] -= acc print ball_pos # create frame frame = simplegui.create_frame("Velocity ball control", WIDTH, HEIGHT)# register event handlersframe.set_draw_handler(draw)frame.set_keydown_handler(keydown)# start frameframe.start() Visualizing Lists and Mutation5 minhttp://www.codeskulptor.org/viz/#examples_lists_mutation.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#################################### Mutation vs. assignment################# Look alike, but differenta = [4, 5, 6]b = [4, 5, 6]print "Original a and b:", a, bprint "Are they same thing?", a is ba[1] = 20print "New a and b:", a, bprint################# Aliasedc = [4, 5, 6]d = cprint "Original c and d:", c, dprint "Are they same thing?", c is dc[1] = 20print "New c and d:", c, dprint################# Copiede = [4, 5, 6]f = list(e)print "Original e and f:", e, fprint "Are they same thing?", e is fe[1] = 20print "New e and f:", e, fprint#################################### Interaction with globalsa = [4, 5, 6]def mutate_part(x): a[1] = xdef assign_whole(x): a = xdef assign_whole_global(x): global a a = xmutate_part(100)print aassign_whole(200)print aassign_whole_global(300)print a Programming Tips - 43 minhttp://www.codeskulptor.org/#examples-tips4.py123456789101112131415161718#################################### Lists (mutable) vs. tuples (immutable)print [4, 5, 6]print (4, 5, 6)print type([4, 5, 6])print type((4, 5, 6))a = [4, 5, 6]a[1] = 100print ab = (4, 5, 6)b[1] = 100print b Practice Exercises for Keyboard (optional)10 minQuiz: Quiz 4b9 questionsQUIZQuiz 4b9 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 19, 11:59 PM PST 1 point1.In Python, [1, 2, 3] is of type list. What is the name of the type of (1, 2, 3)? Pair Triple Array Tuple Set1 point2.Which of the following types of data are immutable in Python? Tuples Strings Numbers Lists Booleans1 point3.Which of the following functions must include a global point declaration in order to change the global variable point?123456point = [0, 0]def function1(): point[0] += 1 point[1] += 2def function2(): point = [50, 50] function2 function11 point4.Consider the following three similar programs. Read them carefully to observe their differences.12345a = range(5)def mutate(a): a[3] = 100mutate(a)print a[3] 12345a = range(5)def mutate(b): a[3] = 100mutate(a)print a[3] 12345a = range(5)def mutate(b): b[3] = 100mutate(a)print a[3] We would like to know whether these all accomplish the same thing. What are the three values, respectively, printed by these three pieces of code? Separate the values only with spaces. 1 point5.In our program, the variable position represents a 2D position on the canvas. We want to be able to change the position by some amount in variable delta. Why is the following code snippet incorrect?1234position = [50, 50]delta = [1, -2]…position = position + delta Note that the ellipses represent that we might have code in between what is shown, but such code is irrelevant and omitted. The numbers in position cannot be changed. One of the elements of delta is negative. The + operator on lists does not mean addition of the numbers in a list. Lists do not support the + operator.1 point6.Consider the following program.12345678a = ["green", "blue", "white", "black"]b = ac = list(a)d = ca[3] = "red"c[2] = a[1]b = a[1 : 3]b[1] = c[2] At the end of this code, to how many list objects do the variables refer? If you run the code and print the variables’ values, you can begin to answer this question. After all, if two variables print differently, they certainly can’t refer to the same object. However, if two variables print the same, you still need to determine whether they refer to the same object. One way is to step through the code while drawing reference diagrams. Another is to mutate one and see if others also mutate. 1 — The four variables each refer to the same list. 3 2 4 — The four variables each refer to different lists.1 point7.Convert the following specification into code. Do the point and rectangle ever overlap? A point starts at [10, 20]. It repeatedly changes position by [3, 0.7] — e.g., under button or timer control. Meanwhile, a rectangle stays in place. Its corners are at [50, 50] (upper left), [180, 50] (upper right), [180, 140] (lower right), and [50, 140] (lower left). To check for overlap, i.e., collision, just run your code and check visually. You do not need to implement a point-rectangle collision test. However, we encourage you to think about how you would implement such a test. Yes No1 point8.Assume we are using acceleration control for a spaceship in a game. That is, we regularly have the following updates: The position is incremented by the time interval multiplied by the velocity. This happens on each draw event.The velocity is incremented by the time interval multiplied by the acceleration. This happens on each draw event.The acceleration is periodically incremented by some fixed vector (the same vector for each step). This could happen on keyboard or timer events.Assume that, initially, the ship is stationary and subject to no acceleration. What sort of trajectory will the spaceship fly in? Either figure this out mathematically, or implement it in CodeSkulptor and see what happens. Unpredictable A non-linear, smooth curve Spiral A straight line1 point9.Write a Python program that initializes a global variable to 5. The keydown event handler updates this global variable by doubling it, while the keyup event handler updates it by decrementing it by 3. What is the value of the global variable after 12 separate key presses, i.e., pressing and releasing one key at a time, and repeating this 12 times in total? To test your code, the global variable’s value should be 35 after 4 key presses. Mini-project #4 - PongMini-project Video11 minhttp://www.codeskulptor.org/#examples-pong_template.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Implementation of classic arcade game Pongimport simpleguiimport random# initialize globals - pos and vel encode vertical info for paddlesWIDTH = 600HEIGHT = 400 BALL_RADIUS = 20PAD_WIDTH = 8PAD_HEIGHT = 80HALF_PAD_WIDTH = PAD_WIDTH / 2HALF_PAD_HEIGHT = PAD_HEIGHT / 2LEFT = FalseRIGHT = True# initialize ball_pos and ball_vel for new bal in middle of table# if direction is RIGHT, the ball's velocity is upper right, else upper leftdef spawn_ball(direction): global ball_pos, ball_vel # these are vectors stored as lists# define event handlersdef new_game(): global paddle1_pos, paddle2_pos, paddle1_vel, paddle2_vel # these are numbers global score1, score2 # these are intsdef draw(canvas): global score1, score2, paddle1_pos, paddle2_pos, ball_pos, ball_vel # draw mid line and gutters canvas.draw_line([WIDTH / 2, 0],[WIDTH / 2, HEIGHT], 1, "White") canvas.draw_line([PAD_WIDTH, 0],[PAD_WIDTH, HEIGHT], 1, "White") canvas.draw_line([WIDTH - PAD_WIDTH, 0],[WIDTH - PAD_WIDTH, HEIGHT], 1, "White") # update ball # draw ball # update paddle's vertical position, keep paddle on the screen # draw paddles # determine whether paddle and ball collide # draw scores def keydown(key): global paddle1_vel, paddle2_vel def keyup(key): global paddle1_vel, paddle2_vel# create frameframe = simplegui.create_frame("Pong", WIDTH, HEIGHT)frame.set_draw_handler(draw)frame.set_keydown_handler(keydown)frame.set_keyup_handler(keyup)# start framenew_game()frame.start() Mini-project Description10 minCode Clinic Tips10 minPeer-graded Assignment: Pong2hhttp://www.codeskulptor.org/#user43_6MWjxkY0pr_0.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158# Implementation of classic arcade game Pongimport simpleguiimport random# initialize globals - pos and vel encode vertical info for paddlesWIDTH = 600HEIGHT = 400 BALL_RADIUS = 20PAD_WIDTH = 8PAD_HEIGHT = 80HALF_PAD_WIDTH = PAD_WIDTH / 2HALF_PAD_HEIGHT = PAD_HEIGHT / 2LEFT = FalseRIGHT = Trueball_line_width = 1ball_pos = [WIDTH / 2, HEIGHT / 2]ball_vel = [-1, -1] # pixels per ticktime = 0paddle1_pos = HEIGHT / 2paddle2_pos = HEIGHT / 2paddle1_vel, paddle2_vel = 0, 0score1, score2 = 0, 0# initialize ball_pos and ball_vel for new bal in middle of table# if direction is RIGHT, the ball's velocity is upper right, else upper leftdef spawn_ball(direction): global ball_pos, ball_vel # these are vectors stored as lists ball_pos = [WIDTH / 2, HEIGHT / 2] ball_vel[1] = - random.randrange(1, 3) if direction == RIGHT: ball_vel[0] = (random.randrange(2, 4)) #pass if direction == LEFT: ball_vel[0] = - (random.randrange(2, 4)) #pass # define event handlersdef new_game(): global paddle1_pos, paddle2_pos, paddle1_vel, paddle2_vel # these are numbers global score1, score2 # these are ints score1, score2 = 0, 0 spawn_ball(RIGHT) def draw(canvas): global score1, score2, paddle1_pos, paddle2_pos, ball_pos, ball_vel # draw mid line and gutters canvas.draw_line([WIDTH / 2, 0],[WIDTH / 2, HEIGHT], 1, "White") canvas.draw_line([PAD_WIDTH, 0],[PAD_WIDTH, HEIGHT], 1, "White") canvas.draw_line([WIDTH - PAD_WIDTH, 0],[WIDTH - PAD_WIDTH, HEIGHT], 1, "White") # hit the top or bottom if (ball_pos[1] + BALL_RADIUS + ball_line_width &gt;= HEIGHT) or \ (ball_pos[1] - BALL_RADIUS - ball_line_width &lt;= 0): ball_vel[1] = - ball_vel[1] # update ball ball_pos[0] += ball_vel[0] ball_pos[1] += ball_vel[1] # draw ball canvas.draw_circle(ball_pos, BALL_RADIUS, ball_line_width, 'White', 'White') # update paddle's vertical position, keep paddle on the screen # draw paddles if paddle1_pos + paddle1_vel &gt;= HALF_PAD_HEIGHT and \ paddle1_pos + paddle1_vel &lt;= HEIGHT - HALF_PAD_HEIGHT: paddle1_pos += paddle1_vel if paddle2_pos + paddle2_vel &gt;= HALF_PAD_HEIGHT and \ paddle2_pos + paddle2_vel &lt;= HEIGHT - HALF_PAD_HEIGHT: paddle2_pos += paddle2_vel #print paddle2_pos - HALF_PAD_HEIGHT canvas.draw_polygon([[0, paddle1_pos - HALF_PAD_HEIGHT], \ [PAD_WIDTH, paddle1_pos - HALF_PAD_HEIGHT], \ [PAD_WIDTH, paddle1_pos + HALF_PAD_HEIGHT], \ [0, paddle1_pos + HALF_PAD_HEIGHT]], 1, 'White', 'White') canvas.draw_polygon([[WIDTH - PAD_WIDTH, paddle2_pos - HALF_PAD_HEIGHT], \ [WIDTH, paddle2_pos - HALF_PAD_HEIGHT], \ [WIDTH, paddle2_pos + HALF_PAD_HEIGHT], \ [WIDTH - PAD_WIDTH, paddle2_pos + HALF_PAD_HEIGHT]], 1, 'White', 'White') # determine whether paddle and ball collide # hit the gutters if (ball_pos[0] + BALL_RADIUS + ball_line_width &gt;= WIDTH - PAD_WIDTH): # hit the paddle2 if ball_pos[1] &lt;= paddle2_pos + HALF_PAD_HEIGHT and \ ball_pos[1] &gt;= paddle2_pos - HALF_PAD_HEIGHT: ball_vel[0] = - 1.1 * ball_vel[0] ball_vel[1] = 1.1 * ball_vel[1] else: score1 += 1 spawn_ball(LEFT) if (ball_pos[0] - BALL_RADIUS - ball_line_width &lt;= PAD_WIDTH): # hit the paddle1 if ball_pos[1] &lt;= paddle1_pos + HALF_PAD_HEIGHT and \ ball_pos[1] &gt;= paddle1_pos - HALF_PAD_HEIGHT: ball_vel[0] = - 1.1 * ball_vel[0] ball_vel[1] = 1.1 * ball_vel[1] else: score2 += 1 spawn_ball(RIGHT) #print ball_vel[0] # draw scores canvas.draw_text(str(score1), (WIDTH/4, HEIGHT/4), 50, 'White') canvas.draw_text(str(score2), (WIDTH*3/4, HEIGHT/4), 50, 'White') def keydown(key): global paddle1_vel, paddle2_vel vel = 4 # palyer 1 if key == simplegui.KEY_MAP["w"]: paddle1_vel -= vel if key == simplegui.KEY_MAP["s"]: paddle1_vel += vel if key == simplegui.KEY_MAP["w"] and key == simplegui.KEY_MAP["s"]: pass # player 2 if key == simplegui.KEY_MAP["up"]: paddle2_vel -= vel if key == simplegui.KEY_MAP["down"]: paddle2_vel += vel if key == simplegui.KEY_MAP["down"] and key == simplegui.KEY_MAP["up"]: pass def keyup(key): global paddle1_vel, paddle2_vel # palyer 1 if key == simplegui.KEY_MAP["w"]: paddle1_vel = 0 if key == simplegui.KEY_MAP["s"]: paddle1_vel = 0 if key == simplegui.KEY_MAP["w"] and key == simplegui.KEY_MAP["s"]: pass # player 2 if key == simplegui.KEY_MAP["up"]: paddle2_vel = 0 if key == simplegui.KEY_MAP["down"]: paddle2_vel = 0 if key == simplegui.KEY_MAP["down"] and key == simplegui.KEY_MAP["up"]: pass# create frameframe = simplegui.create_frame("Pong", WIDTH, HEIGHT)frame.set_draw_handler(draw)frame.set_keydown_handler(keydown)frame.set_keyup_handler(keyup)new_game = frame.add_button('Restart', new_game, 100)# start frame#new_game()frame.start() http://www.codeskulptor.org/#user43_6MWjxkY0pr_1.py1234567def new_game(): global paddle1_pos, paddle2_pos, paddle1_vel, paddle2_vel # these are numbers global score1, score2 # these are ints paddle1_pos = HEIGHT / 2 paddle2_pos = HEIGHT / 2 score1, score2 = 0, 0 spawn_ball(RIGHT) Review Your Peers: PongAn Introduction to Interactive Programming in Python (Part 2)Lecture slides can be found [here]Coursera can be found here About this course: This two-part course is designed to help students with very little or no computing background learn the basics of building simple interactive applications. Our language of choice, Python, is an easy-to learn, high-level computer language that is used in many of the computational courses offered on Coursera. To make learning Python easy, we have developed a new browser-based programming environment that makes developing interactive applications in Python simple. These applications will involve windows whose contents are graphical and respond to buttons, the keyboard and the mouse. In part 2 of this course, we will introduce more elements of programming (such as list, dictionaries, and loops) and then use these elements to create games such as Blackjack. Part 1 of this class will culminate in building a version of the classic arcade game “Asteroids”. Upon completing this course, you will be able to write small, but interesting Python programs. The next course in the specialization will begin to introduce a more principled approach to writing programs and solving computational problems that will allow you to write larger and more complex programs. Week 5 - Mouse input, list methods, dictionariesWeek 5a - Mouse Input and More ListsIntroduction2 minMouse input12 minhttp://www.codeskulptor.org/#examples-mouse_input.py1234567891011121314151617181920212223242526272829303132333435363738# Examples of mouse inputimport simpleguiimport math# intialize globalsWIDTH = 450HEIGHT = 300ball_pos = [WIDTH / 2, HEIGHT / 2]BALL_RADIUS = 15ball_color = "Red"# helper functiondef distance(p, q): return math.sqrt( (p[0] - q[0]) ** 2 + (p[1] - q[1]) ** 2)# define event handler for mouse click, drawdef click(pos): global ball_pos, ball_color if distance(pos, ball_pos) &lt; BALL_RADIUS: ball_color = "Green" else: ball_pos = list(pos) ball_color = "Red"def draw(canvas): canvas.draw_circle(ball_pos, BALL_RADIUS, 1, "Black", ball_color)# create frameframe = simplegui.create_frame("Mouse selection", WIDTH, HEIGHT)frame.set_canvas_background("White")# register event handlerframe.set_mouseclick_handler(click)frame.set_draw_handler(draw)# start frameframe.start() List Methods11 minhttp://www.codeskulptor.org/#examples-list_methods.py1234567891011121314151617181920212223242526272829303132333435363738394041424344# Simple task listimport simpleguitasks = []# Handler for buttondef clear(): global tasks tasks = [] # Handler for new taskdef new(task): tasks.append(task) # Handler for remove numberdef remove_num(tasknum): n = int(tasknum) if n &gt; 0 and n &lt;= len(tasks): tasks.pop(n-1)# Handler for remove namedef remove_name(taskname): if taskname in tasks: tasks.remove(taskname) # Handler to draw on canvasdef draw(canvas): n = 1 for task in tasks: pos = 30 * n canvas.draw_text(str(n) + ": " + task, [5, pos], 24, "White") n += 1 # Create a frame and assign callbacks to event handlersframe = simplegui.create_frame("Task List", 600, 400)frame.add_input("New task:", new, 200)frame.add_input("Remove task number:", remove_num, 200)frame.add_input("Remove task:", remove_name, 200)frame.add_button("Clear All", clear)frame.set_draw_handler(draw)# Start the frame animationframe.start() List Examples11 minhttp://www.codeskulptor.org/#examples-list_of_balls.py12345678910111213141516171819202122232425262728293031323334353637383940# Examples of mouse inputimport simpleguiimport math# intialize globalswidth = 450height = 300ball_list = []ball_radius = 15ball_color = "Red"# helper functiondef distance(p, q): return math.sqrt((p[0] - q[0]) ** 2 + (p[1] - q[1]) ** 2)# define event handler for mouse click, drawdef click(pos): ball_list.append(pos)# if distance(ball_pos, pos) &lt; ball_radius:# if ball_color == "Red":# ball_color = "Green"# else:# ball_pos = [pos[0], pos[1]]# ball_color = "Red"def draw(canvas): for ball_pos in ball_list: canvas.draw_circle(ball_pos, ball_radius, 1, "Black", ball_color) # create frameframe = simplegui.create_frame("Mouse selection", width, height)frame.set_canvas_background("White")# register event handlerframe.set_mouseclick_handler(click)frame.set_draw_handler(draw)# start frameframe.start() http://www.codeskulptor.org/#examples-list_selection.py12345678910111213141516171819202122232425262728293031323334353637383940# Examples of mouse inputimport simpleguiimport math# intialize globalswidth = 450height = 300ball_list = []ball_radius = 15# helper functiondef distance(p, q): return math.sqrt((p[0] - q[0]) ** 2 + (p[1] - q[1]) ** 2)# define event handler for mouse click, drawdef click(pos): changed = False for ball in ball_list: if distance([ball[0], ball[1]], pos) &lt; ball_radius: ball[2] = "Green" changed = True if not changed: ball_list.append([pos[0], pos[1], "Red"])def draw(canvas): for ball in ball_list: canvas.draw_circle([ball[0], ball[1]], ball_radius, 1, "Black", ball[2]) # create frameframe = simplegui.create_frame("Mouse selection", width, height)frame.set_canvas_background("White")# register event handlerframe.set_mouseclick_handler(click)frame.set_draw_handler(draw)# start frameframe.start() http://www.codeskulptor.org/#examples-list_removal.py12345678910111213141516171819202122232425262728293031323334353637383940414243# Examples of mouse inputimport simpleguiimport math# intialize globalswidth = 450height = 300ball_list = []ball_radius = 15ball_color = "Red"# helper functiondef distance(p, q): return math.sqrt((p[0] - q[0]) ** 2 + (p[1] - q[1]) ** 2)# define event handler for mouse click, drawdef click(pos): remove = [] for ball in ball_list: if distance(ball, pos) &lt; ball_radius: remove.append(ball) if remove == []: ball_list.append(pos) else: for ball in remove: ball_list.pop(ball_list.index(ball))def draw(canvas): for ball in ball_list: canvas.draw_circle([ball[0], ball[1]], ball_radius, 1, "Black", ball_color) # create frameframe = simplegui.create_frame("Mouse selection", width, height)frame.set_canvas_background("White")# register event handlerframe.set_mouseclick_handler(click)frame.set_draw_handler(draw)# start frameframe.start() Iteration12 minhttp://www.codeskulptor.org/#examples-iteration.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# Iterating over listsdef count_odd(numbers): count = 0 for num in numbers: if num % 2 == 1: count += 1 return countdef check_odd(numbers): for num in numbers: if num % 2 == 1: return True return Falsedef remove_odd(numbers): for num in numbers: if num % 2 == 1: numbers.remove(num)def remove_odd2(numbers): remove = [] for num in numbers: if num % 2 == 1: remove.append(numbers.index(num)) for idx in remove: numbers.pop(idx) def remove_odd3(numbers): remove = [] for num in numbers: if num % 2 == 1: remove.append(num) for num in remove: numbers.remove(num) def remove_odd4(numbers): newnums = [] for num in numbers: if num % 2 == 0: newnums.append(num) return newnums def remove_last_odd(numbers): has_odd = False last_odd = 0 for num in numbers: if num % 2 == 1: has_odd = True last_odd = num if has_odd: numbers.remove(last_odd) def run(): numbers = [1, 7, 2, 34, 8, 7, 2, 5, 14, 22, 93, 48, 76, 15, 7] print numbers remove_last_odd(numbers) print numbers run() Practice Exercises for Mouse and List Methods (optional)10 minQuiz: Quiz 5a9 questionsQUIZQuiz 5a9 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 19, 11:59 PM PST 1 point1.What is the type of the parameter for a mouseclick handler? Refer to the CodeSkulptor documentation. There is no parameter. Boolean String Number List Tuple1 point2.Which of the following expressions mutate, i.e., change, list my_list? If you’ve forgotten what the operations do, you can look in the CodeSkulptor documentation. my_list + [10, 20] my_list.reverse() my_list.append(10) another_list.extend(my_list) my_list.extend([10, 20])1 point3.We want to remove the element at the front of a list. For example, we want the following code to print “apple” and [“pear”, “blueberry”], respectively. What function or method call should replace the question marks?123fruits = ["apple", "pear", "blueberry"]fruit = ???print fruit, fruits fruits.remove(0) fruits[1:] fruits[0] fruits.pop(0) fruits.pop() fruits.remove(“apple”)1 point4.Which of the following uses of range() will generate the list [2, 5, 8, 11, 14]? First, think about what each of these returns, but also try each in CodeSkulptor. range(2, 15) * 3 range(1, 15, 3) range(2, 15, 3)1 point5.To correctly compute the product of a list numbers of numbers, what statement should replace the question marks?1234numbers = …???for n in numbers: product *= n product = numbers[1] product = numbers[0] product = 1 product = [] product = 01 point6.We can loop over strings, too! The following incomplete function is a simple, but inefficient, way to reverse a string. What line of code needs to replace the questions marks for the code to work correctly?1234567def reverse_string(s): """Returns the reversal of the given string.""" ??? for char in s: result = char + result return resultprint reverse_string("hello") result = [] result = “” result = 0 result = “ “1point Imagine a game on a map. At the beginning, we might want to randomly assign each player a starting point. Which of the following expressions may we use in place of the question marks to correctly implement this functionality?1234567891011import randomdef random_point(): """Returns a random point on a 100x100 grid.""" return (random.randrange(100), random.randrange(100))def starting_points(players): """Returns a list of random points, one for each player.""" points = [] for player in players: point = random_point() ??? return points point.append(points) points.append(point) points + point points += point points.extend(point) point.extend(points)1 point8.The following function is supposed to check whether the given list of numbers is in ascending order. For example, we want is_ascending([2, 6, 9, 12, 400]) to return True while is_ascending([4, 8, 2, 13]) should return False.123456def is_ascending(numbers): """Returns whether the given list of numbers is in ascending order.""" for i in range(len(numbers)): if numbers[i+1] &lt; numbers[i]: return False return True However, the function doesn’t quite work. Try it on the suggested tests to verify this for yourself. The easiest fix is to make a small change to the highlighted code. What should it be replaced with? range(len(numbers)) - 1 range(len(numbers) - 1) range(len(numbers - 1)) range(1, len(numbers))1 point9.Turn the following English description into code: Create a list with two numbers, 0 and 1, respectively.For 40 times, add to the end of the list the sum of the last two numbers.What is the last number in the list? To test your code, if you repeat 10 times, rather than 40, your answer should be 89. Week 5b - Dictionaries and ImagesDictionaries12 minhttp://www.codeskulptor.org/#examples-dictionaries.py123456789101112131415161718192021222324252627282930313233343536373839# Cipherimport simpleguiCIPHER = &#123;'a': 'x', 'b': 'c', 'c': 'r', 'd': 'm', 'e': 'l'&#125;message = ""# Encode buttondef encode(): emsg = "" for ch in message: emsg += CIPHER[ch] print message, "encodes to", emsg# Decode buttondef decode(): dmsg = "" for ch in message: for key, value in CIPHER.items(): if ch == value: dmsg += key print message, "decodes to", dmsg# Update message inputdef newmsg(msg): global message message = msg label.set_text(msg) # Create a frame and assign callbacks to event handlersframe = simplegui.create_frame("Cipher", 2, 200, 200)frame.add_input("Message:", newmsg, 200)label = frame.add_label("", 200)frame.add_button("Encode", encode)frame.add_button("Decode", decode)# Start the frame animationframe.start() Images11 minhttp://www.codeskulptor.org/#examples-images.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# Demonstration of a magnifier on a mapimport simplegui# 1521x1818 pixel map of native American language# source - Gutenberg projectimage = simplegui.load_image("http://commondatastorage.googleapis.com/codeskulptor-assets/gutenberg.jpg")# Image dimensionsMAP_WIDTH = 1521MAP_HEIGHT = 1818# Scaling factorSCALE = 3# Canvas sizeCAN_WIDTH = MAP_WIDTH // SCALECAN_HEIGHT = MAP_HEIGHT // SCALE# Size of magnifier pane and initial centerMAG_SIZE = 120mag_pos = [CAN_WIDTH // 2, CAN_HEIGHT // 2]# Event handlers# Move magnifier to clicked positiondef click(pos): global mag_pos mag_pos = list(pos)# Draw map and magnified regiondef draw(canvas): # Draw map canvas.draw_image(image, [MAP_WIDTH // 2, MAP_HEIGHT // 2], [MAP_WIDTH, MAP_HEIGHT], [CAN_WIDTH // 2, CAN_HEIGHT // 2], [CAN_WIDTH, CAN_HEIGHT]) # Draw magnifier map_center = [SCALE * mag_pos[0], SCALE * mag_pos[1]] map_rectangle = [MAG_SIZE, MAG_SIZE] mag_center = mag_pos mag_rectangle = [MAG_SIZE, MAG_SIZE] canvas.draw_image(image, map_center, map_rectangle, mag_center, mag_rectangle) # Create frame for scaled mapframe = simplegui.create_frame("Map magnifier", CAN_WIDTH, CAN_HEIGHT)# register even handlersframe.set_mouseclick_handler(click) frame.set_draw_handler(draw)# Start frameframe.start() Visualizing Iteration13 minhttp://www.codeskulptor.org/viz/#examples_iteration_lists.py12345678910111213141516171819202122232425262728293031323334def square_list1(numbers): """Returns a list of the squares of the numbers in the input.""" result = [] for n in numbers: result.append(n ** 2) return resultdef square_list2(numbers): """Returns a list of the squares of the numbers in the input.""" return [n ** 2 for n in numbers]print square_list1([4, 5, -2])print square_list2([4, 5, -2])def is_in_range(ball): """Returns whether the ball is in the desired range. """ return ball[0] &gt;= 0 and ball[0] &lt;= 100 and ball[1] &gt;= 0 and ball[1] &lt;= 100def balls_in_range1(balls): """Returns a list of those input balls that are within the desired range.""" result = [] for ball in balls: if is_in_range(ball): result.append(ball) return resultdef balls_in_range2(balls): return [ball for ball in balls if is_in_range(ball)]print balls_in_range1([[-5,40], [30,20], [70,140], [60,50]])print balls_in_range2([[-5,40], [30,20], [70,140], [60,50]]) http://www.codeskulptor.org/viz/#examples-dictionaries.py123456789101112131415161718192021222324252627282930313233343536373839# Cipherimport simpleguiCIPHER = &#123;'a': 'x', 'b': 'c', 'c': 'r', 'd': 'm', 'e': 'l'&#125;message = ""# Encode buttondef encode(): emsg = "" for ch in message: emsg += CIPHER[ch] print message, "encodes to", emsg# Decode buttondef decode(): dmsg = "" for ch in message: for key, value in CIPHER.items(): if ch == value: dmsg += key print message, "decodes to", dmsg# Update message inputdef newmsg(msg): global message message = msg label.set_text(msg) # Create a frame and assign callbacks to event handlersframe = simplegui.create_frame("Cipher", 2, 200, 200)frame.add_input("Message:", newmsg, 200)label = frame.add_label("", 200)frame.add_button("Encode", encode)frame.add_button("Decode", decode)# Start the frame animationframe.start() Programming Tips - 510 minPractice Exercises for Dictionaries and Images (optional)10 minQuiz: Quiz 5b9 questionsQUIZQuiz 5b9 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 19, 11:59 PM PST 1 point1.Which of the following expressions corresponds to a dictionary with no elements? () {} None [] &lt;&gt; dict()1 point2.Given an existing dictionary favorites, what Python statement adds the key “fruit” to this dictionary with the corresponding value “blackberry”? favorites{“fruit” : “blackberry”} favorites[“fruit” = “blackberry”] favorites[“fruit” : “blackberry”] favorites[“fruit”] = “blackberry” favorites = {“fruit” : “blackberry”}1 point3.Keys in a dictionary can have which of the following types? Lists Booleans Strings Dictionaries Numbers Tuples1 point4.Values in a dictionary can have which of the following types? Strings Booleans Tuples Dictionaries Numbers Lists1 point5.We often want to loop over all the key/value pairs in a dictionary. Assume the variable my_dict stores a dictionary. One way of looping like this is as follows:123for key in my_dict: value = my_dict[key] … However, there is a better way. We can instead write the following:12for key, value in ???: … What code should replace the question marks so that the two forms are equivalent? Refer to the video on dictionaries or the CodeSkulptor documentation. list(my_dict) my_dict.values() my_dict.items() my_dict.keys() my_dict.keys_values() items(my_dict)1 point6.Conceptually, the purpose of a dictionary is to represent a relationship between two collections of data — each key in the dictionary is related to one value. Which of the following situations are instances of such a relationship? Do not include situations where you have to introduce additional information in order to fit them into such a relationship. Storing where each person lives Storing a sensor’s data samples Storing x and y coordinates of an arbitrary collection of 2-dimensional points Storing x and y coordinates of 2-dimensional points taken from a function, so that each x coordinate occurs at most once.1 point7.In the previous quiz, you were asked to complete the following code:1234567891011import randomdef random_point(): """Returns a random point on a 100x100 grid.""" return (random.randrange(100), random.randrange(100))def starting_points(players): """Returns a list of random points, one for each player.""" points = [] for player in players: point = random_point() ??? return points Now, we want to rewrite starting_points using a list comprehension. Which list comprehensions could replace the following question marks?123def starting_points(players): """Returns a list of random points, one for each player.""" return ??? Refer to this week’s “Visualizing iteration” video for examples of list comprehensions. Also, try each example in CodeSkulptor before answering the question. [random_point for players] [random_point(player) for player in players] [for player in players: random_point()] [random_point() for player in players] [random_point() for p in players] [random_point for player in players]1 point8.You have the following code. The goal is to display a portion of the image, rescaling it to fill the canvas.123456789101112import simpleguiframe_size = [200, 200]image_size = [1521, 1818]def draw(canvas): canvas.draw_image(image, image_size, [image_size[0] / 2, image_size[1] / 2], [frame_size[0] / 2, frame_size[1] / 2], frame_size)frame = simplegui.create_frame("test", frame_size[0], frame_size[1])frame.set_draw_handler(draw)image = simplegui.load_image("http://commondatastorage.googleapis.com/codeskulptor-assets/gutenberg.jpg")frame.start() Run it, and observe that nothing is displayed in the frame. What is the problem? The source arguments in draw_image are incorrect. We are trying to load pixels that are not within the image, and thus the draw fails. The destination arguments in draw_image are incorrect. We aren’t specifying values that would draw the image on this size canvas. The file doesn’t exist. The file is not an image. One or more of the draw_image arguments are of the wrong type.1 point9.Write a CodeSkulptor program that loads and draws the following image:1http://commondatastorage.googleapis.com/codeskulptor-assets/alphatest.png with a source center of [220, 100] and a source size of [100, 100]. What one word appears in the canvas? If a letter is capitalized in the image, enter it as a capital. Note that you do have to position the image as stated to see the correct word. Mini-project #5 - MemoryMini-project Video12 minhttp://www.codeskulptor.org/#examples-memory_template.py123456789101112131415161718192021222324252627282930313233343536# implementation of card game - Memoryimport simpleguiimport random# helper function to initialize globalsdef new_game(): pass # define event handlersdef mouseclick(pos): # add game state logic here pass # cards are logically 50x100 pixels in size def draw(canvas): pass# create frame and add a button and labelsframe = simplegui.create_frame("Memory", 800, 100)frame.add_button("Reset", new_game)label = frame.add_label("Turns = 0")# register event handlersframe.set_mouseclick_handler(mouseclick)frame.set_draw_handler(draw)# get things rollingnew_game()frame.start()# Always remember to review the grading rubric http://www.codeskulptor.org/#examples-memory_states.py123456789101112131415161718192021222324252627282930313233# simple state example for Memoryimport simplegui # define event handlersdef new_game(): global state state = 0 def buttonclick(): global state if state == 0: state = 1 elif state == 1: state = 2 else: state = 1 def draw(canvas): canvas.draw_text(str(state) + " card exposed", [30, 62], 24, "White")# create frame and add a button and labelsframe = simplegui.create_frame("Memory states", 200, 100)frame.add_button("Restart", new_game, 200)frame.add_button("Simulate mouse click", buttonclick, 200)# register event handlersframe.set_draw_handler(draw)# get things rollingnew_game()frame.start() Mini-project Description10 minhttp://www.codeskulptor.org/#examples-memory_template.py123456789101112131415161718192021222324252627282930313233343536# implementation of card game - Memoryimport simpleguiimport random# helper function to initialize globalsdef new_game(): pass # define event handlersdef mouseclick(pos): # add game state logic here pass # cards are logically 50x100 pixels in size def draw(canvas): pass# create frame and add a button and labelsframe = simplegui.create_frame("Memory", 800, 100)frame.add_button("Reset", new_game)label = frame.add_label("Turns = 0")# register event handlersframe.set_mouseclick_handler(mouseclick)frame.set_draw_handler(draw)# get things rollingnew_game()frame.start()# Always remember to review the grading rubric Code Clinic Tips10 minPeer-graded Assignment: Memory 2hhttp://www.codeskulptor.org/#user43_L8JriAHgg6_0.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# implementation of card game - Memoryimport simpleguiimport randomnum = 8CardNumber = range(num)*2exposed = [False]*len(CardNumber)#exposed = [True, False]*(len(CardNumber)/2)pre = Nonepost = Nonecounter = 0# helper function to initialize globalsdef new_game(): global CardNumber, state, counter, exposed random.shuffle(CardNumber) exposed = [False]*len(CardNumber) state = 0 counter = 0 label.set_text("Turns = " + str(counter)) # define event handlersdef mouseclick(pos): # add game state logic here global exposed, state, pre, post, CardNumber, counter idx = pos[0] / 50 if state == 0: state = 1 exposed[idx] = True pre = idx counter = 1 elif state == 1: if exposed[idx] == False: exposed[idx] = True state = 2 post = idx else: if exposed[idx] == False: exposed[idx] = True if CardNumber[pre]==CardNumber[post]: exposed[pre] = True exposed[post] = True else: exposed[pre] = False exposed[post] = False counter += 1 state = 1 pre = idx label.set_text("Turns = " + str(counter)) #print state,counter # cards are logically 50x100 pixels in size def draw(canvas): global CardNumber for i in range(16): if exposed[i]: canvas.draw_text(str(CardNumber[i]), [9+50*i, 75], 72, "White") else: canvas.draw_polygon([(i*50, 0), (50+i*50, 0), (50+i*50, 100), (i*50, 100)], 2, "Red", "Green")# create frame and add a button and labelsframe = simplegui.create_frame("Memory", 800, 100)frame.add_button("Reset", new_game)label = frame.add_label('Turns = 0')# register event handlersframe.set_mouseclick_handler(mouseclick)frame.set_draw_handler(draw)# get things rollingnew_game()frame.start()# Always remember to review the grading rubric Review Your Peers: MemoryWeek 6 - Classes and object-oriented programmingLearn the basics of object-oriented programming in Python using classes, work with tiled images Week 6a - ClassesObject-oriented Programming - 19 minhttp://www.codeskulptor.org/#examples-oo-character.py123456789101112131415161718192021222324252627class Character: def __init__(self, name, initial_health): self.name = name self.health = initial_health self.inventory = [] def __str__(self): s = "Name: " + self.name s += " Health: " + str(self.health) s += " Inventory: " + str(self.inventory) return s def grab(self, item): self.inventory.append(item) def get_health(self): return self.health def example(): me = Character("Bob", 20) print str(me) me.grab("pencil") me.grab("paper") print str(me) print "Health:", me.get_health() example() Object-oriented Programming - 2 8 minhttp://www.codeskulptor.org/#examples-oo-ball.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137# ball physics code for generic 2D domain# the functions inside() and normal() encode the shape of the ennvironmentimport simpleguiimport randomimport math# Canvas sizewidth = 600height = 400# Ball traitsradius = 20color = "White"# math helper functiondef dot(v, w): return v[0] * w[0] + v[1] * w[1]class RectangularDomain: def __init__(self, width, height): self.width = width self.height = height self.border = 2 # return if bounding circle is inside the domain def inside(self, center, radius): in_width = ((radius + self.border) &lt; center[0] &lt; (self.width - self.border - radius)) in_height = ((radius + self.border) &lt; center[1] &lt; (self.height - self.border - radius)) return in_width and in_height # return a unit normal to the domain boundary point nearest center def normal(self, center): left_dist = center[0] right_dist = self.width - center[0] top_dist = center[1] bottom_dist = self.height - center[1] if left_dist &lt; min(right_dist, top_dist, bottom_dist): return (1, 0) elif right_dist &lt; min(left_dist, top_dist, bottom_dist): return (-1, 0) elif top_dist &lt; min(bottom_dist, left_dist, right_dist): return (0, 1) else: return (0, -1) # return random location def random_pos(self, radius): x = random.randrange(radius, self.width - radius - self.border) y = random.randrange(radius, self.height - radius - self.border) return [x, y] # Draw boundary of domain def draw(self, canvas): canvas.draw_polygon([[0, 0], [self.width, 0], [self.width, self.height], [0, self.height]], self.border*2, "Red") class CircularDomain: def __init__(self, center, radius): self.center = center self.radius = radius self.border = 2 # return if bounding circle is inside the domain def inside(self, center, radius): dx = center[0] - self.center[0] dy = center[1] - self.center[1] dr = math.sqrt(dx ** 2 + dy ** 2) return dr &lt; (self.radius - radius - self.border) # return a unit normal to the domain boundary point nearest center def normal(self, center): dx = center[0] - self.center[0] dy = center[1] - self.center[1] dr = math.sqrt(dx ** 2 + dy ** 2) return [dx / dr, dy / dr] # return random location def random_pos(self, radius): r = random.random() * (self.radius - radius - self.border) theta = random.random() * 2 * math.pi x = r * math.cos(theta) + self.center[0] y = r * math.sin(theta) + self.center[1] return [x, y] # Draw boundary of domain def draw(self, canvas): canvas.draw_circle(self.center, self.radius, self.border*2, "Red") class Ball: def __init__(self, radius, color, domain): self.radius = radius self.color = color self.domain = domain self.pos = self.domain.random_pos(self.radius) self.vel = [random.random() + .1, random.random() + .1] # bounce def reflect(self): norm = self.domain.normal(self.pos) norm_length = dot(self.vel, norm) self.vel[0] = self.vel[0] - 2 * norm_length * norm[0] self.vel[1] = self.vel[1] - 2 * norm_length * norm[1] # update ball position def update(self): self.pos[0] += self.vel[0] self.pos[1] += self.vel[1] if not self.domain.inside(self.pos, self.radius): self.reflect() # draw def draw(self, canvas): canvas.draw_circle(self.pos, self.radius, 1, self.color, self.color) # generic update code for ball physicsdef draw(canvas): ball.update() field.draw(canvas) ball.draw(canvas)field = RectangularDomain(width, height)# field = CircularDomain([width/2, height/2], 180)ball = Ball(radius, color, field) frame = simplegui.create_frame("Ball physics", width, height)frame.set_draw_handler(draw)frame.start() Working with Objects13 minhttp://www.codeskulptor.org/#examples-particle_class.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Particle class example used to simulate diffusion of moleculesimport simpleguiimport random# global constantsWIDTH = 600HEIGHT = 400PARTICLE_RADIUS = 5COLOR_LIST = ["Red", "Green", "Blue", "White"]DIRECTION_LIST = [[1,0], [0, 1], [-1, 0], [0, -1]]# definition of Particle classclass Particle: # initializer for particles def __init__(self, position, color): self.position = position self.color = color # method that updates position of a particle def move(self, offset): self.position[0] += offset[0] self.position[1] += offset[1] # draw method for particles def draw(self, canvas): canvas.draw_circle(self.position, PARTICLE_RADIUS, 1, self.color, self.color) # string method for particles def __str__(self): return "Particle with position = " + str(self.position) + " and color = " + self.color# draw handlerdef draw(canvas): for p in particle_list: p.move(random.choice(DIRECTION_LIST)) for p in particle_list: p.draw(canvas)# create frame and register draw handlerframe = simplegui.create_frame("Particle simulator", WIDTH, HEIGHT)frame.set_draw_handler(draw)# create a list of particlesparticle_list = []for i in range(100): p = Particle([WIDTH / 2, HEIGHT / 2], random.choice(COLOR_LIST)) particle_list.append(p)# start frameframe.start() http://www.codeskulptor.org/#examples-particle_testing_template.py1234567891011121314151617181920212223242526272829303132333435363738# Testing template for Particle class#################################################### Student should add code for the Particle class here #################################################### Test code for the Particle classp = Particle([20, 20], "Red")print pprint type(p)p.move([10, 20])print pp.move([-15, -25])print pprintq = Particle([15, 30], "Green")print qprint type(q)q.move([0, 0])print q#################################################### Output from test#Particle with position = [20, 20] and color = Red#&lt;class '__main__.Particle'&gt;#Particle with position = [30, 40] and color = Red#Particle with position = [15, 15] and color = Red##Particle with position = [15, 30] and color = Green#&lt;class '__main__.Particle'&gt;#Particle with position = [15, 30] and color = Green Classes for Blackjack11 minhttp://www.codeskulptor.org/#examples-blackjack.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144# Mini-project #6 - Blackjackimport simpleguiimport random# load card sprite - 950x392 - source: jfitz.comCARD_SIZE = (73, 98)card_images = simplegui.load_image("http://commondatastorage.googleapis.com/codeskulptor-assets/cards.jfitz.png")CARD_BACK_SIZE = (71, 96)card_back = simplegui.load_image("http://commondatastorage.googleapis.com/codeskulptor-assets/card_back.png") # initialize global variablesdeck = []in_play = Falseoutcome = ""score = 0# define globals for cardsSUITS = ['C', 'S', 'H', 'D']RANKS = ['A', '2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K']VALUES = &#123;'A':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9, 'T':10, 'J':10, 'Q':10, 'K':10&#125;# define card classclass Card: def __init__(self, suit, rank): if (suit in SUITS) and (rank in RANKS): self.suit = suit self.rank = rank else: print "Invalid card: ", self.suit, self.rank def __str__(self): return self.suit + self.rank def get_suit(self): return self.suit def get_rank(self): return self.rank def draw(self, canvas, pos): card_loc = (card_size[0] * (0.5 + RANKS.index(self.rank)), card_size[1] * (0.5 + SUITS.index(self.suit))) canvas.draw_image(card_images, card_loc, card_size, [pos[0] + card_size[0] / 2, pos[1] + card_size[1] / 2], card_size) # define hand classclass Hand: def __init__(self): pass # replace with your code def __str__(self): pass # replace with your code def add_card(self, card): pass # replace with your code # count aces as 1, if the hand has an ace, then add 10 to hand value if don't bust def get_value(self): pass # replace with your code def busted(self): pass # replace with your code def draw(self, canvas, p): pass # replace with your code # define deck classclass Deck: def __init__(self): pass # replace with your code # add cards back to deck and shuffle def shuffle(self): pass # replace with your code def deal_card(self): pass # replace with your code#define callbacks for buttonsdef deal(): global outcome, in_play # your code goes here outcome = "" in_play = Truedef hit(): pass # replace with your code below # if the hand is in play, hit the player # if busted, assign an message to outcome, update in_play and score def stand(): pass # replace with your code below # if hand is in play, repeatedly hit dealer until his hand has value 17 or more # assign a message to outcome, update in_play and scoredef draw(canvas): pass # replace with your code below# initialization frameframe = simplegui.create_frame("Blackjack", 600, 600)frame.set_canvas_background("Green")#create buttons and canvas callbackframe.add_button("Deal", deal, 200)frame.add_button("Hit", hit, 200)frame.add_button("Stand", stand, 200)frame.set_draw_handler(draw)# deal an initial hand# get things rollingframe.start()# Grading rubric - 18 pts total (scaled to 100)# 1 pt - The program opens a frame with the title "Blackjack" appearing on the canvas.# 3 pts - The program displays 3 buttons ("Deal", "Hit" and "Stand") in the control area. (1 pt per button)# 2 pts - The program graphically displays the player's hand using card sprites. # (1 pt if text is displayed in the console instead) # 2 pts - The program graphically displays the dealer's hand using card sprites. # Displaying both of the dealer's cards face up is allowable when evaluating this bullet. # (1 pt if text displayed in the console instead)# 1 pt - Hitting the "Deal" button deals out new hands to the player and dealer.# 1 pt - Hitting the "Hit" button deals another card to the player. # 1 pt - Hitting the "Stand" button deals cards to the dealer as necessary.# 1 pt - The program correctly recognizes the player busting. # 1 pt - The program correctly recognizes the dealer busting. # 1 pt - The program correctly computes hand values and declares a winner. # Evalute based on player/dealer winner messages. # 1 pt - The dealer's hole card is hidden until the hand is over when it is then displayed.# 2 pts - The program accurately prompts the player for an action with the messages # "Hit or stand?" and "New deal?". (1 pt per message)# 1 pt - The program keeps score correctly. Practice Exercises for Classes (part 1) (optional)10 minPractice Exercise for Avatar class (optional)10 minQuiz: Quiz 6a8 questionsQUIZQuiz 6a8 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 26, 11:59 PM PST 1 point1.Every class definition should include an initializer method. What is the name of the initializer method? Refer to the first object-oriented programming video. Note: While you can get away with not having an initializer method, doing so almost always implies using techniques beyond the scope of this course or bad program design. So, beginners should always define an initializer method. init The same as the name of the class init (1 underscore on each side) init (2 underscores on each side)1 point2.In Python, what is the main difference between a function and a method? Methods are defined in built-in library modules, while functions are defined in your own code. There is no difference. They are interchangeable terms. Functions are defined outside of classes, while methods are defined inside of and part of classes. Methods have a parameter named self, while functions do not.1 point3.As an example class, consider the following code from one of the videos:1234567891011121314151617class Character: def __init__(self, name, initial_health): self.name = name self.health = initial_health self.inventory = [] def __str__(self): s = "Name: " + self.name s += " Health: " + str(self.health) s += " Inventory: " + str(self.inventory) return s def grab(self, item): self.inventory.append(item) def get_health(self): return self.health What does the self parameter represent? An object (instance) of the Character class Whatever happens to be passed to it. The method that is being defined The Character class1 point4.Assume you have the following class and method definition, parts of which have been omitted.123456class My_Class: … def my_method(self, value1, value2): """Assumes its inputs are two values and does something.""" …my_object = My_Class() The last line defines the variable my_object as an object of My_Class class. Which of the following is proper syntax for using the method on this object? My_Class.my_method(my_object, 1, 2) my_method(my_object, 1, 2) My_Class.my_object.my_method(1, 2) my_object.my_method(1, 2) my_method(My_Class, 1, 2)1 point5.We want to have balls that move around. Which of the following designs represents encapsulation best? 12345678910# centers : A list of points, the balls' center pointscenters = …# radii : A list of numbers, the balls' radiiradii = …def move(ball_number, move_vector): """ Changes the position of the ball by the given vector. """ centers[ball_number][0] += move_vector[0] centers[ball_number][1] += move_vector[1] 123456789101112class Ball: def __init__(self, c, r): self.center = c self.radius = r def move(self, move_vector): """ Changes the position of the ball by the given vector. """ self.center[0] += move_vector[0] self.center[1] += move_vector[1]# balls : A list of Ball objectsballs = … 123456789101112131415161718class Ball: def __init__(self, c, r): self.center = c self.radius = r def get_position(self): return self.center def set_position(self, new_position): self.center = new_position# balls : A list of Ball objectsballs = …def move(ball, move_vector): """ Changes the position of the ball by the given vector. """ position = ball.get_position() position[0] += move_vector[0] position[1] += move_vector[1] ball.set_position(position) 123456789101112class Ball: def __init__(self, c, r): self.center = c self.radius = r# balls : A list of Ball objectsballs = …def move(ball, move_vector): """ Changes the position of the ball by the given vector. """ ball.center[0] += move_vector[0] ball.center[1] += move_vector[1] 1 point6.A common feature in many object-oriented languages is method overloading. In this quiz question, you will learn by example what overloading is and whether or not Python supports it. Turn the following English description into code. Start a class definition. We’ll call the class Overload.Define an init method. Along with the standard self it has one parameter. The method does nothing useful for this example — use the Python do-nothing statement pass for the body.Define a second init method. Along with self it has two parameters. This method also does nothing useful.Outside of the class, we want to create two Overload objects. If Python supports overloading, you will be able to create an Overload object with one argument, and create another Overload object with two arguments. Does Python support overloading? No Yes1 point7.First, complete the following class definition:12345678910111213141516171819202122232425262728class BankAccount: def __init__(self, initial_balance): """Creates an account with the given balance.""" self.balance = initial_balance self.fees = 0 def deposit(self, amount): """Deposits the amount into the account.""" self.balance += amount - self.fees def withdraw(self, amount): """ Withdraws the amount from the account. Each withdrawal resulting in a negative balance also deducts a penalty fee of 5 dollars from the balance. """ self.balance -= amount if self.balance &lt; 0: self.fees += 5 def get_balance(self): """Returns the current balance in the account.""" return self.balance def get_fees(self): """Returns the total fees ever deducted from the account.""" return self.fees The deposit and withdraw methods each change the account balance. The withdraw method also deducts a fee of 5 dollars from the balance if the withdrawal (before any fees) results in a negative balance. Since we also have the method get_fees you will need to have a variable to keep track of the fees paid. Here’s one possible test of the class. It should print the values 10 and 5, respectively, since the withdrawal incurs a fee of 5 dollars.1234my_account = BankAccount(10)my_account.withdraw(15)my_account.deposit(20)print my_account.get_balance(), my_account.get_fees() Copy-and-paste the following much longer test. What two numbers are printed at the end? Enter the two numbers, separated only by spaces.1234567891011121314151617181920212223242526272829303132333435363738394041424344my_account = BankAccount(10)my_account.withdraw(5)my_account.deposit(10)my_account.withdraw(5)my_account.withdraw(15)my_account.deposit(20)my_account.withdraw(5)my_account.deposit(10)my_account.deposit(20)my_account.withdraw(15)my_account.deposit(30)my_account.withdraw(10)my_account.withdraw(15)my_account.deposit(10)my_account.withdraw(50)my_account.deposit(30)my_account.withdraw(15)my_account.deposit(10)my_account.withdraw(5)my_account.deposit(20)my_account.withdraw(15)my_account.deposit(10)my_account.deposit(30)my_account.withdraw(25)my_account.withdraw(5)my_account.deposit(10)my_account.withdraw(15)my_account.deposit(10)my_account.withdraw(10)my_account.withdraw(15)my_account.deposit(10)my_account.deposit(30)my_account.withdraw(25)my_account.withdraw(10)my_account.deposit(20)my_account.deposit(10)my_account.withdraw(5)my_account.withdraw(15)my_account.deposit(10)my_account.withdraw(5)my_account.withdraw(15)my_account.deposit(10)my_account.withdraw(5)print my_account.get_balance(), my_account.get_fees() 1 point8.We will again use the BankAccount class from the previous problem. You should be able to use the same definition for both problems. Of course, a bank with only one account will go out of business, so we want our BankAccount class to work correctly with many accounts. Naturally, each bank account should have its own balance, with deposits and withdrawals going to the appropriate account. Similarly, the penalty fees for each account should be kept separate.12345678910111213141516171819202122232425262728class BankAccount: def __init__(self, initial_balance): """Creates an account with the given balance.""" self.balance = initial_balance self.fees = 0 def deposit(self, amount): """Deposits the amount into the account.""" self.balance += amount - self.fees def withdraw(self, amount): """ Withdraws the amount from the account. Each withdrawal resulting in a negative balance also deducts a penalty fee of 5 dollars from the balance. """ self.balance -= amount if self.balance &lt; 0: self.fees += 5 def get_balance(self): """Returns the current balance in the account.""" return self.balance def get_fees(self): """Returns the total fees ever deducted from the account.""" return self.fees Here’s one possible test with multiple accounts. It should print the values 10, 5, 5, and 0.12345678account1 = BankAccount(10)account1.withdraw(15)account2 = BankAccount(15)account2.deposit(10)account1.deposit(20)account2.withdraw(20)print account1.get_balance(), account1.get_fees(), account2.get_balance(), account2.get_fees() Copy-and-paste the following much longer test. What four numbers are printed at the end? Enter the four numbers, separated only by spaces.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556account1 = BankAccount(20)account1.deposit(10)account2 = BankAccount(10)account2.deposit(10)account2.withdraw(50)account1.withdraw(15)account1.withdraw(10)account2.deposit(30)account2.withdraw(15)account1.deposit(5)account1.withdraw(10)account2.withdraw(10)account2.deposit(25)account2.withdraw(15)account1.deposit(10)account1.withdraw(50)account2.deposit(25)account2.deposit(25)account1.deposit(30)account2.deposit(10)account1.withdraw(15)account2.withdraw(10)account1.withdraw(10)account2.deposit(15)account2.deposit(10)account2.withdraw(15)account1.deposit(15)account1.withdraw(20)account2.withdraw(10)account2.deposit(5)account2.withdraw(10)account1.deposit(10)account1.deposit(20)account2.withdraw(10)account2.deposit(5)account1.withdraw(15)account1.withdraw(20)account1.deposit(5)account2.deposit(10)account2.deposit(15)account2.deposit(20)account1.withdraw(15)account2.deposit(10)account1.deposit(25)account1.deposit(15)account1.deposit(10)account1.withdraw(10)account1.deposit(10)account2.deposit(20)account2.withdraw(15)account1.withdraw(20)account1.deposit(5)account1.deposit(10)account2.withdraw(20)print account1.get_balance(), account1.get_fees(), account2.get_balance(), account2.get_fees() Week 6b - Tiled ImagesTiled Images15 minhttp://www.codeskulptor.org/#examples-tiled_images.py12345678910111213141516171819202122232425262728293031323334353637383940# demo for drawing using tiled imagesimport simplegui# define globals for cardsRANKS = ('A', '2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K')SUITS = ('C', 'S', 'H', 'D')# card sprite - 950x392CARD_CENTER = (36.5, 49)CARD_SIZE = (73, 98)card_image = simplegui.load_image("http://commondatastorage.googleapis.com/codeskulptor-assets/cards.jfitz.png")# define card classclass Card: def __init__(self, suit, rank): self.rank = rank self.suit = suit def draw(self, canvas, loc): i = RANKS.index(self.rank) j = SUITS.index(self.suit) card_pos = [CARD_CENTER[0] + i * CARD_SIZE[0], CARD_CENTER[1] + j * CARD_SIZE[1]] canvas.draw_image(card_image, card_pos, CARD_SIZE, loc, CARD_SIZE)# define draw handler def draw(canvas): one_card.draw(canvas, (155, 90))# define frame and register draw handlerframe = simplegui.create_frame("Card draw", 300, 200)frame.set_draw_handler(draw)# createa cardone_card = Card('S', '6')frame.start() Visualizing Objects8 minhttp://www.codeskulptor.org/viz/#examples_points.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475################### Object creation and use# Mutation with Aliasingclass Point1: def __init__(self, x, y): self.x = x self.y = y def set_x(self, newx): self.x = newx def get_x(self): return self.xp = Point1(4, 5)q = Point1(4, 5)r = pp.set_x(10)print p.get_x()print q.get_x()print r.get_x()################### Object shared state# Mutation of shared stateclass Point2: def __init__(self, coordinates): self.coords = coordinates def set_coord(self, index, value): self.coords[index] = value def get_coord(self, index): return self.coords[index]coordinates = [4, 5]p = Point2(coordinates)q = Point2(coordinates)r = Point2([4, 5])p.set_coord(0, 10)print p.get_coord(0)print q.get_coord(0)print r.get_coord(0)################### Objects not sharing stateclass Point3: def __init__(self, coordinates): self.coords = list(coordinates) def set_coord(self, index, value): self.coords[index] = value def get_coord(self, index): return self.coords[index]coordinates = [4, 5]p = Point3(coordinates)q = Point3(coordinates)r = Point3([4, 5])p.set_coord(0, 10)print p.get_coord(0)print q.get_coord(0)print r.get_coord(0) Programming Tips - 613 minhttp://www.codeskulptor.org/#examples-tips6.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#################### Broken codeclass ball: def ball(pos, rad): position = pos radius = rad return ball def get_position(): return positionb = ball([0,0], 10)print get_position(b)#################### Fixed codeclass Ball: def __init__(self, pos, rad): self.position = pos self.radius = rad def get_position(self): return self.positionb = Ball([0,0], 10)print b.get_position()################### Example whiledef countdown(n): """Print the values from n to 0.""" i = n while i &gt;= 0: print i i -= 1countdown(5)################### Collatzdef collatz(n): """Prints the values in the Collatz sequence for n.""" i = n while i &gt; 1: print i if i % 2 == 0: i = i / 2 else: i = 3 * i + 1 colnatz(1000)################## Timeouti = 1while i &gt; 0: i += 1 print "Done!" Practice Exercises for Classes (part 2) (optional)10 minQuiz:Quiz 6b8 questionsQUIZQuiz 6b8 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineNovember 26, 11:59 PM PST 1 point1.What is the position of the center of the top-left card (Ace of Clubs, A♣) in the tiled image discussed in the “Tiled images“ video? Remember that each card in this tiled image has size 73 x 98 pixels. (Note that the tiled image used in the current version of your Blackjack mini-project is slightly smaller.) (36.5, 49) (5 73 + 36.5, 1 98 + 49) (0, 0) (73, 98)1 point2.What is the position of the center of the bottom-right card (King of Diamonds, K♦) in the tiled image discussed in the “Tiled images” video? Again, remember that each card in this tiled image has size 73 x 98 pixels. Enter two numbers, separated only by spaces. 1 point3.When using Dropbox to store images for use with CodeSkulptor, what should the www portion of the DropBox URL be replaced by? Refer to the video on tiled images. dl jpg www html gif1 point4.Within the init method, the new object should be returned with what code? return whatever_the_object_is_named (Use the appropriate variable name.) No return statement is needed in init. return return self1 point5.One way of understanding code is to think about other code that accomplishes the same thing — i.e., given the same starting values, it returns and/or mutates the same values. This following defines one way to concatenate multiple lists. For example, list_extend_many([[1,2], [3], [4, 5, 6], [7]]) returns [1, 2, 3, 4, 5, 6, 7] and doesn’t mutate anything.1234567def list_extend_many(lists): """Returns a list that is the concatenation of all the lists in the given list-of-lists.""" result = [] for l in lists: result.extend(l) return result Which of the following definitions are equivalent? I.e., which always produce the same output for the same input, and never mutate the input or any global variable? 1234567def list_extend_many(lists): result = [] i = 0 while i &lt;= len(lists): result.extend(lists[i]) i += 1 return result 1234567def list_extend_many(lists): result = [] i = 0 while i &lt; len(lists): result.extend(lists[i]) i += 1 return result 1234567def list_extend_many(lists): result = [] i = 0 while i &lt; len(lists): result += lists[i] i += 1 return result 1234567def list_extend_many(lists): result = [] i = 0 while i &lt; len(lists): i += 1 result.extend(lists[i]) return result 1 point6.Which of the following programs would never end if it weren’t for CodeSkulptor’s timeout? Assume no break or return statement is used in the elided loop bodies. You might want to add a print statement to each loop to better understand the behavior. 1234n = 1while n &gt; 0: … # Assume this doesn't modify n. n += 1 12while True: … 1234n = 1000while n &gt; 0: … # Assume this doesn't modify n. n -= 1 123my_list = …for x in my_list: … # Assume this doesn't mutate my_list. 1 point7.Convert the following English description into code. Initialize n to be 1000. Initialize numbers to be a list of numbers from 2 to n but not including n.With results starting as the empty list, repeat the following as long as numbers contains any numbers.Add the first number in numbers to the end of results.Remove every number in numbers that is evenly divisible by (has no remainder when divided by) the number that you had just added to results.How long is results? To test your code, when n is instead 100, the length of results is 25. 12345678910n=1000number = range(2,n)r = []while len(number)&gt;0: r.extend([number[0]]) for i in number: if i % r[-1]==0: number.remove(i)print len(r) 1 point8.We can use loops to simulate natural processes over time. Write a program that calculates the populations of two kinds of “wumpuses” over time. At the beginning of year 1, there are 1000 slow wumpuses and 1 fast wumpus. This one fast wumpus is a new mutation. Not surprisingly, being fast gives it an advantage, as it can better escape from predators. Each year, each wumpus has one offspring. (We’ll ignore the more realistic niceties of sexual reproduction, like distinguishing males and females.). There are no further mutations, so slow wumpuses beget slow wumpuses, and fast wumpuses beget fast wumpuses. Also, each year 40% of all slow wumpuses die each year, while only 30% of the fast wumpuses do. So, at the beginning of year one there are 1000 slow wumpuses. Another 1000 slow wumpuses are born. But, 40% of these 2000 slow wumpuses die, leaving a total of 1200 at the end of year one. Meanwhile, in the same year, we begin with 1 fast wumpus, 1 more is born, and 30% of these die, leaving 1.4. (We’ll also allow fractional populations, for simplicity.) Beginning of Year Slow Wumpuses Fast Wumpuses 1 1000 1 2 1200 1.4 3 1440 1.96 … … … Enter the first year in which the fast wumpuses outnumber the slow wumpuses. Remember that the table above shows the populations at the start of the year. 12345678s=1000f=1n=1while f&lt;=s: f=2*f*0.7 s=2*s*0.6 n+=1print f,s,n Mini-project #6 - BlackjackMini-project Video14 minhttp://www.codeskulptor.org/#examples-blackjack_template.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133# Mini-project #6 - Blackjackimport simpleguiimport random# load card sprite - 936x384 - source: jfitz.comCARD_SIZE = (72, 96)CARD_CENTER = (36, 48)card_images = simplegui.load_image("http://storage.googleapis.com/codeskulptor-assets/cards_jfitz.png")CARD_BACK_SIZE = (72, 96)CARD_BACK_CENTER = (36, 48)card_back = simplegui.load_image("http://storage.googleapis.com/codeskulptor-assets/card_jfitz_back.png") # initialize some useful global variablesin_play = Falseoutcome = ""score = 0# define globals for cardsSUITS = ('C', 'S', 'H', 'D')RANKS = ('A', '2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K')VALUES = &#123;'A':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9, 'T':10, 'J':10, 'Q':10, 'K':10&#125;# define card classclass Card: def __init__(self, suit, rank): if (suit in SUITS) and (rank in RANKS): self.suit = suit self.rank = rank else: self.suit = None self.rank = None print "Invalid card: ", suit, rank def __str__(self): return self.suit + self.rank def get_suit(self): return self.suit def get_rank(self): return self.rank def draw(self, canvas, pos): card_loc = (CARD_CENTER[0] + CARD_SIZE[0] * RANKS.index(self.rank), CARD_CENTER[1] + CARD_SIZE[1] * SUITS.index(self.suit)) canvas.draw_image(card_images, card_loc, CARD_SIZE, [pos[0] + CARD_CENTER[0], pos[1] + CARD_CENTER[1]], CARD_SIZE) # define hand classclass Hand: def __init__(self): pass # create Hand object def __str__(self): pass # return a string representation of a hand def add_card(self, card): pass # add a card object to a hand def get_value(self): # count aces as 1, if the hand has an ace, then add 10 to hand value if it doesn't bust pass # compute the value of the hand, see Blackjack video def draw(self, canvas, pos): pass # draw a hand on the canvas, use the draw method for cards # define deck class class Deck: def __init__(self): pass # create a Deck object def shuffle(self): # shuffle the deck pass # use random.shuffle() def deal_card(self): pass # deal a card object from the deck def __str__(self): pass # return a string representing the deck#define event handlers for buttonsdef deal(): global outcome, in_play # your code goes here in_play = Truedef hit(): pass # replace with your code below # if the hand is in play, hit the player # if busted, assign a message to outcome, update in_play and score def stand(): pass # replace with your code below # if hand is in play, repeatedly hit dealer until his hand has value 17 or more # assign a message to outcome, update in_play and score# draw handler def draw(canvas): # test to make sure that card.draw works, replace with your code below card = Card("S", "A") card.draw(canvas, [300, 300])# initialization frameframe = simplegui.create_frame("Blackjack", 600, 600)frame.set_canvas_background("Green")#create buttons and canvas callbackframe.add_button("Deal", deal, 200)frame.add_button("Hit", hit, 200)frame.add_button("Stand", stand, 200)frame.set_draw_handler(draw)# get things rollingdeal()frame.start()# remember to review the gradic rubric Mini-project Description10 minCode Clinic Tips10 minPeer-graded Assignment: Blackjack2hhttp://www.codeskulptor.org/#user43_fY8KwOeCws_0.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236# Mini-project #6 - Blackjackimport simpleguiimport random# load card sprite - 936x384 - source: jfitz.comCARD_SIZE = (72, 96)CARD_CENTER = (36, 48)card_images = simplegui.load_image("http://storage.googleapis.com/codeskulptor-assets/cards_jfitz.png")CARD_BACK_SIZE = (72, 96)CARD_BACK_CENTER = (36, 48)card_back = simplegui.load_image("http://storage.googleapis.com/codeskulptor-assets/card_jfitz_back.png") # initialize some useful global variablesin_play = Falseoutcome = ""score = 0# define globals for cardsSUITS = ('C', 'S', 'H', 'D')RANKS = ('A', '2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K')VALUES = &#123;'A':1, '2':2, '3':3, '4':4, '5':5, '6':6, '7':7, '8':8, '9':9, 'T':10, 'J':10, 'Q':10, 'K':10&#125;# define card classclass Card: def __init__(self, suit, rank): if (suit in SUITS) and (rank in RANKS): self.suit = suit self.rank = rank else: self.suit = None self.rank = None print "Invalid card: ", suit, rank def __str__(self): return self.suit + self.rank def get_suit(self): return self.suit def get_rank(self): return self.rank def draw(self, canvas, pos): card_loc = (CARD_CENTER[0] + CARD_SIZE[0] * RANKS.index(self.rank), CARD_CENTER[1] + CARD_SIZE[1] * SUITS.index(self.suit)) canvas.draw_image(card_images, card_loc, CARD_SIZE, [pos[0] + CARD_CENTER[0], pos[1] + CARD_CENTER[1]], CARD_SIZE) # define hand classclass Hand: def __init__(self): # create Hand object self.hand = [] def __str__(self): # return a string representation of a hand s = "Hand contains" for i in self.hand: s+=" "+str(i) return s def add_card(self, card): # add a card object to a hand self.hand.append(card) def get_value(self): # count aces as 1, if the hand has an ace, then add 10 to hand value if it doesn't bust # compute the value of the hand, see Blackjack video ace = False value = 0 for card in self.hand: rank = card.get_rank() if rank == "A": ace = True value += VALUES[rank] if ace: if value + 10 &lt;= 21: return value+10 else: return value else: return value def draw(self, canvas, pos): # draw a hand on the canvas, use the draw method for cards for card in self.hand: card.draw(canvas, pos) pos[0] += CARD_SIZE[0] # define deck class class Deck: def __init__(self): # create a Deck object self.deck = [Card(suit,rank) for suit in SUITS for rank in RANKS] def shuffle(self): # shuffle the deck # use random.shuffle() random.shuffle(self.deck) def deal_card(self): # deal a card object from the deck return self.deck.pop() def __str__(self): # return a string representing the deck s = "Deck contains" for i in self.deck: s+=" "+str(i) return s#define event handlers for buttonsdef deal(): global outcome, in_play, score # your code goes here if in_play: score -= 1 in_play = True outcome = "Hit or stand?" # create a Deck object and shuffle the deck global deck deck = Deck() deck.shuffle() # create Hand object global dealer, player dealer = Hand() player = Hand() # deal a card object from the deck and add a card object to a hand player.add_card(deck.deal_card()) dealer.add_card(deck.deal_card()) player.add_card(deck.deal_card()) dealer.add_card(deck.deal_card()) #print dealer, player def hit(): # replace with your code below global in_play # if the hand is in play, hit the player if in_play: global player, deck, outcome, score outcome = "Hit or stand?" player.add_card(deck.deal_card()) # if busted, assign a message to outcome, update in_play and score if player.get_value()&gt;21: outcome = "the player has busted. New deal?" in_play = False score -= 1 def stand(): # replace with your code below # if hand is in play, repeatedly hit dealer until his hand has value 17 or more global in_play #print "before " +str(in_play) if in_play: global dealer, deck, outcome, player, score while dealer.get_value() &lt; 17: dealer.add_card(deck.deal_card()) #print dealer.get_value() # assign a message to outcome, update in_play and score if dealer.get_value() &gt; 21: outcome = "the dealer has busted. New deal?" score += 1 elif player.get_value() &lt;= dealer.get_value(): outcome = "the dealer wins. New deal?" score -= 1 else: outcome = "the player wins. New deal?" score += 1 in_play = False #print in_play# draw handler def draw(canvas): # test to make sure that card.draw works, replace with your code below global outcome canvas.draw_text(outcome, (30, 300), 36, 'Blue') canvas.draw_text("Dealer", (30, 140), 48, 'Black') global dealer dealer.draw(canvas, [30, 150]) dealer_pos = [30, 150] global in_play #print in_play if in_play: card_loc = (CARD_CENTER[0] + CARD_SIZE[0], CARD_CENTER[1] + CARD_SIZE[1]) #print card_loc canvas.draw_image(card_back, CARD_CENTER, CARD_SIZE, [dealer_pos[0] + CARD_CENTER[0], dealer_pos[1] + CARD_CENTER[1]], CARD_SIZE) else: card_loc = (CARD_CENTER[0] + CARD_SIZE[0] * RANKS.index(dealer.hand[0].get_rank()), CARD_CENTER[1] + CARD_SIZE[1] * SUITS.index(dealer.hand[0].get_suit())) canvas.draw_image(card_images, card_loc, CARD_SIZE, [dealer_pos[0] + CARD_CENTER[0], dealer_pos[1] + CARD_CENTER[1]], CARD_SIZE) global player canvas.draw_text("Player: "+str(player.get_value()), (30, 370), 48, 'Black') player.draw(canvas, [30, 380]) canvas.draw_text("Blackjack", (200, 50), 48, 'Black') global score canvas.draw_text("Score: "+str(score), (350, 100), 48, 'Red') # initialization frameframe = simplegui.create_frame("Blackjack", 600, 600)frame.set_canvas_background("Green")#create buttons and canvas callbackframe.add_button("Deal", deal, 200)frame.add_button("Hit", hit, 200)frame.add_button("Stand", stand, 200)frame.set_draw_handler(draw)# get things rollingdeal()frame.start()# remember to review the gradic rubric Review Your Peers: BlackjackWeek 7 - Basic game physics, spritesUnderstand the math of acceleration and friction, work with sprites, add sound to your game Week 7a - More ClassesAcceleration and Friction14 minSpaceship Class7 minSound5 minQuiz: Quiz 7a7 questions Week 7b - SpritesSprite Class14 minProgramming Tips - 720 minPractice Exercises for Sprites and Sound (optional)10 minQuiz: Quiz 7b9 questions Mini Project #7 - SpaceshipMini-project Video13 minMini-project Description10 minCode Clinic Tips10 minPeer-graded Assignment: Spaceship2hReview Your Peers: Spaceship##Learn the basics of object-oriented programming in Python using classes, work with tiled images ##Learn the basics of object-oriented programming in Python using classes, work with tiled images Principles of Computing (Part 1)Lecture slides can be found [here]Coursera can be found here About this course: This two-part course builds upon the programming skills that you learned in our Introduction to Interactive Programming in Python course. We will augment those skills with both important programming practices and critical mathematical problem solving skills. These skills underlie larger scale computational problem solving and programming. The main focus of the class will be programming weekly mini-projects in Python that build upon the mathematical and programming principles that are taught in the class. To keep the class fun and engaging, many of the projects will involve working with strategy-based games. In part 1 of this course, the programming aspect of the class will focus on coding standards and testing. The mathematical portion of the class will focus on probability, combinatorics, and counting with an eye towards practical applications of these concepts in Computer Science. Recommended Background - Students should be comfortable writing small (100+ line) programs in Python using constructs such as lists, dictionaries and classes and also have a high-school math background that includes algebra and pre-calculus. #Lecture slides can be found [here]Coursera can be found here primary #Lecture slides can be found [here]Coursera can be found here primary #Lecture slides can be found [here]Coursera can be found here primary]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Summary: Online Courses Seeking for Machine Learning Engineering job]]></title>
    <url>%2F2017%2F09%2F19%2FSummary%20Online%20Courses%20Seeking%20for%20Machine%20Learning%20Engineering%20job%2F</url>
    <content type="text"><![CDATA[课程基于背景，主要选择 Coursera 和 Udacity 作为知识输入，Edx 还没接触。 Coursera Stanford Machine Learning | Coursera 用时一星期这是我机器学习入门课程，从这门网课学到了很多的基本概念，算法。 UW Machine Learning | Coursera 一个Week用时一天 9:00-24:00（复习巩固时间另算）过笔试，面试要求之一，ML算法推导。这是我目前在上的网课进度：3/4，原本这门课有6部分，Capstone演示很精彩，不知道什么原因裁成了4部分，这门课会教你怎么用一个模型，以及怎么搭建这个模型，包括推导。 Machine Learning Foundations: A Case Study Approach | Coursera Machine Learning: Regression | Coursera Machine Learning: Classification | Coursera Stanford CS229过笔试，面试要求之一，ML算法推导。目前在上的课程。由于精力有限，进度很慢。 Stanford Algorithms | Coursera 一个Week用时一天 9:00-24:00（复习巩固时间另算）过笔试要求之一，CS算法。前三门我上完了，挺费心的，很多编程作业都需要再优化一下，学完这个算是算法入门了吧，2018秋招笔试题大部分都会有思路，规定时间内做不出来是因为熟练度的问题 Divide and Conquer, Sorting and Searching, and Randomized Algorithms | Coursera Graph Search, Shortest Paths, and Data Structures | Coursera Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming | Coursera Data Mining | Coursera 一个Week用时一天 9:00-24:00（复习巩固时间另算）职位方向之一，数据挖掘。我只上完了前两门课，原因有二：时间原因，编程作业有问题（环境配置问题）。第一门课对于想找数据分析工作的帮助会很大，这里不详说。第二门会讲基本概念以及应用，可惜编程的环境配置有问题。如果要从事数据挖掘就继续学下去。 Data Visualization | Coursera Text Retrieval and Search Engines | Coursera Deep Learning | Coursera 一个Week用时半天职位方向之一，DL。我目前在上的课程，讲的很基础，有些是对Stanford Machine Learning | Coursera的拓展延伸 Neural Networks and Deep Learning | Coursera CS231n职位方向之一，DL。目前在上的课程。由于精力有限，进度很慢。只是把CNN那一部分看完了。 Fundamentals of Computing | Coursera职位要求：精通python。下周开课，查缺补漏，向精通python努力 程序设计与算法 | Coursera职位要求：精通C/C++。下周开课，向熟练C/C++努力 Udacity Machine Learning Engineer Nanodegree | Udacity 用时1个月，目前Capstone简历项目经历撰写。主要偏应用，实践。包括监督学习，非监督学习，强化学习，深度学习。项目不难，但是课程量很大，知识点很多。 Deep Learning职位方向之一，DL。对初学者不太友好，没有 MLND 中的DL讲的细致。 How to Use Git and GitHub 完成技能要求之一，Git and Github。讲的很全，由于我一直独立编程，所以我大部分都没有用到。 Intro to Computer Science 完成职位要求：精通python。学完这门课应该算是了解python，时间紧的朋友建议放弃这门课。这门课课程量很大，通过python讲CS，进而引入爬虫。学完这门课，我改写了一个小程序专门爬Udacity课程目录，从而方便写我的博客。 Programming Foundations with Python 完成，用时1-2天职位要求：精通python。学完这门课应该算是入门python。强烈推荐函数，用类，写类。]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>Udacity</tag>
        <tag>Job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Ng Deep Learning Specialization Notebook]]></title>
    <url>%2F2017%2F08%2F28%2FCoursera%20Ng%20Deep%20Learning%20Specialization%20Notebook%2F</url>
    <content type="text"><![CDATA[For quick searchingCourse can be found hereVideo in YouTubeLecture Slides can be found in my Github(PDF version) If you want to break into AI, this Specialization will help you do so. Deep Learning is one of the most highly sought after skills in tech. We will help you become good at Deep Learning. In five courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory, but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach. You will also hear from many top leaders in Deep Learning, who will share with you their personal stories and give you career advice. AI is transforming multiple industries. After finishing this specialization, you will likely find creative ways to apply it to your work. We will help you master Deep Learning, understand how to apply it, and build a career in AI. Neural Networks and Deep LearningCourse can be found hereLecture slides can be found hereAbout this course: If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new “superpower” that will let you build AI systems that just weren’t possible a few years ago. In this course, you will learn the foundations of deep learning. When you finish this class, you will: Understand the major technology trends driving Deep Learning Be able to build, train and apply fully connected deep neural networks Know how to implement efficient (vectorized) neural networks Understand the key parameters in a neural network’s architecture This course also teaches you how Deep Learning actually works, rather than presenting only a cursory or surface-level description. So after completing it, you will be able to apply deep learning to a your own applications. If you are looking for a job in AI, after this course you will also be able to answer basic interview questions. This is the first course of the Deep Learning Specialization. Who is this class for: Prerequisites: Expected: - Programming: Basic Python programming skills, with the capability to work effectively with data structures. Recommended: - Mathematics: Matrix vector operations and notation. - Machine Learning: Understanding how to frame a machine learning problem, including how data is represented will be beneficial. If you have taken my Machine Learning Course here, you have much more than the needed level of knowledge. Week 1 Introduction to deep learningBe able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today. Learning Objectives Understand the major trends driving the rise of deep learning.Be able to explain how deep learning is applied to supervised learning.Understand what are the major categories of models (such as CNNs and RNNs), and when they should be applied.Be able to recognize the basics of when deep learning will (or will not) work well. Welcome to the Deep Learning SpecializationWelcome5 minIntroduction to Deep LearningWhat is a neural network?7 minSupervised Learning with Neural Networks8 minWhy is Deep Learning taking off?10 minAbout this Course2 minFrequently Asked Questions10 minCourse Resources1 minHow to use Discussion Forums10 minPractice QuestionsQuiz: Introduction to deep learning10 questionsQUIZIntroduction to deep learning10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 17, 11:59 PM PDT 1 point1.What does the analogy “AI is the new electricity” refer to? AI is powering personal devices in our homes and offices, similar to electricity. AI runs on computers and is thus powered by electricity, but it is letting computers do things not possible before. Through the “smart grid”, AI is delivering a new wave of electricity. Similar to electricity starting about 100 years ago, AI is transforming multiple industries.CorrectYes. AI is transforming many fields from the car industry to agriculture to supply-chain… 1 point2.Which of these are reasons for Deep Learning recently taking off? (Check the three options that apply.) Deep learning has resulted in significant improvements in important applications such as online advertising, speech recognition, and image recognition.CorrectThese were all examples discussed in lecture 3. Neural Networks are a brand new field.Un-selected is correct We have access to a lot more data.CorrectYes! The digitalization of our society has played a huge role in this. We have access to a lot more computational power.CorrectYes! The development of hardware, perhaps especially GPU computing, has significantly improved deep learning algorithms’ performance. 1 point3.Recall this diagram of iterating over different ML ideas. Which of the statements below are true? (Check all that apply.) Being able to try out ideas quickly allows deep learning engineers to iterate more quickly. Faster computation can help speed up how long a team takes to iterate to a good idea. It is faster to train on a big dataset than a small dataset. Recent progress in deep learning algorithms has allowed us to train good models faster (even without changing the CPU/GPU hardware).1 point4.When an experienced deep learning engineer works on a new problem, they can usually use insight from previous problems to train a good model on the first try, without needing to iterate multiple times through different models. True/False? TrueThis should not be selectedNo. Finding the characteristics of a model is key to have good performance. Although experience can help, it requires multiple iterations to build a good model. False1 point5.Which one of these plots represents a ReLU activation function? Figure 1: Figure 2: Figure 3: Figure 4:1 point6.Images for cat recognition is an example of “structured” data, because it is represented as a structured array in a computer. True/False? TrueThis should not be selectedNo. Images for cat recognition is an example of “unstructured” data. False1 point7.A demographic dataset with statistics on different cities’ population, GDP per capita, economic growth is an example of “unstructured” data because it contains data coming from different sources. True/False? True False1 point8.Why is an RNN (Recurrent Neural Network) used for machine translation, say translating English to French? (Check all that apply.) It can be trained as a supervised learning problem.CorrectYes. We can train it on many pairs of sentences x (English) and y (French). It is strictly more powerful than a Convolutional Neural Network (CNN).This should not be selectedNo. RNN and CNN are two distinct classes of models, with their own advantages and disadvantages. It is applicable when the input/output is a sequence (e.g., a sequence of words).CorrectYes. An RNN can map from a sequence of english words to a sequence of french words. RNNs represent the recurrent process of Idea-&gt;Code-&gt;Experiment-&gt;Idea-&gt;…. 1 point9.In this diagram which we hand-drew in lecture, what do the horizontal axis (x-axis) and vertical axis (y-axis) represent? x-axis is the amount of datay-axis (vertical axis) is the performance of the algorithm. x-axis is the performance of the algorithmy-axis (vertical axis) is the amount of data. x-axis is the amount of datay-axis is the size of the model you train. x-axis is the input to the algorithmy-axis is outputs.1 point10.Assuming the trends described in the previous question’s figure are accurate (and hoping you got the axis labels right), which of the following are true? (Check all that apply.) Increasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly. Decreasing the size of a neural network generally does not hurt an algorithm’s performance, and it may help significantly. Increasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly. Decreasing the training set size generally does not hurt an algorithm’s performance, and it may help significantly. Heroes of Deep Learning (Optional)Geoffrey Hinton interview40 minWeek 2 Neural Networks BasicsLearn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models. Learning Objectives Build a logistic regression model, structured as a shallow neural networkImplement the main steps of an ML algorithm, including making predictions, derivative computation, and gradient descent.Implement computationally efficient, highly vectorized, versions of models.Understand how to compute derivatives for logistic regression, using a backpropagation mindset.Become familiar with Python and NumpyWork with iPython NotebooksBe able to implement vectorization across multiple training examples Logistic Regression as a Neural NetworkBinary Classification8 minLogistic Regression5 minLogistic Regression Cost Function8 minGradient Descent11 minDerivatives7 minMore Derivative Examples10 minComputation graph3 minDerivatives with a Computation Graph14 minLogistic Regression Gradient Descent6 minGradient Descent on m Examples8 minPython and VectorizationVectorization8 minMore Vectorization Examples6 minVectorizing Logistic Regression7 minVectorizing Logistic Regression’s Gradient Output9 minBroadcasting in Python11 minA note on python/numpy vectors6 minQuick tour of Jupyter/iPython Notebooks3 minExplanation of logistic regression cost function (optional)7 minPractice QuestionsQuiz: Neural Network Basics10 questionsQUIZNeural Network Basics10 questionsTo Pass80% or higherDeadlineSeptember 24, 11:59 PM PDT 1 point1.What does a neuron compute? A neuron computes an activation function followed by a linear function (z = Wx + b) A neuron computes a function g that scales the input x linearly (Wx + b) A neuron computes a linear function (z = Wx + b) followed by an activation function A neuron computes the mean of all features before applying the output to an activation function1 point2.Which of these is the “Logistic Loss”? $\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = -( y^{(i)}\log(\hat{y}^{(i)}) + (1- y^{(i)})\log(1-\hat{y}^{(i)}))$ $\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \mid y^{(i)} - \hat{y}^{(i)} \mid$ $\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \mid y^{(i)} - \hat{y}^{(i)} \mid^{2}$ $\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = max(0, y^{(i)} - \hat{y}^{(i)})$1 point3.Suppose img is a (32,32,3) array, representing a 32x32 image with 3 color channels red, green and blue. How do you reshape this into a column vector? x = img.reshape((1,3232,3)) x = img.reshape((32*32,3)) x = img.reshape((32323,1)) x = img.reshape((3,32*32))1 point4.Consider the two following random arrays “a” and “b”:123a = np.random.randn(2, 3) # a.shape = (2, 3)b = np.random.randn(2, 1) # b.shape = (2, 1)c = a + b What will be the shape of “c”? c.shape = (2, 1) c.shape = (2, 3) The computation cannot happen because the sizes don’t match. It’s going to be “Error”! c.shape = (3, 2)1 point5.Consider the two following random arrays “a” and “b”:123a = np.random.randn(4, 3) # a.shape = (4, 3)b = np.random.randn(3, 2) # b.shape = (3, 2)c = a*b What will be the shape of “c”? The computation cannot happen because the sizes don’t match. It’s going to be “Error”! c.shape = (4,2) c.shape = (3, 3) c.shape = (4, 3)1 point6.Suppose you have $$n_x$$ input features per example. Recall that $$X = [x^{(1)} x^{(2)} … x^{(m)}]$$. What is the dimension of X? $$(m,n_x)$$ $$(m,1)$$ $$(n_x, m)$$ $$(1,m)$$1 point7.Recall that “np.dot(a,b)” performs a matrix multiplication on a and b, whereas “a*b” performs an element-wise multiplication. Consider the two following random arrays “a” and “b”:123a = np.random.randn(12288, 150) # a.shape = (12288, 150)b = np.random.randn(150, 45) # b.shape = (150, 45)c = np.dot(a,b) What is the shape of c? The computation cannot happen because the sizes don’t match. It’s going to be “Error”! c.shape = (12288, 45) c.shape = (12288, 150) c.shape = (150,150)1 point8.Consider the following code snippet:12345# a.shape = (3,4)# b.shape = (4,1)for i in range(3): for j in range(4): c[i][j] = a[i][j] + b[j] How do you vectorize this? c = a.T + b.T c = a + b.T c = a + b c = a.T + b1 point9.Consider the following code:123a = np.random.randn(3, 3)b = np.random.randn(3, 1)c = a*b What will be c? (If you’re not sure, feel free to run this in python to find out). This will invoke broadcasting, so b is copied three times to become (3,3), and $$*$$ is an element-wise product so c.shape will be (3, 3) This will invoke broadcasting, so b is copied three times to become (3, 3), and $$*$$ invokes a matrix multiplication operation of two 3x3 matrices so c.shape will be (3, 3) This will multiply a 3x3 matrix a with a 3x1 vector, thus resulting in a 3x1 vector. That is, c.shape = (3,1). It will lead to an error since you cannot use “*” to operate on these two matrices. You need to instead use np.dot(a,b)1 point10.Consider the following computation graph. What is the output J? J = (c - 1)*(b + a) J = (a - 1) * (b + c) J = ab + bc + a*c J = (b - 1) * (c + a) Programming AssignmentsDeep Learning Honor Code2 minDeep Learning Honor Code We strongly encourage students to form study groups, and discuss the lecture videos (including in-video questions). We also encourage you to get together with friends to watch the videos together as a group. However, the answers that you submit for the review questions should be your own work. For the programming exercises, you are welcome to discuss them with other students, discuss specific algorithms, properties of algorithms, etc.; we ask only that you not look at any source code written by a different student, nor show your solution code to other students. You are also not allowed to post your code publicly on github. Programming Assignment FAQ10 minPython Basics with numpy (optional)1hPython Basics with numpy (optional)Welcome to your first (Optional) programming exercise of the deep learning specialization. In this assignment you will: Learn how to use numpy. Implement some basic core deep learning functions such as the softmax, sigmoid, dsigmoid, etc… Learn how to handle data by normalizing inputs and reshaping images. Recognize the importance of vectorization. Understand how python broadcasting works. This assignment prepares you well for the upcoming assignment. Take your time to complete it and make sure you get the expected outputs when working through the different exercises. In some code blocks, you will find a “#GRADED FUNCTION: functionName” comment. Please do not modify it. After you are done, submit your work and check your results. You need to score 70% to pass. Good luck :) !Open Notebook Practice Programming Assignment: Python Basics with numpy (optional)1hLogistic Regression with a Neural Network mindset2hLogistic Regression with a Neural Network mindsetWelcome to the first (required) programming exercise of the deep learning specialization. In this notebook you will build your first image recognition algorithm. You will build a cat classifier that recognizes cats with 70% accuracy! As you keep learning new techniques you will increase it to 80+ % accuracy on cat vs. non-cat datasets. By completing this assignment you will: Work with logistic regression in a way that builds intuition relevant to neural networks. Learn how to minimize the cost function. Understand how derivatives of the cost are used to update parameters. Take your time to complete this assignment and make sure you get the expected outputs when working through the different exercises. In some code blocks, you will find a “#GRADED FUNCTION: functionName” comment. Please do not modify these comments. After you are done, submit your work and check your results. You need to score 70% to pass. Good luck :) !Open Notebook Programming Assignment: Logistic Regression with a Neural Network mindsetHeroes of Deep Learning (Optional)Pieter Abbeel interview16 minWeek 3 Shallow neural networksLearn to build a neural network with one hidden layer, using forward propagation and backpropagation. Learning Objectives Understand hidden units and hidden layersBe able to apply a variety of activation functions in a neural network.Build your first forward and backward propagation with a hidden layerApply random initialization to your neural networkBecome fluent with Deep Learning notations and Neural Network RepresentationsBuild and train a neural network with one hidden layer. Shallow Neural NetworkNeural Networks Overview4 minNeural Network Representation5 minComputing a Neural Network’s Output9 minVectorizing across multiple examples9 minExplanation for Vectorized Implementation7 minActivation functions10 minWhy do you need non-linear activation functions?5 minDerivatives of activation functions7 minGradient descent for Neural Networks9 minBackpropagation intuition (optional)15 minRandom Initialization7 minPractice QuestionsQuiz: Shallow Neural Networks10 questionsQUIZShallow Neural Networks10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 1, 11:59 PM PDT 1 point1.Which of the following are true? (Check all that apply.) X is a matrix in which each column is one training example. a[2] denotes the activation vector of the 2nd layer. a2 denotes the activation vector of the 2nd layer for the 12th training example. a[2]4 is the activation output by the 4th neuron of the 2nd layer X is a matrix in which each row is one training example. a2 denotes activation vector of the 12th layer on the 2nd training example. a[2]4 is the activation output of the 2nd layer for the 4th training example1 point2.The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. True/False? True False1 point3.Which of these is a correct vectorized implementation of forward propagation for layer l, where 1≤l≤L? Z[l]=W[l]A[l]+b[l]A[l+1]=gl+1 Z[l]=W[l]A[l−1]+b[l]A[l]=gl Z[l]=W[l−1]A[l]+b[l−1]A[l]=gl Z[l]=W[l]A[l]+b[l]A[l+1]=gl1 point4.You are building a binary classifier for recognizing cucumbers (y=1) vs. watermelons (y=0). Which one of these activation functions would you recommend using for the output layer? ReLU Leaky ReLU sigmoid tanh1 point5.Consider the following code:12A = np.random.randn(4,3)B = np.sum(A, axis = 1, keepdims = True) What will be B.shape? (If you’re not sure, feel free to run this in python to find out). (1, 3) (, 3) (4, ) (4, 1)1 point6.Suppose you have built a neural network. You decide to initialize the weights and biases to be zero. Which of the following statements is true? Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons. Each neuron in the first hidden layer will perform the same computation in the first iteration. But after one iteration of gradient descent they will learn to compute different things because we have “broken symmetry”. Each neuron in the first hidden layer will compute the same thing, but neurons in different layers will compute different things, thus we have accomplished “symmetry breaking” as described in lecture. The first hidden layer’s neurons will perform different computations from each other even in the first iteration; their parameters will thus keep evolving in their own way.1 point7.Logistic regression’s weights w should be initialized randomly rather than to all zeros, because if you initialize to all zeros, then logistic regression will fail to learn a useful decision boundary because it will fail to “break symmetry”, True/False? True False1 point8.You have built a network using the tanh activation for all the hidden units. You initialize the weights to relative large values, using np.random.randn(..,..)*1000. What will happen? This will cause the inputs of the tanh to also be very large, causing the units to be “highly activated” and thus speed up learning compared to if the weights had to start from small values. It doesn’t matter. So long as you initialize the weights randomly gradient descent is not affected by whether the weights are large or small. This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow. This will cause the inputs of the tanh to also be very large, thus causing gradients to also become large. You therefore have to set α to be very small to prevent divergence; this will slow down learning.1 point9.Consider the following 1 hidden layer neural network: Which of the following statements are True? (Check all that apply). W[1] will have shape (2, 4) b[1] will have shape (4, 1) W[1] will have shape (4, 2) b[1] will have shape (2, 1) W[2] will have shape (1, 4) b[2] will have shape (4, 1) W[2] will have shape (4, 1) b[2] will have shape (1, 1)1 point10.In the same network as the previous question, what are the dimensions of Z[1] and A[1]? Z[1] and A[1] are (1,4) Z[1] and A[1] are (4,m) Z[1] and A[1] are (4,1) Z[1] and A[1] are (4,2) Programming AssignmentPlanar data classification with a hidden layer2h 30mProgramming Assignment: Planar data classification with a hidden layerHeroes of Deep Learning (Optional)Ian Goodfellow interview14 minWeek 4 Deep Neural NetworksUnderstand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision. See deep neural networks as successive blocks put one after each other Build and train a deep L-layer Neural Network Analyze matrix and vector dimensions to check neural network implementations. Understand how to use a cache to pass information from forward propagation to back propagation. Understand the role of hyperparameters in deep learning Deep Neural NetworkDeep L-layer neural network5 minForward Propagation in a Deep Network7 minGetting your matrix dimensions right11 minWhy deep representations?10 minBuilding blocks of deep neural networks8 minForward and Backward Propagation10 minParameters vs Hyperparameters7 minWhat does this have to do with the brain?3 minPractice QuestionsQuiz: Key concepts on Deep Neural Networks10 questionsQUIZKey concepts on Deep Neural Networks10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 5, 11:59 PM PST 1 point1.What is the “cache” used for in our implementation of forward propagation and backward propagation? We use it to pass variables computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations. It is used to cache the intermediate values of the cost function during training. It is used to keep track of the hyperparameters that we are searching over, to speed up computation. We use it to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.1 point2.Among the following, which ones are “hyperparameters”? (Check all that apply.) learning rate α number of iterations bias vectors b[l] number of layers L in the neural network activation values a[l] weight matrices W[l] size of the hidden layers n[l]1 point3.Which of the following statements is true? The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers. The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers.1 point4.Vectorization allows you to compute forward propagation in an L-layer neural network without an explicit for-loop (or any other explicit iterative loop) over the layers l=1, 2, …,L. True/False? True False1 point5.Assume we store the values for n[l] in an array called layers, as follows: layer_dims = [nx, 4,3,2,1]. So layer 1 has four hidden units, layer 2 has 3 hidden units and so on. Which of the following for-loops will allow you to initialize the parameters for the model? 1234for(i in range(1, len(layer_dims)/2)): parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01 parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01 1234for(i in range(1, len(layer_dims)/2)): parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01 parameter[‘b’ + str(i)] = np.random.randn(layers[i-1], 1) * 0.01 1234for(i in range(1, len(layer_dims))): parameter[‘W’ + str(i)] = np.random.randn(layers[i-1], layers[i])) * 0.01 parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01 1234for(i in range(1, len(layer_dims))): parameter[‘W’ + str(i)] = np.random.randn(layers[i], layers[i-1])) * 0.01 parameter[‘b’ + str(i)] = np.random.randn(layers[i], 1) * 0.01 1 point6.Consider the following neural network. How many layers does this network have? The number of layers L is 4. The number of hidden layers is 3. The number of layers L is 3. The number of hidden layers is 3. The number of layers L is 4. The number of hidden layers is 4. The number of layers L is 5. The number of hidden layers is 4.1 point7.During forward propagation, in the forward function for a layer l you need to know what is the activation function in a layer (Sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer l, since the gradient depends on it. True/False? True False1 point8.There are certain functions with the following properties: (i) To compute the function using a shallow network circuit, you will need a large network (where we measure size by the number of logic gates in the network), but (ii) To compute it using a deep network circuit, you need only an exponentially smaller network. True/False? True False1 point9.Consider the following 2 hidden layer neural network: Which of the following statements are True? (Check all that apply). W[1] will have shape (4, 4) b[1] will have shape (4, 1) W[1] will have shape (3, 4) b[1] will have shape (3, 1) W[2] will have shape (3, 4) b[2] will have shape (1, 1) W[2] will have shape (3, 1) b[2] will have shape (3, 1) W[3] will have shape (3, 1) b[3] will have shape (1, 1) W[3] will have shape (1, 3) b[3] will have shape (3, 1)1 point10.Whereas the previous question used a specific network, in the general case what is the dimension of W^{[l]}, the weight matrix associated with layer l? W[l] has shape (n[l−1],n[l]) W[l] has shape (n[l],n[l+1]) W[l] has shape (n[l+1],n[l]) W[l] has shape (n[l],n[l−1]) Programming AssignmentsBuilding your Deep Neural Network: Step by Step2h 30mProgramming Assignment: Building your deep neural network: Step by StepDeep Neural Network - Application1hProgramming Assignment: Deep Neural Network ApplicationImproving Deep Neural Networks: Hyperparameter tuning, Regularization and OptimizationCourse can be found hereLecture slides can be found here About this course: This course will teach you the “magic” of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow. After 3 weeks, you will: Understand industry best-practices for building deep learning applications. Be able to effectively use the common neural network “tricks”, including initialization, L2 and dropout regularization, Batch normalization, gradient checking, Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence. Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance Be able to implement a neural network in TensorFlow. This is the second course of the Deep Learning Specialization. Who is this class for: This class is for: - Learners that took the first course of the specialization: “Neural Networks and Deep Learning” - Anyone that already understands fully-connected neural networks, and wants to learn the practical aspects of making them work well. Week 1 Practical aspects of Deep LearningLearning Objectives Recall that different types of initializations lead to different resultsRecognize the importance of initialization in complex neural networks.Recognize the difference between train/dev/test setsDiagnose the bias and variance issues in your modelLearn when and how to use regularization methods such as dropout or L2 regularization.Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with themUse gradient checking to verify the correctness of your backpropagation implementation Setting up your Machine Learning ApplicationTrain / Dev / Test sets12 minBias / Variance8 minBasic Recipe for Machine Learning6 minRegularizing your neural networkRegularization9 minWhy regularization reduces overfitting?7 minDropout Regularization9 minUnderstanding Dropout7 minOther regularization methods8 minSetting up your optimization problemNormalizing inputs5 minVanishing / Exploding gradients6 minWeight Initialization for Deep Networks6 minNumerical approximation of gradients6 minGradient checking6 minGradient Checking Implementation Notes5 minPractice QuestionsQuiz: Practical aspects of deep learning10 questionsQUIZPractical aspects of deep learning10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 15, 11:59 PM PDT 1 point1.If you have 10,000,000 examples, how would you split the train/dev/test set? 60% train . 20% dev . 20% test 98% train . 1% dev . 1% test 33% train . 33% dev . 33% test1 point2.The dev and test set should: Come from the same distribution Come from different distributions Be identical to each other (same (x,y) pairs) Have the same number of examples1 point3.If your Neural Network model seems to have high variance, what of the following would be promising things to try? Add regularization Get more training data Increase the number of units in each hidden layer Get more test data Make the Neural Network deeper1 point4.You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.) Increase the regularization parameter lambda Decrease the regularization parameter lambda Get more training data Use a bigger neural network1 point5.What is weight decay? A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration. A technique to avoid vanishing gradient by imposing a ceiling on the values of the weights. The process of gradually decreasing the learning rate during training. Gradual corruption of the weights in the neural network if it is trained on noisy data.1 point6.What happens when you increase the regularization hyperparameter lambda? Weights are pushed toward becoming smaller (closer to 0) Weights are pushed toward becoming bigger (further from 0) Doubling lambda should roughly result in doubling the weights Gradient descent taking bigger steps with each iteration (proportional to lambda)1 point7.With the inverted dropout technique, at test time: You apply dropout (randomly eliminating units) and do not keep the 1/keep_prob factor in the calculations used in training You apply dropout (randomly eliminating units) but keep the 1/keep_prob factor in the calculations used in training. You do not apply dropout (do not randomly eliminate units), but keep the 1/keep_prob factor in the calculations used in training. You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training1 point8.Increasing the parameter keep_prob from (say) 0.5 to 0.6 will likely cause the following: (Check the two that apply) Increasing the regularization effect Reducing the regularization effect Causing the neural network to end up with a higher training set error Causing the neural network to end up with a lower training set error1 point9.Which of these techniques are useful for reducing variance (reducing overfitting)? (Check all that apply.) Dropout Data augmentation Vanishing gradient Xavier initialization Gradient Checking Exploding gradient L2 regularization1 point10.Why do we normalize the inputs x? It makes the parameter initialization faster It makes the cost function faster to optimize It makes it easier to visualize the data Normalization is another word for regularization–It helps to reduce variance Programming assignmentsInitialization1hProgramming Assignment: InitializationRegularization1h 30mProgramming Assignment: Regularization1hhttps://www.coursera.org/learn/deep-neural-network/programming/SXQaI Gradient Checking1hProgramming Assignment: Gradient Checking1hhttps://www.coursera.org/learn/deep-neural-network/programming/n6NBD Heroes of Deep Learning (Optional)Yoshua Bengio interview25 minWeek 2 Optimization algorithmsLearning Objectives Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam Use random minibatches to accelerate the convergence and improve the optimization Know the benefits of learning rate decay and apply it to your optimization Optimization algorithmsMini-batch gradient descent11 minUnderstanding mini-batch gradient descent11 minExponentially weighted averages5 minUnderstanding exponentially weighted averages9 minBias correction in exponentially weighted averages4 minGradient descent with momentum9 minRMSprop7 minAdam optimization algorithm7 minLearning rate decay6 minThe problem of local optima5 minPractice QuestionsQuiz: Optimization algorithms10 questionsQUIZOptimization algorithms10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 22, 11:59 PM PDT 1 point1.Which notation would you use to denote the 3rd layer’s activations when the input is the 7th example from the 8th minibatch? a[8]{3}(7) a[8]{7}(3) a[3]{8}(7) a[3]{7}(8)1 point2.Which of these statements about mini-batch gradient descent do you agree with? You should implement mini-batch gradient descent without an explicit for-loop over different mini-batches, so that the algorithm processes all mini-batches at the same time (vectorization). Training one epoch (one pass through the training set) using mini-batch gradient descent is faster than training one epoch using batch gradient descent. One iteration of mini-batch gradient descent (computing on a single mini-batch) is faster than one iteration of batch gradient descent.1 point3.Why is the best mini-batch size usually not 1 and not m, but instead something in-between? If the mini-batch size is 1, you end up having to process the entire training set before making any progress. If the mini-batch size is 1, you lose the benefits of vectorization across examples in the mini-batch. If the mini-batch size is m, you end up with batch gradient descent, which has to process the whole training set before making progress. If the mini-batch size is m, you end up with stochastic gradient descent, which is usually slower than mini-batch gradient descent.1 point4.Suppose your learning algorithm’s cost J, plotted as a function of the number of iterations, looks like this: Which of the following do you agree with? If you’re using mini-batch gradient descent, something is wrong. But if you’re using batch gradient descent, this looks acceptable. If you’re using mini-batch gradient descent, this looks acceptable. But if you’re using batch gradient descent, something is wrong. Whether you’re using batch gradient descent or mini-batch gradient descent, this looks acceptable. Whether you’re using batch gradient descent or mini-batch gradient descent, something is wrong.1 point5.Suppose the temperature in Casablanca over the first three days of January are the same: Jan 1st: θ1=10oCJan 2nd: θ210oC(We used Fahrenheit in lecture, so will use Celsius here in honor of the metric world.) Say you use an exponentially weighted average with β=0.5 to track the temperature: v0=0, vt=βvt−1+(1−β)θt. If v2 is the value computed after day 2 without bias correction, and vcorrected2 is the value you compute with bias correction. What are these values? (You might be able to do this without a calculator, but you don’t actually need one. Remember what is bias correction doing.) v2=10, vcorrected2=10 v2=7.5, vcorrected2=7.5 v2=10, vcorrected2=7.5 v2=7.5, vcorrected2=101 point6.Which of these is NOT a good learning rate decay scheme? Here, t is the epoch number. α=1t√α0 α=11+2∗tα0 α=etα0 α=0.95tα01 point7.You use an exponentially weighted average on the London temperature dataset. You use the following to track the temperature: vt=βvt−1+(1−β)θt. The red line below was computed using β=0.9. What would happen to your red curve as you vary β? (Check the two that apply) Decreasing β will shift the red line slightly to the right. Increasing β will shift the red line slightly to the right. Decreasing β will create more oscillation within the red line. Increasing β will create more oscillations within the red line.1 point8.Consider this figure: These plots were generated with gradient descent; with gradient descent with momentum (β = 0.5) and gradient descent with momentum (β = 0.9). Which curve corresponds to which algorithm? (1) is gradient descent. (2) is gradient descent with momentum (large β) . (3) is gradient descent with momentum (small β) (1) is gradient descent with momentum (small β), (2) is gradient descent with momentum (small β), (3) is gradient descent (1) is gradient descent. (2) is gradient descent with momentum (small β). (3) is gradient descent with momentum (large β) (1) is gradient descent with momentum (small β). (2) is gradient descent. (3) is gradient descent with momentum (large β)1 point9.Suppose batch gradient descent in a deep network is taking excessively long to find a value of the parameters that achieves a small value for the cost function J(W[1],b[1],…,W[L],b[L]). Which of the following techniques could help find parameter values that attain a small value forJ? (Check all that apply) Try better random initialization for the weights Try tuning the learning rate α Try mini-batch gradient descent Try using Adam Try initializing all the weights to zero1 point10.Which of the following statements about Adam is False? Adam combines the advantages of RMSProp and momentum Adam should be used with batch gradient computations, not with mini-batches. We usually use “default” values for the hyperparameters β1,β2 and ε in Adam (β1=0.9, β2=0.999, ε=10−8) The learning rate hyperparameter α in Adam usually needs to be tuned. Programming assignmentOptimization2hProgramming Assignment: Optimization30 minhttps://www.coursera.org/learn/deep-neural-network/programming/Ckiv2 Heroes of Deep Learning (Optional)Yuanqing Lin interview13 minWeek 3 Hyperparameter tuning, Batch Normalization and Programming FrameworksMaster the process of hyperparameter tuning Hyperparameter tuningTuning process7 minUsing an appropriate scale to pick hyperparameters8 minHyperparameters tuning in practice: Pandas vs. Caviar6 minBatch NormalizationNormalizing activations in a network8 minFitting Batch Norm into a neural network12 minWhy does Batch Norm work?11 minBatch Norm at test time5 minMulti-class classificationSoftmax Regression11 minTraining a softmax classifier10 minIntroduction to programming frameworksDeep learning frameworks4 minTensorFlow16 minPractice QuestionsQuiz:Hyperparameter tuning, Batch Normalization, Programming Frameworks10 questionsQUIZHyperparameter tuning, Batch Normalization, Programming Frameworks10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 29, 11:59 PM PDT 1 point1.If searching among a large number of hyperparameters, you should try values in a grid rather than random values, so that you can carry out the search more systematically and not rely on chance. True or False? True False1 point2.Every hyperparameter, if set poorly, can have a huge negative impact on training, and so all hyperparameters are about equally important to tune well. True or False? True False1 point3.During hyperparameter search, whether you try to babysit one model (“Panda” strategy) or train a lot of models in parallel (“Caviar”) is largely determined by: Whether you use batch or mini-batch optimization The presence of local minima (and saddle points) in your neural network The amount of computational power you can access The number of hyperparameters you have to tune1 point4.If you think β (hyperparameter for momentum) is between on 0.9 and 0.99, which of the following is the recommended way to sample a value for beta? 12r = np.random.rand()beta = r*0.09 + 0.9 12r = np.random.rand()beta = 1-10**(- r - 1) 12r = np.random.rand()beta = 1-10**(- r + 1) 12r = np.random.rand()beta = r*0.9 + 0.09 1 point5.Finding good hyperparameter values is very time-consuming. So typically you should do it once at the start of the project, and try to find very good hyperparameters so that you don’t ever have to revisit tuning them again. True or false? True False1 point6.In batch normalization as presented in the videos, if you apply it on the lth layer of your neural network, what are you normalizing? z[l] b[l] W[l] a[l]1 point7.In the normalization formula z(i)norm=z(i)−μσ2+ε√, why do we use epsilon? To speed up convergence To avoid division by zero To have a more accurate normalization In case μ is too small1 point8.Which of the following statements about γ and β in Batch Norm are true? There is one global value of γ∈R and one global value of β∈R for each layer, and applies to all the hidden units in that layer. β and γ are hyperparameters of the algorithm, which we tune via random sampling. They set the mean and variance of the linear variable z[l] of a given layer. They can be learned using Adam, Gradient descent with momentum, or RMSprop, not just with gradient descent. The optimal values are γ=σ2+ε−−−−−√, and β=μ.1 point9.After training a neural network with Batch Norm, at test time, to evaluate the neural network on a new example you should: Perform the needed normalizations, use μ and σ2 estimated using an exponentially weighted average across mini-batches seen during training. Skip the step where you normalize using μ and σ2 since a single test example cannot be normalized. Use the most recent mini-batch’s value of μ and σ2 to perform the needed normalizations. If you implemented Batch Norm on mini-batches of (say) 256 examples, then to evaluate on one test example, duplicate that example 256 times so that you’re working with a mini-batch the same size as during training.1 point10.Which of these statements about deep learning programming frameworks are true? (Check all that apply) Even if a project is currently open source, good governance of the project helps ensure that the it remains open even in the long term, rather than become closed or modified to benefit only one company. Deep learning programming frameworks require cloud-based machines to run. A programming framework allows you to code up deep learning algorithms with typically fewer lines of code than a lower-level language such as Python. Programming assignmentTensorflow3hProgramming Assignment:TensorflowStructuring Machine Learning ProjectsAbout this course: You will learn how to build a successful machine learning project. If you aspire to be a technical leader in AI, and know how to set direction for your team’s work, this course will show you how. Much of this content has never been taught elsewhere, and is drawn from my experience building and shipping many deep learning products. This course also has two “flight simulators” that let you practice decision-making as a machine learning project leader. This provides “industry experience” that you might otherwise get only after years of ML work experience. After 2 weeks, you will: Understand how to diagnose errors in a machine learning system, and Be able to prioritize the most promising directions for reducing error Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance Know how to apply end-to-end learning, transfer learning, and multi-task learning I’ve seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time. This is a standalone course, and you can take this so long as you have basic machine learning knowledge. This is the third course in the Deep Learning Specialization. Who is this class for: Pre-requisites: - This course is aimed at individuals with basic knowledge of machine learning, who want to know how to set technical direction and prioritization for their work. - It is recommended that you take course one and two of this specialization (Neural Networks and Deep Learning, and Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization) prior to beginning this course. Course can be found here Week 1primary Week 2primary Week 3Week 4# Week 1Week 2Week 3Week 4# Week 1Week 2Week 3Week 4primary]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018校招算法工程师]]></title>
    <url>%2F2017%2F08%2F26%2F2018%E6%A0%A1%E6%8B%9B%E7%AC%94%E8%AF%95%E9%A2%98%E8%A7%A3%E7%AD%94%2F</url>
    <content type="text"><![CDATA[抛砖引玉 2018阿里校招笔试Question 11234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npN = 9intersections = np.array([[0,3],[1,5],[2,7],[3,3],[4,5],[5,7],[6,9],[7,3],[8,5]])M = 14roads = np.array([[0,1,4],[0,7,8],[1,2,8],[1,7,11],[2,3,7],[2,5,4],[2,8,2],[3,4,9],[3,5,14],[4,5,10],[5,6,2],[6,8,6],[6,7,1],[7,8,7]])s = 0t = 4def WaitTime(CurrentTime,permision): if CurrentTime % (2*permision) in [x for x in range(permision)]: # Green light return 0 else: return (2*permision) - CurrentTime % (2*permision) def CanGo(source): NodeWithTime = &#123;&#125; for i in range(len(roads[roads[:,0] == source])): NodeWithTime[roads[roads[:,0] == source][i,1]] = roads[roads[:,0] == source][i,2] for i in range(len(roads[roads[:,1] == source])): NodeWithTime[roads[roads[:,1] == source][i,0]] = roads[roads[:,1] == source][i,2] return NodeWithTime def minTravelTime(N,intersections,M,roads,s,t): Nodes = [s] A = &#123;&#125; A[s] = 0 for source in Nodes: permision = intersections[source][1] NodeWithTime = CanGo(source) for determination in NodeWithTime.keys(): value = A[source] + WaitTime(A[source],permision) + NodeWithTime[determination] if determination in A.keys(): A[determination] = min(value, A[determination]) else: A[determination] = value if determination not in Nodes: Nodes.extend([determination]) #print Nodes #print A return A[t] print minTravelTime(N,intersections,M,roads,s,t)# 28 Question 212345678910def number(k): count = 1 iteration = 1 while count &lt; k: count += iteration +1 iteration += 1 startposition = count - iteration return k - startposition 2018 京东笔试Question 1123456def calc(n): count = 1 while count &lt; k: count += iteration +1 iteration += 1 return iteration 80% Question 2brute force20%try12345678910111213def calc(n): if n == 1: return 1 else: sum = n*n for a in range(2,n+1): for b in range(1,n+1): if (a**b &lt; n) and (a**(b+1) &gt; n): for i in range(1, b+1): sum += i*n/(b) elif a**b == n: sum += n return sum 2018 didi11234567891011121314151617181920#coding = utf-8import sysif __name__ == &quot;__main__&quot;: # 读取第一行的n n = int(sys.stdin.readline().strip()) line = sys.stdin.readline().strip() # 把每一行的数字分隔后转化成int列表 values = map(int, line.split()) #values.sort() value_set = set(values) count = 0 for i in value_set: values.remove(i) if i in values: count += 1 elif i == 0: count += 1 print count 212345678910111213141516#coding = utf-8import sysif __name__ == &quot;__main__&quot;: # 读取第一行的n n = int(sys.stdin.readline().strip()) if n == 1: print 1 a = [2,3,5] while True: for i in range(len(a)): for j in range(len(a)): result = a[i] * a[j] if result &lt; a[-1]: a.append(result) a.sort() 头条11234567891011121314151617181920212223242526272829303132#coding = utf-8import sysif __name__ == &quot;__main__&quot;: # 读取第一行的n first_line = map(int,sys.stdin.readline().strip().split()) n = first_line[0] m = first_line[1] c = first_line[2] dicti = &#123;&#125; for i in range(n): # 读取每一行 line = sys.stdin.readline().strip() # 把每一行的数字分隔后转化成int列表 values = map(int, line.split()) if len(values) &gt; 1: dicti[tuple(values[1:])] = i+1 else: dicti[tuple()] = i+1 sth = &#123;&#125; for i in range(1,c+1): a = [] for keys in dicti.keys(): if i in keys: a += dicti[keys] sth[i] = a+a count = 0 for key, value in sth.iteritems(): for i in range(len(value)-1): if ((value[i]+m) % n) &gt; value[i+1]: count += 1 print count 21234567891011121314151617181920#coding = utf-8import sysif __name__ == &quot;__main__&quot;: # 读取第一行的n n = int(sys.stdin.readline().strip()) second_line = map(int,sys.stdin.readline().strip().split()) check_num = int(sys.stdin.readline().strip()) for i in range(check_num): # 读取每一行 line = sys.stdin.readline().strip() # 把每一行的数字分隔后转化成int列表 values = map(int, line.split()) count = 0 sth = second_line[values[0]-1:values[1]] while values[2] in sth: count += 1 sth.remove(values[2]) print count 科大讯飞10.812345678910111213141516171819202122people = raw_input()count = 0while len(people) != 0: if people[0] == &apos;L&apos;: count += 1 people = people[1:] print people else: if &apos;L&apos; in people: L_index = people.index(&apos;L&apos;) if (L_index + 1) &lt; (len(people) - 1): people = &apos;R&apos;+people[L_index+1:] print people else: people = &apos;R&apos; print people else: print people count += len(people) breakprint peopleprint count 20.412345678910111213141516171819202122232425n = raw_input()course = []dicti = &#123;&#125;repeat_key = []for i in range(int(n)): line = raw_input() course += [line.split()] #print &apos;course&apos;+str(course) if course[i][0] in dicti.keys(): repeat_key += [course[i][0]] #print &apos;dicti[course[i][0]]&apos;+str(dicti[course[i][0]]) #print &apos;[course[i][1]]&apos;+str([course[i][1]]) dicti[course[i][0]] += [course[i][1]] else: dicti[course[i][0]] = [course[i][1]]#print course#print dicti#print repeat_keyif len(repeat_key) == 0: print &apos;YES&apos;else: repeat_key.sort() for i in repeat_key: print i +&apos; &apos;+ &apos; &apos;.join(dicti[i]) 123456501 20452123 20452322 20452601 20452822 204527 1234311 20452123 20452243 204531 312345n = raw_input()name = []for i in range(int(n)): group = raw_input() name += [group] souhu11234567891011121314f_l = raw_input()arr = f_l.split('/')path = []for i in range(len(arr)): if arr[i] == '.': pass elif arr[i] == '..': path.pop() else: path += [arr[i]]sth = '/'.join(path)sth = sth[:-1]print sth 21234567891011121314151617181920212223242526f_l = raw_input()n = int(f_l)s_l = raw_input()arr = map(int, s_l.split(' '))# print s_l#print arrcount = 0while len(arr) &gt; 1: if arr[0] &lt; arr[-1]: count += arr[0]*2 #print count arr.pop(0) elif arr[0] &gt; arr[-1]: count += arr[-1]*2 #print count arr.pop(-1) else: count += arr[0]*2 #print count arr.pop(0) arr.pop(-1)if len(arr) == 0: print countelse: print count+arr[0] 美的1 文件中不重复的url12345678910111213141516171819202122232425262728293031323334def get_next_target(page): start_link = page.find(&apos;http&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;http&apos;, start_link) end_quote = page.find(&apos;http&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef union(a, b): if b not in a: a.append(b) def get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: union(links,url) page = page[endpos:] else: break return linksfile_path = &quot;G:\intro to DA\Data Wrangling with MongoDB - Udacity.html&quot;HtmlFile = open(file_path, &apos;r&apos;)context = HtmlFile.read() # type: stringlinks = get_all_links(context)HtmlFile.close()urlfile = open(&apos;URL.txt&apos;, &apos;w&apos;)for url in links: url.write(&quot;%s\n&quot; % item)urlfile.close() 2 实现前向传播算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import numpy as npdef layer_sizes(X, Y): &quot;&quot;&quot; Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer &quot;&quot;&quot; n_x = X.shape[0] # size of input layer n_h = 4 n_y = Y.shape[0] # size of output layer return (n_x, n_h, n_y)def initialize_parameters(n_x, n_h, n_y): &quot;&quot;&quot; Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) &quot;&quot;&quot; np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random. W1 = np.random.randn(n_h,n_x) * 0.01 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y,n_h) * 0.01 b2 = np.zeros((n_y,1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;&quot;W1&quot;: W1, &quot;b1&quot;: b1, &quot;W2&quot;: W2, &quot;b2&quot;: b2&#125; return parametersdef forward_propagation(X, parameters): &quot;&quot;&quot; Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot; &quot;&quot;&quot; # Retrieve each parameter from the dictionary &quot;parameters&quot; W1 = parameters[&quot;W1&quot;] b1 = parameters[&quot;b1&quot;] W2 = parameters[&quot;W2&quot;] b2 = parameters[&quot;b2&quot;] # Implement Forward Propagation to calculate A2 (probabilities) Z1 = np.dot(W1, X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = &#123;&quot;Z1&quot;: Z1, &quot;A1&quot;: A1, &quot;Z2&quot;: Z2, &quot;A2&quot;: A2&#125; return A2, cachen_x = layer_sizes(X, Y)[0]n_y = layer_sizes(X, Y)[2]# Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs = &quot;W1, b1, W2, b2, parameters&quot;.parameters = initialize_parameters(n_x, n_h, n_y)W1 = parameters[&quot;W1&quot;]b1 = parameters[&quot;b1&quot;]W2 = parameters[&quot;W2&quot;]b2 = parameters[&quot;b2&quot;]A2, cache = forward_propagation(X, parameters) 31234567891011121314151617181920212223242526272829303132```# Face ++25%改进未完成```pythonf = Truewhile f: s_n = raw_input() n = int(s_n) if n == -1: break else: s = 0 s_f = 0.0 for i in range(n): s1 = raw_input() if &apos;.&apos; in s1: s_li = s_f.split(&apos;.&apos;) s_l, s_r = long(s_li[0]), long(s_li[1]) li = s1.split(&apos;.&apos;) left, right = long(li[0]), long(li[1]) lll = s_l + left rrr = s_r + right n1 = float(s1) else: n1 = int(s1) s += n1 print s baidu1当时未完成12345678910111213141516171819202122232425262728293031323334s_n = raw_input()n = int(s_n)def f(n): lis = [2,3,5,7] lis_d = [] for i in range(1,6): lis_i = [] for j in range(10**i,10**(i+1)): lis_ = lis[:] fl = [] for k in lis_: if j % k == 0: fl.append(True) break if True not in fl: lis_i.append(j) lis.append(j) #print 'lis_i: ' + str(lis_i) lis__ = lis_i[:] if len(lis__) &gt; 0: for ii in lis__: s_ii = str(ii) r_s_ii = s_ii[::-1] r_ii = int(r_s_ii) if r_ii != ii and r_ii in lis__: lis_d.append(ii) if len(lis_d) == n: return lis_dprint f(n) 小红书190%123456789101112131415161718192021222324s = raw_input()f = Truenum = len(s) / 3ss = s.replace("RED", "", num)li = '1234567890'max_n = -1s_n = ''for i in range(len(ss)): si = ss[i] if si in li: f = False s_n += si n = int(s_n) if n &gt; max_n: max_n = n else: s_n = ''if max_n &gt; 0: print max_n if f: print -1 2stay stuned.123456789101112131415161718192021s = raw_input()mo = s_mo.split(' ')n_mo = map(int, mo)a,b,n = n_mo[0], n_mo[1], n_mo[2]ss = ['r','e','d']ite = ss[:]for i in range(n): l_re = [] for j in ss: for k in ite: re = j + k l_re.append(re) ss = l_re[:]#judgefor i in range(len(ss)): ch = ss[i] ch.replace('r','1',len(ss)) ch.replace('e','0',len(ss)) ch.replace('d','0',len(ss) 380%12345678910111213141516171819202122232425262728293031s_n = raw_input()n = int(s_n)s_mo = raw_input()mo = s_mo.split(' ')n_mo = map(int, mo)# print n_mos_sum = raw_input()sum_ = int(s_sum)li = n_mo[:]count = 1if sum_ in li: print 1else: while min(li) &lt; sum_: l_re = [] for i in li: for j in n_mo: re = i + j #if re not in li: l_re.append(re) li = l_re[:] count += 1 if sum_ in li: print count break if min(li) &gt; sum_: print -1 宜信1少了半小时答题，差一点就搞定了1234567891011121314151617181920212223s = raw_input()mo = s.split(' ')n_mo = map(int, mo)N, W = n_mo[0], n_mo[1]if W &gt;= 1: li = [] for i in range(N): s_i = str(i) li.append(s_i) itr = li[:] for j in range(W-1): l_re = [] for k1 in li: for k2 in itr: if k1 != k2: re = k1 + k2 l_re.append(re) li = l_re[:] print (W**N - len(li)) 新浪面试一面：主要招的是视频和图像方向的 自我介绍 做的项目 CNN权值 CNN介绍 kmeans GMM 区别 物体运动快慢 视频编解码 美团1210% 1234567891011121314151617181920212223242526272829303132333435s = raw_input()s = s.split(' ')s_n, s_m = s[0], s[1]c = len(s_n) / 10c_m = len(s_n) % 10if c == 0: n = int(s_n)if c &gt; 0: su = 0 for k in range(c): for kk in range(-1, -11, -1): tem = int(s_n[kk+10*k]) * 10**(-(kk+10*k)-1) #print tem su = su + tem for kk_ in range(-1, -c_m, -1): tem = int(s_n[kk_+10*c]) * 10**(-(kk_+10*c)-1) su = su + tem n = su m = int(s_m)#print n, ml = [str(x) for x in range(1,n+1)]ll = ''.join(l)#print llfor i in range(m): num = raw_input() p = ll.find(num) ll = num + ll[:p] + ll[p+1:] #print ll for j in ll: print j 深信服科技校园招聘研发试题–卷B150%12345s = raw_input()a = s.decode('unicode_escape')a = a[1:-1]print a 225%123456789101112131415161718192021222324252627282930313233343536373839s = raw_input()def f(s, set_i_o): set_00 = s + set_i_o[0] set_01 = set_i_o[1] set_10 = set_i_o[0] set_11 = set_i_o[1] + s return (set_00, set_01), (set_10, set_11)c = 0arr = [('','')]# * 2**(c)while c &lt; len(s): l = len(arr) for i in range(l): a, b = f(s[c], arr[i]) arr += [a] #print arr arr += [b] arr = arr[2**c:] #print arr c += 1o = [None] * len(arr) for i in range(len(arr)): o[i] = arr[i][1]+arr[i][0]s_o = set(o) for j in s_o: print j``` # 人人## 1AC```pythonn = int(raw_input())print (n-1)*n*2 - (n-1)*n + 1 250%123456789101112131415161718192021222324252627282930s = raw_input()arr = map(int, s.split(' '))n,m,k = arr[0], arr[1], arr[2]#print n,m,kl = []for i in range(k): tem = raw_input() arr = map(int, tem.split(' ')) #print arr arr = tuple(arr) l.append(arr) #print arr,la = sorted(l, key=lambda t: t[0]) b = sorted(l, key=lambda t: t[1])#print a, bif a == b: p = '' if (1,1) not in a: a = [(1,1)] + a if (n,m) not in a: a = a + [(n,m)] for i in range(len(a)-1): nd = a[i+1][0] - a[i][0] nr = a[i+1][1] - a[i][1] p += 'D'*nd p += 'R'*nr print pelse: print "Impossible" 3浪潮一面 自我介绍 突出点 性格明天之前给二面消息，三天内给offer]]></content>
  </entry>
  <entry>
    <title><![CDATA[Coursera UW Machine Learning Specialization Notebook]]></title>
    <url>%2F2017%2F08%2F19%2FCoursera%20UW%20Machine%20Learning%20Specialization%20Notebook%2F</url>
    <content type="text"><![CDATA[For quick searchingCourse can be found hereNotes can be found in my Github This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data. Machine Learning Foundations: A Case Study ApproachCourse can be found hereLecture slides can be found hereNotes can be found in my Github About this course: Do you have data and wonder what it can tell you? Do you need a deeper understanding of the core ways in which machine learning can improve your business? Do you want to be able to converse with specialists about anything from regression and classification to deep learning and recommender systems? In this course, you will get hands-on experience with machine learning from a series of practical case-studies. At the end of the first course you will have studied how to predict house prices based on house-level features, analyze sentiment from user reviews, retrieve documents of interest, recommend products, and search for images. Through hands-on practice with these use cases, you will be able to apply machine learning methods in a wide range of domains. This first course treats the machine learning method as a black box. Using this abstraction, you will focus on understanding tasks of interest, matching these tasks to machine learning tools, and assessing the quality of the output. In subsequent courses, you will delve into the components of this black box by examining models and algorithms. Together, these pieces form the machine learning pipeline, which you will use in developing intelligent applications. Learning Outcomes: By the end of this course, you will be able to: -Identify potential applications of machine learning in practice. -Describe the core differences in analyses enabled by regression, classification, and clustering. -Select the appropriate machine learning task for a potential application. -Apply regression, classification, clustering, retrieval, recommender systems, and deep learning. -Represent your data as features to serve as input to machine learning models. -Assess the model quality in terms of relevant error metrics for each task. -Utilize a dataset to fit a model to analyze new data. -Build an end-to-end application that uses machine learning at its core. -Implement these techniques in Python. Welcome to Machine Learning Foundations: A Case Study Approach! By joining this course, you’ve taken a first step in becoming a machine learning expert. You will learn a broad range of machine learning methods for deriving intelligence from data, and by the end of the course you will be able to implement actual intelligent applications. These applications will allow you to perform predictions, personalized recommendations and retrieval, and much more. If you continue with the subsequent courses in the Machine Learning specialization, you will delve deeper into the methods and algorithms, giving you the power to develop and deploy new machine learning services. To begin, we recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. These assignments—one per Module 2 through 6—will walk you through Python implementations of intelligent applications for: Predicting house prices Analyzing the sentiment of product reviews Retrieving Wikipedia articles Recommending songs Classifying images with deep learning Week 1 WelcomeMachine learning is everywhere, but is often operating behind the scenes. This introduction to the specialization provides you with insights into the power of machine learning, and the multitude of intelligent applications you personally will be able to develop and deploy upon completion. We also discuss who we are, how we got here, and our view of the future of intelligent applications. Why you should learn machine learning with usImportant Update regarding the Machine Learning Specialization10 minSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: intro.pdf Welcome to this course and specialization41 secWho we are5 minMachine learning is changing the world3 minWhy a case study approach?7 minSpecialization overview6 minWho this specialization is for and what you will be able to doHow we got into ML3 minWho is this specialization for?4 minWhat you’ll be able to do57 secThe capstone and an example intelligent application6 minThe future of intelligent applications2 minGetting started with the tools for the courseReading: Getting started with Python, IPython Notebook &amp; GraphLab Create10 minReading: where should my files go?10 minGetting started with Python and the IPython NotebookDownload the IPython Notebook used in this lesson to follow along10 minStarting an IPython Notebook5 minCreating variables in Python7 minConditional statements and loops in Python8 minCreating functions and lambdas in Python3 minGetting started with SFrames for data engineering and analysisDownload the IPython Notebook used in this lesson to follow along10 minStarting GraphLab Create &amp; loading an SFrame4 minCanvas for data visualization4 minInteracting with columns of an SFrame4 minUsing .apply() for data transformation5 minWeek 2 Regression: Predicting House PricesThis week you will build your first intelligent application that makes predictions from data. We will explore this idea within the context of our first case study, predicting house prices, where you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,…). This is just one of the many places where regression can be applied.Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression. You will also examine how to analyze the performance of your predictive model and implement regression in practice using an iPython notebook. Linear regression modelingSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: regression-intro-annotated.pdf Predicting house prices: A case study in regression1 minWhat is the goal and how might you naively address it?3 minLinear Regression: A Model-Based Approach5 minAdding higher order effects4 minEvaluating regression modelsEvaluating overfitting via training/test split6 minTraining/test curves4 minAdding other features2 minOther regression examples3 minSummary of regressionRegression ML block diagram5 minQuiz: Regression9 questionsPredicting house prices: IPython NotebookDownload the IPython Notebook used in this lesson to follow along10 minLoading &amp; exploring house sale data7 minSplitting the data into training and test sets2 minLearning a simple regression model to predict house prices from house size3 minEvaluating error (RMSE) of the simple model2 minVisualizing predictions of simple model with Matplotlib4 minInspecting the model coefficients learned1 minExploring other features of the data6 minLearning a model to predict house prices from more features3 minApplying learned models to predict price of an average house5 minApplying learned models to predict price of two fancy houses7 minProgramming assignmentReading: Predicting house prices assignment10 minQuiz: Predicting house prices3 questionsWeek 3 Classification: Analyzing SentimentHow do you guess whether a person felt positively or negatively about an experience, just from a short review they wrote? In our second case study, analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,…).This task is an example of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis, image classification. You will analyze the accuracy of your classifier, implement an actual classifier in an iPython notebook, and take a first stab at a core piece of the intelligent application you will build and deploy in your capstone. Classification modelingSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: classification-annotated.pdf Analyzing the sentiment of reviews: A case study in classification38 secWhat is an intelligent restaurant review system?4 minExamples of classification tasks4 minLinear classifiers5 minDecision boundaries3 minEvaluating classification modelsTraining and evaluating a classifier4 minWhat’s a good accuracy?3 minFalse positives, false negatives, and confusion matrices6 minLearning curves5 minClass probabilities1 minSummary of classificationClassification ML block diagram3 minQuiz: Classification7 questionsAnalyzing sentiment: IPython NotebookDownload the IPython Notebook used in this lesson to follow along10 minLoading &amp; exploring product review data2 minCreating the word count vector2 minExploring the most popular product4 minDefining which reviews have positive or negative sentiment4 minTraining a sentiment classifier3 minEvaluating a classifier &amp; the ROC curve4 minApplying model to find most positive &amp; negative reviews for a product4 minExploring the most positive &amp; negative aspects of a product4 minProgramming assignmentReading: Analyzing product sentiment assignment10 minQuiz: Analyzing product sentiment11 questionsWeek 4 Clustering and Similarity: Retrieving DocumentsA reader is interested in a specific news article and you want to find a similar articles to recommend. What is the right notion of similarity? How do I automatically search over documents to find the one that is most similar? How do I quantitatively represent the documents in the first place? In this third case study, retrieving documents, you will examine various document representations and an algorithm to retrieve the most similar subset. You will also consider structured representations of the documents that automatically group articles by similarity (e.g., document topic). You will actually build an intelligent document retrieval system for Wikipedia entries in an iPython notebook. Algorithms for retrieval and measuring similarity of documentsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: clustering-intro-annotated.pdf Document retrieval: A case study in clustering and measuring similarity35 secWhat is the document retrieval task?1 minWord count representation for measuring similarity6 minPrioritizing important words with tf-idf3 minCalculating tf-idf vectors5 minRetrieving similar documents using nearest neighbor search2 minClustering models and algorithmsClustering documents task overview2 minClustering documents: An unsupervised learning task4 mink-means: A clustering algorithm3 minOther examples of clustering6 minSummary of clustering and similarityClustering and similarity ML block diagram7 minQuiz: Clustering and Similarity6 questionsDocument retrieval: IPython NotebookDownload the IPython Notebook used in this lesson to follow along10 minLoading &amp; exploring Wikipedia data5 minExploring word counts5 minComputing &amp; exploring TF-IDFs7 minComputing distances between Wikipedia articles5 minBuilding &amp; exploring a nearest neighbors model for Wikipedia articles3 minExamples of document retrieval in action4 minProgramming assignmentReading: Retrieving Wikipedia articles assignment10 minQuiz: Retrieving Wikipedia articles9 questionsMachine Learning: RegressionCourse can be found hereSummary can be found in my Github About this course: Case Study - Predicting Housing Prices In our first case study, predicting house prices, you will create models that predict a continuous value (price) from input features (square footage, number of bedrooms and bathrooms,…). This is just one of the many places where regression can be applied. Other applications range from predicting health outcomes in medicine, stock prices in finance, and power usage in high-performance computing, to analyzing which regulators are important for gene expression. In this course, you will explore regularized linear regression models for the task of prediction and feature selection. You will be able to handle very large sets of features and select between models of various complexity. You will also analyze the impact of aspects of your data – such as outliers – on your selected models and predictions. To fit these models, you will implement optimization algorithms that scale to large datasets. Learning Outcomes: By the end of this course, you will be able to: -Describe the input and output of a regression model. -Compare and contrast bias and variance when modeling data. -Estimate model parameters using optimization algorithms. -Tune parameters with cross validation. -Analyze the performance of the model. -Describe the notion of sparsity and how LASSO leads to sparse solutions. -Deploy methods to select between models. -Exploit the model to form predictions. -Build a regression model to predict prices using a housing dataset. -Implement these techniques in Python. Week 1WelcomeRegression is one of the most important and broadly used machine learning and statistics tools out there. It allows you to make predictions from data by learning the relationship between features of your data and some observed, continuous-valued response. Regression is used in a massive number of applications ranging from predicting stock prices to understanding gene regulatory networks. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have. What is this course about?Slides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: intro.pdf Welcome!1 minWhat is the course about?3 minOutlining the first half of the course5 minOutlining the second half of the course5 minAssumed background4 minReading: Software tools you’ll need10 minSimple Linear RegressionOur course starts from the most basic regression model: Just fitting a line to data. This simple model for forming predictions from a single, univariate feature of the data is appropriately called “simple linear regression”. In this module, we describe the high-level regression task and then specialize these concepts to the simple linear regression case. You will learn how to formulate a simple regression model and fit the model to data using both a closed-form solution as well as an iterative optimization algorithm called gradient descent. Based on this fitted function, you will interpret the estimated model parameters and form predictions. You will also analyze the sensitivity of your fit to outlying observations. You will examine all of these concepts in the context of a case study of predicting house prices from the square feet of the house. Regression fundamentalsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: week1_simpleregression-annotated.pdf A case study in predicting house prices1 minRegression fundamentals: data &amp; model8 minRegression fundamentals: the task2 minRegression ML block diagram4 minThe simple linear regression model, its use, and interpretationThe simple linear regression model2 minThe cost of using a given line6 minUsing the fitted line6 minInterpreting the fitted line6 minAn aside on optimization: one dimensional objectivesDefining our least squares optimization objective3 minFinding maxima or minima analytically7 minMaximizing a 1d function: a worked example2 minFinding the max via hill climbing6 minFinding the min via hill descent3 minChoosing stepsize and convergence criteria6 minAn aside on optimization: multidimensional objectivesGradients: derivatives in multiple dimensions5 minGradient descent: multidimensional hill descent6 minFinding the least squares lineComputing the gradient of RSS7 minApproach 1: closed-form solution5 minOptional reading: worked-out example for closed-form solution10 minApproach 2: gradient descent7 minOptional reading: worked-out example for gradient descent10 minComparing the approaches1 minDiscussion and summary of simple linear regressionDownload notebooks to follow along10 minInfluence of high leverage points: exploring the data4 minInfluence of high leverage points: removing Center City7 minInfluence of high leverage points: removing high-end towns3 minAsymmetric cost functions3 minA brief recap1 minQuiz: Simple Linear Regression7 questions Programming assignmentReading: Fitting a simple linear regression model on housing data10 minQuiz: Fitting a simple linear regression model on housing data4 questions Week 2 Multiple RegressionThe next step in moving beyond simple linear regression is to consider “multiple regression” where multiple features of the data are used to form predictions. More specifically, in this module, you will learn how to build models of more complex relationship between a single variable (e.g., ‘square feet’) and the observed response (like ‘house sales price’). This includes things like fitting a polynomial to your data, or capturing seasonal changes in the response value. You will also learn how to incorporate multiple input variables (e.g., ‘square feet’, ‘# bedrooms’, ‘# bathrooms’). You will then be able to describe how all of these models can still be cast within the linear regression framework, but now using multiple “features”. Within this multiple regression framework, you will fit models to data, interpret estimated coefficients, and form predictions. Here, you will also implement a gradient descent algorithm for fitting a multiple regression model. Multiple features of one inputSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: week2_multipleregression-annotated.pdf Multiple regression intro30 secPolynomial regression3 minModeling seasonality8 minWhere we see seasonality3 minRegression with general features of 1 input2 minIncorporating multiple inputsMotivating the use of multiple inputs4 minDefining notation3 minRegression with features of multiple inputs3 minInterpreting the multiple regression fit7 minSetting the stage for computing the least squares fitOptional reading: review of matrix algebra10 minThis section involves some use of matrix algebra. If you’d like to brush up on it, we recommend a short tutorial. Rewriting the single observation model in vector notation6 minRewriting the model for all observations in matrix notation4 minComputing the cost of a D-dimensional curve9 minComputing the least squares D-dimensional curveComputing the gradient of RSS3 minApproach 1: closed-form solution3 minDiscussing the closed-form solution4 minApproach 2: gradient descent2 minFeature-by-feature update9 minAlgorithmic summary of gradient descent approach4 minSummarizing multiple regressionA brief recap1 minQuiz: Multiple Regression9 questions Programming assignment 1Reading: Exploring different multiple regression models for house price prediction10 minQuiz: Exploring different multiple regression models for house price prediction8 questions Programming assignment 2Numpy tutorial10 minMore information on Numpy, beyond this tutorial, can be found in the Numpy getting started guide. Reading: Implementing gradient descent for multiple regression10 minQuiz: Implementing gradient descent for multiple regression5 questions Week 3 Assessing PerformanceHaving learned about linear regression models and algorithms for estimating the parameters of such models, you are now ready to assess how well your considered method should perform in predicting new data. You are also ready to select amongst possible models to choose the best performing. This module is all about these important topics of model selection and assessment. You will examine both theoretical and practical aspects of such analyses. You will first explore the concept of measuring the “loss” of your predictions, and use this to define training, test, and generalization error. For these measures of error, you will analyze how they vary with model complexity and how they might be utilized to form a valid assessment of predictive performance. This leads directly to an important conversation about the bias-variance tradeoff, which is fundamental to machine learning. Finally, you will devise a method to first select amongst models and then assess the performance of the selected model. The concepts described in this module are key to all machine learning problems, well-beyond the regression setting addressed in this course. Defining how we assess performanceSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: week3_assessingperformance-annotated.pdf Assessing performance intro32 secWhat do we mean by “loss”?4 min3 measures of loss and their trends with model complexityTraining error: assessing loss on the training set7 minGeneralization error: what we really want8 minTest error: what we can actually compute4 minDefining overfitting2 minTraining/test split1 min3 sources of error and the bias-variance tradeoffIrreducible error and bias6 minVariance and the bias-variance tradeoff6 minError vs. amount of data6 minOPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of errorFormally defining the 3 sources of error14 minFormally deriving why 3 sources of error20 minPutting the pieces togetherTraining/validation/test split for model selection, fitting, and assessment7 minA brief recap1 minQuiz: Assessing Performance13 questions Programming assignmentReading: Exploring the bias-variance tradeoff10 minQuiz: Exploring the bias-variance tradeoff4 questions Week 4 Ridge RegressionYou have examined how the performance of a model varies with increasing model complexity, and can describe the potential pitfall of complex models becoming overfit to the training data. In this module, you will explore a very simple, but extremely effective technique for automatically coping with this issue. This method is called “ridge regression”. You start out with a complex model, but now fit the model in a manner that not only incorporates a measure of fit to the training data, but also a term that biases the solution away from overfitted functions. To this end, you will explore symptoms of overfitted functions and use this to define a quantitative measure to use in your revised optimization objective. You will derive both a closed-form and gradient descent algorithm for fitting the ridge regression objective; these forms are small modifications from the original algorithms you derived for multiple regression. To select the strength of the bias away from overfitting, you will explore a general-purpose method called “cross validation”. You will implement both cross-validation and gradient descent to fit a ridge regression model and select the regularization constant. Characteristics of overfit modelsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: week4_ridgeregression-annotated.pdf Symptoms of overfitting in polynomial regression2 minDownload the notebook and follow along10 minNext, we will see a demo illustrating the concept of overfitting. We recommend you download the IPython Notebook used in the demo to follow along. (The second and third parts of this notebook will be used to demonstrate ridge regression and LASSO; two techniques to address overfitting.) IPython Notebook:Overfitting_Demo_Ridge_Lasso.ipynb.zip Overfitting demo7 minOverfitting for more general multiple regression models3 minThe ridge objectiveBalancing fit and magnitude of coefficients7 minThe resulting ridge objective and its extreme solutions5 minHow ridge regression balances bias and variance1 minDownload the notebook and follow along10 minRidge regression demo9 minThe ridge coefficient path4 minOptimizing the ridge objectiveComputing the gradient of the ridge objective5 minApproach 1: closed-form solution6 minDiscussing the closed-form solution5 minApproach 2: gradient descent9 minTying up the loose endsSelecting tuning parameters via cross validation3 minK-fold cross validation5 minHow to handle the intercept6 minA brief recap1 minQuiz: Ridge Regression9 questions Programming Assignment 1Reading: Observing effects of L2 penalty in polynomial regression10 minQuiz: Observing effects of L2 penalty in polynomial regression7 questions Programming Assignment 2Reading: Implementing ridge regression via gradient descent10 minQuiz: Implementing ridge regression via gradient descent8 questions Week 5 Feature Selection &amp; LassoA fundamental machine learning task is to select amongst a set of features to include in a model. In this module, you will explore this idea in the context of multiple regression, and describe how such feature selection is important for both interpretability and efficiency of forming predictions. To start, you will examine methods that search over an enumeration of models including different subsets of features. You will analyze both exhaustive search and greedy algorithms. Then, instead of an explicit enumeration, we turn to Lasso regression, which implicitly performs feature selection in a manner akin to ridge regression: A complex model is fit based on a measure of fit to the training data plus a measure of overfitting different than that used in ridge. This lasso method has had impact in numerous applied domains, and the ideas behind the method have fundamentally changed machine learning and statistics. You will also implement a coordinate descent algorithm for fitting a Lasso model. Coordinate descent is another, general, optimization technique, which is useful in many areas of machine learning. Feature selection via explicit model enumerationSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: week5_lassoregression-annotated.pdf The feature selection task3 minAll subsets6 minComplexity of all subsets3 minGreedy algorithms7 minComplexity of the greedy forward stepwise algorithm2 minFeature selection implicitly via regularized regressionCan we use regularization for feature selection?3 minThresholding ridge coefficients?4 minThe lasso objective and its coefficient path7 minGeometric intuition for sparsity of lasso solutionsVisualizing the ridge cost7 minVisualizing the ridge solution6 minVisualizing the lasso cost and solution7 minDownload the notebook and follow along10 minLasso demo5 minSetting the stage for solving the lassoWhat makes the lasso objective different3 minCoordinate descent5 minNormalizing features3 minCoordinate descent for least squares regression (normalized features)8 minOptimizing the lasso objectiveCoordinate descent for lasso (normalized features)5 minAssessing convergence and other lasso solvers2 minCoordinate descent for lasso (unnormalized features)1 minOPTIONAL ADVANCED MATERIAL: Deriving the lasso coordinate descent updateDeriving the lasso coordinate descent update19 minTying up loose endsChoosing the penalty strength and other practical issues with lasso5 minA brief recap3 minQuiz: Feature Selection and Lasso7 questions Programming Assignment 1Reading: Using LASSO to select features10 minQuiz: Using LASSO to select features6 questions Programming Assignment 2Reading: Implementing LASSO using coordinate descent10 minQuiz: Implementing LASSO using coordinate descent8 questions Week 6Nearest Neighbors &amp; Kernel RegressionUp to this point, we have focused on methods that fit parametric functions—like polynomials and hyperplanes—to the entire dataset. In this module, we instead turn our attention to a class of “nonparametric” methods. These methods allow the complexity of the model to increase as more data are observed, and result in fits that adapt locally to the observations. We start by considering the simple and intuitive example of nonparametric methods, nearest neighbor regression: The prediction for a query point is based on the outputs of the most related observations in the training set. This approach is extremely simple, but can provide excellent predictions, especially for large datasets. You will deploy algorithms to search for the nearest neighbors and form predictions based on the discovered neighbors. Building on this idea, we turn to kernel regression. Instead of forming predictions based on a small set of neighboring observations, kernel regression uses all observations in the dataset, but the impact of these observations on the predicted value is weighted by their similarity to the query point. You will analyze the theoretical performance of these methods in the limit of infinite training data, and explore the scenarios in which these methods work well versus struggle. You will also implement these techniques and observe their practical behavior. Motivating local fitsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: week6_NNkernelregression-annotated.pdf Limitations of parametric regression3 minNearest neighbor regression1-Nearest neighbor regression approach8 minDistance metrics4 min1-Nearest neighbor algorithm3 mink-Nearest neighbors and weighted k-nearest neighborsk-Nearest neighbors regression7 mink-Nearest neighbors in practice3 minWeighted k-nearest neighbors4 minKernel regressionFrom weighted k-NN to kernel regression6 minGlobal fits of parametric models vs. local fits of kernel regression6 mink-NN and kernel regression wrapupPerformance of NN as amount of data grows7 minIssues with high-dimensions, data scarcity, and computational complexity3 mink-NN for classification1 minA brief recap1 minQuiz: Nearest Neighbors &amp; Kernel Regression7 questionsProgramming AssignmentReading: Predicting house prices using k-nearest neighbors regression10 minQuiz: Predicting house prices using k-nearest neighbors regression8 questionsClosing RemarksIn the conclusion of the course, we will recap what we have covered. This represents both techniques specific to regression, as well as foundational machine learning concepts that will appear throughout the specialization. We also briefly discuss some important regression techniques we did not cover in this course. We conclude with an overview of what’s in store for you in the rest of the specialization. What we’ve learnedSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: closing.pdf Simple and multiple regression4 minAssessing performance and ridge regression7 minFeature selection, lasso, and nearest neighbor regression4 minSummary and what’s ahead in the specializationWhat we covered and what we didn’t cover5 minThank you!1 minMachine Learning: ClassificationCourse can be found hereLecture slides can be found hereSummary can be found in my Github About this course: Case Studies: Analyzing Sentiment &amp; Loan Default Prediction In our case study on analyzing sentiment, you will create models that predict a class (positive/negative sentiment) from input features (text of the reviews, user profile information,…). In our second case study for this course, loan default prediction, you will tackle financial data, and predict when a loan is likely to be risky or safe for the bank. These tasks are an examples of classification, one of the most widely used areas of machine learning, with a broad array of applications, including ad targeting, spam detection, medical diagnosis and image classification. In this course, you will create classifiers that provide state-of-the-art performance on a variety of tasks. You will become familiar with the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting. In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent. You will implement these technique on real-world, large-scale machine learning tasks. You will also address significant tasks you will face in real-world applications of ML, including handling missing data and measuring precision and recall to evaluate a classifier. This course is hands-on, action-packed, and full of visualizations and illustrations of how these techniques will behave on real data. We’ve also included optional content in every module, covering advanced topics for those who want to go even deeper! Learning Objectives: By the end of this course, you will be able to: -Describe the input and output of a classification model. -Tackle both binary and multiclass classification problems. -Implement a logistic regression model for large-scale classification. -Create a non-linear model using decision trees. -Improve the performance of any model using boosting. -Scale your methods with stochastic gradient ascent. -Describe the underlying decision boundaries. -Build a classification model to predict sentiment in a product review dataset. -Analyze financial data to predict loan defaults. -Use techniques for handling missing data. -Evaluate your models using precision-recall metrics. -Implement these techniques in Python (or in the language of your choice, though Python is highly recommended). Week 1Welcome !Classification is one of the most widely used techniques in machine learning, with a broad array of applications, including sentiment analysis, ad targeting, spam detection, risk assessment, medical diagnosis and image classification. The core goal of classification is to predict a category or class y from some inputs x. Through this course, you will become familiar with the fundamental models and algorithms used in classification, as well as a number of core machine learning concepts. Rather than covering all aspects of classification, you will focus on a few core techniques, which are widely used in the real-world to get state-of-the-art performance. By following our hands-on approach, you will implement your own algorithms on multiple real-world tasks, and deeply grasp the core techniques needed to be successful with these approaches in practice. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have. Welcome to the courseImportant Update regarding the Machine Learning Specialization 10 minHello Machine Learning learners, Please know that due to unforeseen circumstances, courses 5 and 6 - Recommender Systems &amp; Dimensionality Reduction and An Intelligent Application with Deep Learning - will not be launching as part of the Machine Learning Specialization. We understand this may come as very disappointing news and we’re deeply sorry for this inconvenience. If you have paid for these courses or have received financial aid from Coursera, you will remain eligible to earn your Specialization Certificate upon successfully completing courses 1-4 of the Specialization. If you paid for courses 5 &amp; 6 via a pre-payment toward the Specialization, Coursera has provided you with free access to two other courses offered by the University of Washington: Computational Neuroscience and Data Manipulation at Scale: Systems and Algorithms. An email has been sent out with specific instructions on how to enroll in these courses. If you individually paid for either x or y course, you will receive a refund within the next two weeks. If you have any questions or would like to request a refund, please feel free to contact Coursera’s 24/7 learner support team via the Request a Refund article in the Learner Help Center. The last day to request a refund will be April 30, 2017. We value you as a Coursera learner and want to ensure that your experience with the Machine Learning Specialization remains a positive one. Regards, The Coursera Team Slides presented in this module 10 minFor those interested, the slides presented in the videos for this module can be downloaded here: intro.pdf Welcome to the classification course, a part of the Machine Learning Specialization 1 minhttps://www.coursera.org/learn/ml-classification/lecture/YMpzf/welcome-to-the-classification-course-a-part-of-the-machine-learning What is this course about? 6 minhttps://www.coursera.org/learn/ml-classification/lecture/qZhKx/what-is-this-course-about Impact of classification 1 minhttps://www.coursera.org/learn/ml-classification/lecture/OnpWH/impact-of-classification Course overview and detailsCourse overview 3 minhttps://www.coursera.org/learn/ml-classification/lecture/84fuF/course-overview Outline of first half of course 5 minhttps://www.coursera.org/learn/ml-classification/lecture/LyubT/outline-of-first-half-of-course Outline of second half of course 5 minhttps://www.coursera.org/learn/ml-classification/lecture/z1g9k/outline-of-second-half-of-course Assumed background 3 minhttps://www.coursera.org/learn/ml-classification/lecture/IindM/assumed-background Let’s get started! 45 sechttps://www.coursera.org/learn/ml-classification/lecture/AktDn/lets-get-started Reading: Software tools you’ll need 10 minSoftware tools you’ll need for this courseHow this specialization was designed. The learning approach in this specialization is to start from use cases and then dig into algorithms and methods, what we call a case-studies approach. We are very excited about this approach, since it has worked well in several other courses. The first course, Machine Learning: Foundations, was focused on understanding how ML can be used in various cases studies. The second course, Machine Learning: Regression, was focused on models that predict a continuous value from input features. The follow on courses will dig into more details of algorithms and methods of other ML areas. We expect all learners to have taken the first and second course, before taking this course. Classification - A Machine Learning Approach. This course focuses classification, one of the most important types of data analysis, with a wide range of applications. After successfully completing this course, you will be able to use classification methods in practice, implement some of the most fundamental algorithms in this area, and choose the right model for your task. You will become familiar with the most successful techniques, which are most widely used in practice, including logistic regression, decision trees and boosting. In addition, you will be able to design and implement the underlying algorithms that can learn these models at scale, using stochastic gradient ascent. Programming assignment formatAlmost every module will be associated with one or two programming assignments. The goal of these assignments is to have hands-on experience on the techniques we discuss in lectures. To test your implementations, you will be asked questions in a quiz following the assignment. You will be implementing core classification techniques or other ML concepts from scratch in most modules. In a few module, you will also explore fundamental ML concepts, such as regularization or precision-recall, using existing implementations of ML algorithms, with the goal of gaining proficiency in the ML concepts. Why PythonIn this course, we are going to use the Python programming language to build several intelligent applications that use machine learning. Python is a simple scripting language that makes it easy to interact with data. Furthermore, Python has a wide range of packages that make it easy to get started and build applications, from the simplest ones to the most complex. Python is widely used in industry, and is becoming the de facto language for data science in industry. (R is another alternative language. However, R tends to be significantly less scalable and has very few deployment tools, thus it is seldomly used for production code in industry. It is possible, but discouraged to use R in this specialization.) We will also encourage the use the IPython Notebook in our assignments. The IPython Notebook is a simple interactive environment for programming with Python, which makes it really easy to share your results. Think about it as a combination of a Python terminal and a wiki page. Thus, you can combine code, plots and text to explain what you did. (You are not required to use IPython Notebook in the assignments, and should have no problem using straight up Python if you prefer.) Useful software toolsAlthough you will be implementing algorithms from scratch in various assignments, some software tools will be useful in the process. In particular, there are four types of data tools that would be helpful: Data manipulation: to help you slice-and-dice the data, create new features, and clean the data. Matrix operations: in the inner loops of your algorithms, you will do various matrix operations, and libraries focus on these will speed-up your code significantly. Plotting library: so you can visualize data and models. Pre-implemented ML algorithms: in some assignments where we are focusing on exploring ML classification models, you will use a pre-implemented ML algorithms to help focus your efforts on the fundamentals. 1.Tools for data manipulation For data manipulation, we recommend using SFrame, an open-source, highly-scalable Python library for data manipulation. An alternative is the Pandas library. A huge advantage of SFrame over Pandas is that with SFrame, you are not limited to datasets that fit in memory, which allows you to deal with large datasets, even on a laptop. (The SFrame API is very similar to Pandas’ API. Here is a doc showing the relationship between the two of them.) 2.Tools for matrix operation For matrix operations, we strongly recommend Numpy, an open-source Python library that provides fast performance, for data that fits in memory. 3.Tools for plotting For plotting, we strongly recommend you use Matplotlib, an open-source Python library with extensive plotting functionality. 4.Tools with pre-implemented ML algorithms For the few assignments where you will be using pre-implemented ML algorithms, we recommend you use GraphLab Create, which we used in the first and second course, a package we have been working on for many years now, and has seen an exciting adoption curve, especially in industry with folks building real applications. A popular alternative is to use scikit-learn. GraphLab Create is more scalable than scikit-learn and simpler to use when your data is not numeric vectors. On the other hand, scikit-learn is open-source. In this course, most of the assignments are about implementing algorithms from scratch, so this choice is more flexible than in the first course. We are happy, however, for you to use any tool(s) of your liking. As you will notice, we are only grading the output of your programs, so the specific software tool is not the focus of the course. More details on using other tools are at the end of this doc. It’s important to emphasize that this specialization is not about providing training for a specific software package. The goal of the specialization is for your effort to be spent on learning the fundamental concepts and algorithms behind machine learning in a hands-on fashion. These concepts transcend any single package. What you learn here you can use whether you write code from scratch, use any existing ML packages out there, or any that may be developed in the future. We are happy to hear that so many of you are enjoying this approach so far! 5.Licenses for SFrame &amp; GraphLab Create The SFrame package is available in open-source under a permissive BSD license. So, you will always be able to use SFrames for free. GraphLab Create is free on a 1-year, renewable license for educational purposes, including Coursera. The reason we suggest you use GraphLab Create for this course is because this software will make it much easier for you see machine learning in action and to help you complete your assignments quickly. Upgrade GraphLab CreateIf you are using GraphLab Create and already have it installed, please make sure you upgrade to the latest version! The simplest way to do this is to: Open the GraphLab Launcher.Click on ‘TERMINAL’.On the terminal window, type:pip install --upgrade graphlab-create ResourcesThese are some good resources you can explore, if you are using the recommended software tools: In the first course of this ML specialization, Machine Learning Foundations, we provided many tutorials and getting started guides. We recommend you go over those before tackling this course.There are many Python resources available online. Here is a good place for documentation.For SFrame &amp; GraphLab Create, there is also a lot of information available online. Here are some starting points: the User Guide and detailed API docs.For Numpy, here is a getting started guide. We will also provide a tutorial when it’s time to use it. Installing the recommended software toolsIf you choose to use the recommended tools, you have two options: downloading and installing the required software or using a prepackaged version on a free instance on Amazon EC2. 1.Option 1: Downloading and installing all software on your own machine Download and install Python, IPython Notebook, Numpy, SFrame and GraphLab Create. You can find the instructions here. 2.Option 2: Using a free Amazon EC2 with all the software pre-installed If you do not have a 64-bit computer, you will not be able to run GraphLab Create. Additionally, some of you may want a simple experience where you don’t have to download the course content and install everything locally. Here, we’ll address these situations! Amazon EC2 offers free cloud computing hours with what they call micro instances. These instances are all we need to do the work for this course. We have created an image for one such instance that is easy to launch and contains all the course content. This will allow you to run everything you need for this course in the cloud for free, without having to install anything locally. (You do need to create an Amazon EC2 account and have internet access.) You can find step-by-step instructions here: https://turi.com/download/install-graphlab-create-aws-coursera.html We note that installing all the software on your own local machine may be the right option for most people; especially since you can run locally everything without needing to be online to do the homeworks. But, the option using Amazon EC2 should be a great alternative. Github repository with starter codeIn each module of the course, we have a reading with the assignments for that module as well as some starter code. For those interested, the starter code and demos used in this course are also available in a public Github repository: https://github.com/learnml/machine-learning-specialization Using other software packagesWe strongly encourage you to use the recommended software packages for this course, since they will allow you to learn the fundamental concepts more quickly. However, you are welcome to use others. Here are a few notes if you do so. 1.Installing other software tools In the instructions above, you will be using the GraphLab Launcher, which will automatically install Python, IPython Notebook, Numpy, Matplotlib, SFrame and GraphLab Create. If you don’t use the GraphLab Launcher, you will need to install each of these tools separately, by following the pages linked above. Anaconda is a good tool to help simplify some of this installation. 2.If you are using SFrame, but not GraphLab Create GraphLab Create uses SFrame under the hood, but you can use just SFrame for most assignments. If you choose to do so, in the starter code for the assignments, you should change the line import graphlab import sframe import sframeand everything should work with just some small modifications, e.g., the calls: graphlab.SFrame(...)will become sframe.SFrame(...) 3.If you are using other software tools out there You are welcome to use other packages, e.g., scikit-learn instead of GraphLab Create, or Pandas instead of SFrame, or even R instead of Python. If you choose to use all these different packages, we will provide the datasets (in standard CSV format) and the assignment questions will not depend specifically on the recommended tools. Linear Classifiers &amp; Logistic RegressionLinear classifiers are amongst the most practical classification methods. For example, in our sentiment analysis case-study, a linear classifier associates a coefficient with the counts of each word in the sentence. In this module, you will become proficient in this type of representation. You will focus on a particularly useful type of linear classifier called logistic regression, which, in addition to allowing you to predict a class, provides a probability associated with the prediction. These probabilities are extremely useful, since they provide a degree of confidence in the predictions. In this module, you will also be able to construct features from categorical inputs, and to tackle classification problems with more than two class (multiclass problems). You will examine the results of these techniques on a real-world product sentiment analysis task. Linear classifiersSlides presented in this module 10 minFor those interested, the slides presented in the videos for this module can be downloaded here: logistic-regression-model-annotated.pdf Linear classifiers: A motivating example 2 minhttps://www.coursera.org/learn/ml-classification/lecture/HNKIj/linear-classifiers-a-motivating-example Intuition behind linear classifiers 3 minhttps://www.coursera.org/learn/ml-classification/lecture/lCBwS/intuition-behind-linear-classifiers Decision boundaries 3 minhttps://www.coursera.org/learn/ml-classification/lecture/NIdE0/decision-boundaries Linear classifier model 5 minhttps://www.coursera.org/learn/ml-classification/lecture/XBc9n/linear-classifier-model Effect of coefficient values on decision boundary 2 minhttps://www.coursera.org/learn/ml-classification/lecture/Qy2js/effect-of-coefficient-values-on-decision-boundary Using features of the inputs 2 minhttps://www.coursera.org/learn/ml-classification/lecture/WHIMY/using-features-of-the-inputs Class probabilitiesPredicting class probabilities 1 minhttps://www.coursera.org/learn/ml-classification/lecture/j4Ji0/predicting-class-probabilities Review of basics of probabilities 6 minhttps://www.coursera.org/learn/ml-classification/lecture/p6rtM/review-of-basics-of-probabilities Review of basics of conditional probabilities 8 minhttps://www.coursera.org/learn/ml-classification/lecture/Cun2N/review-of-basics-of-conditional-probabilities Using probabilities in classification 2 minhttps://www.coursera.org/learn/ml-classification/lecture/f0nhO/using-probabilities-in-classification Logistic regressionPredicting class probabilities with (generalized) linear models 5 minhttps://www.coursera.org/learn/ml-classification/lecture/OV5Kt/predicting-class-probabilities-with-generalized-linear-models The sigmoid (or logistic) link function 4 minhttps://www.coursera.org/learn/ml-classification/lecture/KXvGC/the-sigmoid-or-logistic-link-function Logistic regression model 5 minhttps://www.coursera.org/learn/ml-classification/lecture/OJQXu/logistic-regression-model Effect of coefficient values on predicted probabilities 7 minhttps://www.coursera.org/learn/ml-classification/lecture/JkEEH/effect-of-coefficient-values-on-predicted-probabilities Overview of learning logistic regression models 2 minhttps://www.coursera.org/learn/ml-classification/lecture/GuxAJ/overview-of-learning-logistic-regression-models Practical issues for classificationEncoding categorical inputs 4 minhttps://www.coursera.org/learn/ml-classification/lecture/kCY0D/encoding-categorical-inputs Multiclass classification with 1 versus all 7 minhttps://www.coursera.org/learn/ml-classification/lecture/N7QA6/multiclass-classification-with-1-versus-all Summarizing linear classifiers &amp; logistic regressionRecap of logistic regression classifier 1 minhttps://www.coursera.org/learn/ml-classification/lecture/laPcB/recap-of-logistic-regression-classifier Quiz: Linear Classifiers &amp; Logistic Regression 5 questionsQUIZLinear Classifiers &amp; Logistic Regression5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineAugust 20, 11:59 PM PDT 1 point1.(True/False) A linear classifier assigns the predicted class based on the sign of Score(x)=wTh(x). True False1 point2.(True/False) For a conditional probability distribution over y|x, where y takes on two values (+1, -1, i.e. good review, bad review) P(y=+1|x)+P(y=−1|x)=1. True False1 point3.Which function does logistic regression use to “squeeze” the real line to [0, 1]? Logistic function Absolute value function Zero function1 point4.If Score(x)=wTh(x)&gt;0, which of the following is true about P(y=+1|x)? P(y = +1 | x) &lt;= 0.5 P(y = +1 | x) &gt; 0.5 Can’t say anything about P(y = +1 | x)1 point5.Consider training a 1 vs. all multiclass classifier for the problem of digit recognition using logistic regression. There are 10 digits, thus there are 10 classes. How many logistic regression classifiers will we have to train? Programming AssignmentPredicting sentiment from product reviews 10 minQuiz: Predicting sentiment from product reviews 12 questionsQUIZPredicting sentiment from product reviews12 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineAugust 20, 11:59 PM PDT 1 point1.How many weights are greater than or equal to 0? 1 point2.Of the three data points in sample_test_data, which one has the lowest probability of being classified as a positive review? First Second Third1 point3.Which of the following products are represented in the 20 most positive reviews? Snuza Portable Baby Movement Monitor MamaDoo Kids Foldable Play Yard Mattress Topper, Blue Britax Decathlon Convertible Car Seat, Tiffany Safety 1st Exchangeable Tip 3 in 1 Thermometer1 point4.Which of the following products are represented in the 20 most negative reviews? The First Years True Choice P400 Premium Digital Monitor, 2 Parent Unit JP Lizzy Chocolate Ice Classic Tote Set Peg-Perego Tatamia High Chair, White Latte Safety 1st High-Def Digital Monitor1 point5.What is the accuracy of the sentiment_model on the test_data? Round your answer to 2 decimal places (e.g. 0.76). 1 point6.Does a higher accuracy value on the training_data always imply that the classifier is better? Yes, higher accuracy on training data always implies that the classifier is better. No, higher accuracy on training data does not necessarily imply that the classifier is better.1 point7.Consider the coefficients of simple_model. There should be 21 of them, an intercept term + one for each word in significant_words. How many of the 20 coefficients (corresponding to the 20 significant_words and excluding the intercept term) are positive for the simple_model? 1 point8.Are the positive words in the simple_model also positive words in the sentiment_model? Yes No1 point9.Which model (sentiment_model or simple_model) has higher accuracy on the TRAINING set? Sentiment_model Simple_model1 point10.Which model (sentiment_model or simple_model) has higher accuracy on the TEST set? Sentiment_model Simple_model1 point11.Enter the accuracy of the majority class classifier model on the test_data. Round your answer to two decimal places (e.g. 0.76). 1 point12.Is the sentiment_model definitely better than the majority class classifier (the baseline)? Yes No Week 2Learning Linear ClassifiersOnce familiar with linear classifiers and logistic regression, you can now dive in and write your first learning algorithm for classification. In particular, you will use gradient ascent to learn the coefficients of your classifier from data. You first will need to define the quality metric for these tasks using an approach called maximum likelihood estimation (MLE). You will also become familiar with a simple technique for selecting the step size for gradient ascent. An optional, advanced part of this module will cover the derivation of the gradient for logistic regression. You will implement your own learning algorithm for logistic regression from scratch, and use it to learn a sentiment analysis classifier. Maximum likelihood estimationSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: logistic-regression-learning-annotated.pdf Goal: Learning parameters of logistic regression2 minhttps://www.coursera.org/learn/ml-classification/lecture/uxALW/goal-learning-parameters-of-logistic-regression Intuition behind maximum likelihood estimation4 minData likelihood8 minFinding best linear classifier with gradient ascent3 minGradient ascent algorithm for learning logistic regression classifierReview of gradient ascent6 minLearning algorithm for logistic regression3 minExample of computing derivative for logistic regression5 minInterpreting derivative for logistic regression5 minSummary of gradient ascent for logistic regression2 minChoosing step size for gradient ascent/descentChoosing step size5 minCareful with step sizes that are too large4 minRule of thumb for choosing step size3 min(VERY OPTIONAL LESSON) Deriving gradient of logistic regression(VERY OPTIONAL) Deriving gradient of logistic regression: Log trick4 min(VERY OPTIONAL) Expressing the log-likelihood3 min(VERY OPTIONAL) Deriving probability y=-1 given x2 min(VERY OPTIONAL) Rewriting the log likelihood into a simpler form8 min(VERY OPTIONAL) Deriving gradient of log likelihood8 minSummarizing learning linear classifiersRecap of learning logistic regression classifiers1 minQuiz: Learning Linear Classifiers6 questionsQUIZLearning Linear Classifiers6 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineAugust 27, 11:59 PM PDT 1 point1.(True/False) A linear classifier can only learn positive coefficients. True False1 point2.(True/False) In order to train a logistic regression model, we find the weights that maximize the likelihood of the model. True False1 point3.(True/False) The data likelihood is the product of the probability of the inputs x given the weights w and response y. True False1 point4.Questions 4 and 5 refer to the following scenario. Consider the setting where our inputs are 1-dimensional. We have data x y 2.5 +1 0.3 -1 2.8 +1 0.5 +1 and the current estimates of the weights are w0=0 and w1=1. (w0: the intercept, w1: the weight for x). Calculate the likelihood of this data. Round your answer to 2 decimal places. 12345678910111213141516171819202122import numpy as npx = np.array([2.5,0.3,2.8,0.5])y = np.array([1,0,1,1])w0 = 0w1 = 1def ypre(x,w0,w1): score = w0 + x * w1 return sigmoid(score)def sigmoid(score): return 1.0/(1+ (np.exp(-score)))lik = 1for i in range(len(x)): if i == 1: lik *= (1 - ypre(x[i], 0, 1)) else: lik *= (ypre(x[i], 0, 1))print lik# 0.230765141474 1 point5.Refer to the scenario given in Question 4 to answer the following: Calculate the derivative of the log likelihood with respect to w1. Round your answer to 2 decimal places. 123456789def der(hx, ytrue, ypre): return hx * (ytrue - ypre)sum = 0for i in range(len(x)): sum += der(x[i],y[i],ypre(x[i],0,1)) print sum# 0.366590721926 1 point6.Which of the following is true about gradient ascent? Select all that apply. It is an iterative algorithm It only updates a few of the parameters, not all of them It finds the maximum by “hill climbing” Programming AssignmentImplementing logistic regression from scratch10 minQuiz: Implementing logistic regression from scratch8 questionsQUIZImplementing logistic regression from scratch8 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineAugust 27, 11:59 PM PDT 1 point1.How many reviews in amazon_baby_subset.gl contain the word perfect? 1 point2.Consider the feature_matrix that was obtained by converting our data to NumPy format. How many features are there in the feature_matrix? 1 point3.Assuming that the intercept is present, how does the number of features in feature_matrix relate to the number of features in the logistic regression model? Let x = [number of features in feature_matrix] and y = [number of features in logistic regression model]. y = x - 1 y = x y = x + 1 None of the above1 point4.Run your logistic regression solver with provided parameters. As each iteration of gradient ascent passes, does the log-likelihood increase or decrease? It increases. It decreases. None of the above1 point5.We make predictions using the weights just learned. How many reviews were predicted to have positive sentiment? 1 point6.What is the accuracy of the model on predictions made above? (round to 2 digits of accuracy) 1 point7.We look at “most positive” words, the words that correspond most strongly with positive reviews. Which of the following words is not present in the top 10 “most positive” words? love easy great perfect cheap1 point8.Similarly, we look at “most negative” words, the words that correspond most strongly with negative reviews. Which of the following words is not present in the top 10 “most negative” words? need work disappointed even return Overfitting &amp; Regularization in Logistic RegressionAs we saw in the regression course, overfitting is perhaps the most significant challenge you will face as you apply machine learning approaches in practice. This challenge can be particularly significant for logistic regression, as you will discover in this module, since we not only risk getting an overly complex decision boundary, but your classifier can also become overly confident about the probabilities it predicts. In this module, you will investigate overfitting in classification in significant detail, and obtain broad practical insights from some interesting visualizations of the classifiers’ outputs. You will then add a regularization term to your optimization to mitigate overfitting. You will investigate both L2 regularization to penalize large coefficient values, and L1 regularization to obtain additional sparsity in the coefficients. Finally, you will modify your gradient ascent algorithm to learn regularized logistic regression classifiers. You will implement your own regularized logistic regression classifier from scratch, and investigate the impact of the L2 penalty on real-world sentiment analysis data. Overfitting in classificationSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: logistic-regression-overfitting-annotated.pdf Evaluating a classifier 3 minhttps://www.coursera.org/learn/ml-classification/lecture/RzxaQ/evaluating-a-classifier Review of overfitting in regression3 minOverfitting in classification5 minVisualizing overfitting with high-degree polynomial features3 minOverconfident predictions due to overfittingOverfitting in classifiers leads to overconfident predictions5 minVisualizing overconfident predictions4 min(OPTIONAL) Another perspecting on overfitting in logistic regression8 minL2 regularized logistic regressionPenalizing large coefficients to mitigate overfitting5 minL2 regularized logistic regression4 minVisualizing effect of L2 regularization in logistic regression5 minLearning L2 regularized logistic regression with gradient ascent7 minSparse logistic regressionSparse logistic regression with L1 regularization7 minSummarizing overfitting &amp; regularization in logistic regressionRecap of overfitting &amp; regularization in logistic regression58 secQuiz: Overfitting &amp; Regularization in Logistic Regression8 questionsQUIZOverfitting &amp; Regularization in Logistic Regression8 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineAugust 27, 11:59 PM PDT 1 point1.Consider four classifiers, whose classification performance is given by the following table: Classification error on training set Classification error on validation set Classifier 1 0.2 0.6 Classifier 2 0.8 0.6 Classifier 3 0.2 0.2 Classifier 4 0.5 0.4 Which of the four classifiers is most likely overfit? Classifier 1 Classifier 2 Classifier 3 Classifier 41 point2.Suppose a classifier classifies 23100 examples correctly and 1900 examples incorrectly. Compute error by hand. Round your answer to 3 decimal places. 1 point3.(True/False) Accuracy and error measured on the same dataset always sum to 1. True False1 point4.Which of the following is NOT a correct description of complex models? Complex models accommodate many features. Complex models tend to produce lower training error than simple models. Complex models tend to generalize better than simple models. Complex models tend to exhibit high variance in response to perturbation in the training data. Complex models tend to exhibit low bias, capturing many patterns in the training data that simple models may have missed.1 point5.Which of the following is a symptom of overfitting in the context of logistic regression? Select all that apply. Large estimated coefficients Good generalization to previously unseen data Simple decision boundary Complex decision boundary Overconfident predictions of class probabilities1 point6.Suppose we perform L2 regularized logistic regression to fit a sentiment classifier. Which of the following plots does NOT describe a possible coefficient path? Choose all that apply. Note. Assume that the algorithm runs for a wide range of L2 penalty values and each coefficient plot is zoomed out enough to capture all long-term trends. 1 point7.Suppose we perform L1 regularized logistic regression to fit a sentiment classifier. Which of the following plots does NOT describe a possible coefficient path? Choose all that apply. Note. Assume that the algorithm runs for a wide range of L1 penalty values and each coefficient plot is zoomed out enough to capture all long-term trends. 1 point8.In the context of L2 regularized logistic regression, which of the following occurs as we increase the L2 penalty λ? Choose all that apply. The L2 norm of the set of coefficients gets smaller Region of uncertainty becomes narrower, i.e., the classifier makes predictions with higher confidence. Decision boundary becomes less complex Training error decreases The classifier has lower variance Some features are excluded from the classifier Programming AssignmentLogistic Regression with L2 regularization10 minQuiz: Logistic Regression with L2 regularization8 questionsQUIZLogistic Regression with L2 regularization8 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineAugust 27, 11:59 PM PDT 1 point1.In the function feature_derivative_with_L2, was the intercept term regularized? Yes No1 point2.Does the term with L2 regularization increase or decrease the log likelihood ℓℓ(w)? Increases Decreases1 point3.Which of the following words is not listed in either positive_words or negative_words? love disappointed great money quality1 point4.Questions 5 and 6 use the coefficient plot of the words in positive_words and negative_words. (True/False) All coefficients consistently get smaller in size as the L2 penalty is increased. True False1 point5.Questions 5 and 6 use the coefficient plot of the words in positive_words and negative_words. (True/False) The relative order of coefficients is preserved as the L2 penalty is increased. (For example, if the coefficient for ‘cat’ was more positive than that for ‘dog’, this remains true as the L2 penalty increases.) True False1 point6.Questions 7, 8, and 9 ask you about the 6 models trained with different L2 penalties. Which of the following models has the highest accuracy on the training data? Model trained with L2 penalty = 0 Model trained with L2 penalty = 4 Model trained with L2 penalty = 10 Model trained with L2 penalty = 100 Model trained with L2 penalty = 1e3 Model trained with L2 penalty = 1e51 point7.Questions 7, 8, and 9 ask you about the 6 models trained with different L2 penalties. Which of the following models has the highest accuracy on the validation data? Model trained with L2 penalty = 0 Model trained with L2 penalty = 4 Model trained with L2 penalty = 10 Model trained with L2 penalty = 100 Model trained with L2 penalty = 1e3 Model trained with L2 penalty = 1e51 point8.Questions 7, 8, and 9 ask you about the 6 models trained with different L2 penalties. Does the highest accuracy on the training data imply that the model is the best one? Yes No Week 3 Decision TreesAlong with linear classifiers, decision trees are amongst the most widely used classification techniques in the real world. This method is extremely intuitive, simple to implement and provides interpretable predictions. In this module, you will become familiar with the core decision trees representation. You will then design a simple, recursive greedy algorithm to learn decision trees from data. Finally, you will extend this approach to deal with continuous inputs, a fundamental requirement for practical problems. In this module, you will investigate a brand new case-study in the financial sector: predicting the risk associated with a bank loan. You will implement your own decision tree learning algorithm on real loan data. Intuition behind decision treesSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: decision-trees-annotated.pdf Predicting loan defaults with decision trees3 minIntuition behind decision trees1 minTask of learning decision trees from data3 minLearning decision treesRecursive greedy algorithm4 minLearning a decision stump3 minSelecting best feature to split on6 minWhen to stop recursing4 minUsing the learned decision treeMaking predictions with decision trees1 minMulticlass classification with decision trees2 minLearning decision trees with continuous inputsThreshold splits for continuous inputs6 min(OPTIONAL) Picking the best threshold to split on3 minVisualizing decision boundaries5 minSummarizing decision treesRecap of decision trees56 secQuiz: Decision Trees11 questionsQUIZDecision Trees11 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 3, 11:59 PM PDT 1 point1.Questions 1 to 6 refer to the following common scenario: Consider the following dataset: x1 x2 x3 y 1 1 1 +1 0 1 0 -1 1 0 1 -1 0 0 1 +1 Let us train a decision tree with this data. Let’s call this tree T1. What feature will we split on at the root? x1 x2 x3x1: .5x2: .5x3: .25 1 point2.Refer to the dataset presented in Question 1 to answer the following. Fully train T1 (until each leaf has data points of the same output label). What is the depth of T1? 1 point3.Refer to the dataset presented in Question 1 to answer the following. What is the training error of T1? 1 point4.Refer to the dataset presented in Question 1 to answer the following. Now consider a tree T2, which splits on x1 at the root, and splits on x2 in the 1st level, and has leaves at the 2nd level. Note: this is the XOR function on features 1 and 2. What is the depth of T2? 1 point5.Refer to the dataset presented in Question 1 to answer the following. What is the training error of T2? 1 point6.Refer to the dataset presented in Question 1 to answer the following. Which has smaller depth, T1 or T2? T1 T21 point7.(True/False) When deciding to split a node, we find the best feature to split on that minimizes classification error. True False1 point8.If you are learning a decision tree, and you are at a node in which all of its data has the same y value, you should find the best feature to split on create a leaf that predicts the y value of all the data terminate recursions on all branches and return the current tree go back to the PARENT node and select a DIFFERENT feature to split on so that the y values are not all the same at THIS node3: False 1 point8.Let’s say we have learned a decision tree on dataset D. Consider the split learned at the root of the decision tree. Which of the following is true if one of the data points in D is removed and we re-train the tree? The split at the root will be different The split at the root will be exactly the same as before The split could be the same or could be different1 point9.Consider two datasets D1 and D2, where D2 has the same data points as D1, but has an extra feature for each data point. Let T1 be the decision tree trained with D1, and T2 be the tree trained with D2. Which of the following is true? T2 has better training error than T1 T2 has better test error than T1 Too little information to guarantee anything1 point10.(True/False) Logistic regression with polynomial degree 1 features will always have equal or lower training error than decision stumps (depth 1 decision trees). True False1 point11.(True/False) Decision trees (with depth &gt; 1) are always linear classifiers. True False1 point11.(True/False) Decision stumps (depth 1 decision trees) are always linear classifiers. True False Programming Assignment 1Identifying safe loans with decision trees10 minQuiz: Identifying safe loans with decision trees7 questionsQUIZIdentifying safe loans with decision trees7 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 3, 11:59 PM PDT 1 point1.What percentage of the predictions on sample_validation_data did decision_tree_model get correct? 25% 50% 75% 100%1 point2.Which loan has the highest probability of being classified as a safe loan? First Second Third Fourth1 point3.Notice that the probability preditions are the exact same for the 2nd and 3rd loans. Why would this happen? During tree traversal both examples fall into the same leaf node. This can only happen with sheer coincidence.1 point4.Based on the visualized tree, what prediction would you make for this data point? +1 -11 point5.What is the accuracy of decision_tree_model on the validation set, rounded to the nearest .01 (e.g. 0.76)? 1 point6.How does the performance of big_model on the validation set compare to decision_tree_model on the validation set? Is this a sign of overfitting? big_model has higher accuracy on the validation set than decision_tree_model. This is overfitting. big_model has higher accuracy on the validation set than decision_tree_model. This is not overfitting. big_model has lower accuracy on the validation set than decision_tree_model. This is overfitting. big_model has lower accuracy on the validation set than decision_tree_model. This is not overfitting.1 point7.Let us assume that each mistake costs money: Assume a cost of $10,000 per false negative.Assume a cost of $20,000 per false positive.What is the total cost of mistakes made by decision_tree_model on validation_data? Please enter your answer as a plain integer, without the dollar sign or the comma separator, e.g. 3002000. Programming Assignment 2Implementing binary decision trees10 minQuiz: Implementing binary decision trees7 questionsQUIZImplementing binary decision trees7 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 3, 11:59 PM PDT 1 point1.What was the feature that my_decision_tree first split on while making the prediction for test_data[0]? emp_length.4 years grade.A term. 36 months home_ownership.MORTGAGE1 point2.What was the first feature that lead to a right split of test_data[0]? emp_length.&lt; 1 year emp_length.10+ years grade.B grade.D1 point3.What was the last feature split on before reaching a leaf node for test_data[0]? grade.D grade.B term. 36 months grade.A1 point4.Rounded to 2nd decimal point (e.g. 0.76), what is the classification error of my_decision_tree on the test_data? 1 point5.What is the feature that is used for the split at the root node? grade.A term. 36 months term. 60 months home_ownership.OWN1 point6.What is the path of the first 3 feature splits considered along the left-most branch of my_decision_tree? term. 36 months, grade.A, grade.B term. 36 months, grade.A, emp_length.4 years term. 36 months, grade.A, no third feature because second split resulted in leaf1 point7.What is the path of the first 3 feature splits considered along the right-most branch of my_decision_tree? term. 36 months, grade.D, grade.B term. 36 months, grade.D, home_ownership.OWN term. 36 months, grade.D, no third feature because second split resulted in leaf Week 4Preventing Overfitting in Decision TreesOut of all machine learning techniques, decision trees are amongst the most prone to overfitting. No practical implementation is possible without including approaches that mitigate this challenge. In this module, through various visualizations and investigations, you will investigate why decision trees suffer from significant overfitting problems. Using the principle of Occam’s razor, you will mitigate overfitting by learning simpler trees. At first, you will design algorithms that stop the learning process before the decision trees become overly complex. In an optional segment, you will design a very practical approach that learns an overly-complex tree, and then simplifies it with pruning. Your implementation will investigate the effect of these techniques on mitigating overfitting on our real-world loan data set. Overfitting in decision treesSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: decision-trees-overfitting-annotated.pdf A review of overfitting2 minOverfitting in decision trees5 minEarly stopping to avoid overfittingPrinciple of Occam’s razor: Learning simpler decision trees5 minEarly stopping in learning decision trees6 min(OPTIONAL LESSON) Pruning decision trees(OPTIONAL) Motivating pruning8 min(OPTIONAL) Pruning decision trees to avoid overfitting6 min(OPTIONAL) Tree pruning algorithm3 minSummarizing preventing overfitting in decision treesRecap of overfitting and regularization in decision trees1 minQuiz: Preventing Overfitting in Decision Trees11 questionsQUIZPreventing Overfitting in Decision Trees11 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 10, 11:59 PM PDT 1 point1.(True/False) When learning decision trees, smaller depth USUALLY translates to lower training error. True False1 point2.(True/False) If no two data points have the same input values, we can always learn a decision tree that achieves 0 training error. True False1 point3.(True/False) If decision tree T1 has lower training error than decision tree T2, then T1 will always have better test error than T2. True False1 point4.Which of the following is true for decision trees? Model complexity increases with size of the data. Model complexity increases with depth. None of the above1 point5.Pruning and early stopping in decision trees is used to combat overfitting improve training error None of the above1 point6.Which of the following is NOT an early stopping method? Stop when the tree hits a certain depth Stop when node has too few data points (minimum node “size”) Stop when every possible split results in the same amount of error reduction Stop when best split results in too small of an error reduction1 point7.Consider decision tree T1 learned with minimum node size parameter = 1000. Now consider decision tree T2 trained on the same dataset and parameters, except that the minimum node size parameter is now 100. Which of the following is always true? The depth of T2 &gt;= the depth of T1 The number of nodes in T2 &gt;= the number of nodes in T1 The test error of T2 &lt;= the test error of T1 The training error of T2 &lt;= the training error of T11 point8.Questions 8 to 11 refer to the following common scenario: Imagine we are training a decision tree, and we are at a node. Each data point is (x1, x2, y), where x1,x2 are features, and y is the label. The data at this node is: x1 x2 y 0 1 +1 1 0 +1 0 1 +1 1 1 -1 What is the classification error at this node (assuming a majority class classifier)? 1 point9.Refer to the scenario presented in Question 8. If we split on x1, what is the classification error? 1point Refer to the scenario presented in Question 8. If we split on x2, what is the classification error? 1 point11.Refer to the scenario presented in Question 8. If our parameter for minimum gain in error reduction is 0.1, do we split or stop early? Split Stop early Programming AssignmentDecision Trees in Practice10 minQuiz: Decision Trees in Practice14 questionsQUIZDecision Trees in Practice14 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 10, 11:59 PM PDT 1 point1.Given an intermediate node with 6 safe loans and 3 risky loans, if the min_node_size parameter is 10, what should the tree learning algorithm do next? Create a leaf and return it Continue building the tree by finding the best splitting feature1 point2.Assume an intermediate node has 6 safe loans and 3 risky loans. For each of 4 possible features to split on, the error reduction is 0.0, 0.05, 0.1, and 0.14, respectively. If the minimum gain in error reduction parameter is set to 0.2, what should the tree learning algorithm do next? Create a leaf and return it Continue building the tree by using the splitting feature that gives 0.14 error reduction1 point3.Consider the prediction path validation_set[0] with my_decision_tree_old and my_decision_tree_new. For my_decision_tree_new trained with1max_depth = 6, min_node_size = 100, min_error_reduction=0.0 is the prediction path shorter, longer, or the same as the prediction path using my_decision_tree_old that ignored the early stopping conditions 2 and 3? Shorter Longer The same1 point4.Consider the prediction path for ANY new data point. For my_decision_tree_new trained with1max_depth = 6, min_node_size = 100, min_error_reduction=0.0 is the prediction path for a data point always shorter, always longer, always the same, shorter or the same, or longer or the same as for my_decision_tree_old that ignored the early stopping conditions 2 and 3? Always shorter Always longer Always the same Shorter or the same Longer or the same1 point5.For a tree trained on any dataset using parameters1max_depth = 6, min_node_size = 100, min_error_reduction=0.0 what is the maximum possible number of splits encountered while making a single prediction? 1 point6.Is the validation error of the new decision tree (using early stopping conditions 2 and 3) lower than, higher than, or the same as that of the old decision tree from the previous assigment? Higher than Lower than The same1 point7.Which tree has the smallest error on the validation data? model_1 model_2 model_31 point8.Does the tree with the smallest error in the training data also have the smallest error in the validation data? Yes No1 point9.Is it always true that the tree with the lowest classification error on the training set will result in the lowest classification error in the validation set? Yes, this is ALWAYS true. No, this is NOT ALWAYS true.1 point10.Which tree has the largest complexity? model_1 model_2 model_31 point11.Is it always true that the most complex tree will result in the lowest classification error in the validation_set? Yes, this is ALWAYS true. No, this is NOT ALWAYS true.1 point12.Using the complexity definition, which model (model_4, model_5, or model_6) has the largest complexity? model_4 model_5 model_61 point13.model_4 and model_5 have similar classification error on the validation set but model_5 has lower complexity. Should you pick model_5 over model_4? Pick model_5 over model_4 Pick model_4 over model_51 point14.Using the results obtained in this section, which model (model_7, model_8, or model_9) would you choose to use? model_7 model_8 model_9 Handling Missing DataReal-world machine learning problems are fraught with missing data. That is, very often, some of the inputs are not observed for all data points. This challenge is very significant, happens in most cases, and needs to be addressed carefully to obtain great performance. And, this issue is rarely discussed in machine learning courses. In this module, you will tackle the missing data challenge head on. You will start with the two most basic techniques to convert a dataset with missing data into a clean dataset, namely skipping missing values and inputing missing values. In an advanced section, you will also design a modification of the decision tree learning algorithm that builds decisions about missing data right into the model. You will also explore these techniques in your real-data implementation. Basic strategies for handling missing dataSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: decision-trees-missing-values-annotated.pdf Challenge of missing data3 minStrategy 1: Purification by skipping missing data4 minStrategy 2: Purification by imputing missing data4 minStrategy 3: Modify learning algorithm to explicitly handle missing dataModifying decision trees to handle missing data4 minFeature split selection with missing data5 minSummarizing handling missing dataRecap of handling missing data1 minQuiz: Handling Missing Data7 questionsQUIZHandling Missing Data7 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 10, 11:59 PM PDT 1 point1.(True/False) Skipping data points (i.e., skipping rows of the data) that have missing features only works when the learning algorithm we are using is decision tree learning. True False1 point2.What are potential downsides of skipping features with missing values (i.e., skipping columns of the data) to handle missing data? So many features are skipped that accuracy can degrade The learning algorithm will have to be modified You will have fewer data points (i.e., rows) in the dataset If an input at prediction time has a feature missing that was always present during training, this approach is not applicable.1 point3.(True/False) It’s always better to remove missing data points (i.e., rows) as opposed to removing missing features (i.e., columns). True False1 point4.Consider a dataset with N training points. After imputing missing values, the number of data points in the data set is 2 * N N 5 * N1 point5.Consider a dataset with D features. After imputing missing values, the number of features in the data set is 2 * D D 0.5 * D1 point6.Which of the following are always true when imputing missing data? Select all that apply. Imputed values can be used in any classification algorithm Imputed values can be used when there is missing data at prediction time Using imputed values results in higher accuracies than skipping data points or skipping features1 point7.Consider data that has binary features (i.e. the feature values are 0 or 1) with some feature values of some data points missing. When learning the best feature split at a node, how would we best modify the decision tree learning algorithm to handle data points with missing values for a feature? We choose to assign missing values to the branch of the tree (either the one with feature value equal to 0 or with feature value equal to 1) that minimizes classification error. We assume missing data always has value 0. We ignore all data points with missing values. Week 5 BoostingOne of the most exciting theoretical questions that have been asked about machine learning is whether simple classifiers can be combined into a highly accurate ensemble. This question lead to the developing of boosting, one of the most important and practical techniques in machine learning today. This simple approach can boost the accuracy of any classifier, and is widely used in practice, e.g., it’s used by more than half of the teams who win the Kaggle machine learning competitions. In this module, you will first define the ensemble classifier, where multiple models vote on the best prediction. You will then explore a boosting algorithm called AdaBoost, which provides a great approach for boosting classifiers. Through visualizations, you will become familiar with many of the practical aspects of this techniques. You will create your very own implementation of AdaBoost, from scratch, and use it to boost the performance of your loan risk predictor on real data. The amazing idea of boosting a classifierSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: boosting-annotated.pdf The boosting question3 minEnsemble classifiers5 minBoosting5 minAdaBoostAdaBoost overview3 minWeighted error4 minComputing coefficient of each ensemble component4 minReweighing data to focus on mistakes4 minNormalizing weights2 minApplying AdaBoostExample of AdaBoost in action5 minLearning boosted decision stumps with AdaBoost4 minProgramming Assignment 1Exploring Ensemble Methods10 minQuiz: Exploring Ensemble Methods9 questionsQUIZExploring Ensemble Methods9 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 17, 11:59 PM PDT 1 point1.What percentage of the predictions on sample_validation_data did model_5 get correct? 25% 50% 75% 100%1 point2.According to model_5, which loan is the least likely to be a safe loan? First Second Third Fourth1 point3.What is the number of false positives on the validation data? 1 point4.Using the same costs of the false positives and false negatives, what is the cost of the mistakes made by the boosted tree model (model_5) as evaluated on the validation_set? 1 point5.What grades are the top 5 loans? A B C D E1 point6.Which model has the best accuracy on the validation_data? model_10 model_50 model_100 model_200 model_5001 point7.Is it always true that the model with the most trees will perform best on the test/validation set? Yes, a model with more trees will ALWAYS perform better on the test/validation set. No, a model with more trees does not always perform better on the test/validation set.1 point8.Does the training error reduce as the number of trees increases? Yes No1 point9.Is it always true that the test/validation error will reduce as the number of trees increases? Yes, it is ALWAYS true that the test/validation error will reduce as the number of trees increases. No, the test/validation error will not necessarily always reduce as the number of trees increases. Convergence and overfitting in boostingThe Boosting Theorem3 minOverfitting in boosting5 minSummarizing boostingEnsemble methods, impact of boosting &amp; quick recap4 minQuiz:Boosting11 questionsQUIZBoosting11 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 17, 11:59 PM PDT 1 point1.Which of the following is NOT an ensemble method? Gradient boosted trees AdaBoost Random forests Single decision trees1 point2.Each binary classifier in an ensemble makes predictions on an input x as listed in the table below. Based on the ensemble coefficients also listed in the table, what is the final ensemble model’s prediction for x? Classifier coefficient wt Prediction for x Classifier 1 0.61 +1 Classifier 2 0.53 -1 Classifier 3 0.88 -1 Classifier 4 0.34 +1 +1 -11 point3.(True/False) Boosted decision stumps is a linear classifier. True False1 point4.(True/False) For AdaBoost, test error is an appropriate criterion for choosing the optimal number of iterations. True False1 point5.In an iteration in AdaBoost, recall that learning the coefficient w_t for learned weak learner f_t is calculated by $$\displaystyle{\frac{1}{2}\ln{\left( \frac{1-\mathtt{weighted_error}(f_t)}{\mathtt{weighted_error}(f_t)} \right)}}$$If the weighted error of f_t is equal to .25, what is the value of w_t? Round your answer to 2 decimal places. 1 point6.Which of the following classifiers is most accurate as computed on a weighted dataset? A classifier with: weighted error = 0.1 weighted error = 0.3 weighted error = 0.5 weighted error = 0.7 weighted error = 0.991 point7.Imagine we are training a decision stump in an iteration of AdaBoost, and we are at a node. Each data point is (x1, x2, y), where x1,x2 are features, and y is the label. Also included are the weights of the data. The data at this node is: Weight x1 x2 y 0.3 0 1 +1 0.35 1 0 -1 0.1 0 1 +1 0.25 1 1 +1 Suppose we assign the same class label to all data in this node. (Pick the class label with the greater total weight.) What is the weighted error at the node? Round your answer to 2 decimal places. 1 point8.After each iteration of AdaBoost, the weights on the data points are typically normalized to sum to 1. This is used because of issues with numerical instability (underflow/overflow) the weak learners can only learn with normalized weights none of the above1 point9.Consider the following 2D dataset with binary labels. We train a series of weak binary classifiers using AdaBoost. In one iteration, the weak binary classifier produces the decision boundary as follows: Which of the five points (indicated in the second figure) will receive higher weight in the following iteration? Choose all that apply. (1) (2) (3) (4) (5)1 point10.Suppose we are running AdaBoost using decision tree stumps. At a particular iteration, the data points have weights according the figure. (Large points indicate heavy weights.)Which of the following decision tree stumps is most likely to be fit in the next iteration? 1 point11.(True/False) AdaBoost achieves zero training error after a sufficient number of iterations, as long as we can find weak learners that perform better than random chance at each iteration of AdaBoost (i.e., on weighted data). True False Programming Assignment 2Boosting a decision stump10 minQuiz:Boosting a decision stump5 questionsQUIZBoosting a decision stump5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineSeptember 17, 11:59 PM PDTYou can still pass this quiz before the course ends. 1 point1.Recall that the classification error for unweighted data is defined as follows: classification error=# mistakes# all data pointsMeanwhile, the weight of mistakes for weighted data is given by $$WM(α,y^)=∑i=1nαi×1[yi≠y^i].$$If we set the weights α=1 for all data points, how is the weight of mistakes WM(α,ŷ) related to the classification error? WM(α,ŷ) = [classification error] WM(α,ŷ) = [classification error] * [weight of correctly classified data points] WM(α,ŷ) = N * [classification error] WM(α,ŷ) = 1 - [classification error]1 point2.Refer to section Example: Training a weighted decision tree. Will you get the same model as small_data_decision_tree_subset_20 if you trained a decision tree with only 20 data points from the set of points in subset_20? Yes No1 point3.Refer to the 10-component ensemble of tree stumps trained with Adaboost. As each component is trained sequentially, are the component weights monotonically decreasing, monotonically increasing, or neither? Monotonically decreasing Monotonically increasing Neither1 point4.Which of the following best describes a general trend in accuracy as we add more and more components? Answer based on the 30 components learned so far. Training error goes down monotonically, i.e. the training error reduces with each iteration but never increases. Training error goes down in general, with some ups and downs in the middle. Training error goes up in general, with some ups and downs in the middle. Training error goes down in the beginning, achieves the best error, and then goes up sharply. None of the above1 point5.From this plot (with 30 trees), is there massive overfitting as the # of iterations increases? Yes No Week 6 Precision-RecallIn many real-world settings, accuracy or error are not the best quality metrics for classification. You will explore a case-study that significantly highlights this issue: using sentiment analysis to display positive reviews on a restaurant website. Instead of accuracy, you will define two metrics: precision and recall, which are widely used in real-world applications to measure the quality of classifiers. You will explore how the probabilities output by your classifier can be used to trade-off precision with recall, and dive into this spectrum, using precision-recall curves. In your hands-on implementation, you will compute these metrics with your learned classifier on real-world sentiment analysis data. Why use precision &amp; recall as quality metricsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: precision-recall.pdf Case-study where accuracy is not best metric for classification3 minWhat is good performance for a classifier?3 minPrecision &amp; recall explainedPrecision: Fraction of positive predictions that are actually positive5 minRecall: Fraction of positive data predicted to be positive3 minThe precision-recall tradeoffPrecision-recall extremes2 minTrading off precision and recall4 minPrecision-recall curve5 minSummarizing precision-recallRecap of precision-recall1 minQuiz: Precision-Recall9 questionsQUIZPrecision-Recall9 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 1, 11:59 PM PDT 1 point1.Questions 1 to 5 refer to the following scenario: Suppose a binary classifier produced the following confusion matrix. Predicted Positive Predicted Negative Actual Positive 5600 40 Actual Negative 1900 2460 What is the recall of this classifier? Round your answer to 2 decimal places. 1 point2.Refer to the scenario presented in Question 1 to answer the following: (True/False) This classifier is better than random guessing. True False1 point3.Refer to the scenario presented in Question 1 to answer the following: (True/False) This classifier is better than the majority class classifier. True False1 point4.Refer to the scenario presented in Question 1 to answer the following: Which of the following points in the precision-recall space corresponds to this classifier? (1) (2) (3) (4) (5)1 point5.Refer to the scenario presented in Question 1 to answer the following: Which of the following best describes this classifier? It is optimistic It is pessimistic None of the1 point6.Suppose we are fitting a logistic regression model on a dataset where the vast majority of the data points are labeled as positive. To compensate for overfitting to the dominant class, we should Require higher confidence level for positive predictions Require lower confidence level for positive predictions1 point7.It is often the case that false positives and false negatives incur different costs. In situations where false negatives cost much more than false positives, we should Require higher confidence level for positive predictions Require lower confidence level for positive predictions1 point8.We are interested in reducing the number of false negatives. Which of the following metrics should we primarily look at? Accuracy Precision Recall1 point9.Suppose we set the threshold for positive predictions at 0.9. What is the lowest score that is classified as positive? Round your answer to 2 decimal places. Programming AssignmentExploring precision and recall10 minQuiz: Exploring precision and recall13 questionsQUIZExploring precision and recall13 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 1, 11:59 PM PDT 1 point1.Consider the logistic regression model trained on amazon_baby.gl using GraphLab Create. Using accuracy as the evaluation metric, was our logistic regression model better than the majority class classifier? Yes No1 point2.How many predicted values in the test set are false positives? 1 point3.Consider the scenario where each false positive costs $100 and each false negative $1. Given the stipulation, what is the cost associated with the logistic regression classifier’s performance on the test set? Between $0 and $100,000 Between $100,000 and $200,000 Between $200,000 and $300,000 Above $300,0001 point4.Out of all reviews in the test set that are predicted to be positive, what fraction of them are false positives? (Round to the second decimal place e.g. 0.25) 1 point5.Based on what we learned in lecture, if we wanted to reduce this fraction of false positives to be below 3.5%, we would: Discard a sufficient number of positive predictions Discard a sufficient number of negative predictions Increase threshold for predicting the positive class (y^=+1) Decrease threshold for predicting the positive class (y^=+1)1 point6.What fraction of the positive reviews in the test_set were correctly predicted as positive by the classifier? Round your answer to 2 decimal places. 1 point7.What is the recall value for a classifier that predicts +1 for all data points in the test_data? 1 point8.What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9? More reviews are predicted to be positive. Fewer reviews are predicted to be positive.1 point9.Consider the metrics obtained from setting the threshold to 0.5 and to 0.9. Does the recall increase with a higher threshold? Yes No1 point10.Among all the threshold values tried, what is the smallest threshold value that achieves a precision of 96.5% or better? Round your answer to 3 decimal places. 1 point11.Using threshold = 0.98, how many false negatives do we get on the test_data? (Hint: You may use the graphlab.evaluation.confusion_matrix function implemented in GraphLab Create.) 1 point12.Questions 13 and 14 are concerned with the reviews that contain the word baby. Among all the threshold values tried, what is the smallest threshold value that achieves a precision of 96.5% or better for the reviews of data in baby_reviews? Round your answer to 3 decimal places. 1 point13.Questions 13 and 14 are concerned with the reviews that contain the word baby. Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 96.5%? Larger Smaller Week 7 Scaling to Huge Datasets &amp; Online LearningWith the advent of the internet, the growth of social media, and the embedding of sensors in the world, the magnitudes of data that our machine learning algorithms must handle have grown tremendously over the last decade. This effect is sometimes called “Big Data”. Thus, our learning algorithms must scale to bigger and bigger datasets. In this module, you will develop a small modification of gradient ascent called stochastic gradient, which provides significant speedups in the running time of our algorithms. This simple change can drastically improve scaling, but makes the algorithm less stable and harder to use in practice. In this module, you will investigate the practical techniques needed to make stochastic gradient viable, and to thus to obtain learning algorithms that scale to huge datasets. You will also address a new kind of machine learning problem, online learning, where the data streams in over time, and we must learn the coefficients as the data arrives. This task can also be solved with stochastic gradient. You will implement your very own stochastic gradient ascent algorithm for logistic regression from scratch, and evaluate it on sentiment analysis data. Scaling ML to huge datasetsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: online-learning-annotated.pdf Gradient ascent won’t scale to today’s huge datasets3 minTimeline of scalable machine learning &amp; stochastic gradient4 minScaling ML with stochastic gradientWhy gradient ascent won’t scale3 minStochastic gradient: Learning one data point at a time3 minComparing gradient to stochastic gradient3 minUnderstanding why stochastic gradient worksWhy would stochastic gradient ever work?4 minConvergence paths2 minStochastic gradient: Practical tricksShuffle data before running stochastic gradient2 minChoosing step size3 minDon’t trust last coefficients1 min(OPTIONAL) Learning from batches of data3 min(OPTIONAL) Measuring convergence4 min(OPTIONAL) Adding regularization3 minOnline learning: Fitting models from streaming dataThe online learning task3 minUsing stochastic gradient for online learning3 minSummarizing scaling to huge datasets &amp; online learningScaling to huge datasets through parallelization &amp; module recap1 minQuiz: Scaling to Huge Datasets &amp; Online Learning10 questionsQUIZScaling to Huge Datasets &amp; Online Learning10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 8, 11:59 PM PDT 1 point1.(True/False) Stochastic gradient ascent often requires fewer passes over the dataset than batch gradient ascent to achieve a similar log likelihood. True False1 point2.(True/False) Choosing a large batch size results in less noisy gradients True False1 point3.(True/False) The set of coefficients obtained at the last iteration represents the best coefficients found so far. True False1 point4.Suppose you obtained the plot of log likelihood below after running stochastic gradient ascent. Which of the following actions would help the most to improve the rate of convergence? Increase step size Decrease step size Decrease batch size1 point5.Suppose you obtained the plot of log likelihood below after running stochastic gradient ascent. Which of the following actions would help to improve the rate of convergence? Increase batch size Increase step size Decrease step size1 point6.Suppose it takes about 1 milliseconds to compute a gradient for a single example. You run an online advertising company and would like to do online learning via mini-batch stochastic gradient ascent. If you aim to update the coefficients once every 5 minutes, how many examples can you cover in each update? Overhead and other operations take up 2 minutes, so you only have 3 minutes for the coefficient update. 1 point7.In search for an optimal step size, you experiment with multiple step sizes and obtain the following convergence plot. Which line corresponds to the best step size? (1) (2) (3) (4) (5)1 point8.Suppose you run stochastic gradient ascent with two different batch sizes. Which of the two lines below corresponds to the smaller batch size (assuming both use the same step size)? (1) (2)1 point9.Which of the following is NOT a benefit of stochastic gradient ascent over batch gradient ascent? Choose all that apply. Each coefficient step is very fast. Log likelihood of data improves monotonically. Stochastic gradient ascent can be used for online learning. Stochastic gradient ascent can achieve higher likelihood than batch gradient ascent for the same amount of running time. Stochastic gradient ascent is highly robust with respect to parameter choices.1 point10.Suppose we run the stochastic gradient ascent algorithm described in the lecture with batch size of 100. To make 10 passes over a dataset consisting of 15400 examples, how many iterations does it need to run? Programming AssignmentTraining Logistic Regression via Stochastic Gradient Ascent10 minQuiz: Training Logistic Regression via Stochastic Gradient Ascent12 questionsQUIZTraining Logistic Regression via Stochastic Gradient Ascent12 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 8, 11:59 PM PDT 1 point1.In Module 3 assignment, there were 194 features (an intercept + one feature for each of the 193 important words). In this assignment, we will use stochastic gradient ascent to train the classifier using logistic regression. How does the changing the solver to stochastic gradient ascent affect the number of features? Increases Decreases Stays the same1 point2.Recall from the lecture and the earlier assignment, the log likelihood (without the averaging term) is given by $$ℓℓ(w)=∑i=1N((1[yi=+1]−1)wTh(xi)−ln(1+exp(−wTh(xi))))$$whereas the average log likelihood is given by $$ℓℓA(w)=1/N∑i=1N((1[yi=+1]−1)wTh(xi)−ln(1+exp(−wTh(xi))))$$How are the functions ℓℓ(w) and ℓℓA(w) related? ℓℓA(w)=ℓℓ(w) ℓℓA(w)=(1/N)⋅ℓℓ(w) ℓℓA(w)=N⋅ℓℓ(w) ℓℓA(w)=ℓℓ(w)−∥w∥1 point3.Refer to the sub-section Computing the gradient for a single data point. The code block above computed ∂ℓi(w)∂wjfor j = 1 and i = 10. Is this quantity a scalar or a 194-dimensional vector? A scalar A 194-dimensional vector1 point4.Refer to the sub-section Modifying the derivative for using a batch of data points. The code block computed ∑s=ii+B∂ℓs(w)∂wjfor j = 10, i = 10, and B = 10. Is this a scalar or a 194-dimensional vector? A scalar A 194-dimensional vector1 point5.For what value of B is the term ∑s=1B∂ℓs(w)∂wjthe same as the full gradient ∂ℓ(w)∂wj? A numeric answer is expected for this question. Hint: consider the training set we are using now. 1 point6.For what value of batch size B above is the stochastic gradient ascent function logistic_regression_SG act as a standard gradient ascent algorithm? A numeric answer is expected for this question. Hint: consider the training set we are using now. 1 point7.When you set batch_size = 1, as each iteration passes, how does the average log likelihood in the batch change? Increases Decreases Fluctuates1 point8.When you set batch_size = len(feature_matrix_train), as each iteration passes, how does the average log likelihood in the batch change? Increases Decreases Fluctuates1 point9.Suppose that we run stochastic gradient ascent with a batch size of 100. How many gradient updates are performed at the end of two passes over a dataset consisting of 50000 data points? 1 point10.Refer to the section Stochastic gradient ascent vs gradient ascent. In the first figure, how many passes does batch gradient ascent need to achieve a similar log likelihood as stochastic gradient ascent? It’s always better 10 passes 20 passes 150 passes or more1 point11.Questions 11 and 12 refer to the section Plotting the log likelihood as a function of passes for each step size. Which of the following is the worst step size? Pick the step size that results in the lowest log likelihood in the end. 1e-2 1e-1 1e0 1e1 1e21 point12.Questions 11 and 12 refer to the section Plotting the log likelihood as a function of passes for each step size. Which of the following is the best step size? Pick the step size that results in the highest log likelihood in the end. 1e-4 1e-2 1e0 1e1 1e2 Machine Learning: Clustering &amp; RetrievalCourse can be found hereLecture slides can be found hereSummary can be found in my Github About this course: Case Studies: Finding Similar Documents A reader is interested in a specific news article and you want to find similar articles to recommend. What is the right notion of similarity? Moreover, what if there are millions of other documents? Each time you want to a retrieve a new document, do you need to search through all other documents? How do you group similar documents together? How do you discover new, emerging topics that the documents cover? In this third case study, finding similar documents, you will examine similarity-based algorithms for retrieval. In this course, you will also examine structured representations for describing the documents in the corpus, including clustering and mixed membership models, such as latent Dirichlet allocation (LDA). You will implement expectation maximization (EM) to learn the document clusterings, and see how to scale the methods using MapReduce. Learning Outcomes: By the end of this course, you will be able to: -Create a document retrieval system using k-nearest neighbors. -Identify various similarity metrics for text data. -Reduce computations in k-nearest neighbor search by using KD-trees. -Produce approximate nearest neighbors using locality sensitive hashing. -Compare and contrast supervised and unsupervised learning tasks. -Cluster documents by topic using k-means. -Describe how to parallelize k-means using MapReduce. -Examine probabilistic clustering approaches using mixtures models. -Fit a mixture of Gaussian model using expectation maximization (EM). -Perform mixed membership modeling using latent Dirichlet allocation (LDA). -Describe the steps of a Gibbs sampler and how to use its output to draw inferences. -Compare and contrast initialization techniques for non-convex optimization objectives. -Implement these techniques in Python. Week 1 WelcomeClustering and retrieval are some of the most high-impact machine learning tools out there. Retrieval is used in almost every applications and device we interact with, like in providing a set of products related to one a shopper is currently considering, or a list of people you might want to connect with on a social media platform. Clustering can be used to aid retrieval, but is a more broadly useful tool for automatically discovering structure in data, like uncovering groups of similar patients. This introduction to the course provides you with an overview of the topics we will cover and the background knowledge and resources we assume you have. What is this course about?Slides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: intro.pdf Welcome and introduction to clustering and retrieval tasks6 minCourse overview3 minModule-by-module topics covered8 minAssumed background6 minSoftware tools you’ll need for this course10 minGithub repository with starter code In each module of the course, we have a reading with the assignments for that module as well as some starter code. For those interested, the starter code and demos used in this course are also available in a public Github repository: https://github.com/learnml/machine-learning-specialization A big week ahead!10 minWeek 2 Nearest Neighbor SearchWe start the course by considering a retrieval task of fetching a document similar to one someone is currently reading. We cast this problem as one of nearest neighbor search, which is a concept we have seen in the Foundations and Regression courses. However, here, you will take a deep dive into two critical components of the algorithms: the data representation and metric for measuring similarity between pairs of datapoints. You will examine the computational burden of the naive nearest neighbor search algorithm, and instead implement scalable alternatives using KD-trees for handling large datasets and locality sensitive hashing (LSH) for providing approximate nearest neighbors, even in high-dimensional spaces. You will explore all of these ideas on a Wikipedia dataset, comparing and contrasting the impact of the various choices you can make on the nearest neighbor results produced. Introduction to nearest neighbor search and algorithmsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: retrieval-intro-annotated.pdf Retrieval as k-nearest neighbor search2 min1-NN algorithm2 mink-NN algorithm6 minThe importance of data representations and distance metricsDocument representation5 minDistance metrics: Euclidean and scaled Euclidean6 minWriting (scaled) Euclidean distance using (weighted) inner products4 minDistance metrics: Cosine similarity9 minTo normalize or not and other distance considerations6 minQuiz: Representations and metrics6 questionsQUIZRepresentations and metrics6 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 22, 11:59 PM PDT 1 point1.Consider three data points with two features as follows: Among the three points, which two are closest to each other in terms of having the ​smallest Euclidean distance? A and B A and C B and C1 point2.Consider three data points with two features as follows: Among the three points, which two are closest to each other in terms of having the ​largest cosine similarity (or equivalently, ​smallest cosine distance)? A and B A and C B and C1 point3.Consider the following two sentences. Sentence 1: The quick brown fox jumps over the lazy dog.Sentence 2: A quick brown dog outpaces a quick fox.Compute the Euclidean distance using word counts. To compute word counts, turn all words into lower case and strip all punctuation, so that “The” and “the” are counted as the same token. That is, document 1 would be represented as x=[# the,# a,# quick,# brown,# fox,# jumps,# over,# lazy,# dog,# outpaces]where # word is the count of that word in the document. Round your answer to 3 decimal places. sum = 13 1 point4.Consider the following two sentences. Sentence 1: The quick brown fox jumps over the lazy dog.Sentence 2: A quick brown dog outpaces a quick fox.Recall that cosine distance = 1 - cosine similarity = 1−xTy||x||||y||Compute the cosine distance between sentence 1 and sentence 2 using word counts. To compute word counts, turn all words into lower case and strip all punctuation, so that “The” and “the” are counted as the same token. That is, document 1 would be represented as x=[# the,# a,# quick,# brown,# fox,# jumps,# over,# lazy,# dog,# outpaces]where # word is the count of that word in the document. Round your answer to 3 decimal places. 1 point5.(True/False) For positive features, cosine similarity is always between 0 and 1. True False1 point6.Which of the following does not describe the word count document representation? (Note: this is different from TF-IDF document representation.) Ignores the order of the words Assigns a high score to a frequently occurring word Penalizes words that appear in every document Programming Assignment 1Choosing features and metrics for nearest neighbor search10 minQuiz: Choosing features and metrics for nearest neighbor search5 questionsQUIZChoosing features and metrics for nearest neighbor search5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 22, 11:59 PM PDT 1 point1.Among the words that appear in both Barack Obama and Francisco Barrio, take the 5 that appear most frequently in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words? 1 point2.Measure the pairwise distance between the Wikipedia pages of Barack Obama, George W. Bush, and Joe Biden. Which of the three pairs has the smallest distance? Between Obama and Biden Between Obama and Bush Between Biden and Bush1 point3.Collect all words that appear both in Barack Obama and George W. Bush pages. Out of those words, find the 10 words that show up most often in Obama’s page. Which of the following is NOT one of the 10 words? the presidential in act his1 point4.Among the words that appear in both Barack Obama and Phil Schiliro, take the 5 that have largest weights in Obama. How many of the articles in the Wikipedia dataset contain all of those 5 words? 1point Compute the Euclidean distance between TF-IDF features of Obama and Biden. Round your answer to 3 decimal places. Use American-style decimals (e.g. 110.921). Scaling up k-NN search using KD-treesComplexity of brute force search1 minKD-tree representation9 minNN search with KD-trees7 minComplexity of NN search with KD-trees5 minVisualizing scaling behavior of KD-trees4 minApproximate k-NN search using KD-trees7 min(OPTIONAL) A worked-out example for KD-trees10 minQuiz: KD-trees5 questionsQUIZKD-trees5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 22, 11:59 PM PDT 1 point1.Which of the following is not true about KD-trees? It divides the feature space into nested axis-aligned boxes. It can be used only for approximate nearest neighbor search but not for exact nearest neighbor search. It prunes parts of the feature space away from consideration by inspecting smallest possible distances that can be achieved. The query time scales sublinearly with the number of data points and exponentially with the number of dimensions. It works best in low to medium dimension settings.1 point2.Questions 2, 3, 4, and 5 involves training a KD-tree on the following dataset: – X1 X2 – Data point 1 -1.58 -2.01 Data point 2 0.91 3.98 Data point 3 -0.73 4.00 Data point 4 -4.22 1.16 Data point 5 4.19 -2.02 Data point 6 -0.33 2.15 Train a KD-tree by hand as follows: First split using X1 and then using X2. Alternate between X1 and X2 in order. Use “middle-of-the-range” heuristic for each split. Take the maximum and minimum of the coordinates of the member points. Keep subdividing until every leaf node contains two or fewer data points. What is the split value used for the first split? Enter the exact value, as you are expected to obtain a finite number of decimals. Use American-style decimals (e.g. 0.026). 1 point3.Refer to Question 2 for context. What is the split value used for the second split? Enter the exact value, as you are expected to obtain a finite number of decimals. Use American-style decimals (e.g. 0.026). 1 point4.Refer to Question 2 for context. Given a query point (-3, 1.5), which of the data points belong to the same leaf node as the query point? Choose all that apply. Data point 1 Data point 2 Data point 3 Data point 4 Data point 5 Data point 61 point5.Refer to Question 2 for context. Perform backtracking with the query point (-3, 1.5) to perform exact nearest neighbor search. Which of the data points would be pruned from the search? Choose all that apply. Hint: Assume that each node in the KD-tree remembers the tight bound on the coordinates of its member points, as follows: Data point 1 Data point 2 Data point 3 Data point 4 Data point 5 Data point 6 Locality sensitive hashing for approximate NN searchLimitations of KD-trees3 minLSH as an alternative to KD-trees4 minUsing random lines to partition points5 minDefining more bins3 minSearching neighboring bins8 minLSH in higher dimensions4 min(OPTIONAL) Improving efficiency through multiple tables22 minQuiz: Locality Sensitive Hashing5 questionsQUIZLocality Sensitive Hashing5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 22, 11:59 PM PDT 1 point1.(True/False) Like KD-trees, Locality Sensitive Hashing lets us compute exact nearest neighbors while inspecting only a fraction of the data points in the training set. True False1 point2.(True/False) Given two data points with high cosine similarity, the probability that a randomly drawn line would separate the two points is small. True False1 point3.(True/False) The true nearest neighbor of the query is guaranteed to fall into the same bin as the query. True False1 point4.(True/False) Locality Sensitive Hashing is more efficient than KD-trees in high dimensional setting. True False1 point5.Suppose you trained an LSH model and performed a lookup using the bin index of the query. You notice that the list of candidates returned are not at all similar to the query item. Which of the following changes would not produce a more relevant list of candidates? Use multiple tables. Increase the number of random lines/hyperplanes. Inspect more neighboring bins to the bin containing the query. Decrease the number of random lines/hyperplanes. Programming Assignment 2Implementing Locality Sensitive Hashing from scratch10 minQuiz: Implementing Locality Sensitive Hashing from scratch5 questionsQUIZImplementing Locality Sensitive Hashing from scratch5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 22, 11:59 PM PDT 1 point1.What is the document ID of Barack Obama’s article? 1 point2.Which bin contains Barack Obama’s article? Enter its integer index. 1 point3.Examine the bit representations of the bins containing Barack Obama and Joe Biden. In how many places do they agree? 16 out of 16 places (Barack Obama and Joe Biden fall into the same bin) 14 out of 16 places 12 out of 16 places 10 out of 16 places 8 out of 16 places1 point4.Refer to the section “Effect of nearby bin search”. What was the smallest search radius that yielded the correct nearest neighbor for Obama, namely Joe Biden? 1 point5.Suppose our goal was to produce 10 approximate nearest neighbors whose average distance from the query document is within 0.01 of the average for the true 10 nearest neighbors. For Barack Obama, the true 10 nearest neighbors are on average about 0.77. What was the smallest search radius for Barack Obama that produced an average distance of 0.78 or better? Summarizing nearest neighbor searchA brief recap2 minWeek 3 Clustering with k-meansIn clustering, our goal is to group the datapoints in our dataset into disjoint sets. Motivated by our document analysis case study, you will use clustering to discover thematic groups of articles by “topic”. These topics are not provided in this unsupervised learning task; rather, the idea is to output such cluster labels that can be post-facto associated with known topics like “Science”, “World News”, etc. Even without such post-facto labels, you will examine how the clustering output can provide insights into the relationships between datapoints in the dataset. The first clustering algorithm you will implement is k-means, which is the most widely used clustering algorithm out there. To scale up k-means, you will learn about the general MapReduce framework for parallelizing and distributing computations, and then how the iterates of k-means can utilize this framework. You will show that k-means can provide an interpretable grouping of Wikipedia articles when appropriately tuned. Introduction to clusteringSlides presented in this module10 minThe goal of clustering3 minAn unsupervised task6 minHope for unsupervised learning, and some challenge cases4 minClustering via k-meansThe k-means algorithm7 mink-means as coordinate descent6 minSmart initialization via k-means++4 minAssessing the quality and choosing the number of clusters9 minQuiz: k-means9 questionsQUIZk-means9 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 29, 11:59 PM PDT 1 point1.(True/False) k-means always converges to a local optimum. True False1 point2.(True/False) The clustering objective is non-increasing throughout a run of k-means. True False1 point3.(True/False) Running k-means with a larger value of k always enables a lower possible final objective value than running k-means with smaller k. True False1 point4.(True/False) Any initialization of the centroids in k-means is just as good as any other. True False1 point5.(True/False) Initializing centroids using k-means++ guarantees convergence to a global optimum. True False1 point6.(True/False) Initializing centroids using k-means++ costs more than random initialization in the beginning, but can pay off eventually by speeding up convergence. True False1 point7.(True/False) Using k-means++ can only influence the number of iterations to convergence, not the quality of the final assignments (i.e., objective value at convergence). True False4 points8.Consider the following dataset: – X1 X2 Data point 1 -1.88 2.05 Data point 2 -0.71 0.42 Data point 3 2.41 -0.67 Data point 4 1.85 -3.80 Data point 5 -3.69 -1.33 Perform k-means with k=2 until the cluster assignment does not change between successive iterations. Use the following initialization for the centroids: – X1 X2 Cluster 1 2.00 2.00 Cluster 2 -2.00 -2.00 Which of the five data points changed its cluster assignment most often during the k-means run? Data point 1 Data point 2 Data point 3 Data point 4 Data point 51 point9.Suppose we initialize k-means with the following centroids Which of the following best describes the cluster assignment in the first iteration of k-means? Programming AssignmentClustering text data with k-means10 minQuiz: Clustering text data with K-means8 questionsQUIZClustering text data with K-means8 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 29, 11:59 PM PDT 1 point1.Make sure you have the latest versions of the notebook and the file kmeans-arrays.npz Read this post if … you downloaded the files before September 10… you created an Amazon EC2 instance before October 1 I acknowledge.1 point2.(True/False) The clustering objective (heterogeneity) is non-increasing for this example. True False1 point3.Let’s step back from this particular example. If the clustering objective (heterogeneity) would ever increase when running K-means, that would indicate: (choose one) K-means algorithm got stuck in a bad local minimum There is a bug in the K-means code All data points consist of exact duplicates Nothing is wrong. The objective should generally go down sooner or later.1 point4.Refer to the output of K-means for K=3 and seed=0. Which of the three clusters contains the greatest number of data points in the end? Cluster #0 Cluster #1 Cluster #21 point Another way to capture the effect of changing initialization is to look at the distribution of cluster assignments. Compute the size (# of member data points) of clusters for each of the multiple runs of K-means. Look at the size of the largest cluster (most # of member data points) across multiple runs, with seeds 0, 20000, …, 120000. What is the maximum value this quantity takes? 1 point6.Refer to the section “Visualize clusters of documents”. Which of the 10 clusters above contains the greatest number of articles? Cluster 0: artists, actors, film directors, playwrights Cluster 4: professors, researchers, scholars Cluster 5: Australian rules football players, American football players Cluster 7: composers, songwriters, singers, music producers Cluster 9: politicians1 point7.Refer to the section “Visualize clusters of documents”. Which of the 10 clusters above contains the least number of articles? Cluster 1: soccer (association football) players, rugby players Cluster 3: baseball players Cluster 6: female figures from various fields Cluster 7: composers, songwriters, singers, music producers Cluster 8: ice hockey players1 point Another sign of too large K is having lots of small clusters. Look at the distribution of cluster sizes (by number of member data points). How many of the 100 clusters have fewer than 236 articles, i.e. 0.4% of the dataset? MapReduce for scaling k-meansMotivating MapReduce8 minThe general MapReduce abstraction5 minMapReduce execution overview and combiners6 minMapReduce for k-means7 minQuiz: MapReduce for k-means5 questionsQUIZMapReduce for k-means5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineOctober 29, 11:59 PM PDT 1 point1.Suppose we are operating on a 1D vector. Which of the following operation is not data parallel over the vector elements? Add a constant to every element. Multiply the vector by a constant. Increment the vector by another vector of the same dimension. Compute the average of the elements. Compute the sign of each element.1 point2.(True/False) A single mapper call can emit multiple (key,value) pairs. True False1 point3.(True/False) More than one reducer can emit (key,value) pairs with the same key simultaneously. True False1 point4.(True/False) Suppose we are running k-means using MapReduce. Some mappers may be launched for a new k-means iteration even if some reducers from the previous iteration are still running. True False1 point5.Consider the following list of binary operations. Which can be used for the reduce step of MapReduce? Choose all that apply. Hints: The reduce step requires a binary operator that satisfied both of the following conditions. Commutative: OP(x1,x2)=OP(x2,x1)Associative: OP(OP(x1,x2),x3)=OP(x1,OP(x2,x3)) OP1(x1,x2)=max(x1,x2) OP2(x1,x2)=x1+x2−2 OP3(x1,x2)=3x1+2x2 OP4(x1,x2)=x21+x2 OP5(x1,x2)=(x1+x2)/2 Summarizing clustering with k-meansOther applications of clustering7 minWeek 4 Mixture ModelsIn k-means, observations are each hard-assigned to a single cluster, and these assignments are based just on the cluster centers, rather than also incorporating shape information. In our second module on clustering, you will perform probabilistic model-based clustering that provides (1) a more descriptive notion of a “cluster” and (2) accounts for uncertainty in assignments of datapoints to clusters via “soft assignments”. You will explore and implement a broadly useful algorithm called expectation maximization (EM) for inferring these soft assignments, as well as the model parameters. To gain intuition, you will first consider a visually appealing image clustering task. You will then cluster Wikipedia articles, handling the high-dimensionality of the tf-idf document representation considered. Motivating and setting the foundation for mixture modelsSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: mixmodel-EM-annotated.pdf Motiving probabilistic clustering models8 minAggregating over unknown classes in an image dataset6 minUnivariate Gaussian distributions2 minBivariate and multivariate Gaussians7 minMixtures of Gaussians for clusteringMixture of Gaussians6 minInterpreting the mixture of Gaussian terms5 minScaling mixtures of Gaussians for document clustering5 minExpectation Maximization (EM) building blocksComputing soft assignments from known cluster parameters7 min(OPTIONAL) Responsibilities as Bayes’ rule5 minEstimating cluster parameters from known cluster assignments6 minEstimating cluster parameters from soft assignments8 minThe EM algorithmEM iterates in equations and pictures6 minConvergence, initialization, and overfitting of EM9 minRelationship to k-means3 min(OPTIONAL) A worked-out example for EM10 minQuiz: EM for Gaussian mixtures9 questionsQUIZEM for Gaussian mixtures9 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 5, 11:59 PM PST 1 point1.(True/False) While the EM algorithm maintains uncertainty about the cluster assignment for each observation via soft assignments, the model assumes that every observation comes from only one cluster. True False1 point2.(True/False) In high dimensions, the EM algorithm runs the risk of setting cluster variances to zero. True False1 point3.In the EM algorithm, what do the E step and M step represent, respectively? Estimate cluster responsibilities, Maximize likelihood over parameters Estimate likelihood over parameters, Maximize cluster responsibilities Estimate number of parameters, Maximize likelihood over parameters Estimate likelihood over parameters, Maximize number of parameters1 point4.Suppose we have data that come from a mixture of 6 Gaussians (i.e., that is the true data structure). Which model would we expect to have the highest log-likelihood after fitting via the EM algorithm? A mixture of Gaussians with 2 component clusters A mixture of Gaussians with 4 component clusters A mixture of Gaussians with 6 component clusters A mixture of Gaussians with 7 component clusters A mixture of Gaussians with 10 component clusters61 point5.Which of the following correctly describes the differences between EM for mixtures of Gaussians and k-means? Choose all that apply. k-means often gets stuck in a local minimum, while EM tends not to EM is better at capturing clusters of different sizes and orientations EM is better at capturing clusters with overlaps EM is less prone to overfitting than k-means k-means is equivalent to running EM with infinitesimally small diagonal covariances.1 point6.Suppose we are running the EM algorithm. After an E-step, we obtain the following responsibility matrix: Cluster responsibilities Cluster A Cluster B Cluster C Data point 1 0.20 0.40 0.40 Data point 2 0.50 0.10 0.40 Data point 3 0.70 0.20 0.10 Which is the least probable cluster for data point 1? Cluster A Cluster B Cluster C1 point7.Suppose we are running the EM algorithm. After an E-step, we obtain the following responsibility matrix: Cluster responsibilities Cluster A Cluster B Cluster C Data point 1 0.20 0.40 0.40 Data point 2 0.50 0.10 0.40 Data point 3 0.70 0.20 0.10 Suppose also that the data points are as follows: Dataset X Y Z Data point 1 3 1 2 Data point 2 0 0 3 Data point 3 1 3 7 Let us compute the new mean for Cluster A. What is the Z coordinate of the new mean? Round your answer to 3 decimal places.(2*0.2 +3*0.5+7*0.7)/(.2+.5+.7)= 1 point8.Which of the following contour plots describes a Gaussian distribution with diagonal covariance? Choose all that apply. True(1) True(2) True(3) True(4) True(5)2 points9.Suppose we initialize EM for mixtures of Gaussians (using full covariance matrices) with the following clusters: Which of the following best describes the updated clusters after the first iteration of EM? Summarizing mixture modelsA brief recap1 minProgramming Assignment 1Implementing EM for Gaussian mixtures10 minQuiz: Implementing EM for Gaussian mixtures6 questionsQUIZImplementing EM for Gaussian mixtures6 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 5, 11:59 PM PST 1 point1.What is the weight that EM assigns to the first component after running the above codeblock? Round your answer to 3 decimal places. 1 point2.Using the same set of results, obtain the mean that EM assigns the second component. What is the mean in the first dimension? Round your answer to 3 decimal places. 1 point3.Using the same set of results, obtain the covariance that EM assigns the third component. What is the variance in the first dimension? Round your answer to 3 decimal places. 1 point4.Is the loglikelihood plot monotonically increasing, monotonically decreasing, or neither? Monotonically increasing Monotonically decreasing Neither1 point5.Calculate the likelihood (score) of the first image in our data set (img[0]) under each Gaussian component through a call to multivariate_normal.pdf. Given these values, what cluster assignment should we make for this image? Cluster 0 Cluster 1 Cluster 2 Cluster 31 point6.Four of the following images are not in the list of top 5 images in the first cluster. Choose these four. Image 1 Image 2 Image 3 Image 4 Image 5 Image 6 Image 7 Programming Assignment 2Clustering text data with Gaussian mixtures10 minQuiz: Clustering text data with Gaussian mixtures4 questionsQUIZClustering text data with Gaussian mixtures4 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 5, 11:59 PM PST 1 point1.Select all the topics that have a cluster in the model created above. Baseball Basketball Soccer/football Music Politics Law Finance1 point2.Try fitting EM with the random initial parameters you created above. What is the final loglikelihood that the algorithm converges to? Choose the range that contains this value. Less than 2.2e9 Between 2.2e9 and 2.3e9 Between 2.3e9 and 2.4e9 Between 2.4e9 and 2.5e9 Greater than 2.5e91 point3.Is the final loglikelihood larger or smaller than the final loglikelihood we obtained above when initializing EM with the results from running k-means? Initializing EM with k-means led to a larger final loglikelihood Initializing EM with k-means led to a smaller final loglikelihood1 point4.For the above model, out_random_init, use the visualize_EM_clusters method you created above. Are the clusters more or less interpretable than the ones found after initializing using k-means? More interpretable Less interpretable Week 5 Mixed Membership Modeling via Latent Dirichlet AllocationThe clustering model inherently assumes that data divide into disjoint sets, e.g., documents by topic. But, often our data objects are better described via memberships in a collection of sets, e.g., multiple topics. In our fourth module, you will explore latent Dirichlet allocation (LDA) as an example of such a mixed membership model particularly useful in document analysis. You will interpret the output of LDA, and various ways the output can be utilized, like as a set of learned document features. The mixed membership modeling ideas you learn about through LDA for document analysis carry over to many other interesting models and applications, like social network models where people have multiple affiliations. Throughout this module, we introduce aspects of Bayesian modeling and a Bayesian inference algorithm called Gibbs sampling. You will be able to implement a Gibbs sampler for LDA by the end of the module. Introduction to latent Dirichlet allocationSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: LDA-annotated.pdf Mixed membership models for documents3 minAn alternative document clustering model4 minComponents of latent Dirichlet allocation model2 minGoal of LDA inference5 minQuiz: Latent Dirichlet Allocation5 questionsQUIZLatent Dirichlet Allocation5 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 12, 11:59 PM PST 1 point1.(True/False) According to the assumptions of LDA, each document in the corpus contains words about a single topic. True False1 point2.(True/False) Using LDA to analyze a set of documents is an example of a supervised learning task. True False1 point3.(True/False) When training an LDA model, changing the ordering of words in a document does not affect the overall joint probability. True False1 point4.(True/False) Suppose in a trained LDA model two documents have no topics in common (i.e., one document has 0 weight on any topic with non-zero weight in the other document). As a result, a single word in the vocabulary cannot have high probability of occurring in both documents. True False1 point5.(True/False) Topic models are guaranteed to produce weights on words that are coherent and easily interpretable by humans. True False Bayesian inference via Gibbs samplingThe need for Bayesian inference4 minGibbs sampling from 10,000 feet5 minA standard Gibbs sampler for LDA9 minCollapsed Gibbs sampling for LDAWhat is collapsed Gibbs sampling?3 minA worked example for LDA: Initial setup4 minA worked example for LDA: Deriving the resampling distribution7 minUsing the output of collapsed Gibbs sampling4 minSummarizing latent Dirichlet allocationA brief recap1 minQuiz: Learning LDA model via Gibbs sampling10 questionsQUIZLearning LDA model via Gibbs sampling10 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 12, 11:59 PM PST 1 point1.(True/False) Each iteration of Gibbs sampling for Bayesian inference in topic models is guaranteed to yield a higher joint model probability than the previous sample. True False1 point2.(Check all that are true) Bayesian methods such as Gibbs sampling can be advantageous because they Account for uncertainty over parameters when making predictions Are faster than methods such as EM Maximize the log probability of the data under the model Regularize parameter estimates to avoid extreme values1 point3.For the standard LDA model discussed in the lectures, how many parameters are required to represent the distributions defining the topics? [# unique words] [# unique words] * [# topics] [# documents] * [# unique words] [# documents] * [# topics]2 points4.Suppose we have a collection of documents, and we are focusing our analysis to the use of the following 10 words. We ran several iterations of collapsed Gibbs sampling for an LDA model with K=2 topics and alpha=10.0 and gamma=0.1 (with notation as in the collapsed Gibbs sampling lecture). The corpus-wide assignments at our most recent collapsed Gibbs iteration are summarized in the following table of counts: Word Count in topic 1 Count in topic 2 baseball 52 0 homerun 15 0 ticket 9 2 price 9 25 manager 20 37 owner 17 32 company 1 23 stock 0 75 bankrupt 0 19 taxes 0 29 We also have a single document i with the following topic assignments for each word: topic 1 2 1 2 1 word baseball manager ticket price owner Suppose we want to re-compute the topic assignment for the word “manager”. To sample a new topic, we need to compute several terms to determine how much the document likes each topic, and how much each topic likes the word “manager”. The following questions will all relate to this situation. First, using the notation in the slides, what is the value of mmanager,1 (i.e., the number of times the word “manager” has been assigned to topic 1)? 1 point5.Consider the situation described in Question 4. What is the value of ∑wmw,1, where the sum is taken over all words in the vocabulary? 1 point6.Consider the situation described in Question 4. Following the notation in the slides, what is the value of ni,1 for this document i (i.e., the number of words in document i assigned to topic 1)? 1 point7.In the situation described in Question 4, “manager” was assigned to topic 2. When we remove that assignment prior to sampling, we need to decrement the associated counts. After decrementing, what is the value of ni,2? 1 point8.In the situation described in Question 4, “manager” was assigned to topic 2. When we remove that assignment prior to sampling, we need to decrement the associated counts. After decrementing, what is the value of mmanager,2? 1 point9.In the situation described in Question 4, “manager” was assigned to topic 2. When we remove that assignment prior to sampling, we need to decrement the associated counts. After decrementing, what is the value of ∑wmw,2? 2 points10.Consider the situation described in Question 4. As discussed in the slides, the unnormalized probability of assigning to topic 1 is p1=ni,1+αNi−1+Kαmmanager,1+γ∑wmw,1+Vγwhere V is the total size of the vocabulary. Similarly the unnormalized probability of assigning to topic 2 is p2=ni,2+αNi−1+Kαmmanager,2+γ∑wmw,2+VγUsing the above equations and the results computed in previous questions, compute the probability of assigning the word “manager” to topic 1. (Reminder: Normalize across the two topic options so that the probabilities of all possible assignments—topic 1 and topic 2—sum to 1.) Round your answer to 3 decimal places. p1 = (3+10)/(4+210)(20+0.1)/(123+100.1)p2 = (1+10)/(4+210)(36+0.1)/(241+100.1) Programming AssignmentModeling text topics with Latent Dirichlet Allocation10 minQuiz: Modeling text topics with Latent Dirichlet Allocation12 questionsQUIZModeling text topics with Latent Dirichlet Allocation12 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineNovember 12, 11:59 PM PST 1 point1.Identify the top 3 most probable words for the first topic. institute university professor research studies game coach1 point2.What is the sum of the probabilities assigned to the top 50 words in the 3rd topic? Round your answer to 3 decimal places. 1 point3.What is the topic most closely associated with the article about former US President George W. Bush? Use the average results from 100 topic predictions. 1 point4.What are the top 3 topics corresponding to the article about English football (soccer) player Steven Gerrard? Use the average results from 100 topic predictions. science and research team sports music, TV, and film international athletics Great Britain and Australia1 point5.Using the LDA representation, compute the 5000 nearest neighbors for American baseball player Alex Rodriguez. For what value of k is Mariano Rivera the k-th nearest neighbor to Alex Rodriguez? 1 point6.Using the TF-IDF representation, compute the 5000 nearest neighbors for American baseball player Alex Rodriguez. For what value of k is Mariano Rivera the k-th nearest neighbor to Alex Rodriguez? 1 point7.What was the value of alpha used to fit our original topic model? 1 point8.What was the value of gamma used to fit our original topic model? Remember that GraphLab Create uses “beta” instead of “gamma” to refer to the hyperparameter that influences topic distributions over words. 1 point9.How many topics are assigned a weight greater than 0.3 or less than 0.05 for the article on Paul Krugman in the low alpha model? Use the average results from 100 topic predictions. 1 point10.How many topics are assigned a weight greater than 0.3 or less than 0.05 for the article on Paul Krugman in the high alpha model? Use the average results from 100 topic predictions. 1 point11.For each topic of the low gamma model, compute the number of words required to make a list with total probability 0.5. What is the average number of words required across all topics? (HINT: use the get_topics() function from GraphLab Create with the cdf_cutoff argument.) 1 point12.For each topic of the high gamma model, compute the number of words required to make a list with total probability 0.5. What is the average number of words required across all topics? (HINT: use the get_topics() function from GraphLab Create with the cdf_cutoff argument). Week 6 Hierarchical Clustering &amp; Closing RemarksIn the conclusion of the course, we will recap what we have covered. This represents both techniques specific to clustering and retrieval, as well as foundational machine learning concepts that are more broadly useful. We provide a quick tour into an alternative clustering approach called hierarchical clustering, which you will experiment with on the Wikipedia dataset. Following this exploration, we discuss how clustering-type ideas can be applied in other areas like segmenting time series. We then briefly outline some important clustering and retrieval ideas that we did not cover in this course. We conclude with an overview of what’s in store for you in the rest of the specialization. What we’ve learnedSlides presented in this module10 minFor those interested, the slides presented in the videos for this module can be downloaded here: closing-annotated.pdf Module 1 recap10 minModule 2 recap3 minModule 3 recap6 minModule 4 recap7 minHierarchical clustering and clustering for time series segmentationWhy hierarchical clustering?2 minDivisive clustering4 minAgglomerative clustering2 minThe dendrogram4 minAgglomerative clustering details7 minHidden Markov models9 minProgramming AssignmentModeling text data with a hierarchy of clusters10 minQuiz: Modeling text data with a hierarchy of clusters3 questionsQUIZModeling text data with a hierarchy of clusters3 questionsTo Pass33% or higherAttempts3 every 8 hoursDeadlineNovember 19, 11:59 PM PST 1 point1.Make sure you have the latest versions of the notebook. Read this post if … you downloaded the notebook before September 10… you created an Amazon EC2 instance before October 1 I acknowledge. 1 point2.Which diagram best describes the hierarchy right after splitting the ice_hockey_football cluster?football golf 1 point3.Let us bipartition the clusters male_non_athletes and female_non_athletes. Which diagram best describes the resulting hierarchy of clusters for the non-athletes? Note. The clusters for the athletes are not shown to save space. Summary and what’s ahead in the specializationWhat we didn’t cover2 minThank you!1 min]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Coursera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity DLND Notebook]]></title>
    <url>%2F2017%2F08%2F14%2FUdacity%20DLND%20Notebook%2F</url>
    <content type="text"><![CDATA[Not enroll yet.Here‘s preview]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Functions Notebook]]></title>
    <url>%2F2017%2F08%2F11%2FPython%20Functions%20Notebook%2F</url>
    <content type="text"><![CDATA[Notation for errors I met.For quick search Build in functionAassertcode:assert statementexplanation:1234if statement == True: continueelse: report AssertionError Rreadline()read the first line and begin with second linedata:12Title,Released,Label,UK Chart Position,US Chart Position,BPI Certification,RIAA CertificationPlease Please Me,22 March 1963,Parlophone(UK),1,-,Gold,Platinum code:1234with open(datafile, &quot;r&quot;) as f: header = f.readline().split(&quot;,&quot;) for line in f: ... header:[&#39;Title&#39;, &#39;Released&#39;, &#39;Label&#39;, &#39;UK Chart Position&#39;, &#39;US Chart Position&#39;, &#39;BPI Certification&#39;, &#39;RIAA Certification\n&#39;]line:Please Please Me,22 March 1963,Parlophone(UK),1,-,Gold,Platinum Sstrip()can be used to delete the \n codecode:1234header = f.readline().split(&quot;,&quot;)print headerprint header[-1]print header[-1].strip() output:1234[&apos;Title&apos;, &apos;Released&apos;, &apos;Label&apos;, &apos;UK Chart Position&apos;, &apos;US Chart Position&apos;, &apos;BPI Certification&apos;, &apos;RIAA Certification\n&apos;]RIAA CertificationRIAA Certification Imported modulescsvcsv.DictReader()read csv file,default denote the first row as the field labels,line is dict data typecode:12345import csvwith open(datafile, &apos;rb&apos;) as sd: r = csv.DictReader(sd) for line in r: ... more details can be found in ud032 Using CSV Module xlrdxlrd.open_workbook()read xls filecode:1234567import xlrddatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;workbook = xlrd.open_workbook(datafile)sheet = workbook.sheet_by_index(0)data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] details can be found in ud032 Notebook ZipFilecode:12from zipfile import ZipFiledatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud032 Data Wrangling with MongoDB Notebook]]></title>
    <url>%2F2017%2F08%2F11%2FUdacity%20ud032%20Data%20Wrangling%20with%20MongoDB%20Notebook%2F</url>
    <content type="text"><![CDATA[Course can be found here, classroom.wiki Data Extraction FundamentalsIntroQuiz: Action TimeAssessing the Quality of Data Pt. 1Assessing the Quality of Data Pt. 2Tabular FormatsCSV FormatYou can download the datafiles from the Supporting Materials link on this page or the Course Materials page. Pick the data format supported by your spreadsheet application, download the file, open it in the spreadsheet, then export it as “csv” file. Compare the size of the spreadsheet file with the size of the csv file! Beatles Discography (csv) Quiz: Parsing CSV Filescode:123456789101112131415161718192021222324252627282930313233343536# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)# split each line on &quot;,&quot; and then for each line, create a dictionary# where the key is the header title of the field, and the value is the value of that field in the row.# The function parse_file should return a list of dictionaries,# each data line in the file being a single list entry.# Field names and values should not contain extra whitespace, like spaces or newline characters.# You can use the Python string method strip() to remove the extra whitespace.# You have to parse only the first 10 data lines in this exercise,# so the returned list should have 10 entries!import osDATADIR = &quot;&quot;DATAFILE = &quot;beatles-diskography.csv&quot;def parse_file(datafile): data = [] with open(datafile, &quot;r&quot;) as f: for line in f: print line return datadef test(): # a simple test of your implemetation datafile = os.path.join(DATADIR, DATAFILE) d = parse_file(datafile) firstline = &#123;&apos;Title&apos;: &apos;Please Please Me&apos;, &apos;UK Chart Position&apos;: &apos;1&apos;, &apos;Label&apos;: &apos;Parlophone(UK)&apos;, &apos;Released&apos;: &apos;22 March 1963&apos;, &apos;US Chart Position&apos;: &apos;-&apos;, &apos;RIAA Certification&apos;: &apos;Platinum&apos;, &apos;BPI Certification&apos;: &apos;Gold&apos;&#125; tenthline = &#123;&apos;Title&apos;: &apos;&apos;, &apos;UK Chart Position&apos;: &apos;1&apos;, &apos;Label&apos;: &apos;Parlophone(UK)&apos;, &apos;Released&apos;: &apos;10 July 1964&apos;, &apos;US Chart Position&apos;: &apos;-&apos;, &apos;RIAA Certification&apos;: &apos;&apos;, &apos;BPI Certification&apos;: &apos;Gold&apos;&#125; assert d[0] == firstline assert d[9] == tenthline test() solution:123456789101112131415def parse_file(datafile): data = [] with open(datafile, &quot;r&quot;) as f: header = f.readline().split(&quot;,&quot;) count = 0 for line in f: if count == 10: break field = line.split(&quot;,&quot;) entry = &#123;&#125; for i, value in enumerate(field): entry[header[i].strip()] = value.strip() data.append(entry) count += 1 return data You can check the data in the dropdown in the top-left corner of the quiz starter code or download the datafile beatles-diskography.csv in the Supporting Materials below. Python string method strip() will come in handy to get rid of the extra whitespace (that includes newline character at the end of line) Quiz: Problematic LineUsing CSV Modulecsv.DictReader()default denote the first row as the field labelsline is dict data typecode:123456import csvdatafile = &quot;.csv&quot;with open(datafile, &apos;rb&apos;) as sd: r = csv.DictReader(sd) for line in r: ... You can read more about the python csv module at the link below:http://docs.python.org/2/library/csv.html Intro to XLRDYou might find this video a lot more exciting if you try to run this code locally along the video! You can download the datafile from Course Materials. You can also install the xlrd library locally on your computer via python pip and the following command: pip install xlrdThe example code:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import xlrddatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;def parse_file(datafile): workbook = xlrd.open_workbook(datafile) sheet = workbook.sheet_by_index(0) data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] print &quot;\nList Comprehension&quot; print &quot;data[3][2]:&quot;, print data[3][2] print &quot;\nCells in a nested loop:&quot; for row in range(sheet.nrows): for col in range(sheet.ncols): if row == 50: print sheet.cell_value(row, col), ### other useful methods: print &quot;\nROWS, COLUMNS, and CELLS:&quot; print &quot;Number of rows in the sheet:&quot;, print sheet.nrows print &quot;Type of data in cell (row 3, col 2):&quot;, print sheet.cell_type(3, 2) print &quot;Value in cell (row 3, col 2):&quot;, print sheet.cell_value(3, 2) print &quot;Get a slice of values in column 3, from rows 1-3:&quot; print sheet.col_values(3, start_rowx=1, end_rowx=4) print &quot;\nDATES:&quot; print &quot;Type of data in cell (row 1, col 0):&quot;, print sheet.cell_type(1, 0) exceltime = sheet.cell_value(1, 0) print &quot;Time in Excel format:&quot;, print exceltime print &quot;Convert time to a Python datetime tuple, from the Excel float:&quot;, print xlrd.xldate_as_tuple(exceltime, 0) return datadata = parse_file(datafile) Quiz: Reading Excel FilesYou can download the “2013_ERCOT_Hourly_Load_Data.xls” datafile from the Supporting Materials section on this page or from this Course Materials page. Note that the code expects the data to be contained in an archive named “2013_ERCOT_Hourly_Load_Data.xls.zip”, so you will need to change the name of the downloaded archive or modify the code to run the code on your local computer. You can also install the xlrd library locally on your computer via python pip and the following command:pip install xlrd 2013 ERCOT hourly load data code:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/usr/bin/env python&quot;&quot;&quot;Your task is as follows:- read the provided Excel file- find and return the min, max and average values for the COAST region- find and return the time value for the min and max entries- the time values should be returned as Python tuplesPlease see the test function for the expected return format&quot;&quot;&quot;import xlrdfrom zipfile import ZipFiledatafile = &quot;2013_ERCOT_Hourly_Load_Data.xls&quot;def open_zip(datafile): with ZipFile(&apos;&#123;0&#125;.zip&apos;.format(datafile), &apos;r&apos;) as myzip: myzip.extractall()def parse_file(datafile): workbook = xlrd.open_workbook(datafile) sheet = workbook.sheet_by_index(0) ### example on how you can get the data #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] ### other useful methods: # print &quot;\nROWS, COLUMNS, and CELLS:&quot; # print &quot;Number of rows in the sheet:&quot;, # print sheet.nrows # print &quot;Type of data in cell (row 3, col 2):&quot;, # print sheet.cell_type(3, 2) # print &quot;Value in cell (row 3, col 2):&quot;, # print sheet.cell_value(3, 2) # print &quot;Get a slice of values in column 3, from rows 1-3:&quot; # print sheet.col_values(3, start_rowx=1, end_rowx=4) # print &quot;\nDATES:&quot; # print &quot;Type of data in cell (row 1, col 0):&quot;, # print sheet.cell_type(1, 0) # exceltime = sheet.cell_value(1, 0) # print &quot;Time in Excel format:&quot;, # print exceltime # print &quot;Convert time to a Python datetime tuple, from the Excel float:&quot;, # print xlrd.xldate_as_tuple(exceltime, 0) data = &#123; &apos;maxtime&apos;: (0, 0, 0, 0, 0, 0), &apos;maxvalue&apos;: 0, &apos;mintime&apos;: (0, 0, 0, 0, 0, 0), &apos;minvalue&apos;: 0, &apos;avgcoast&apos;: 0 &#125; return datadef test(): open_zip(datafile) data = parse_file(datafile) assert data[&apos;maxtime&apos;] == (2013, 8, 13, 17, 0, 0) assert round(data[&apos;maxvalue&apos;], 10) == round(18779.02551, 10)test() solution:1234567891011121314151617181920212223242526def parse_file(datafile): workbook = xlrd.open_workbook(datafile) sheet = workbook.sheet_by_index(0) cv = sheet.col_values(1, start_rowx=1, end_rowx=None) maxvalue = max(cv) maxposition = cv.index(maxvalue) + 1 minvalue = min(cv) minposition = cv.index(minvalue) + 1 max_exceltime = sheet.cell_value(maxposition, 0) maxtime = xlrd.xldate_as_tuple(max_exceltime, 0) min_exceltime = sheet.cell_value(minposition, 0) mintime = xlrd.xldate_as_tuple(min_exceltime, 0) avgcoast = sum(cv)/len(cv) data = &#123; &apos;maxtime&apos;: maxtime, &apos;maxvalue&apos;: maxvalue, &apos;mintime&apos;: mintime, &apos;minvalue&apos;: minvalue, &apos;avgcoast&apos;: avgcoast &#125; return data Intro to JSONData Modeling in JSONJSON ResourcesExtra InfoIf you’re unfamiliar with JSON, or would just like a refresher, W3Schools has a great tutorial on the subject. JSON Tutorial You can also check out http://www.json.org/ You can find information about Python’s json module on this page of the Python documentation. Note that JSON arrays are interpreted as lists and JSON objects as dictionaries, so you can use the standard Python approaches to inspect JSON data. You’ll get some practice exploring some data of this type in the next quiz. Quiz: JSON PlaygroundYou can check the data in the dropdown in the top-left corner of the quiz starter code. ‘Run locally’ means that you have to download or copy the file contents to your local machine, modify it and run. To be able to do that you need to have Python installed, as well as the requests module. Please see Requests installation documentation. If you have “pip, you can install Requests by running the following command: pip install requestsTo learn more about the requests module, see the documentation here. Quiz: Exploring JSONCheck You Out]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Data Science Community Notebook]]></title>
    <url>%2F2017%2F07%2F24%2FCoursera%20Data%20Science%20Community%20Notebook%2F</url>
    <content type="text"><![CDATA[About this course: This “course” is a community space for Coursera learners to connect with one another to discuss all things Data Science. Talk about the latest trends in the industry, give and receive advice on getting into or progressing a Data Science career, work on some portfolio projects, or just dig into some interesting problems with fellow Data Science enthusiasts. I was invited into Coursera Data Science Community in 2017-07-24 Week 1Welcome!Welcome to the Coursera Data Science community!About the Coursera Data Science community 10 minWelcome to the Coursera Data Science community! We hope you’ll get involved with some interesting Data Science discussions, chat about the latest trends and tools within Data Science, get and give career advice, and meet some great people who share your enthusiasm for Data Science! This “course” doesn’t work in quite the same way as normal courses. There aren’t any course materials so you’ll want to head over to the discussion forums to get started. ResourcesResources to accompany webinars 10 minData Science Career Webinar 1 (for MCS-DS students) _593f80302483112d4d09763e0f9eddc7_MCSDS Data Science Career Webinar 1 (for MCS-DS students) 52 minhttps://www.coursera.org/learn/data-science-community/lecture/xx6ws/data-science-career-webinar-1-for-mcs-ds-students Data Science Career Webinar 2 (for MCS-DS students) 47 minhttps://www.coursera.org/learn/data-science-community/lecture/ayP9T/data-science-career-webinar-2-for-mcs-ds-students]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity st095 Statistics Notebook]]></title>
    <url>%2F2017%2F07%2F19%2FUdacity%20st095%20Statistics%20Notebook%2F</url>
    <content type="text"><![CDATA[This course, classroom can be divided into describe (classroom) and inference statistics.wikiCompleted in 2017/08/07 Orientation!WelcomeIntroQuiz: MnMsQuiz: Definitely blueProblem SetsQuiz: ForumsQuiz: Instructor NotesMy DriveQuiz: Practice with SpreadsheetsQuiz: Calculate sumsCourse WikiYou can get to the course wiki by clicking “Materials” or following this link. Intro to Research MethodsLauren&#39;s Intro VideoQuiz: Believe ResultsQuiz: Measure MemoryDefine ConstructsBBC Memory TestMemory Test DescriptionHere is a brief description of how the test worked, to help you follow the rest of the lesson. The test consisted of three parts: Users were shown 12 photos in the first part.Users were shown another 12 photos in the second part.Users were shown 48 photos in the third part and asked if they saw each photoin the first part, the second part, or neither. After the test, users were given two scores: A “Recognition score”, calculated as the percentage of times they correctly identified whether they saw the face at all, regardless of which part the face was from.A “Temporal memory score”, calculated as the percentage of recognized faces that were identified with the correct part (part 1 or part 2). Users were advised to take a 5 minute break between each part of the test. BBC ScoresGoogle AccountQuiz: BBC MeasurementOperational DefinitionQuiz: ConstructsQuiz: Operational DefinitionsDataQuiz: Sleep and MemoryQuiz: Influence MemoryQuiz: Control for Time of Daylurking variablesextraneous factor Quiz: Same ScoresPopulation parameters (such as mu, or μ) are values that describe the entire population. Sample statistics (such as X-bar, or $\bar{x}$​​ ) are values that describe our sample; we use statistics to estimate the population parameters. Estimates are our best guesses for the population parameters. So, for example, we would use X-bar to estimate mu. Quiz: Sample AverageQuiz: Better SampleRandomnessQuiz: Visualize RelationshipQuiz: True or Not?Golden Arches TheoryMcDonald&#39;sQuiz: World PeaceCausal InferenceQuiz: Benefits of SurveysQuiz: Downsides of SurveysQuiz: PlaceboQuiz: BlindQuiz: Double BlindQuiz: Controlled FactorsRandom AssignmentQuiz: Control for What?Quiz: Katie&#39;s HandClick this link to access the data: Height and hand length Copy and paste the data into your own spreadsheet to create the scatterplot. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Draw ConclusionsProblem Set 1: Intro to statistical research methodsQuiz: 1. All California ResidentsQuiz: 2. Sleepy College StudentsQuiz: 3. Not Enough Sleep…?Quiz: 4. Characteristic of a SampleQuiz: 5. Freshman 15Quiz: 6. Characteristic of a PopulationQuiz: 7. Sample Approximates PopulationQuiz: 8. SAT ScoresQuiz: 9. Survey to FriendsQuiz: 10. Which Are Constructs?Quiz: 11. Which Are Not Constructs?Quiz: 12. Define Operational DefinitionQuiz: 13. Research StudiesQuiz: 14. VariablesQuiz: 15. Variable RelationshipsQuiz: 16. Which Are Hypotheses?Quiz: 17. Lurking AroundQuiz: 18. Symbols = Fun Fun Fun!Quiz: 19. nQuiz: 20. Which Are True?Quiz: 21. Random SampleQuiz: 22. Convenience SamplesQuiz: 23. Sample vs. PopulationQuiz: 24. Classical MusicQuiz: 25. CaffeineQuiz: 26. Video GamesQuiz: 27. English TeachingQuiz: 28. Reading ScoreQuiz: 29. Extreme TemperatureQuiz: 30. Teaching MethodQuiz: 31. Reading ScoresQuiz: 32. Which Is Better?Quiz: 33. All Kinds of VariablesQuiz: 34. LandminesQuiz: 35. LandminesQuiz: 36. Student SatisfactionQuiz: 37. InsomniaQuiz: 38. InsomniaQuiz: 39. SADQuiz: 40. Random AssignmentQuiz: 41. Placebo Control ConditionQuiz: 42. Why Placebo?Quiz: 43. Measuring ConstructsQuiz: 44. ParticipantsQuiz: 45. What Proportion?Quiz: 46. Blind StudiesQuiz: 47. Causality?Quiz: 48. DepressionQuiz: 49. DepressionQuiz: 50. DepressionConclusionconstructopertaional definitionindependent variabledependent variableextraneous or lurking variableexperimental study - casual conclusionobservational study Visualizing DataQuiz: Where Students Are FromQuiz: FrequencyQuiz: US, China, PakistanQuiz: Relative FrequencyQuiz: Range of ProportionsQuiz: Sum Relative FrequenciesQuiz: Proportion from CountriesQuiz: Convert to PercentageQuiz: Range PercentagesQuiz: ContinentsQuiz: Number of Rowsinterval = bin = bucket Quiz: Bin SizeVisualizing DataQuiz: HistogramDifferent Bin SizesClick this link to play around with the histogram applet! (You may need to install the Java plug-in.) Interactivate Histogram Applet Udacity Student Ages Data: Make sure you have one number per line for the applet. Quiz: Smaller BinQuiz: Find Bin SizeQuiz: Most Frequent AgeQuiz: Proportion over 60Quiz: Percentage Under 60Quiz: Younger than 20Quiz: Continent GraphQuiz: Difference Between Graphshistogram: bin x-axis: numerical/quantitativebar: x-axis: categorical/qualitative Quiz: Biased GraphsChanging Bin SizeQuiz: Interpret HistogramQuiz: Skewed DistributionProblem Set 2: Visualizing dataQuiz: 1. Blood TypesThe Σ symbol means the total sum. It is the Greek letter capital sigma. f stands for frequency (count), p stands for proportion. The Σ symbol means the total sum. It is the Greek letter capital sigma. f stands for frequency (count), p stands for proportion. Enter each answer as a number without any special characters, including % signs. Enter proportions as decimals. The Σ symbol means the total sum. It is the Greek letter capital sigma. Quiz: 2. Rare BloodQuiz: 3. Common BloodQuiz: 4. Type AQuiz: 5. GuesstimateQuiz: 6. Analyze StuffQuiz: 7. Ode to nQuiz: 8. Calculate PercentagesQuiz: 9. Common DecadeQuiz: 10. How Old?Quiz: 11. Really Old!Quiz: 12. Birth Year HistogramQuiz: 13. Most Common BinQuiz: 14. When Most Were BornQuiz: 15. Type of DataQuiz: 16. Heights of BarsQuiz: 17. Which Region?Quiz: 18. Calculate Bin SizeQuiz: 19. How to Find nQuiz: 20. How to Analyze ShapeQuiz: 21. Commute TimeQuiz: 22. Commute an HourQuiz: 23. Find Bin WidthQuiz: 24. Analyze HistogramQuiz: 25. Frequency and Bin SizeQuiz: 26. Positively SkewedQuiz: 27. Distribution of What?Quiz: 28. Thinking About DistributionsQuiz: 29. Frequency AxisQuiz: 30. X-Axis Represent!Quiz: 31. Skewnesspositively skewed Quiz: 32. Negatively SkewedQuiz: 33. Normal DistributionQuiz: 34. Table vs. HistogramGoogle Spreadsheet TutorialTutorialLink to Udacians’ Facebook Friends Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” You will work with this data in Lesson 3. Central tendencyQuiz: Which Major?One Number to Describe DataQuiz: Which Number to Choose?Quiz: Mode of Datasetmode: most frequency Quiz: Mode of DistributionQuiz: Mode - Negatively Skewed DistributionQuiz: Mode - Uniform DistributionQuiz: More than One Mode?Quiz: Mode of Categorical DataQuiz: More o&#39; Mode!Quiz: Find the MeanQuiz: Procedure for Finding MeanQuiz: Iterative ProcedureHelpful SymbolsQuiz: Properties of the MeanQuiz: Mean with OutlierQuiz: What Can You Expect?UNCQuiz: Requirement for MedianQuiz: Find the MedianQuiz: Median with OutlierQuiz: Find Median with OutlierMeasures of Centermean median mode Quiz: Order Measures of Center 1Quiz: Order Measures of Center 2Use Measures of Center to CompareQuiz: Udacians&#39; Facebook Friends - MeanQuiz: Udacians&#39; Facebook Friends - MedianQuiz: Formula for Location of MedianQuiz: Wrap Up - Measures of CenterGood Job!Problem Set 3: Central tendencyQuiz: 1. BBC Memory ScoresLink to spreadsheet with sample memory scores: BBC Sample Scores Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: 2. BBC Memory ScoresQuiz: 3. BBC Memory ScoresQuiz: 4. What Distribution?Quiz: 5. What Distribution?Quiz: 6. Normal DistributionQuiz: 7. Positively SkewedQuiz: 8. MeanQuiz: 9. MedianQuiz: 10. ModeQuiz: 11. Deal or No Deal? (Median)Deal or No Deal? Quiz: 12. Deal or No Deal? (Mode)Quiz: 13. Deal or No Deal? (Number)Quiz: 14. Deal or No Deal? (Mean vs. Median)Quiz: 15. Deal or No Deal? (Mean)Quiz: 16. Deal or No Deal? (Proportion)Quiz: 17. Deal or No Deal? (Frequency)Quiz: 18. Deal or No Deal? (Distribution)Quiz: 19. Deal or No Deal? (Center)Quiz: 20. NHL (Mean)Round to the nearest hundredth (xx.xx) National Hockey League Quiz: 21. NHL (Mode)Quiz: 22. NHL (Median)Quiz: 23. Which Distributions?Quiz: 24. Median Given HistogramVariabilityQuiz: Social Networkers&#39; SalariesQuiz: Should You Get an Account?Quiz: What&#39;s the Difference?Quiz: Quantify SpreadQuiz: Does Range Change?Quiz: Mark Z the OutlierChop Off the TailsQuiz: Where Is Q1?quartile Quiz: Q3 - Q1Quiz: IQRIQR=Q3-Q1 Quiz: What Is an Outlier?Quiz: Define Outlier&lt; Q1 - 1.5 IQR> Q3 + 1.5 IQR Quiz: Match BoxplotsQuiz: Mean Within IQR?Problem with IQRQuiz: Measure VariabilityQuiz: Calculate MeanQuiz: Deviation from MeanQuiz: Average DeviationQuiz: Equation for Average DeviationQuiz: Be Happy and Get Rid of NegativesQuiz: Absolute DeviationsQuiz: Average Absolute DeviationQuiz: Formula for Avg. Abs. Dev.Quiz: Squared DeviationsQuiz: Sum of SquaresQuiz: Average Squared Deviationvariance Quiz: Avg. Squared Dev. in WordsQuiz: One DimensionStandard Deviationsquare root of variance Quiz: Calculate SDQuiz: SD Social NetworkersQuiz: SD in WordsQuiz: Spreadsheet SDClick here to access the data: Sample Social Networkers’ Salary Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Click here to access the data: Sample Social Networkers’ Salary Enter your answer as a number without any special characters, including $ or commas. Link to Sample Social Networkers’ Salary Copy and paste this data into your own spreadsheet to calculate the standard deviation. Point of SDQuiz: Find ValuesQuiz: Sample SDQuiz: Bessel&#39;s CorrectionStandard Deviation of sample is smaller than Standard Deviation of population, so we use n-1 to estimate the Standard Deviation of population(sample Standard Deviation) Sample Standard deviation(Standard deviation of population):$$s = \sqrt{\frac{\sum (x_i - \bar{x})^2}{n - 1}}$$Variance:$$\frac{\sum (x_i - \bar{x})^2}{n -1}$$Standard deviation of sample$$s = \sqrt{\frac{\sum (x_i - \bar{x})^2}{n}}$$ Clarifying Sample SDJelly BeansProblem Set 4: VariabilityQuiz: 1. Udacians&#39; Facebook Friends (Mean)Click here for the link to the data: Udacians’ Facebook Friends Quiz: 2. Udacians&#39; Facebook Friends (Avg. Dev)Quiz: 3. Udacians&#39; Facebook Friends (SS)Quiz: 4. Udacians&#39; Facebook Friends (Variance)Quiz: 5. Udacians&#39; Facebook Friends (Std Dev)Quiz: 6. Udacians&#39; Facebook Friends (mean +/- std dev)Quiz: 7. Udacians&#39; Facebook Friends (proportion)Quiz: 8. Udacians&#39; Facebook Friends (sample sd)Quiz: 9. Class ExamQuiz: 10. Where&#39;s Your Score? (sd = 5)Quiz: 11. Where&#39;s Your Score? (sd = 2.5)Quiz: 12. Where&#39;s Your Score? (sd = 10)Quiz: 13. BBC Sample Scores (Std. Dev.)Link to BBC Sample Scores Quiz: 14. BBC Sample Scores (Variability)Lessons 1-4 Review/AssessmentQuiz: IQuiz: IIQuiz: IIIQuiz: IVQuiz: VQuiz: VIQuiz: VIIQuiz: VIIIQuiz: IXQuiz: XQuiz: XIQuiz: XIIQuiz: XIIIQuiz: XIVQuiz: XVQuiz: XVIQuiz: XVIIQuiz: XVIIIQuiz: XIXQuiz: XXQuiz: XXIQuiz: XXIIQuiz: XXIIIQuiz: XXIVQuiz: XXVQuiz: XXVIQuiz: XXVIIQuiz: XXVIIIQuiz: XXIXQuiz: XXXQuiz: XXXIStandardizingQuiz: ChessUSCF DistributionQuiz: Absolute or RelativeQuiz: Relative Frequency HistogramQuiz: Proportion Between 170 and 210Quiz: Proportion Between 180 and 200Quiz: More DetailInfinitely SmallQuiz: Continuous DistributionTheoretical Normal DistributionZnumber of standard deviations away from the mean UnpopularQuiz: Katie - SDs BelowQuiz: Andy - SDs BelowQuiz: Who&#39;s More Unpopular?Quiz: Formula for Number of SDsZ-ScoreLink to poll: How many Facebook friends do you have?$z = \frac{x-\mu}{\sigma}$ Quiz: Negative Z-ScoreQuiz: Mean of Standardized DistributionQuiz: SD of Standardized DistributionStandard Normal DistributionQuiz: Popular ChrisQuiz: Convert to Z-ScoreQuiz: Convert to Popularity ScoreProblem Set 5: StandardizingQuiz: 1. Which Distribution Is Which?Quiz: 2. Z-ScoresQuiz: 3. OkCupidQuiz: 4. Social MediaQuiz: 5. Z-Scores of UsageQuiz: 6. Where on the Distribution?Quiz: 7. Mean and SDQuiz: 8. IQ 125Quiz: 9. IQ 150Quiz: 10. ScoresQuiz: 11. Grade on a CurveQuiz: 12. ExtremeQuiz: 13. SDQuiz: 14. SJSU FootballQuiz: 15. ExamsQuiz: 16. Which SD?Quiz: 17. Closest to MeanQuiz: 18. Farthest from MeanQuiz: 19. True or False?Quiz: 20. BBC - SleepQuiz: 21. BBC - RecognitionQuiz: 22. BBC - TemporalQuiz: 23. BBC - What Score?Normal DistributionIntro to the PDFprobability density function Quiz: ProbabilityGet to Know the PDFQuiz: Probability GreaterQuiz: Probability LessQuiz: 2 SDs Below or AboveQuiz: Proportion of Facebook FriendsQuiz: More than 262Quiz: Between 118 and 226Quiz: Less than 240Z-TableHere is a link to the z-table shown in the video:z-table Quiz: Using the Z-TableKarmaUdacity forums no longer use the karma system Katie describes. However, you can still access the dataset Katie created by clicking here: Average Karma points per post. Quiz: Average Karma Points per PostQuiz: SD of Karma Points per PostQuiz: Integer SDsQuiz: Less than 5Quiz: More than 20Quiz: Between 10 and 16Quiz: Top 5%Great Job!Link to visualize the area under the curve Problem Set 6: Normal DistributionQuiz: 1. HeightsQuiz: CHALLENGE 2. HeightsQuiz: 3. HousesQuiz: 4. HousesQuiz: 5. HousesQuiz: 6. HousesQuiz: 7. Greater than 108Quiz: 8. Less than 76Quiz: 9. Between 65 and 90Quiz: 10. Between 80 and 95Quiz: 11. Top 30%Quiz: 12. Greater than 1.64Quiz: 13. Less than -2.33Quiz: 14. Top 40%Quiz: 15. MeaningsQuiz: 16. 64th PercentileSampling DistributionsQuiz: Compare Sample MeansQuiz: Gambling in VegasA tetrahedral die will result in a 1, 2, 3, or 4 on each roll. If you’re curious about how this works, you can read this thread. Write your answer as a proportion. Quiz: Tetrahedral DieQuiz: Total Number of SamplesQuiz: Mean of Each SampleQuiz: Mean of Sample MeansQuiz: Sampling Distributiondistribution of sample means = Sampling DistributionMean of each sample:1, 1.5, 2, 2.5, 1.5, 2, 2.5, 3, 2, 2.5, 3, 3.5, 2.5, 3, 3.5, 4 Copy and paste the sample means into WolframAlpha and hit enter. Pay particular attention to the histogram showing the frequency of each mean. Quiz: Probability Mean &gt; or = 3Quiz: What We Need to Compare the MeansQuiz: Calculate SDsSE: standard deviation of all our sample means Quiz: Relationship Between SDsQuiz: Ratio of SDs$$SE = \frac{ \sigma }{ \sqrt{n} }$$$\sigma$: population standard deviationSE: standard deviation of distribution of sample means(sampling distribution) Quiz: SD of Sampling DistributionThe Central Limit TheoremSE: standard error Quiz: Roll 1 DieNOTE: Unfortunately, the rolling die simulation we used to link to here is no longer valid. Don’t worry! You have enough information now to answer this question without it. Give it your best shot, and you’ll see how the applet works in the solution video. An alternative simulation can be found here. Quiz: Roll 2 DiceQuiz: Find Standard ErrorThe “standard deviation of the sampling distribution” is also known as the standard error.The mean is solved for by adding up the different outcomes (1, 2, 3, 4, 5, 6) and dividing by the number of outcomes (6), yielding a result of 3.5 Quiz: Roll 5 DiceQuiz: Standard Error for Avg of 5 DiceQuiz: Standard Error When n IncreasesQuiz: Shape of Distribution When n IncreasesSimulation AppletLink to awesome simulation applet M&amp;MsQuiz: M&amp;M CLTUsing Sampling DistributionKloutWhat is Klout and how does it work? Quiz: Klout ParametersLink to Klout data Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Your calculations should automatically save in Google Drive. Quiz: Klout Sampling Distribution (Mean)Quiz: Klout Sampling Distribution (SD)Quiz: Sampling Distribution ShapeWhat Do You Get with a Good Klout Score?Quiz: Location of Mean on DistributionQuiz: Probability of Obtaining MeanQuiz: Does Low Probability = Causation?Quiz: Increase Sample SizeQuiz: Location of MeanQuiz: Probability of MeanQuiz: Something FunProblem Set 7: Sampling DistributionsQuiz: 1. Central Limit TheoremQuiz: 2. Location of Sample MeanQuiz: 3. Average DifferenceQuiz: 4. Increase Sample SizeQuiz: 5. Standard ErrorQuiz: 6. n and σQuiz: 7. n and x-barQuiz: 8. Mean of Sample MeansQuiz: 9. Standard ErrorQuiz: 10. Z-ScoreQuiz: 11. ProbabilityQuiz: 12. Mean n = 25Quiz: 13. Standard ErrorQuiz: 14. ProbabilityQuiz: 15. Probability DecreasedQuiz: 16. Population Distribution ShapeQuiz: 17. Sampling Distribution ShapeQuiz: 18. Mean of Sampling DistributionQuiz: 19. SD of Sampling DistributionQuiz: 20. Which Distribution?Quiz: 21. Greater or Less?Quiz: CHALLENGE 22. What Sample Size?Final projecthere EstimationSummaryQuiz: Mean of Treated Populationpoint estimate Quiz: Population Mean vs. Sample MeanQuiz: Percent of Sample MeansApproximate Margin of Errormargin of error: $\frac{2\sigma}{\sqrt{n}}$ Interval Estimate for Population MeanQuiz: Confidence Interval BoundsQuiz: Exact Z-ScoresSampling DistributionQuiz: 95% CI with Exact Z-ScoresQuiz: Generalize Point EstimateQuiz: Generalize CIQuiz: CI Range for Larger Sample SizeQuiz: CI When n = 250Bigger Sample, Smaller CIQuiz: Z for 98% CILink to z-table Quiz: Find 98% CICritical Values of Z-1.96 and 1.96 are the critical values of z for 95% confidence Quiz: Engagement RatioLink to Engagement Ratio data Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Enter your answer as a number without any special characters, including commas. Hypothesis Testing SongQuiz: Point Estimate Engagement RatioQuiz: Standard ErrorQuiz: CI BoundsQuiz: Generalize CIMargin of ErrorThe margin of error is half the width of the confidence interval. Quiz: Rate Engagement and LearningLink to rate your engagement and learning in this class Results from SampleQuiz: What Statistics?Quiz: Sampling DistributionsQuiz: Z-Scores of Sample MeansQuiz: Probability Sample Mean Is at Least…Quiz: What Does This Mean?Wrap-UpProblem Set 8: EstimationQuiz: 2. Larger RangeQuiz: 3. Increase Sample SizeQuiz: 4. Increase Population SDQuiz: 5. 95% CIQuiz: 6. Critical Values 95% CIQuiz: 7. Standard ErrorQuiz: 8. ProbabilityLink to z-table Write the probability as a proportion.Hint: Theoretically, z-scores can be anything from -infinity to +infinity. What does it say about the probability if you get really high or really low z-scores? Quiz: 9. Margin of ErrorQuiz: 10. 95% CIQuiz: 11. Interpret CIQuiz: 12. Critical Values 99% CIQuiz: 13. Standard ErrorQuiz: 14. ProbabilityQuiz: 15. Margin of ErrorQuiz: 16. 99% CIQuiz: 17. Interpret CIHypothesis testingQuiz: Likely or unlikelyQuiz: Likely or UnlikelyQuiz: Alpha Levels0.05 5%0.01 1%0.001 0.1% Quiz: Z-Critical Value 0.05critical regionz-critical value Quiz: Critical Values 0.01Quiz: Critical Values 0.001Critical RegionsNote that when it comes to constructing a hypothesis test, it is best to choose a significant level before you perform the test. You can report the results as significant at a certain critical level after obtaining your result, but it is important that you are not ‘fishing’ for results before you see the results in your sample. This article has some additional information about understanding significance levels and p-values in hypothesis testing. Quiz: Significance $\alpha$ level z-critical value 0.05 1.65 0.01 2.32 0.001 3.08 z-score significant at 3.14 p&lt;0.001 DartsQuiz: Z-ScoreQuiz: Two-Tailed Critical Values 0.05Quiz: Two-Tailed TestQuiz: Two-Tailed ProbabilityQuiz: Two-Tailed Critical Values 0.01Quiz: Two-Tailed Critical Values 0.001HypothesesQuiz: Fail to Reject the NullQuiz: Evidence to Reject the NullQuiz: Mean and SDLink to Learning and Engagement Data (use this data for quiz) Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Link to Full Learning and Engagement Results (optional) Quiz: Null HypothesisQuiz: Alternative HypothesisOne tailed or two tailedQuiz: Conduct Hypothesis Testwhat does it mean to reject the null? our sample mean falls within the critical region the z-score of our sample mean is greater than the z-critical value the probability of obtaining the sample mean is less than the alpha level Quiz: Critical Values 0.05Quiz: Z-Score of Sample MeanQuiz: Results of Hypothesis TestQuiz: Increase Sample SizeQuiz: Reject or Fail to RejectQuiz: Probability of Obtaining MeanQuiz: Decision ErrorsType 1 error: reject true H0Type 2 error: retain false H0 Quiz: Hot BeverageQuiz: RainingQuiz: What Happened?Quiz: What Happened?Prone to MisinterpretationsTo Finish This Lesson…Hypothesis TestingQuiz: Increase Engagement?Problem Set 9: Hypothesis testingQuiz: 1. True or False?Quiz: 2. Null HypothesisQuiz: 3. Alternative HypothesisQuiz: 4. Standard ErrorQuiz: 5. Z-ScoreQuiz: 6. Z-Critical ValueQuiz: 7. Statistical DecisionQuiz: 8. ConclusionQuiz: 9. Type I ErrorQuiz: 10. HypothesesQuiz: 11. Standard ErrorQuiz: 12. Z-ScoreQuiz: 13. Valentine&#39;s DayLessons 5-9 Review/AssessmentQuiz: 1. Find z-scoreQuiz: 2. ProbabilityQuiz: 3. Z-score furthest from meanQuiz: 4. Find x given z-scoreQuiz: 5. ProportionQuiz: 6. ProbabilityQuiz: 7. Compare using distributionsQuiz: 8. Standard Normal DistributionQuiz: 9. ProbabilityQuiz: 10. Z-scoreQuiz: 11. PercentileQuiz: 12. Distribution of meansQuiz: 13. Standard ErrorQuiz: 14. Z-score of meanQuiz: 15. Probability of sample meanQuiz: 16. UnlikelyQuiz: 17. Proportion of samplesQuiz: 18. Proportion of samplesQuiz: 19. Increase nQuiz: 20. Point estimateQuiz: 21. 95% CIQuiz: 22. HypothesesQuiz: 23. DecisionQuiz: 24. Probability of Type I errorQuiz: 25. Find sigmat-TestsQuiz: t-Distribution$t = \frac{mean difference}{S}$S: population standard deviation estimated by standard deviation of sample means GuinnessQuiz: Degrees of FreedomQuiz: DF - Choose n NumbersQuiz: DF - Add to 10Quiz: DF - Marginal TotalsDF - Sample SDn-1 in sample standard deviation can be denoted as effctive sample size t-TableLink to t-Table Quiz: One-Tailed t-TestQuiz: Two-Tailed t-TestQuiz: Bounds of Areat-statistic: t critical value Quiz: Affect t-StatisticOne-Sample t-Test$t=\frac{\bar{x}-\mu_0}{s/\sqrt{n}}$ Quiz: Increase tQuiz: FinchesQuiz: Finches - n and DFLink to Finch beak width data Quiz: Finches - Mean and sQuiz: Finches - Find t-StatisticQuiz: Finches - Decisionp-value: probability of getting this t-statistic Quiz: P-Valuereject the null when the p-value is less than the $\alpha$ level Quiz: Visualize P-ValueQuiz: Find P-ValueLink to GraphPad Remember, our sample has 8 values, and our t-statistic is 0.977. Quiz: Rent - t-Critical ValuesQuiz: Rent - t-StatisticQuiz: Rent - DecisionQuiz: Rent - Cohen&#39;s dstandardized mean differenceCohen’s d =$$\frac{\bar{x} - \mu_0}{s}$$ Quiz: Rent - CIQuiz: Rent - Find CIQuiz: Rent - Margin of ErrorQuiz: Rent - Increase nDependent SamplesDependent t-test for paired sampleswithin-subject designs two conditions pre-test, post-test growth over time(longitudinal study) Quiz: KeyboardsLink to Keyboards data Quiz: Keyboards: Point Estimate for Differencepoint estimate is based on samples Quiz: Keyboards - SD of DifferencesQuiz: Keyboards - t-StatisticQuiz: Keyboards - t-Critical ValuesQuiz: Keyboards - DecisionQuiz: Keyboards - Cohen&#39;s dCohen’s d = difference of means / sample standard deviation estimated by samples Quiz: Keyboards - CI for Dependent SamplesNotation for DifferenceTypes of Designsrepeated measures design Effect Sizeeffect size=mean difference Quiz: Everyday MeaningTypes of Effect-Size Measures difference measures mean difference standarized difference Cohen’s d Correlation measures $r^2$ Statistical SignificanceCohen&#39;s d$d=\frac{\bar{x}-\mu}{s}$s: standard deviation of sample means r^2coefficieng of determination0-1$$r^2 = \frac{t^2}{t^2 + df}$$t: not the t-criticaldf: degrees of freedom0: the variables are not at all related Quiz: Compute r^2Report ResultsAPA stylet(df) = x.xx, p=.xx, directiont(24) = -2.50, p&lt;.05, one-tailed Report CI ResultsReport CI Results 2APA style-CIsconfidence interval on the mean difference; 95%CI = (4,6) Report Results Effect SizeCohen’s d, $r^2$d = x.xx$r^2$ = .xx One-Sample t-Testformulas:df = n-1$SEM = \frac{s}{\sqrt{n}}$s: sample standard deviation$t = \frac{\bar{x} - \mu}{SEM}$$\bar{x}$: sample mean$\mu$: population meanCI = $\bar{x}\pm$ margin of errormargin of error = t-critical * SEMCohen’s d = $\frac{\bar{x}-\mu}{s}$$r^2 = \frac{t^2}{t^2 + df}$t: not the t-critical MuQuiz: Dependent VariableQuiz: TreatmentQuiz: Null HypothesisQuiz: Alternative HypothesisHypothesesQuiz: Which-Tailed Test?Quiz: Degrees of FreedomQuiz: t-Criticalcan be found directly via t-table Quiz: SEMstandard error of meansample standard deviation / sqrt(n) Quiz: Mean Differencesample mean and population mean Quiz: t-StatisticQuiz: Critical RegionQuiz: P-ValueQuiz: Statistically SignificantQuiz: Meaningful ResultsQuiz: Cohen&#39;s dQuiz: r^2Quiz: Margin of ErrorQuiz: Compute CIProblem Set 10: t-TestsQuiz: 1. Normal vs. t-DistributionQuiz: 2. Z vs. tQuiz: 3. Vocabularies - Type of StudyQuiz: 4. Vocabularies - Independent VariableQuiz: 5. Vocabularies - Dependent VariableQuiz: 6. Vocabularies - Null HypothesisQuiz: 7. Vocabularies - Alternative HypothesisQuiz: 8. Vocabularies - One- or Two-Tailed?Quiz: 9. Vocabularies - t-Critical ValueQuiz: 10. Vocabularies - Mean and SDQuiz: 11. Vocabularies - t-StatisticQuiz: 12. Cell Phone Law: Type of StudyQuiz: 13. Cell Phone Law: Dependent VariableQuiz: 14. Cell Phone Law: Independent VariableQuiz: 15. Cell Phone Law: Null HypothesisQuiz: 16. Cell Phone Law: Alt. HypothesisQuiz: 17. Cell Phone Law: t-TestQuiz: 18. Cell Phone Law: Difference ScoresQuiz: 19. Cell Phone Law: t-Critical ValueQuiz: 20. Cell Phone Law: Standard ErrorQuiz: 21. Cell Phone Law: t-StatisticQuiz: 22. Cell Phone Law: DecisionQuiz: 23. Cell Phone Law: Cohen&#39;s dQuiz: 24. Cell Phone Law: CIMake sure you use the t-critical value for a two-tailed test, even if you are doing a one-tailed test. This is because you center the confidence interval around your point estimate, rather than letting one bound go to positive or negative infinity depending on the direction of your test. t-Tests continuedIndependent Samplesindependent samples experimental observational Standard Errorstandard error =$$\frac{s}{\sqrt{n}} = \frac{\sqrt{s^2_1 + s^2_2}}{\sqrt{n}} = \sqrt{\frac{s^2_1 + s^2_2}{n}} = \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}}$$$$df = n_1 + n_2 -2$$t-statistic = $\frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{SE}$ Quiz: Meal PricesClick here to access the data: Meal prices in Gettysburg and Wilma If you would like to perform any calculations, copy and paste the data into your own spreadsheet. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Average Meal PriceQuiz: SD for Meal PriceQuiz: Meal Price SEMQuiz: Meal Price t-StatisticQuiz: Calculate t-StatisticQuiz: t-Critical ValuesQuiz: Gettysburg or Wilma?Acne MedicationQuiz: Acne Medication t-StatisticQuiz: Acne Medication - t-Critical ValuesQuiz: Acne Medication - DecisionWho Has More Shoes?Quiz: Mean Number of ShoesClick here to access the data: Pairs of shoes owned by males and females Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Shoes - Standard ErrorQuiz: Shoes - t-StatisticQuiz: Shoes - DecisionQuiz: Shoes - 95% CIQuiz: Shoes - Calculate CIQuiz: Gender and ShoesQuiz: Pooled Variance Sum of SquaresAssumes samples are approximately the same size$$SS^2_p = \frac{SS_1 + SS_2}{df_1 +df_2}$$SS: sum of squares$SS^2_p$: pooled variance Quiz: Calculate Pooled VarianceQuiz: Corrected Standard Errorstandard error :$$\sqrt{\frac{S^2_1}{n_1} + \frac{S^2_2}{n_2}}$$corrected Standard Error:$$\sqrt{\frac{S^2_p}{n_1} + \frac{S^2_p}{n_2}}$$ Quiz: t-StatisticQuiz: t-Critical and DecisionAssumptionsstandard error:$$\sqrt{\frac{S^2_1}{n_1} + \frac{S^2_2}{n_2}}$$corrected Standard Error:$$\sqrt{\frac{S^2_p}{n_1} + \frac{S^2_p}{n_2}}$$T-test assumption X and Y should be random samples from two different independent populations Populations are approximately normal Sample data can extimate population variances Population variances are roughly equal Problem Set 11: t-Tests continuedQuiz: 1. t-TestQuiz: 2. t-TestQuiz: 3. t-TestQuiz: 4. t-Critical ValueQuiz: 5. P-ValueQuiz: 6. P-ValueQuiz: 7. t-StatisticQuiz: 8. True or False?Quiz: 9. Sum of SquaresQuiz: 10. Pooled VarianceQuiz: 11. Standard ErrorQuiz: 12. t-StatisticQuiz: 13. DecisionQuiz: 14. Independent VariableQuiz: 15. Dependent VariableQuiz: 16. Null HypothesisQuiz: 17. Degrees of FreedomQuiz: 18. t-Critical ValuesQuiz: 19. t-StatisticQuiz: 20. DecisionQuiz: 21. Cohen&#39;s dQuiz: 22. Percent of VariabilityOne-way ANOVAQuiz: IntuitionQuiz: Number of t-TestsQuiz: Extended t-Test Numeratorgrand mean Quiz: Grand Meangrand mean sometimes equal to mean of sample means Quiz: Between-Group VariabilityQuiz: Significantly Different MeansQuiz: Sample Variability and Significancewithin-group variability: the variability of indivisual samples ANOVAANOVA: analysis of varianceone-way ANOVA: one independent vaiable Quiz: HypothesesQuiz: Within-Group VariabilityWithin-Group Variability greater, test statistics smalller Quiz: Between-Group VariabilityBetween-Group Variability greater, test statistics greater Quiz: F-RatioF-Ratio = Between-Group Variability / Within-Group Variability Quiz: Visualize Statistical OutcomeQuiz: Formalize Within-Group VariabilityFormula for F-Ratio$$F = \frac{Between-Group Variability}{Within-Group Variability} = \frac{n\sum(\bar{x}_k - \bar{x}_G)^2/(k-1)}{\sum(\bar{x}_i - \bar{x}_k)^2/(N-k)} = \frac{SS_{between}/df_{between}}{SS_{within}/df_{within}} = \frac{MS_{between}}{MS_{within}}$$ Quiz: Degrees of Freedom$$df_{total} = N-1$$$$df_{between} + df_{within} = (k-1) + (N-k) =N-1$$ Total Variation$$SS_{between} + SS_{within} = SS_{total} = \sum(\bar{x}_i - \bar{x}_G)^2$$ Quiz: F-DistributionF-Distribution ShapeQuiz: Table for F-CriticalClick here if you’d like to check out the F-table! Quiz: Sample Means and Grand MeanQuiz: SS BetweenQuiz: SS WithinQuiz: Degrees of FreedomQuiz: Mean SquaresQuiz: F-StatisticQuiz: F-CriticalQuiz: DecisionProblem Set 12: One-way ANOVAQuiz: 1. Within-Group VariabilityQuiz: 2. Between-Group Variability and F-RatioQuiz: 3. Source of VariationQuiz: 4. Between-Group VariabilityQuiz: 5. True or False?Quiz: 6. Large F-StatisticQuiz: 7. Degrees of FreedomQuiz: 8. DecisionQuiz: 9. Degrees of FreedomQuiz: 10. DecisionQuiz: 11. Null HypothesisQuiz: 12. Alternative HypothesisQuiz: 13. SS BetweenQuiz: 14. SS WithinQuiz: 15. Degrees of FreedomQuiz: 16. MSQuiz: 17. F-StatisticQuiz: 18. F-Critical Value3.8853 Quiz: 19. DecisionQuiz: 20. DF Total26 Quiz: 21. Null HypothesisQuiz: 22. N28=3+24+1 Quiz: 23. DecisionQuiz: 24. SS Between150=200-50 Quiz: 25. True or False?True ANOVA continuedQuiz: Cows and foodQuiz: Grand meanQuiz: Group meansQuiz: SS betweenQuiz: SS withinQuiz: Degrees of FreedomQuiz: Mean squaresQuiz: F statisticQuiz: F critical and decisionQuiz: Deviation from grand meanQuiz: SS totalQuiz: ConclusionMultiple Comparison Testscompare all of the means with each other Quiz: Tukeys HSDTukey’s Honostly Significant Difference(HSD) =$$q^{\star} \sqrt{\frac{MS_{within}}{n}} = q^{\star} \frac{S_p}{\sqrt{n}}$$q: Studentized Range Statisticn: sample sizeLink to Studentized Range Statistic (q) Table for alpha = 0.05df is the degrees of freedom for within-group variability; k is the number of groups/samples in your study Quiz: Which differences are significantif difference between sample means greater than Tukey’s HSD, we can conclude that two sample is Honostly Significant Difference Quiz: Cohens d for multiple comparisons$$d = \frac{\bar{x_1} - \bar{x_2}}{S_p} = \frac{\bar{x_1} - \bar{x_2}}{\sqrt{MS_{within}}}$$ Quiz: Eta squaredpropotion of total variation that is due to between-group differences(explained variation)$$\eta^2 = \frac{SS_{between}}{SS_{total}}$$ Quiz: Calculate eta squaredQuiz: Range of eta squaredQuiz: Software outputQuiz: Missing mean differencesQuiz: Different sample sizesQuiz: Grand meanQuiz: SS between$$SS_{between} = \sum n_k (\bar{x_k} - \bar{x_G})^2$$sample mean minus grand mean Quiz: SS withinQuiz: Degrees of freedomQuiz: MS and FQuiz: Proportion due to drug type$$\eta^2 = \frac{SS_{between}}{SS_{total}}$$$\eta^2$ propotion of the difference between tumor reduction can be explained by the different treatments Quiz: Powertype II error: failing to reject the null when we should have larger samples result in higher power lower within-group variability leads to higher power choosing treatments with strong effect sizes will increase power ANOVA Assumptions and Wrap Up$$F = \frac{Between-Group Variability}{Within-Group Variability} = \frac{n\sum(\bar{x}_k - \bar{x}_G)^2/(k-1)}{\sum(\bar{x}_i - \bar{x}_k)^2/(N-k)} = \frac{SS_{between}/df_{between}}{SS_{within}/df_{within}} = \frac{MS_{between}}{MS_{within}}$$$$q^{\star} \sqrt{\frac{MS_{within}}{n}} = q^{\star} \frac{S_p}{\sqrt{n}}$$$$\eta^2 = \frac{SS_{between}}{SS_{total}}$$ Problem Set 13: ANOVA continuedQuiz: 1. Decision3.3158retain Quiz: 2. Decision3.2874reject Quiz: 3. SS WithinSStotal - SSbetween Quiz: 4. Symbolize SS WithinSS{within} is also called SS{error} because this is the variation that can’t be explained by the independent variable. Quiz: 5. Degrees of Freedomk-1N-k Quiz: 6. HypothesesQuiz: 7. Post-hoc TestsWe conduct post-hoc tests in order to determine which pair(s) of groups were significantly different, since ANOVA only tells us that at least one pair is significantly different. Quiz: 8. Tukey&#39;s HSDsmaller than HSD, retain Quiz: 9. Tukey&#39;s HSDQuiz: 10. η^2Quiz: 11. η^2 MeaningQuiz: 12. Decision0.05 F(1,30)-critical = 4.1709 Quiz: 13. DecisionQuiz: 14. SS BetweenQuiz: 15. Normality AssumptionQuiz: 16. Homogeneity of VarianceQuiz: 17. Grand MeanQuiz: 18. SS BetweenQuiz: 19. Degrees of FreedomQuiz: 20. MS BetweenQuiz: 21. DecisionQuiz: 22. ConclusionQuiz: 23. η^2Quiz: 24. η^2 MeaningQuiz: 25. Explained ProportionQuiz: 26. Tukey&#39;s HSDCorrelationRelationshipsQuiz: The Variables x and yQuiz: Show RelationshipQuiz: ScatterplotQuiz: Stronger RelationshipQuiz: As x IncreasesQuiz: Strength and DirectionCorrelation CoefficientCorrelation Coefficient r: Pearson’s r$$r = \frac{cov(x,y)}{S_x \cdot S_y} = \frac{cov_{x,y}}{S_x \cdot S_y}$$$r^2$ = % of the variation in Y explained by the variation in X.$r^2$: coefficient of determination Quiz: Match with rQuiz: Age in Months and YearsQuiz: Hours Asleep vs. AwakeQuiz: Create ScatterplotData from poll Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Quiz: Calculate rGoogle Spreadsheets function to calculate r:=PEARSON(start cell for variable x:end cell for variable x, start cell for variable y:end cell for variable y) Quiz: StrongerQuiz: Hypothesis Testing for ρ$\rho$: true correlation for population.correlation between variable and population Quiz: Testing for SignificanceQuiz: CI for ρQuiz: Find pGraphPad QuickCalcs Page Quiz: Add OutlierAdd the outlier (20, 8) to the data for age and number of pets (i.e., age = 20, number of pets = 8). Calculate the new correlation coefficient. Correlation vs. CausationHere is a link to the original comic Fallaciesambigous temporal precedencethird variable problempost hoc fallacy Problem Set 14: CorrelationQuiz: 1. Positive DirectionsQuiz: 2. Order Correlation CoefficientsQuiz: 3. Interpret ResultsQuiz: 4. Unlikely Value of ρQuiz: 5. ρ CIQuiz: 6. Decision Based on t-StatisticQuiz: 7. Writing Down Dreams0.95Dream Data Quiz: 8. RelationshipsQuiz: 9. No CorrelationQuiz: 10. Direction of RelationshipQuiz: 11. Approximate CorrelationQuiz: 12. Approximate CorrelationQuiz: 13. Increase Sample SizeWhen we were doing t-tests, as the sample size increased, we could reach significance with a smaller difference between the means. That’s because increasing the sample size makes it harder to get a large difference by chance. What’s the analogous idea here? Quiz: 14. Direction of RelationshipQuiz: 15. Estimate r0.58 Quiz: 16. Compute rQuiz: 17. Coefficient of DeterminationQuiz: 18. Calculate pQuiz: 19. DecisionQuiz: 20. ConclusionQuiz: 21. Decision Based on pRegressionIntro to Linear RegressionAirplane FlightsQuiz: Symbolize Regression EquationTry not to get confused with the typical way that the equation for the regression line is often presented: y = mx + b. There are a variety of standard ways for symbolizing the slope and y-intercept, and in this class we’ll use a to represent the y-intercept and b to represent the slope. In general, try not to memorize equations or formulas, because this will hinder your ability to understand the same formulas when different symbols are used to symbolize the same coefficients. Instead, it’s far better to understand the format of an equation or formula so you can recognize what each symbol means. Quiz: Guess Best Fit LineMinimize Sum of Squares$$b = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = r(\frac{S_y}{S_x})$$r: Pearson’s rWe have decided to symbolize the regression line by y = bx + a, where b represents the slope and a represents the y-intercept.Since b = r(standard deviation of y-values)/(standard deviation of x-values), we can also symbolize the regression line like this:y = r(standard deviation of y-values)/(standard deviation of x-values)x + a Quiz: Calculate rQuiz: Calculate Standard DeviationsQuiz: Calculate SlopeQuiz: Find y-InterceptQuiz: What Point Does the Line Go Through?Quiz: Calculate MeansQuiz: Calculate y-InterceptQuiz: Travel 4000 MilesQuiz: Additional Cost per MileQuiz: Cost to Travel 0 MilesQuiz: Travel on a BudgetQuiz: Which Has More Error?Standard Error of EstimateStandard Error Of Estimate =$$\frac{\sum(y - \hat{y})^2}{N - 2}$$N: number of points Confidence IntervalsQuiz: Hypothesis Testing for SlopeQuiz: t-Test for Slopedf = N -2N: number of x points R OutputFactors Affecting Linear RegressionSummary of Linear RegressionIntro to Multiple RegressionR: multiple correlation coefficient$R^2$: propotion of variability in Y explained by out set of predictors Quiz: Alcohol, Religiosity, &amp; Self-EsteemQuiz: Make PredictionsQuiz: RelationshipQuiz: CausationAppletsProblem Set 15: RegressionQuiz: 1. Stem Cells &amp; Vision - SlopeQuiz: 2. Stem Cells &amp; Vision - r^2Quiz: 3. Stem Cells &amp; Vision - y-InterceptQuiz: 4. Stem Cells &amp; Vision - Regression EqnQuiz: 5. Stem Cells &amp; Vision - # of Stem CellsQuiz: 6. Stem Cells &amp; Vision - PhotoreceptorsQuiz: 7. Estimate SlopeQuiz: 8. Indicate SlopeQuiz: 9. CI for SlopeQuiz: 10. SymbolsQuiz: 11. Make PredictionsQuiz: 12. Standard ErrorQuiz: 13. r^2Quiz: 14. Definition of SlopeQuiz: 15. Definition of y-InterceptQuiz: 16. Definition of y-HatQuiz: 17. Definition of Std Error of EstimateQuiz: 18. Describe RelationshipQuiz: 19. Describe RelationshipQuiz: 20. Describe RelationshipChi-Squared testsQuiz: Scales of MeasurementRatio dataOrdinal dataInterval data If you would like some additional discussion on the scales of measurement, then take a look at this very nice little tutorial. As you watch the tutorial, think about what properties must a measurement have for us to consider it a nominal scale or an ordinal scale, etc. Quiz: Choose Type of DataNon-Parametric TestsQuiz: Mount ShastaQuiz: Expected FrequenciesQuiz: Observed FrequencyQuiz: Hypotheses PercentQuiz: Hypotheses FrequencyQuiz: Expected Frequenciesχ^2 Goodness-of-Fit TestQuiz: χ^2 Statistic successful unsuccessful expected frequency 33 67 observed frequency 41 59 $$\chi^2 = \sum \frac{(f_o - f_e)^2}{f_e}$$$f_o$: observed frequency$f_e$: expected frequency Quiz: Observed Equals ExpectedQuiz: χ^2 ValuesQuiz: Degrees of FreedomQuiz: Which Has More df?the more categories we have, the more degree of freedoms we have and the larger $\chi^2$ statistics will be. We need higher critical value to reject the null hpyothesis. Quiz: Calculate χ^2 StatisticQuiz: Find dfdf = 1 = Number of categories - 1 Quiz: Calculate pP value equals 0.0891Not significant at either level GraphPad QuickCalcs The instructor should have said, “Calculate the one-tailed p-value.” GraphPad’s Chi-Square calculator reports the p-value as two-sided, but should be taken as the one-sided p-value we desire. χ^2 Test for IndependenceQuiz: Remember DetailsElizabeth Loftus: Eyewitness Testimony Source: Loftus, E.F. &amp; Palmer, J.C. (1974). Reconstruction of automobile destruction: An example of the interaction between language and memory. Journal of Verbal Learning and Verbal Behavior. 13, 585-589. Quiz: Broken Glass Hit Smashed Control Total Yes 7 16 6 29 No 43 34 44 121 Total 50 50 50 150 Null Hypothesis: Student’s response for whether or not they saw broken glass is independent of the wording used in questionExpected frequencies:For (Yes, Hit) is (29/150) * 50For (No, Hit) is (121/150) * 50 Quiz: Expected FrequenciesQuiz: Calculate χ^2 StatisticQuiz: Degrees of Freedomdf = 2 = (number of rows - 1)(number of columns - 1) Quiz: DecisionGraphPad QuickCalcsChi-Square Table Quiz: Effect SizeChi-square Test for Independence$$Cramer’s V (\phi_c) = \sqrt{\frac{\chi^2}{n(k - 1)}}$$k: smaller of number of rows or columns (2)n: total number (150) Quiz: Calculate Cramér&#39;s VAssumptions and RestrictionsAvoid dependent observationsAvoid small expected frequencies (large n) SummaryCongratsProblem Set 16: Chi-Squared testsQuiz: 1. Dependent VariableRanking Quiz: 2. Scale of MeasurementOrdinal Quiz: 3. Dependent Variablehelp Quiz: 4. Scale of MeasurementNominal Quiz: 5. Dependent Variabletime Quiz: 6. Scale of Measurementratio Quiz: 7. Dependent Variableerror Quiz: 8. Scale of Measurementratio Quiz: 9. Specify Level of MeasurementQuiz: 10. Select χ^2 TestQuiz: 11. Null Hypothesisnot Quiz: 12. Which χ^2 Test? outcome frequencies expected frequencies 1 8 4 2 4 4 3 1 4 4 8 4 5 3 4 6 0 4 good nominal Quiz: 13. Degrees of Freedom5 Quiz: 14. χ^2 Critical ValueQuiz: 15. Calculate χ^2 StatisticQuiz: 16. DecisionQuiz: 17. ConclusionQuiz: 18. Null Hypothesis Cabin Steerage Total Yes 299 186 485 No 280 526 806 Total 579 712 1291 independent Quiz: 19. Which χ^2 Test?Indenpence nominal Quiz: 20. Degrees of Freedom1 Quiz: 21. χ^2 Critical ValueQuiz: 22. Calculate χ^2Quiz: 23. DecisionQuiz: 24. ConclusionLessons 10-16 Review/AssessmentQuiz: 1Quiz: 2Quiz: 3Quiz: 4Quiz: 5Quiz: 6Quiz: 70.3/15 Quiz: 8120*0.02+5.24 Quiz: 9Quiz: 10Quiz: 11Quiz: 12Quiz: 13Quiz: 14Quiz: 15Quiz: 16Quiz: 17Quiz: 18Quiz: 19Quiz: 20Quiz: 21Quiz: 22Quiz: 23Quiz: 24Quiz: 25Quiz: 26Quiz: 27Quiz: 28Quiz: 29Quiz: 30Final ProjectFinal project details can be found [here](https://www.udacity.com/wiki/ud201/final_project. Here are helpful tips and suggestions as you work on your final project!]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity Courses]]></title>
    <url>%2F2017%2F07%2F19%2FUdacity%20Courses%2F</url>
    <content type="text"><![CDATA[Almost done… Courses are not Learned Course Classroom Code Complete Data Analyst Data Analysis and Visualization classroom ud404 0 Deploying a Hadoop Cluster classroom ud1000 0 A/B Testing classroom ud257 0 Data Visualization and D3.js classroom ud507 0 Data Analysis with R classroom ud651 0 Data Wrangling with MongoDB classroom ud032 0 Real-Time Analytics with Apache Storm classroom ud381 0 Database Systems Concepts &amp; Design classroom ud150 0 Business Analyst A/B Testing for Business Analysts classroom ud979 0 Time Series Forecasting classroom ud980 0 Segmentation and Clustering classroom ud981 0 Classification Models classroom ud978 0 Problem Solving with Advanced Analytics classroom ud976 0 Creating an Analytical Dataset classroom ud977 0 Digital Marketing Intro to Data Science classroom ud359 0 How to Build a Startup classroom ep245 0 Health Informatics in the Cloud classroom ud809 0 Algorithm Computability, Complexity &amp; Algorithms classroom ud061 0 High Performance Computing classroom ud281 0 Machine Learning Model Building and Validation classroom ud919 0 Deploying a Hadoop Cluster classroom ud1000 0 Intro to Hadoop and MapReduce classroom ud617 0 Intro to Artificial Intelligence classroom cs271 0 Big Data Analytics in Healthcare classroom ud758 0 Human-Computer Interaction classroom ud400 0 Self Driving Car Artificial Intelligence for Robotics classroom cs373 0 Artificial Intelligence Knowledge-Based AI: Cognitive Systems classroom ud409 0 Artificial Intelligence classroom ud954 0 Computer Science Computer Networking classroom ud436 0 Intro to Algorithms classroom cs215 0 Technical Interview classroom ud513 0 Intro to Information Security classroom ud459 0 Compilers: Theory and Practice classroom ud168 0 Python Design of Computer Programs classroom cs212 0 Git and GitHub Version Control with Git classroom ud123 0 GitHub &amp; Collaboration classroom ud456 0 React Client-Server Communication classroom ud897 0 Front End Nano Degree Localization Essentials classroom ud610 0 Full Stack Developing Scalable Apps in Python classroom ud858 0 Linux Command Line Basics classroom ud595 0 Configuring Linux Web Servers classroom ud299 0 Dynamic Web Applications with Sinatra classroom ud268 0 Intro to Relational Databases classroom ud197 0 Shell Workshop classroom ud206 0 Network Security classroom ud199 0 Deploying Applications with Heroku classroom ud272 0 Android Java Programming Basics classroom cs046 0 classroom ud 0 classroom ud 0 classroom ud 0 Have learned Course Classroom Code Complete(.0-1.) Statistics Intro to Statistics classroom st101 1 Statistics st095 st095 1 Intro to Descriptive Statistics classroom ud827 1 Intro to Inferential Statistics classroom ud201 0.9 Data Analyst Intro to Data Analysis classroom ud170 1 Data Visualization in Tableau classroom ud1006 1 Machine Learning Intro to Machine Learning classroom ud120 1 Machine Learning classroom ud262 1 Reinforcement Learning classroom ud600 1 Deep Learning classroom ud730 1 Front End Writing READMEs classroom ud777 1 Full Stack Web Development classroom ud253 0.5 Programming Foundations with Python classroom ud036 1 Programming Languages classroom cs262 1 Networking for Web Developers classroom ud256 1 Git and GitHub How to Use Git and GitHub classroom ud775 1 Python Introduction to Python classroom ud1110 0 Intro to Computer Science classroom cs101 1 | classroom | ud | 1 | classroom | ud | 1 | classroom | ud | 1]]></content>
      <tags>
        <tag>Course</tag>
        <tag>Udacity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera S Algorithms Specialization Notebook]]></title>
    <url>%2F2017%2F07%2F17%2FCoursera%20S%20Algorithms%20Specialization%20Notebook%2F</url>
    <content type="text"><![CDATA[Writing this blog for quick searching, resume and interview.Shortest Paths Revisited, NP-Complete Problems and What To Do About Them.stay stuned…Courses can be found hereSummary can be found here Algorithms are the heart of computer science, and the subject has countless practical applications as well as intellectual depth. This specialization is an introduction to algorithms for learners with at least a little programming experience. The specialization is rigorous but emphasizes the big picture and conceptual understanding over low-level implementation and mathematical details. After completing this specialization, you will be well-positioned to ace your technical interviews and speak fluently about algorithms with other programmers and computer scientists. About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004. He has taught and published extensively on the subject of algorithms and their applications. Divide and Conquer, Sorting and Searching, and Randomized AlgorithmsCourse can be found hereLecture slides can be found hereSummary can be found in my Github About this course: The primary topics in this part of the specialization are: asymptotic (“Big-oh”) notation, sorting and searching, divide and conquer (master method, integer and matrix multiplication, closest pair), and randomized algorithms (QuickSort, contraction algorithm for min cuts). Who is this class for: Learners with at least a little bit of programming experience who want to learn the essentials of algorithms. In a University computer science curriculum, this course is typically taken in the third year. Week 1Introduction;“big-oh” notation and asymptotic analysis. I. INTRODUCTIONWelcome and Week 1 Overview 10 minWELCOME: Welcome to the Algorithms Specialization! Here’s an overview of the first week of material. INTRODUCTION: The first set of lectures for this week is meant to give you the flavor of the course, and hopefully get you excited about it. We begin by discussing algorithms in general and why they’re so important, and then use the problem of multiplying two integers to illustrate how algorithmic ingenuity can often improve over more straightforward or naive solutions. We discuss the Merge Sort algorithm in detail, for several reasons: it’s a practical and famous algorithm that you should all know; it’s a good warm-up to get you ready for more intricate algorithms; and it’s the canonical introduction to the “divide and conquer” algorithm design paradigm. These lectures conclude by describing several guiding principles for how we’ll analyze algorithms in this course. ASYMPTOTIC ANALYSIS: The second set of lectures for this week is an introduction to big-oh notation and its relatives, which belongs in the vocabulary of every serious programmer and computer scientist. The goal is to identify a “sweet spot” of granularity for reasoning about algorithms — we want to suppress second-order details like constant factors and lower-order terms, and focus on how the running time of an algorithm scales as the input size grows large. PREREQUISITES: This course is not an introduction to programming, and it assumes that you have basic programming skills in a language such as Python, Java, or C. There are several outstanding free online courses that teach basic programming. We also use mathematical analysis as needed to understand how and why algorithms and data structures really work. If you need a refresher on the basics of proofs (induction, contradiction, etc.), I recommend the lecture notes “Mathematics for Computer Science” by Lehman and Leighton (see separate Resources pages). DISCUSSION FORUMS: The discussion forums play a crucial role in massive online courses like this one. If you have trouble understanding a lecture or completing an assignment, you should turn to the forums for help. After you’ve mastered the lectures and assignments for a given week, I hope you’ll contribute to the forums and help out your fellow students. While I won’t have time to carefully monitor the discussion forums, I’ll check in and answer questions whenever I find the time. VIDEOS AND SLIDES: Videos can be streamed or downloaded and watched offline (recommended for commutes, etc.). We are also providing PDF lecture slides (typed versions of what’s written in the lecture videos), as well as subtitle files (in English and in some cases other languages as well). And if you find yourself wishing that I spoke more quickly or more slowly, note that you can adjust the video speed to accommodate your preferred pace. HOMEWORK #1: The first problem set consists of 5 multiple choice problems, mostly about Merge Sort and asymptotic notation. The first programming assignment asks you to implement one or more of the integer multiplication algorithms covered in lecture. SUGGESTED READINGS FOR WEEK 1: Abbreviations in suggested readings refer to the following textbooks: CLRS - Cormen, Leiserson, Rivest, and Stein, Introduction to Algorithms (3rd edition) DPV - Dasgupta, Papadimitriou, and Vazirani, Algorithms KT - Kleinberg and Tardos, Algorithm Design SW - Sedgewick and Wayne, Algorithms (4th edition) CLRS: Chapters 2 and 3 DPV: Sections 0.3, 2.1, 2.3 KT: Sections 2.1, 2.2, 2.4, and 5.1 SW: Sections 1.4 and 2.2 Overview, Resources, and Policies 10 minCourse overviewAlgorithms are the heart of computer science, and the subject has countless practical applications as well as intellectual depth. This specialization is an introduction to algorithms for learners with at least a little programming experience. The specialization is rigorous but emphasizes the big picture and conceptual understanding over low-level implementation and mathematical details. After completing this specialization, you will have a greater mastery of algorithms than almost anyone without a graduate degree in the subject. Specific topics in Part 1 of the specialization include: “Big-oh” notation, sorting and searching, divide and conquer (master method, integer and matrix multiplication, closest pair), and randomized algorithms (QuickSort, contraction algorithm for min cuts). Resources Mathematics for Computer Science (by Eric Lehman and Tom Leighton):book Course PoliciesVideo Lectures Lectures will be made available weekly, for a total of four weeks. In a given week, there will be roughly three hours of material. Some weeks include extra optional material (some review, some advanced topics). Below each lecture video there is a PDF of typed versions of the slides. You can download the videos of the lectures for offline viewing. The video player supports speeding up and slowing down the pace. Weekly Programming Assignments and Problem Sets Every week there will be a new problem set and a new programming assignment. For each problem set you are allowed a maximum of two attempts in a 12-hour period (we’ll use the best score). For each programming assignment you’re allowed a maximum of 10 attempts in a 12-hour period (we’ll use the best score). For the final exam, you’re allowed one attempt per 24 hours. Grading To pass a problem set, you must get at least 4 of the 5 questions correct (80%). To pass a programming assignment, you must get all of the answers correct (100%). To pass the final exam, you must get at least 70% of the total points (7 out of 10). To pass the course, you must pass all of the problem sets, all of the programming assignments, and the final exam. Theory Problems These are totally optional theory questions (no deadlines or credit given). We encourage you to attempt these questions and discuss them in the forums to develop a deeper understanding of the design and analysis of algorithms. Lecture slides 10 minSee the zip file below (which includes slides for both Part 1 and Part 2 of the specialization). Also available from the video lectures on a lecture-by-lecture basis. algo1slides.zip Why Study Algorithms?4 minInteger Multiplication 8 minKaratsuba Multiplication 12 minAbout the Course17 minMerge Sort: Motivation and Example8 minMerge Sort: Pseudocode12 minMerge Sort: Analysis 9 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/wW9On/merge-sort-analysis Guiding Principles for Analysis of Algorithms 15 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/q5fV4/guiding-principles-for-analysis-of-algorithms II. ASYMPTOTIC ANALYSISThe Gist 14 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/o2NpH/the-gist Big-Oh Notation 4 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/KGtUp/big-oh-notation Basic Examples 7 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/mb8bV/basic-examples Big Omega and Theta7 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/SxSch/big-omega-and-theta Additional Examples [Review - Optional]7 minProblem Set #1Quiz: Problem Set #1 5 questions Programming Assignment #1Quiz: Programming Assignment #1 1 questionIn this programming assignment you will implement one or more of the integer multiplication algorithms described in lecture. To get the most out of this assignment, your program should restrict itself to multiplying only pairs of single-digit numbers. You can implement the grade-school algorithm if you want, but to get the most out of the assignment you’ll want to implement recursive integer multiplication and/or Karatsuba’s algorithm. So: what’s the product of the following two 64-digit numbers? 3141592653589793238462643383279502884197169399375105820974944592 2718281828459045235360287471352662497757247093699959574966967627 [TIP: before submitting, first test the correctness of your program on some small test cases of your own devising. Then post your best test cases to the discussion forums to help your fellow students!] [Food for thought: the number of digits in each input number is a power of 2. Does this make your life easier? Does it depend on which algorithm you’re implementing?] The numeric answer should be typed in the space below. So if your answer is 1198233847, then just type 1198233847 in the space provided without any space / commas / any other punctuation marks. (We do not require you to submit your code, so feel free to use any programming language you want — just type the final numeric answer in the following space.) Week 2Divide-and-conquer basics;the master method for analyzing divide and conquer algorithms. III. DIVIDE &amp; CONQUER ALGORITHMSWeek 2 Overview 10 minDIVIDE AND CONQUER ALGORITHMS: The next set of lectures discusses three non-trivial examples of the divide and conquer algorithm design paradigm. The first is for counting the number of inversions in an array. This problem is related to measuring similarity between two ranked lists, which in turn is relevant for making good recommendations to someone based on your knowledge of their and others’ preferences (“collaborative filtering”). The second algorithm is Strassen’s mind-blowing recursive algorithm for matrix multiplication, which improves over the obvious iterative method. The third algorithm, which is more advanced and is optional material, is for computing the closest pair of points in the plane. THE MASTER METHOD: These lectures cover a “black-box” method for solving recurrences. You can then immediately determine the running time of most of the divide-and-conquer algorithms that you’ll ever see! (Including Karatsuba’s integer multiplication algorithm from Week 1.) The proof is a nice generalization of the recursion tree method that we used to analyze MergeSort. Ever wonder about the mysterious three cases of the Master Method? Watch these videos and hopefully all will become clear. HOMEWORK: Problem Set #2 has five questions that should give you practice with divide-and-conquer algorithms and the Master Method. Programming assignment #2 asks you to implement the counting inversions algorithm (from Part III) in whatever programming language you please, run it on a quite large input, and enter the answer. SUGGESTED READINGS FOR WEEK 2: CLRS Chapter 4 (except Section 4.3), and Sections 28.1 and 33.4 DPV Sections 2.2 and 2.5 KT Sections 5.2-5.5 O(n log n) Algorithm for Counting Inversions I12 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/GFmmJ/o-n-log-n-algorithm-for-counting-inversions-i O(n log n) Algorithm for Counting Inversions II1 6 minhttps://www.coursera.org/learn/algorithms-divide-conquer/lecture/IUiUk/o-n-log-n-algorithm-for-counting-inversions-ii Strassen’s Subcubic Matrix Multiplication Algorithm22 minO(n log n) Algorithm for Closest Pair I [Advanced - Optional]31 minO(n log n) Algorithm for Closest Pair II [Advanced - Optional]18 minIV. THE MASTER METHODMotivation7 minFormal Statement10 minExamples13 minProof I9 minInterpretation of the 3 Cases10 minProof II16 minProblem Set #2Quiz: Problem Set #2 5 questions Optional Theory Problems (Batch #1) 10 minThe following problems are for those of you looking to challenge yourself beyond the required problem sets and programming questions. Most of these have been given in Stanford’s CS161 course, Design and Analysis of Algorithms, at some point. They are completely optional and will not be graded. While they vary in level, many are pretty challenging, and we strongly encourage you to discuss ideas and approaches with your fellow students on the “Theory Problems” discussion form. You are given as input an unsorted array of n distinct numbers, where n is a power of 2. Give an algorithm that identifies the second-largest number in the array, and that uses at most n+log2n−2 comparisons.You are a given a unimodal array of n distinct elements, meaning that its entries are in increasing order up until its maximum element, after which its elements are in decreasing order. Give an algorithm to compute the maximum element that runs in O(log n) time.You are given a sorted (from smallest to largest) array A of n distinct integers which can be positive, negative, or zero. You want to decide whether or not there is an index i such that A[i] = i. Design the fastest algorithm that you can for solving this problem.You are given an n by n grid of distinct numbers. A number is a local minimum if it is smaller than all of its neighbors. (A neighbor of a number is one immediately above, below, to the left, or the right. Most numbers have four neighbors; numbers on the side have three; the four corners have two.) Use the divide-and-conquer algorithm design paradigm to compute a local minimum with only O(n) comparisons between pairs of numbers. (Note: since there are n2 numbers in the input, you cannot afford to look at all of them. Hint: Think about what types of recurrences would give you the desired upper bound.) Programming Assignment #2Quiz: Programming Assignment #2 1 questionDownload the following text file: IntegerArray.txtThis file contains all of the 100,000 integers between 1 and 100,000 (inclusive) in some order, with no integer repeated. Your task is to compute the number of inversions in the file given, where the ith row of the file indicates the ith entry of an array. Because of the large size of this array, you should implement the fast divide-and-conquer algorithm covered in the video lectures. The numeric answer for the given input file should be typed in the space below. So if your answer is 1198233847, then just type 1198233847 in the space provided without any space / commas / any other punctuation marks. You can make up to 5 attempts, and we’ll use the best one for grading. (We do not require you to submit your code, so feel free to use any programming language you want — just type the final numeric answer in the following space.) [TIP: before submitting, first test the correctness of your program on some small test files or your own devising. Then post your best test cases to the discussion forums to help your fellow students!] Week 3The QuickSort algorithm and its analysis;probability review. V. QUICKSORT - ALGORITHMWeek 3 Overview10 minQUICKSORT - THE ALGORITHM (Part V). One of the greatest algorithms ever, and our first example of a randomized algorithm. These lectures go over the pseudocode — the high-level approach, how to partition an array around a pivot element in linear time with minimal extra storage, and the ramifications of different pivot choices — and explain how the algorithm works. QUICKSORT - THE ANALYSIS (Part VI). These lectures prove that randomized QuickSort (i.e., with random pivot choices) runs in O(n log n) time on average. The analysis is as elegant as the algorithm itself, and is based on a “decomposition principle” that is often useful in the analysis of randomized algorithms. Note that there are some accompanying lecture notes for this part (available for download underneath each video). Also, it may be helpful to watch the first probability review video (below) before watching this sequence. PROBABILITY REVIEW (Part VII). This first of these videos reviews the concepts from discrete probability that are necessary for the QuickSort analysis — sample spaces, events, random variables, expectation, and linearity of expectation. The second video covers just two topics, although quite tricky ones! (Namely, conditional probability and independence.) You need to review these two topics (via this video or some other source, as you wish) before studying the analysis of the randomized contraction algorithm in Week 4. HOMEWORK: This week’s quiz will help you understand QuickSort and probability more deeply. Programming Assignment #3 asks you to implement QuickSort and compute the number of comparisons that it makes for three different pivot rules. SUGGESTED READINGS FOR WEEK 3: CLRS Chapter 7 KT Section 13.5 SW Section 2.3 Quicksort: Overview12 minPartitioning Around a Pivot24 minCorrectness of Quicksort [Review - Optional]10 minChoosing a Good Pivot22 minVI. QUICKSORT - ANALYSISAnalysis I: A Decomposition Principle21 minAnalysis II: The Key Insight11 minAnalysis III: Final Calculations8 minVII. PROBABILITY REVIEWProbability Review I25 minProbability Review II17 minProblem Set #3Quiz: Problem Set #35 questions Programming Assignment #3Quiz: Programming Assignment #33 questionsQuickSort.txt Week 4Linear-time selection;graphs, cuts, and the contraction algorithm. VIII. LINEAR-TIME SELECTIONWeek 4 Overview10 minPart VIII — LINEAR-TIME SELECTION (Required). These lectures study the problem of computing the ith smallest element of an input array (e.g., the median). It’s easy to solve this problem in O(n log n) time using sorting, but we can do better! The required material goes over a super-practical randomized algorithm, very much in the spirit of QuickSort, that has linear expected running time. Don’t forget that it takes linear time just to read the input! The analysis is somewhat different than what we studied for QuickSort, but is equally slick. Basically, there’s a cool way to think about the progress the algorithm makes in terms of a simple coin-flipping experiment. Linearity of expectation (yes, it’s back…) seals the deal. Part VIII — LINEAR-TIME SELECTION (Optional). I couldn’t resist covering some advanced related material. The first is an algorithm that has more Turing Award-winning authors than any other algorithm that I know of. It is a deterministic (i.e., no randomization allowed) linear-time algorithm for the Selection problem, based on an ingenious “median-of-medians” idea for guaranteeing good pivot choices. (There are some accompanying lectures notes for this part, available for download underneath each video.) The second optional topic answers the question “can we do better?” for sorting, unfortunately in the negative. That is, a counting argument shows that there is no “comparison-based” sorting algorithm (like MergeSort, QuickSort, or HeapSort) with worst-case running time better than n log n. Part IX — GRAPHS AND THE CONTRACTION ALGORITHM. The second set of lectures for this week is a segue from randomized algorithms to graph algorithms. We first review graphs and the most standard ways of representing them (most commonly, by adjacency lists). We then consider the random contraction algorithm, discovered by Karger “only” 20ish years ago (while a PhD student here at Stanford). This algorithm solves the minimum cut problem — given an undirected graph, separate the vertices into two non-empty groups to minimize the number of “crossing edges”. Such problems come up when reasoning about, for example, physical networks, social networks, and images. This algorithm was perhaps the first strong evidence that graph problems could be added to the long list of “killer applications” of random sampling. Don’t tune out before the final plot twist — a simple but useful trick for transforming an algorithm that almost always fails into one that almost always succeeds. HOMEWORK: Problem Set #4 has five questions about the randomized selection algorithm, cuts in graphs, and the contraction algorithm. Programming Assignment #4 asks you to implement the contraction algorithm and use it to compute the min cut of the graph that we provide. SUGGESTED READINGS FOR WEEK 3: CLRS Chapter 9, 22 (Only 22.1) DPV Chapter 3 (only 3.1) KT Chapter 13, Sections 13.2,13.5 SW Chapter 4, Section 4.1 Randomized Selection - Algorithm21 minRandomized Selection - Analysis20 minDeterministic Selection - Algorithm [Advanced - Optional]16 minDeterministic Selection - Analysis I [Advanced - Optional]22 minDeterministic Selection - Analysis II [Advanced - Optional]12 minOmega(n log n) Lower Bound for Comparison-Based Sorting [Advanced - Optional]13 minIX. GRAPHS AND THE CONTRACTION ALGORITHMGraphs and Minimum Cuts15 minGraph Representations14 minRandom Contraction Algorithm8 minAnalysis of Contraction Algorithm30 minCounting Minimum Cuts7 minProblem Set #4Quiz: Problem Set #45 questions Optional Theory Problems (Batch #2)10 min Programming Assignment #4Quiz: Programming Assignment #41 question Final Exam (1 attempt per 24 hours)Info and FAQ for final exam10 minQuiz: Final Exam10 questions Graph Search, Shortest Paths, and Data StructuresCourse can be found hereLecture slides can be found hereSummary can be found in my Github About this course: The primary topics in this part of the specialization are: data structures (heaps, balanced search trees, hash tables, bloom filters), graph primitives (applications of breadth-first and depth-first search, connectivity, shortest paths), and their applications (ranging from deduplication to social network analysis). Who is this class for: Learners with at least a little bit of programming experience who want to learn the essentials of algorithms. In a University computer science curriculum, this course is typically taken in the third year. Week 1Breadth-first and depth-first search;computing strong components;applications. X. GRAPH SEARCH AND CONNECTIVITY (Week 1)Week 1 Overview10 minWELCOME: Welcome to Part 2 of the Algorithms Specialization: Graph Search, Shortest Paths, and Data Structures! Like the previous part, the course will have four weeks of lectures and assignments, followed by a final exam. Here are the highlights of the course’s first week. SUMMARY: Week 1 is all about graph search and its applications (section X). We’ll cover a selection of fundamental primitives for reasoning about graphs. One very cool aspect of this material is that all of the algorithms that we’ll cover are insanely fast (linear time with small constants); and, it can be quite subtle to understand why they work! The culmination of these lectures — computing the strongly connected components of a directed graph with just two passes of depth-first search — vividly illustrates the point that fast algorithms often require deep insight into the structure of the problem that you’re solving. There are also lecture notes for this last topic (available for download underneath the videos). (If you’re feeling REALLY motivated, you might read up on Tarjan’s earlier algorithm for computing SCCs that needs only a single DFS pass!) THE VIDEOS: We begin with an overview video, which gives some reasons why you should care about graph search, a general strategy for searching a graph without doing any redundant work, and a high-level introduction to the two most important search strategies, breadth-first search (BFS) and depth-first search (DFS). The second video discusses BFS in more detail, including applications to computing shortest paths and the connected components of an undirected graph. The third video drills down on DFS, and shows how to use it to compute a toplogical ordering of a directed acyclic graph (i.e., to sequence tasks in a way that respects precedence constraints). The fourth and fifth videos cover the description and analysis, respectively, of the aforementioned algorithm for computing SCCs. A final optional video — hopefully one of the more fun ones in the course — discusses the structure of the Web, and lightly touches on topics like Google’s PageRank algorithm and the “six degrees of separation” phenomenon in social networks. DISCUSSION FORUMS: The discussion forums play a crucial role in massive online courses like this one. If you have trouble understanding a lecture or completing an assignment, you should turn to the forums for help. After you’ve mastered the lectures and assignments for a given week, I hope you’ll contribute to the forums and help out your fellow students. While I won’t have time to carefully monitor the discussion forums, I’ll check in and answer questions whenever I find the time. VIDEOS AND SLIDES: Videos can be streamed or downloaded and watched offline (recommended for commutes, etc.). We are also providing PDF lecture slides (typed versions of what’s written in the lecture videos), as well as subtitle files (in English and in some cases other languages as well). And if you find yourself wishing that I spoke more quickly or more slowly, note that you can adjust the video speed to accommodate your preferred pace. THE HOMEWORK: Problem Set #1 should help you solidify your understanding of graph representations and graph search. The programming assignment asks you to implement the SCC algorithm from the lectures, and report your findings about the SCCs of a large graph. Programming Assignment #1 is the most difficult one of the course (and one of the more difficult ones in the entire specialization); as always, I encourage you to use the discussion forums to exchange ideas, tips, and test cases. If you can get through the first week of the course, it should be all downhill from there! SUGGESTED READINGS FOR WEEK 1: Abbreviations in suggested readings refer to the following textbooks: CLRS - Cormen, Leiserson, Rivest, and Stein, Introduction to Algorithms (3rd edition)DPV - Dasgupta, Papadimitriou, and Vazirani, Algorithms KT - Kleinberg and Tardos, Algorithm Design SW - Sedgewick and Wayne, Algorithms (4th edition) CLRS Chapter 22 DPV Chapter 3 KT Chapter 3, Section 3.5, 3.6 SW Chapter 4, Section 4.1,4.2 Overview, Resources, and Policies10 minResources Mathematics for Computer Science (by Eric Lehman and Tom Leighton): https://www.cs.princeton.edu/courses/archive/fall06/cos341/handouts/mathcs.pdf Lecture slides10 minGraph Search - Overview23 minBreadth-First Search (BFS): The Basics14 minBFS and Shortest Paths7 minBFS and Undirected Connectivity13 minDepth-First Search (DFS): The Basics7 minTopological Sort21 minComputing Strong Components: The Algorithm29 minComputing Strong Components: The Analysis26 minStructure of the Web [Optional]18 minProblem Set #1Quiz: Problem Set #15 questions Optional Theory Problems (Week 1)10 minIn the 2SAT problem, you are given a set of clauses, where each clause is the disjunction of two literals (a literal is a Boolean variable or the negation of a Boolean variable). You are looking for a way to assign a value “true” or “false” to each of the variables so that all clauses are satisfied — that is, there is at least one true literal in each clause. For this problem, design an algorithm that determines whether or not a given 2SAT instance has a satisfying assignment. (Your algorithm does not need to exhibit a satisfying assignment, just decide whether or not one exists.) Your algorithm should run in O(m+n) time, where m and n are the number of clauses and variables, respectively. [Hint: strongly connected components.] Programming Assignment #1Quiz: Programming Assignment #11 question Week 2Dijkstra’s shortest-path algorithm. XI. DIJKSTRA’S SHORTEST-PATH ALGORITHM (Week 2)Week 2 Overview10 minSUMMARY: This week is the climax of all our work on graph search — Dijkstra’s shortest-path algorithm, surely one of the greatest hits of algorithms (Part XI). It works in any directed graph with non-negative edge lengths, and it computes the shortest paths from a source vertex to all other vertices. Particularly nice is the blazingly fast implementation that uses a heap data structure (more on heaps next week). THE HOMEWORK: Problem Set #2 should solidify your understanding of Dijkstra’s shortest-path algorithm. In the programming assignment you’ll implement Dijkstra’s algorithm. You can just implement the basic version, but those of you who want a bigger challenge are encouraged to devise a heap-based implementation. SUGGESTED READINGS FOR WEEK 2: CLRS Chapter 24 (Sections 3,4) DPV Sections 4.4 KT Section 4.4 SW Section 4.4 Dijkstra’s Shortest-Path Algorithm20 minDijkstra’s Algorithm: Examples12 minCorrectness of Dijkstra’s Algorithm19 minDijkstra’s Algorithm: Implementation and Running Time26 minProblem Set #2Quiz: Problem Set #25 questions Optional Theory Problems (Week 2)10 minIn lecture we define the length of a path to be the sum of the lengths of its edges. Define the bottleneck of a path to be the maximum length of one of its edges. A mininum-bottleneck path between two vertices s and t is a path with bottleneck no larger than that of any other s-t path. Show how to modify Dijkstra’s algorithm to compute a minimum-bottleneck path between two given vertices. The running time should be O(mlogn), as in lecture.We can do better. Suppose now that the graph is undirected. Give a linear-time (O(m)) algorithm to compute a minimum-bottleneck path between two given vertices.What if the graph is directed? Can you compute a minimum-bottleneck path between two given vertices faster than O(mlogn)? Programming Assignment #2Quiz: Programming Assignment #21 question Week 3Heaps;balanced binary search trees. XII. HEAPS (Week 3)Week 3 Overview10 minSUMMARY: This week we start our discussion of data structures, a huge (and hugely useful) topic. This week covers heaps and balanced binary search trees. My primary goals here are to teach you the operations that these data structures support (along with their running times), and to develop your intuition about which data structures are useful for which sorts of problems. THE VIDEOS: Part XII begins with a short overview video in which I explain my approach to teaching data structures in this course. The next two videos discuss heaps and some of their applications (required), and some details about how they are typically implemented (optional, recommended for hardcore programmer/computer science types). Part XIII discusses balanced binary search trees — the supported operations and canonical uses (required) and a glimpse at what’s “under the hood” (optional). THE HOMEWORK: Problem Set #3 should solidify your understanding of heaps and search trees. In the programming assignment you’ll implement a slick data structure application covered in the lectures (median maintenance). SUGGESTED READINGS FOR WEEK 3: CLRS Chapter 6,11,12,13 DPV Section 4.5 SW Section 3.3, 3.4 Data Structures: Overview4 minHeaps: Operations and Applications18 minHeaps: Implementation Details [Advanced - Optional]20 minXIII. BALANCED BINARY SEARCH TREES (Week 3)Balanced Search Trees: Operations and Applications10 minBinary Search Tree Basics, Part I13 minBinary Search Tree Basics, Part II30 minRed-Black Trees21 minRotations [Advanced - Optional]7 minInsertion in a Red-Black Tree [Advanced]14 minProblem Set #3Quiz: Problem Set #35 questions Programming Assignment #3Quiz: Programming Assignment #31 question Week 4Hashing;bloom filters. XIV. HASHING: THE BASICS (Week 4)Week 4 Overview10 minSUMMARY: This week wraps up our discussion of data structures. We’ll finish with a bang by covering hash tables — certainly one of the most important data structures — in detail. First (Part XIV) we discuss hash table basics — the supported operations, the types of problems they’re useful for, and an overview of implementation approaches. Part XV has one required video about “pathological data sets” for hash functions, and three optional videos that cover great stuff — how randomization dodges pathological data sets, a construction of simple hash functions that are guaranteed to (in expectation over the choice of a random hash function) spread every data set out evenly, and performance analyses of hashing with both chaining and open addressing. Part XVI contains two required videos on the implementation and performance of bloom filters, which are like hash tables except that they are insanely space-efficient and occasionally suffer from false positives. THE HOMEWORK: Problem Set #4 should solidify your understanding of hash tables and bloom filters. In the programming assignment you’ll implement another slick data structure application covered in the lectures (solving the 2-SUM problem using hash tables). SUGGESTED READINGS FOR WEEK 4: CLRS Chapter 11KT Chapter 13 (Section 13.6)SW Section 3.5 Hash Tables: Operations and Applications19 minHash Tables: Implementation Details, Part I18 minHash Tables: Implementation Details, Part II22 minXV. UNIVERSAL HASHING (Week 4)Pathological Data Sets and Universal Hashing Motivation21 minUniversal Hashing: Definition and Example [Advanced - Optional]25 minUniversal Hashing: Analysis of Chaining [Advanced - Optional]18 minHash Table Performance with Open Addressing [Advanced - Optional]15 minXVI. BLOOM FILTERS (Week 4)Bloom Filters: The Basics15 minBloom Filters: Heuristic Analysis13 minProblem Set #4Quiz: Problem Set #45 questions Optional Theory Problems (Week 4)10 minRecall that a set H of hash functions (mapping the elements of a universe U to the buckets {0,1,2,…,n−1}) is universal if for every distinct x,y∈U, the probability Prob[h(x)=h(y)] that x and y collide, assuming that the hash function h is chosen uniformly at random from H, is at most 1/n. In this problem you will prove that a collision probability of 1/n is essentially the best possible. Precisely, suppose that H is a family of hash functions mapping U to {0,1,2,…,n−1}, as above. Show that there must be a pair x,y∈U of distinct elements such that, if h is chosen uniformly at random from H, then Prob[h(x)=h(y)]≥1n−1|U|. Programming Assignment #4Quiz: Programming Assignment #41 question Final Exam (1 attempt per 24 hours)Info and FAQ for final exam10 min Unlike the problem sets, you have only ONE attempt per 24 hours. Thus DO NOT START THE EXAM until you are ready. There are 10 questions, and each is worth 1 point. Many are of the “select all that apply” type, rather than the strict multiple choice format that is more common on the problem sets. You will receive partial credit for each option that you leave correctly checked or correctly unchecked. (So if you mark 4 out of the 5 options for a problem correctly, you’ll receive 0.8 out of the 1 point.) You need 7 points total to pass the exam. All questions are about material covered in the required (i.e., non-optional) videos. Roughly half of the questions follow fairly directly from the lecture material (assuming that you understand it thoroughly). Roughly a quarter of the questions are variations on problem set questions. Roughly a quarter of the questions demand more thought, for example by asking you to consider whether certain results from lecture do or do not extend to more general situations. Good luck! Quiz: Final Exam10 questions Greedy Algorithms, Minimum Spanning Trees, and Dynamic ProgrammingCourse can be found hereLecture slides and solutions can be found here About this course: The primary topics in this part of the specialization are: greedy algorithms (scheduling, minimum spanning trees, clustering, Huffman codes) and dynamic programming (knapsack, sequence alignment, optimal search trees). Who is this class for: Learners with at least a little bit of programming experience who want to learn the essentials of algorithms. In a University computer science curriculum, this course is typically taken in the third year. Week 1Two motivating applications;selected review;introduction to greedy algorithms;a scheduling application;Prim’s MST algorithm. XVII. TWO MOTIVATING APPLICATIONS (Week 1)Week 1 Overview 10 minWELCOME: Welcome to Part 3 of the Algorithms Specialization: Greedy Algorithms, Minimum Spanning Trees, and Dynamic Programming! Like the previous two parts, the course will have four weeks of lectures and assignments, followed by a final exam. Here are the highlights of the course’s first week. TWO MOTIVATING APPLICATIONS: We begin with a fairly non-technical discussion of two motivating applications — distributed shortest-path routing in the Internet, and sequence alignment — to build excitement for the tools that you’ll acquire later in this course (and also in Part 4 of the specialization). INTRODUCTION TO GREEDY ALGORITHMS: The focus of this week and the next is the greedy algorithm design paradigm. These two non-technical videos discuss the pros and cons of this paradigm and describe a cool application to the optimal management of the contents of a cache. A SCHEDULING APPLICATION: Scheduling problems come up all the time (e.g., how should a shared resource be allocated?) and greedy algorithms are often useful for them. We’ll discuss a specific success story — minimizing the weighted sum of completion times of a bunch of tasks — in detail. The correctness proof furnishes a particularly clean example of an “exchange argument.” PRIM’S MST ALGORITHM: The minimum spanning tree (MST) problem, in addition to enjoying several applications, is a uniquely great problem for the study of greedy algorithms. Unusually, several different greedy algorithms always compute an optimal solution. We begin here with the Dijkstra-esque Prim’s algorithm. The correctness proof requires understanding the subtly beautiful structure of cuts in graphs, while its blazingly fast implementation relies on a deft application of the heap data structure. VIDEOS AND SLIDES: A reminder that videos can be streamed or downloaded and watched offline (recommended for commutes, etc.). We are also providing PDF lecture slides (typed versions of what’s written in the lecture videos), as well as subtitle files (in English and in some cases other languages as well). And if you find yourself wishing that I spoke more quickly or more slowly, note that you can adjust the video speed to accommodate your preferred pace. HOMEWORK #1: The first problem set consists of 5 problems, about greedy scheduling algorithms and minimum spanning trees. The first programming assignment asks you to implement some of the algorithms that we’ve covered, run them on large inputs, and enter the answer. For the seasoned programmers out there looking for an additional challenge, try doing the programming assignments in a programming language that you don’t already know! DISCUSSION FORUMS: Discussion forums play an absolutely crucial role in massive online courses. If you have trouble understanding a lecture or completing an assignment, you should turn to the forums for help. After you’ve mastered the lectures and assignments for a given week, I hope you’ll contribute to the forums and help out your fellow learners. While I won’t have time to carefully monitor the discussion forums, I’ll check in and answer questions whenever I find the time. SUGGESTED READINGS FOR WEEK 1: Abbreviations in suggested readings refer to the following textbooks: CLRS - Cormen, Leiserson, Rivest, and Stein, Introdution to Algorithms (3rd edition) DPV - Dasgupta, Papadimitriou, and Vazirani, Algorithms KT - Kleinberg and Tardos, Algorithm Design SW - Sedgewick and Wayne, Algorithms (4th edition) CLRS: Chapter 16 (Sections 1 and 2) and Chapter 23 DPV: Sections 5.1.1, 5.1.2, and 5.1.5 KT: Sections 4.1, 4.2, 4.3, and 4.5 SW: Section 4.3 Overview, Resources, and Policies 10 minCourse overviewAlgorithms are the heart of computer science, and the subject has countless practical applications as well as intellectual depth. This specialization is an introduction to algorithms for learners with at least a little programming experience. The specialization is rigorous but emphasizes the big picture and conceptual understanding over low-level implementation and mathematical details. After completing this specialization, you will have a greater mastery of algorithms than almost anyone without a graduate degree in the subject. Specific topics in Part 3 of the specialization include: greedy algorithms (scheduling, minimum spanning trees, clustering, Huffman codes) and dynamic programming (knapsack, sequence alignment, optimal search trees). ResourcesMathematics for Computer Science (by Eric Lehman and Tom Leighton): https://www.cs.princeton.edu/courses/archive/fall06/cos341/handouts/mathcs.pdf Course PoliciesVideo Lectures Lectures will be made available weekly, for a total of four weeks. In a given week, there will generally be two hours of required material, plus additional optional material (some review, some advanced topics). Below each lecture video there is a PDF of typed versions of the slides. You can download the videos of the lectures for offline viewing. The video player supports speeding up and slowing down the pace. Weekly Programming Assignments and Problem Sets Each week there will be a new problem set and a new programming assignment. For each problem set you are allowed a maximum of two attempts in a 12-hour period (we’ll use the best score). For each programming assignment you’re allowed a maximum of 10 attempts in a 12-hour period (we’ll use the best score). For the final exam, you’re allowed one attempt per 24 hours. Grading To pass a problem set, you must get at least 4 of the 5 questions correct (80%). To pass a programming assignment, you must get all of the answers correct (100%). To pass the final exam, you must get at least 70% of the total points (14 out of 20). To pass the course, you must pass all of the problem sets, all of the programming assignments, and the final exam. Theory Problems These are totally optional theory questions (no deadlines or credit given). We encourage you to attempt these questions and discuss them in the forums to develop a deeper understanding of the design and analysis of algorithms. Lecture slides 10 minSee the zip file below (which includes slides for both Part 3 and Part 4 of the specialization). Also available from the video lectures on a lecture-by-lecture basis. Algo2Typed.zip Application: Internet Routing 10 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/0VcrE/application-internet-routingDijkstra’s algorithmBellman-Ford algorithm Application: Sequence Alignment 8 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/ekVkk/application-sequence-alignmentBrute-force search: Try all possible alignments, remember the bestone. XVIII. INTRODUCTION TO GREEDY ALGORITHMS (Week 1)Introduction to Greedy Algorithms 12 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/WHe2b/introduction-to-greedy-algorithmsDANGER: Most greedy algorithms are NOT correct. (Even if yourintuition says otherwise!) Application: Optimal Caching 10 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/VMnNW/application-optimal-cachingThe “furthest-in-future” algorithm is optimal (i.e., minimizes the number of cache misses).Least Recently Used (LRU) XIX. A SCHEDULING APPLICATION (Week 1)Problem Definition 5 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/ZvZGZ/problem-definitionGoal: Minimize the weighted sum of completion times: A Greedy Algorithm 12 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/Jo6gK/a-greedy-algorithmGuess (1): Order jobs by decreasing value of wj - lj .Guess (2): Order wj/lj .Alg#1 not (always) correct.Alg#2 (order by decreasing ratio wj=lj ‘s) is always correct. not obvious! - proof coming up next Correctness Proof - Part I 6 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/rFQ5P/correctness-proof-part-iProof: By an Exchange Argument. Correctness Proof - Part II 4 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/mrXwO/correctness-proof-part-iiCost-Benefit Analysis,Upshot: Cost of exchange wi lj . [Ci goes up by lj ] Benefit of exchange is wj li . [Cj goes down by li ]Note: i &gt; j $\rightarrow$ wi/li &lt; wj/lj $\rightarrow$ wi lj &lt; wj li $\rightarrow$ COST &lt; BENEFIT $\rightarrow$ Swap improves $\sigma^{\star}$, contradicts optimality of $\sigma^{\star}$. Handling Ties [Advanced - Optional] 7 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/YuoAV/handling-ties-advanced-optional XX. PRIM’S MINIMUM SPANNING TREE ALGORITHM (Week 1)MST Problem Definition 11 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/9D5ML/mst-problem-definitionspanning tree Prim’s MST Algorithm 7 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/tQ6gK/prims-mst-algorithmPrim’s MST Algorithm Correctness Proof I 15 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/15UXn/correctness-proof-iProof of Prim’s algorithm outputs a spanning tree.(1) Algorithm maintains invariant that T spans X(2) Can’t get stuck with X $\neq$ V(3) No cycles ever get created in T. Correctness Proof II 8 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/hYzal/correctness-proof-iiProof of Prim’s algorithm always outputs a minimum-costspanning tree.The Cut Property Proof of Cut Property [Advanced - Optional] 11 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/UImix/proof-of-cut-property-advanced-optional Fast Implementation I 14 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/bYMq1/fast-implementation-iwhy O(m) time per iteration?Because computer does not know which vertice is in or not in X and V-X, so computer iterate through all edges which v in X and w in V-X. Check: Can initialize heap with O( m + n log n ) = O(mlog n)preprocessing.m: iterate through m edges and put it into heapnlogn: might be time of heap insert operation Fast Implementation II 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/qzdR8/fast-implementation-iiO(mlog n) timem: (n - 1) Inserts during preprocessing(n - 1) Extract-Mins (one per iteration of while loop)O[(n-1) + (n-1) log n] = O(m+mlog n) = O(mlog n) Problem Set #1Quiz: Problem Set #1 5 questionsQUIZProblem Set #15 questionsTo Pass80% or higherAttempts2 every 12 hoursDeadlineJuly 23, 11:59 PM PDT 1 point1.We are given as input a set of $n$ requests (e.g., for the use of an auditorium), with a known start time $s_i$ and finish time $t_i$ for each request $i$. Assume that all start and finish times are distinct. Two requests conflict if they overlap in time — if one of them starts between the start and finish times of the other. Our goal is to select a maximum-cardinality subset of the given requests that contains no conflicts. (For example, given three requests consuming the intervals [0,3], [2,5], and [4,7], we want to return the first and third requests.) We aim to design a greedy algorithm for this problem with the following form: At each iteration we select a new request i, including it in the solution-so-far and deleting from future consideration all requests that conflict with i. Which of the following greedy rules is guaranteed to always compute an optimal solution? At each iteration, pick the remaining request which requires the least time (i.e., has the smallest value of $t_i−s_i$) (breaking ties arbitrarily).This should not be selectedExercise: find a counterexample that uses only three requests. At each iteration, pick the remaining request with the fewest number of conflicts with other remaining requests (breaking ties arbitrarily).This should not be selectedThere are counterexamples, though finding one requires a bit of work! At each iteration, pick the remaining request with the earliest finish time.CorrectLet Rj denote the requests with the j earliest finish times. Prove by induction on j that this greedy algorithm selects the maximum-number of non-conflicting requests from Sj. At each iteration, pick the remaining request with the earliest start time.This should not be selectedWhat if there is a request with start time 0 and an extremely large finish time? 1 point2.We are given as input a set of $n$ jobs, where job $j$ has a processing time $p_j$ and a deadline $d_j$. Recall the definition of completion times $C_j$ from the video lectures. Given a schedule (i.e., an ordering of the jobs), we define the lateness $l_j$ of job $j$ as the amount of time $C_j−d_j$ after its deadline that the job completes, or as 0 if Cj≤dj. Our goal is to minimize the maximum lateness, maxj$l_j$. Which of the following greedy rules produces an ordering that minimizes the maximum lateness? You can assume that all processing times and deadlines are distinct. Schedule the requests in increasing order of processing time pj Schedule the requests in increasing order of the product dj$\cdot$pj None of the other answers are correct.This should not be selectedOne of the proposed greedy rules works. Try an exchange argument. Schedule the requests in increasing order of deadline djCorrectProof by an exchange argument, analogous to minimizing the weighted sum of completion times. 1 point3.In this problem you are given as input a graph T=(V,E) that is a tree (that is, T is undirected, connected, and acyclic). A perfect matching of T is a subset F$\subset$E of edges such that every vertex v∈V is the endpoint of exactly one edge of F. Equivalently, F matches each vertex of T with exactly one other vertex of T. For example, a path graph has a perfect matching if and only if it has an even number of vertices. Consider the following two algorithms that attempt to decide whether or not a given tree has a perfect matching. The degree of a vertex in a graph is the number of edges incident to it. (The two algorithms differ only in the choice of v in line 5.) Algorithm A:123456789While T has at least one vertex: If T has no edges: halt and output &quot;T has no perfect matching.&quot; Else: Let v be a vertex of T with maximum degree. Choose an arbitrary edge e incident to v. Delete e and its two endpoints from T.[end of while loop]Halt and output &quot;T has a perfect matching.&quot; Algorithm B:123456789While T has at least one vertex: If T has no edges: halt and output &quot;T has no perfect matching.&quot; Else: Let v be a vertex of T with minimum non-zero degree. Choose an arbitrary edge e incident to v. Delete e and its two endpoints from T.[end of while loop]Halt and output &quot;T has a perfect matching.&quot; Is either algorithm correct? Both algorithms always correctly determine whether or not a given tree graph has a perfect matching.This should not be selectedConsider a three-hop path. Neither algorithm always correctly determines whether or not a given tree graph has a perfect matching.This should not be selectedTry to prove correctness by induction. Algorithm A always correctly determines whether or not a given tree graph has a perfect matching; algorithm B does not.This should not be selectedConsider a three-hop path. Algorithm B always correctly determines whether or not a given tree graph has a perfect matching; algorithm A does not.CorrectAlgorithm A can fail, for example, on a three-hop path. Correctness of algorithm B can be proved by induction on the number of vertices in T. Note that the tree property is used to argue that there must be a vertex with degree 1; if there is a perfect matching, it must include the edge incident to this vertex. 1 point4.Consider an undirected graph G=(V,E) where every edge e∈E has a given cost ce. Assume that all edge costs are positive and distinct. Let T be a minimum spanning tree of G and P a shortest path from the vertex s to the vertex t. Now suppose that the cost of every edge e of G is increased by 1 and becomes ce+1. Call this new graph G′. Which of the following is true about G′? T must be a minimum spanning tree but P may not be a shortest s-t path.CorrectThe positive statement has many proofs (e.g., via the Cut Property). For the negative statement,think about two different paths from s to t that contain a different number of edges. T may not be a minimum spanning tree but P is always a shortest s-t path. T is always a minimum spanning tree and P is always a shortest s-t path.This should not be selectedThink about two different paths from s to t that contain a different number of edges. T may not be a minimum spanning tree and P may not be a shortest s-t path.This should not be selectedWhat does the Cut Property tell you about G′ vs. G? 1 point5.Suppose T is a minimum spanning tree of the connected graph G. Let H be a connected induced subgraph of G. (I.e., H is obtained from G by taking some subset S$\subseteq$V of vertices, and taking all edges of E that have both endpoints in S. Also, assume H is connected.) Which of the following is true about the edges of T that lie in H? You can assume that edge costs are distinct, if you wish. [Choose the strongest true statement.] For every G and H, these edges form a spanning tree (but not necessary minimum-cost) of HThis should not be selectedSuppose G is a triangle and H is an edge. For every G and H, these edges form a minimum spanning tree of HThis should not be selectedHow do you know that they form a connected subgraph of H? For every G and H, these edges are contained in some minimum spanning tree of HCorrectProof via the Cut Property (cuts in G correspond to cuts in H with only fewer crossing edges). For every G and H and spanning tree TH of H, at least one of these edges is missing from THThis should not be selectedThink about the Cut Property. Optional Theory Problems (Week 1)10 minThe following problems are for those of you looking to challenge yourself beyond the required problem sets and programming questions. They are completely optional and will not be graded. While they vary in level, many are pretty challenging, and we strongly encourage you to discuss ideas and approaches with your fellow students on the “Theory Problems” discussion forum. Consider a connected undirected graph G with not necessarily distinct edge costs. Consider two different minimum-cost spanning trees of G, T and T′. Is there necessarily a sequence of minimum-cost spanning trees T=T0,T1,T2,…,Tr=T′ with the property that each consecutive pair Ti,Ti+1 of MSTs differ by only a single edge swap? Prove the statement or exhibit a counterexample. Consider the following algorithm. The input is a connected undirected graph with edge costs (distinct, if you prefer). The algorithm proceeds in iterations. If the current graph is a spanning tree, then the algorithm halts. Otherwise, it picks an arbitrary cycle of the current graph and deletes the most expensive edge on the cycle. Is this algorithm guaranteed to compute a minimum-cost spanning tree? Prove it or exhibit a counterexample. Consider the following algorithm. The input is a connected undirected graph with edge costs (distinct, if you prefer). The algorithm proceeds in phases. Each phase adds some edges to a tree-so-far and reduces the number of vertices in the graph (when there is only 1 vertex left, the MST is just the empty set). In a phase, we identify the cheapest edge ev incident on each vertex v of the current graph. Let F={ev} be the collection of all such edges in the current phase. Obtain a new (smaller) graph by contracting all of the edges in F — so that each connected component of F becomes a single vertex in the new graph — discarding any self-loops that result. Let T denote the union of all edges that ever get contracted in a phase of this algorithm. Is T guaranteed to be a minimum-cost spanning tree? Prove it or exhibit a counterexample. Programming Assignment #1Quiz: Programming Assignment #1 3 questionsQUIZProgramming Assignment #13 questionsTo Pass100% or higherAttempts10 every 12 hoursDeadlineJuly 23, 11:59 PM PDTcode can be found here 1 point1.In this programming problem and the next you’ll code up the greedy algorithms from lecture for minimizing the weighted sum of completion times.. Download the text file below. jobs.txtThis file describes a set of jobs with positive and integral weights and lengths. It has the format [number_of_jobs] [job_1_weight] [job_1_length] [job_2_weight] [job_2_length] … For example, the third line of the file is “74 59”, indicating that the second job has weight 74 and length 59. You should NOT assume that edge weights or lengths are distinct. Your task in this problem is to run the greedy algorithm that schedules jobs in decreasing order of the difference (weight - length). Recall from lecture that this algorithm is not always optimal. IMPORTANT: if two jobs have equal difference (weight - length), you should schedule the job with higher weight first. Beware: if you break ties in a different way, you are likely to get the wrong answer. You should report the sum of weighted completion times of the resulting schedule — a positive integer — in the box below. ADVICE: If you get the wrong answer, try out some small test cases to debug your algorithm (and post your test cases to the discussion forum). 1 point2.For this problem, use the same data set as in the previous problem. Your task now is to run the greedy algorithm that schedules jobs (optimally) in decreasing order of the ratio (weight/length). In this algorithm, it does not matter how you break ties. You should report the sum of weighted completion times of the resulting schedule — a positive integer — in the box below. 1 point3.In this programming problem you’ll code up Prim’s minimum spanning tree algorithm. Download the text file below. edges.txtThis file describes an undirected graph with integer edge costs. It has the format [number_of_nodes] [number_of_edges] [one_node_of_edge_1] [other_node_of_edge_1] [edge_1_cost] [one_node_of_edge_2] [other_node_of_edge_2] [edge_2_cost] … For example, the third line of the file is “2 3 -8874”, indicating that there is an edge connecting vertex #2 and vertex #3 that has cost -8874. You should NOT assume that edge costs are positive, nor should you assume that they are distinct. Your task is to run Prim’s minimum spanning tree algorithm on this graph. You should report the overall cost of a minimum spanning tree — an integer, which may or may not be negative — in the box below. IMPLEMENTATION NOTES: This graph is small enough that the straightforward O(mn) time implementation of Prim’s algorithm should work fine. OPTIONAL: For those of you seeking an additional challenge, try implementing a heap-based version. The simpler approach, which should already give you a healthy speed-up, is to maintain relevant edges in a heap (with keys = edge costs). The superior approach stores the unprocessed vertices in the heap, as described in lecture. Note this requires a heap that supports deletions, and you’ll probably need to maintain some kind of mapping between vertices and their positions in the heap. Week 2Kruskal’s MST algorithm and applications to clustering;advanced union-find (optional). XXI. KRUSKAL’S MINIMUM SPANNING TREE ALGORITHM (Week 2)Week 2 Overview 10 minKRUSKAL’S MST ALGORITHM: Last week we covered Prim’s MST algorithm and a blazingly fast implementation of it. There are several reasons for studying a second greedy MST algorithm, due to Kruskal. First, it’s a beautiful algorithm, worthy of a greatest hits compilation. Second, to obtain a super-fast implementation of it, we’ll need to learn about the simple but fundamental “Union-Find” data structure. The third reason is covered in the next section… CLUSTERING: Clustering is an important form of unsupervised learning (i.e., extracting patterns from unlabeled data). These two videos discuss how Kruskal’s MST algorithm suggests flexible and useful greedy approaches to clustering problems. UNION-FIND LECTURES: This is purely optional material about advanced implementations and analysis of the union-find data structure. For those of you looking for some seriously next-level (but beautiful) material, check it out when you get a chance. HOMEWORK #2: The second problem set is all about MSTs. The second programming assignment asks you to implement the greedy clustering algorithm from lecture. For part (a), a straightforward implementation should suffice. Part (b) involves a graph that is likely too big to fit in your computer’s memory, so answering this part might take some ingenuity. SUGGESTED READINGS FOR WEEK 2: CLRS Chapter 21 and Chapter 23 (Section 2)DPV Sections 5.1.3 and 5.1.4KT Sections 4.5-4.7SW Sections 1.5 and 4.3 Kruskal’s MST Algorithm 7 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/PLdBf/kruskals-mst-algorithmKruskal’s MST Algorithm Correctness of Kruskal’s Algorithm 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/U3ukN/correctness-of-kruskals-algorithm Implementing Kruskal’s Algorithm via Union-Find I 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/e0TJP/implementing-kruskals-algorithm-via-union-find-i FIND(X): Return name of group that X belongs to.UNION(Ci , Cj ): Fuse groups Ci , Cj into a single one. Implementing Kruskal’s Algorithm via Union-Find II 13 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/TvDMg/implementing-kruskals-algorithm-via-union-find-ii O(mlog n) total (Matching Prim’s algorithm) MSTs: State-of-the-Art and Open Questions [Advanced - Optional] 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/Wt9aw/msts-state-of-the-art-and-open-questions-advanced-optional XXII. CLUSTERING (Week 2)Application to Clustering 11 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/QWubN/application-to-clustering Call points p &amp; q separated if they’re assigned to different clusters. Initially, each point in a separate cluster Repeat until only k clusters: Let p, q = closest pair of separated points (determines thecurrent spacing)-Merge the clusters containing p &amp; q into a single cluster. Correctness of Clustering Algorithm 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/7lWTf/correctness-of-clustering-algorithm XXIII. ADVANCED UNION-FIND (Week 2)Lazy Unions [Advanced - Optional] 10 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/WX3tg/lazy-unions-advanced-optional Union-by-Rank [Advanced - Optional] 12 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/RF7aH/union-by-rank-advanced-optional Analysis of Union-by-Rank [Advanced - Optional] 14 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/r2nA3/analysis-of-union-by-rank-advanced-optional Path Compression [Advanced - Optional]14 minPath Compression: The Hopcroft-Ullman Analysis I [Advanced - Optional]9 minPath Compression: The Hopcroft-Ullman Analysis II [Advanced - Optional]11 minThe Ackermann Function [Advanced - Optional]16 minPath Compression: Tarjan’s Analysis I [Advanced - Optional]14 minPath Compression: Tarjan’s Analysis II [Advanced - Optional]13 minProblem Set #2Quiz: Problem Set #2 5 questions Due in 6 daysQUIZProblem Set #25 questionsTo Pass80% or higherAttempts2 every 12 hoursDeadlineJuly 30, 11:59 PM PDT 1 point1.Suppose we are given a directed graph G=(V,E) in which every edge has a distinct positive edge weight. A directed graph is acyclic if it has no directed cycle. Suppose that we want to compute the maximum-weight acyclic subgraph of G (where the weight of a subgraph is the sum of its edges’ weights). Assume that G is weakly connected, meaning that there is no cut with no edges crossing it in either direction. Here is an analog of Prim’s algorithm for directed graphs. Start from an arbitrary vertex s, initialize S={s} and F=∅. While S≠V, find the maximum-weight edge (u,v) with one endpoint in S and one endpoint in V−S. Add this edge to F, and add the appropriate endpoint to S. Here is an analog of Kruskal’s algorithm. Sort the edges from highest to lowest weight. Initialize F=∅. Scan through the edges; at each iteration, add the current edge i to F if and only if it does not create a directed cycle. Which of the following is true? Only the modification of Kruskal’s algorithm always computes a maximum-weight acyclic subgraph.This should not be selectedFour vertices suffice for a counterexample. Both algorithms always compute a maximum-weight acyclic subgraph. Both algorithms might fail to compute a maximum-weight acyclic subgraph.CorrectIndeed. Any ideas for a correct algorithm? Only the modification of Prim’s algorithm always computes a maximum-weight acyclic subgraph.1 point2.Consider a connected undirected graph G with edge costs that are not necessarily distinct. Suppose we replace each edge cost $c_e$ by $−c_e$; call this new graph G′. Consider running either Kruskal’s or Prim’s minimum spanning tree algorithm on G′, with ties between edge costs broken arbitrarily, and possibly differently, in each algorithm. Which of the following is true? Prim’s algorithm computes a maximum-cost spanning tree of G but Kruskal’s algorithm might not. Both algorithms compute a maximum-cost spanning tree of G, but they might compute different ones.CorrectDifferent tie-breaking rules generally yield different spanning trees. Both algorithms compute the same maximum-cost spanning tree of G.This should not be selectedDoes the spanning tree produced by either algorithm depend on how ties between edge costs are broken? Kruskal’s algorithm computes a maximum-cost spanning tree of G but Prim’s algorithm might not.1 point3.Consider the following algorithm that attempts to compute a minimum spanning tree of a connected undirected graph G with distinct edge costs. First, sort the edges in decreasing cost order (i.e., the opposite of Kruskal’s algorithm). Initialize T to be all edges of G. Scan through the edges (in the sorted order), and remove the current edge from T if and only if it lies on a cycle of T. Which of the following statements is true? The output of the algorithm will never have a cycle, but it might not be connected. The algorithm always outputs a spanning tree, but it might not be a minimum cost spanning tree. The output of the algorithm will always be connected, but it might have cycles. The algorithm always outputs a minimum spanning tree.CorrectDuring the iteration in which an edge is removed, it was on a cycle C of T. By the sorted ordering, it must be the maximum-cost edge of C. By an exchange argument, it cannot be a member of any minimum spanning tree. Since every edge deleted by the algorithm belongs to no MST, and its output is a spanning tree (no cycles by construction, connected by the Lonely Cut Corollary), its output must be the (unique) MST. 1 point4.Consider an undirected graph G=(V,E) where edge e∈E has cost $c_e$. A minimum bottleneck spanning tree T is a spanning tree that minimizes the maximum edge cost $max_{e∈T}c_e$. Which of the following statements is true? Assume that the edge costs are distinct. A minimum bottleneck spanning tree is not always a minimum spanning tree, but a minimum spanning tree is always a minimum bottleneck spanning tree.CorrectFor the positive statement, recall the following (from correctness of Prim’s algorithm): for every edge e of the MST, there is a cut (A,B) for which e is the cheapest one crossing it. This implies that every other spanning tree has maximum edge cost at least as large. For the negative statement, use a triangle with one extra high-cost edge attached. A minimum bottleneck spanning tree is always a minimum spanning tree and a minimum spanning tree is always a minimum bottleneck spanning tree.This should not be selectedIs the minimum bottleneck spanning tree unique (even with distinct edge costs)? A minimum bottleneck spanning tree is not always a minimum spanning tree and a minimum spanning tree is not always a minimum bottleneck spanning tree. A minimum bottleneck spanning tree is always a minimum spanning tree but a minimum spanning tree is not always a minimum bottleneck spanning tree.1 point5.You are given a connected undirected graph G with distinct edge costs, in adjacency list representation. You are also given the edges of a minimum spanning tree T of G. This question asks how quickly you can recompute the MST if we change the cost of a single edge. Which of the following are true? [RECALL: It is not known how to deterministically compute an MST from scratch in O(m) time, where m is the number of edges of G.] [Check all that apply.] Suppose e∈T and we increase the cost of e. Then, the new MST can be recomputed in O(m) deterministic time.CorrectLet A,B be the two connected components of T−{e}. Edge e no longer belongs to the new MST if and only if it is no longer the cheapest edge crossing the cut (A,B) (this can be checked in O(m) time). If f is the new cheapest edge crossing (A,B), then the new MST is T−{e}∪{f}. Suppose e∈T and we decrease the cost of e. Then, the new MST can be recomputed in O(m) deterministic time.CorrectThe MST does not change (by the Cut Property), so no re-computation is needed. Suppose e∉T and we decrease the cost of e. Then, the new MST can be recomputed in O(m) deterministic time.CorrectLet C be the cycle of T∪{e}. Edge e belongs to the new MST if and only if it is no longer the most expensive edge of C (this can be checked in O(n) time). If f is the new most expensive edge of C, then the new MST is T∪{e}−{f}. Suppose e∉T and we increase the cost of e. Then, the new MST can be recomputed in O(m) deterministic time.CorrectThe MST does not change (by the Cycle Property of the previous problem), so no re-computation is needed. Optional Theory Problems (Week 2) 10 minThe following problems are for those of you looking to challenge yourself beyond the required problem sets and programming questions. They are completely optional and will not be graded. While they vary in level, many are pretty challenging, and we strongly encourage you to discuss ideas and approaches with your fellow students on the “Theory Problems” discussion forum. Consider a connected undirected graph G with edge costs, which need not be distinct. Prove the following statement or provide a counterexample: for every MST T of G, there exists a way to sort G’s edges in nondecreasing order of cost so that Kruskal’s algorithm outputs the tree T. Recall the definition of a minimum bottleneck spanning tree from Problem Set #2. Give a linear-time (i.e., O(m)) algorithm for computing a minimum bottleneck spanning tree of a connected undirected graph. [Hint: make use of a non-trivial linear-time algorithm discussed in Part 1 of the specialization.] Consider a connected undirected graph G with distinct edge costs that are positive integers between 1 and n3, where n is the number of vertices of G. How fast can you compute the MST of G? Read about matroids correctly computes a maximum-weight basis. For the matroid of spanning trees of a graph, this algorithm becomes Kruskal’s algorithm. Can you formulate an analog of Prim’s MST algorithm for matroids? Prove that our analysis of union-find with lazy unions and union by rank (but without path compression) is asymptotically optimal (i.e., there are sequences of operations where you do Θ(logn) work on most of the operations). Prove that in our union-find data structure with lazy unions, union by rank, and path compression, some operations might require Θ(logn) time. Programming Assignment #2Quiz: Programming Assignment #2 2 questions Due in 6 daysQUIZProgramming Assignment #22 questionsTo Pass100% or higherAttempts10 every 12 hoursDeadlineJuly 30, 11:59 PM PDT 1 point1.In this programming problem and the next you’ll code up the clustering algorithm from lecture for computing a max-spacing k-clustering. Download the text file below. clustering1.txt This file describes a distance function (equivalently, a complete graph with edge costs). It has the following format: [number_of_nodes] [edge 1 node 1] [edge 1 node 2] [edge 1 cost] [edge 2 node 1] [edge 2 node 2] [edge 2 cost] … There is one edge (i,j) for each choice of 1≤i&lt;j≤n, where n is the number of nodes. For example, the third line of the file is “1 3 5250”, indicating that the distance between nodes 1 and 3 (equivalently, the cost of the edge (1,3)) is 5250. You can assume that distances are positive, but you should NOT assume that they are distinct. Your task in this problem is to run the clustering algorithm from lecture on this data set, where the target number k of clusters is set to 4. What is the maximum spacing of a 4-clustering? ADVICE: If you’re not getting the correct answer, try debugging your algorithm using some small test cases. And then post them to the discussion forum! 1 point2.In this question your task is again to run the clustering algorithm from lecture, but on a MUCH bigger graph. So big, in fact, that the distances (i.e., edge costs) are only defined implicitly, rather than being provided as an explicit list. The data set is below. clustering_big.txt The format is: [# of nodes] [# of bits for each node’s label] [first bit of node 1] … [last bit of node 1] [first bit of node 2] … [last bit of node 2] … For example, the third line of the file “0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1” denotes the 24 bits associated with node #2. The distance between two nodes u and v in this problem is defined as the Hamming distance— the number of differing bits — between the two nodes’ labels. For example, the Hamming distance between the 24-bit label of node #2 above and the label “0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1” is 3 (since they differ in the 3rd, 7th, and 21st bits). The question is: what is the largest value of k such that there is a k-clustering with spacing at least 3? That is, how many clusters are needed to ensure that no pair of nodes with all but 2 bits in common get split into different clusters? NOTE: The graph implicitly defined by the data file is so big that you probably can’t write it out explicitly, let alone sort the edges by cost. So you will have to be a little creative to complete this part of the question. For example, is there some way you can identify the smallest distances without explicitly looking at every pair of nodes? https://github.com/dbarabanov/coursera/blob/master/algorithms_2/assignment_2/question_1.py Week 3Huffman codes; introduction to dynamic programming. XXIV. HUFFMAN CODES (Week 3)Week 3 Overview 10 minHUFFMAN CODES: Everybody loves compression. It means more songs on your smartphone, faster downloads, and so on. Huffman coding is a fundamental type of lossless compression, used for example in the MP3 standard. In these videos we’ll learn about the optimality of Huffman codes, and a blazingly fast greedy algorithm for computing them. DYNAMIC PROGRAMMING: This week also introduces the dynamic programming design paradigm; next week we’ll see a selection of killer applications. For now, we content ourselves with the development of a linear-time algorithm for a relatively simple problem, computing a maximum-weight independent set of a path graph. We conclude the week by zooming out and identifying the key principles of dynamic programming. HOMEWORK #3: The third problem set and programming assignment reinforce the concepts and algorithms studied in the lectures, and the programming assignment asks you to implement the greedy algorithm for Huffman coding and the dynamic programming algorithm for finding a maximum-weight independent set of a path. SUGGESTED READINGS FOR WEEK 3: CLRS Chapter 16 (Section 3) and Chapter 15 (Section 3) DPV Sections 5.2 and 6.7 KT Sections 4.8, 6.1, 6.2 SW Section 5.5 Introduction and Motivation 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/plgXS/introduction-and-motivation fixed-lengthvariable-length(prefix free) Problem Definition 10 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/IWxVe/problem-definition A binary tree T minimizing the average encoding lengthL. A Greedy Algorithm 16 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/ZIJwv/a-greedy-algorithm Huffman’s Algorithm A More Complex Example 4 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/rTB4s/a-more-complex-example Correctness Proof I 10 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/eSz8f/correctness-proof-i Correctness Proof II 12 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/l3Ss5/correctness-proof-ii XXV. INTRODUCTION TO DYNAMIC PROGRAMMING (Week 3)Introduction: Weighted Independent Sets in Path Graphs 7 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/WENc1/introduction-weighted-independent-sets-in-path-graphs max-weight independent set WIS in Path Graphs: Optimal Substructure 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/t9XAF/wis-in-path-graphs-optimal-substructure Upshot: A max-weight IS must be either(i) a max-weight IS of G%\prime% or(ii) $v_n$ + a max-weight IS of G%\prime\prime% WIS in Path Graphs: A Linear-Time Algorithm 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/w040v/wis-in-path-graphs-a-linear-time-algorithm A[0] = 0; A[1] = w1For i = 2, 3, … n:A[i] = max{ A[i - 1] , A[i - 2] + wi } WIS in Path Graphs: A Reconstruction Algorithm 6 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/TZgJM/wis-in-path-graphs-a-reconstruction-algorithm Let S = Null; While i &gt;= 1 [scan through array from right to left] If A[i - 1] &gt;= A[i - 2] + wi [i.e. case 1 wins] Decrease i by 1 Else [i.e., case 2 wins] Add vi to S, decrease i by 2 Return S Principles of Dynamic Programming 7 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/VEc7L/principles-of-dynamic-programming Problem Set #3Quiz: Problem Set #35 questionsQUIZProblem Set #35 questionsTo Pass80% or higherAttempts2 every 12 hoursDeadlineAugust 6, 11:59 PM PDT 1 point1.Consider an alphabet with five letters, {a,b,c,d,e}, and suppose we know the frequencies fa=0.32, fb=0.25, fc=0.2, fd=0.18, and fe=0.05. What is the expected number of bits used by Huffman’s coding scheme to encode a 1000-letter document?a:0.32, b:0.25, c:0.2, de:0.23a:0.32, b:0.25, cde: 0.43ab: 0.57, cde: 0.43abcdea:00, b:01, c:10, d: 110, e:1110.322 + 0.252 + 0.22 + 0.183 + 0.053 = 2.232.231000 = 2230 2230 2400 3000 34501 point2.Under a Huffman encoding of n symbols, how long (in terms of number of bits) can a codeword possibly be? log2nThis should not be selectedThis is the best-case scenario for the length of the longest codeword; in general the longest codeword can be longer than this. n n-1CorrectFor the lower bound, take frequencies proportional to powers of 2. For the upper bound, note that the total number of merges is exactly n−1. lnn1 point3.Which of the following statements holds for Huffman’s coding scheme? If the most frequent letter has frequency less than 0.33, then all letters will be coded with at least two bits.Such a letter will endure a merge in at least two iterations: the last one (which involves all letters), and at least one previous iteration. In the penultimate iteration, if the letter has not yet endured a merge, at least one of the two other remaining subtrees has cumulative frequency at least (1−.33)/2&gt;.33, so the letter will get merged in this iteration. A letter with frequency at least 0.5 might get encoded with two or more bits. If a letter’s frequency is at least 0.4, then the letter will certainly be coded with only one bit.This should not be selectedCounterexample: frequencies 40%, 40%, and 20%. If the most frequent letter has frequency less than 0.5, then all letters will be coded with more than one bit.This should not be selectedCounterexample: frequencies 40%, 30%, and 30%. 1 point4.Which of the following is true for our dynamic programming algorithm for computing a maximum-weight independent set of a path graph? (Assume there are no ties.) If a vertex is excluded from the optimal solution of a subproblem, then it is excluded from the optimal solutions of all bigger subproblems. As long as the input graph has at least two vertices, the algorithm never selects the minimum-weight vertex. If a vertex is excluded from the optimal solution of two consecutive subproblems, then it is excluded from the optimal solutions of all bigger subproblems.CorrectBy induction, since the optimal solution to a subproblem depends only on the solutions of the previous two subproblems. The algorithm always selects the maximum-weight vertex.1 point5.Recall our dynamic programming algorithm for computing the maximum-weight independent set of a path graph. Consider the following proposed extension to more general graphs. Consider an undirected graph with positive vertex weights. For a vertex v, obtain the graph G′(v) by deleting v and its incident edges from G, and obtain the graph G′′(v) from G by deleting v, its neighbors, and all of the corresponding incident edges from G. Let OPT(H) denote the value of a maximum-weight independent set of a graph H. Consider the formula OPT(G)=max{OPT(G′(v)),wv+OPT(G′′(v))}, where v is an arbitrary vertex of G of weight wv. Which of the following statements is true? The formula is always correct in trees, but does not lead to an efficient dynamic programming algorithm. The formula is always correct in general graphs, and it leads to an efficient dynamic programming algorithm. The formula is correct in path graphs but is not always correct in trees.This should not be selectedDo weighted independent sets in trees have nice optimal substructure? The formula is always correct in trees, and it leads to an efficient dynamic programming algorithm.CorrectIndeed. What running time can you get? Programming Assignment #3Quiz: Programming Assignment #33 questionsQUIZProgramming Assignment #33 questionsTo Pass100% or higherAttempts 10 every 12 hoursDeadlineAugust 6, 11:59 PM PDT 1 point1.In this programming problem and the next you’ll code up the greedy algorithm from the lectures on Huffman coding. Download the text file below. huffman.txtThis file describes an instance of the problem. It has the following format: [number_of_symbols] [weight of symbol #1] [weight of symbol #2] … For example, the third line of the file is “6852892,” indicating that the weight of the second symbol of the alphabet is 6852892. (We’re using weights instead of frequencies, like in the “A More Complex Example” video.) Your task in this problem is to run the Huffman coding algorithm from lecture on this data set. What is the maximum length of a codeword in the resulting Huffman code? ADVICE: If you’re not getting the correct answer, try debugging your algorithm using some small test cases. And then post them to the discussion forum! 1 point2.Continuing the previous problem, what is the minimum length of a codeword in your Huffman code? 1 point3.In this programming problem you’ll code up the dynamic programming algorithm for computing a maximum-weight independent set of a path graph. Download the text file below. mwis.txtThis file describes the weights of the vertices in a path graph (with the weights listed in the order in which vertices appear in the path). It has the following format: [number_of_vertices] [weight of first vertex] [weight of second vertex] … For example, the third line of the file is “6395702,” indicating that the weight of the second vertex of the graph is 6395702. Your task in this problem is to run the dynamic programming algorithm (and the reconstruction procedure) from lecture on this data set. The question is: of the vertices 1, 2, 3, 4, 17, 117, 517, and 997, which ones belong to the maximum-weight independent set? (By “vertex 1” we mean the first vertex of the graph—there is no vertex 0.) In the box below, enter a 8-bit string, where the ith bit should be 1 if the ith of these 8 vertices is in the maximum-weight independent set, and 0 otherwise. For example, if you think that the vertices 1, 4, 17, and 517 are in the maximum-weight independent set and the other four vertices are not, then you should enter the string 10011010 in the box below. Week 4Advanced dynamic programming:the knapsack problem,sequence alignment,and optimal binary search trees. XXVI. THE KNAPSACK PROBLEM (Week 4)Week 4 Overview 10 minDYNAMIC PROGRAMMING BOOT CAMP: This week is devoted to the dynamic programming design paradigm and a selection of killer applications: the famous Knapsack problem in Part XXVI, the sequence alignment problem in Part XXVII, and computing optimal binary search trees in Part XXVIII. HOMEWORK #4: The fourth problem set and programming assignment reinforce the concepts and algorithms studied in the lectures, and the programming assignment asks you to implement a dynamic programming algorithm for the Knapsack problem. SUGGESTED READINGS FOR WEEK 3: CLRS Chapter 15 DPV Chapter 6 KT Sections 6.3-6.6 The Knapsack Problem 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/LIgLJ/the-knapsack-problemDeveloping a Dynamic Programming AlgorithmStep 1: Formulate recurrence [optimal solution as function ofsolutions to smaller subproblems] based on a structure of anoptimal solution.case 1 and case 2 with proofed A Dynamic Programming Algorithm 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/0n68L/a-dynamic-programming-algorithmknapsack algorithm Example [Review - Optional] 12 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/LADQc/example-review-optional XXVII. SEQUENCE ALIGNMENT (Week 4)Optimal Substructure 13 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/QJkyp/optimal-substructure A Dynamic Programming Algorithm 12 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/tNmae/a-dynamic-programming-algorithm XXVIII. OPTIMAL BINARY SEARCH TREES (Week 4)Problem Definition 12 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/GKCeN/problem-definition Optimal Substructure 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/rUDLu/optimal-substructure Proof of Optimal Substructure 6 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/0qjbs/proof-of-optimal-substructure A Dynamic Programming Algorithm I 9 minhttps://www.coursera.org/learn/algorithms-greedy/lecture/3wrTN/a-dynamic-programming-algorithm-i A Dynamic Programming Algorithm II9 minProblem Set #4Quiz: Problem Set #4 5 questionsQUIZProblem Set #45 questionsTo Pass80% or higherAttempts2 every 12 hoursDeadlineAugust 13, 11:59 PM PDT 1 point1.Consider a variation of the Knapsack problem where we have two knapsacks, with integer capacities W1 and W2. As usual, we are given n items with positive values and positive integer weights. We want to pick subsets S1,S2 with maximum total value (i.e., ∑i∈S1vi+∑i∈S2vi) such that the total weights of S1 and S2 are at most W1 and W2, respectively. Assume that every item fits in either knapsack (i.e., wi≤min{W1,W2} for every item i). Consider the following two algorithmic approaches. (1) Use the algorithm from lecture to pick a a max-value feasible solution S1 for the first knapsack, and then run it again on the remaining items to pick a max-value feasible solution S2 for the second knapsack. (2) Use the algorithm from lecture to pick a max-value feasible solution for a knapsack with capacity W1+W2, and then split the chosen items into two sets S1,S2 that have size at most W1 and W2, respectively. Which of the following statements is true? Neither algorithm is guaranteed to produce an optimal feasible solution to the original problem.CorrectIndeed. Can you devise from scratch a dynamic programming algorithm that correctly solves the problem? Algorithm (1) is guaranteed to produce an optimal feasible solution to the original problem but algorithm (2) is not. Algorithm (2) is guaranteed to produce an optimal feasible solution to the original problem but algorithm (1) is not. Algorithm (1) is guaranteed to produce an optimal feasible solution to the original problem provided W1=W2.This should not be selectedThere’s a counterexample with 4 items. 1 point2.Recall the dynamic programming algorithms from lecture for the Knapsack and sequence alignment problems. Both fill in a two-dimensional table using a double-for loop. Suppose we reverse the order of the two for loops. (I.e., cut and paste the second for loop in front of the first for loop, without otherwise changing the text in any way.) Are the resulting algorithms still well defined and correct? Both algorithms remain well defined and correct after reversing the order of the for loops.CorrectThe necessary subproblem solutions are still available for constant-time lookup. The Knapsack algorithm remains well defined and correct after reversing the order of the for loops, but the sequence alignment algorithm does not. The sequence alignment algorithm remains well defined and correct after reversing the order of the for loops, but the Knapsack algorithm does not. Neither algorithm remains well defined and correct after reversing the order of the for loops.1 point3.Consider an instance of the optimal binary search tree problem with 7 keys (say 1,2,3,4,5,6,7 in sorted order) and frequencies w1=.05,w2=.4,w3=.08,w4=.04,w5=.1,w6=.1,w7=.23. What is the minimum-possible average search time of a binary search tree with these keys? 2.18Correct 2.42 2.9 2.081 point4.The following problems all take as input two strings X and Y, of length m and n, over some alphabet Σ. Which of them can be solved in O(mn) time? [Check all that apply.] Compute the length of a longest common subsequence of X and Y. (Recall a subsequence need not be consecutive. For example, the longest common subsequence of “abcdef” and “afebcd” is “abcd”.)CorrectSimilar dynamic programming to sequence alignment, with one subproblem for each Xi and Yj. Alternatively, this reduces to sequence alignment by setting the gap penalty to 1 and making the penalty of matching two different characters to be very large. Assume that X and Y have the same length n. Does there exist a permutation f, mapping each i∈{1,2,…,n} to a distinct f(i)∈{1,2,…,n}, such that Xi=Yf(i) for every i=1,2,…,n?CorrectThis problem can be solved in O(n) time, without dynamic programming. Just count the frequency of each symbol in each string. The permutation f exists if and only if every symbol occurs exactly the same number of times in each string. Consider the following variation of sequence alignment. Instead of a single gap penalty αgap, you’re given two numbers a and b. The penalty of inserting k gaps in a row is now defined as ak+b, rather than kαgap. Other penalties (for matching two non-gaps) are defined as before. The goal is to compute the minimum-possible penalty of an alignment under this new cost model.CorrectVariation on the original sequence alignment dynamic program. With each subproblem, you need to keep track of what gaps you insert, since the costs you incur in the current position depend on whether or not the previous subproblems inserted gaps. Blows up the number of subproblems and running time by a constant factor. Compute the length of a longest common substring of X and Y. (A substring is a consecutive subsequence of a string. So “bcd” is a substring of “abcdef”, whereas “bdf” is not.)CorrectSimilar dynamic programming to sequence alignment, with one subproblem for each Xi and Yj. 1 point5.Recall our dynamic programming algorithms for maximum-weight independent set, sequence alignment, and optimal binary search trees. The space requirements of these algorithms are proportional to the number of subproblems that get solved: Θ(n) (where n is the number of vertices), Θ(mn) (where m,n are the lengths of the two strings), and Θ(n2) (where n is the number of keys), respectively. Suppose we only want to compute the value of an optimal solution (the final answer of the first, forward pass) and don’t care about actually reconstructing an optimal solution (i.e., we skip the second, reverse pass over the table). How much space do you then really need to run each of three algorithms? Θ(1), Θ(n), and Θ(n2) Θ(n), Θ(mn), and Θ(n2)This should not be selectedBy throwing away old subproblems that you won’t ever need again, you can save space. Θ(1), Θ(n), and Θ(n)This should not be selectedHow do you avoid keeping track of Θ(n2) subproblem solutions when computing an optimal binary search tree? Θ(1), Θ(1), and Θ(n) Optional Theory Problems (Week 4) 10 minGive a dynamic programming algorithm that computes an optimal binary search tree and runs in O(n2) time. Programming Assignment #4Quiz: Programming Assignment #4 2 questionsQUIZProgramming Assignment #42 questionsTo Pass100% or higherAttempts10 every 12 hoursDeadlineAugust 13, 11:59 PM PDT 1 point1.In this programming problem and the next you’ll code up the knapsack algorithm from lecture. Let’s start with a warm-up. Download the text file below. knapsack1.txtThis file describes a knapsack instance, and it has the following format: [knapsack_size][number_of_items] [value_1] [weight_1] [value_2] [weight_2] … For example, the third line of the file is “50074 659”, indicating that the second item has value 50074 and size 659, respectively. You can assume that all numbers are positive. You should assume that item weights and the knapsack capacity are integers. In the box below, type in the value of the optimal solution. ADVICE: If you’re not getting the correct answer, try debugging your algorithm using some small test cases. And then post them to the discussion forum! 1 point2.This problem also asks you to solve a knapsack instance, but a much bigger one. Download the text file below. knapsack_big.txtThis file describes a knapsack instance, and it has the following format: [knapsack_size][number_of_items] [value_1] [weight_1] [value_2] [weight_2] … For example, the third line of the file is “50074 834558”, indicating that the second item has value 50074 and size 834558, respectively. As before, you should assume that item weights and the knapsack capacity are integers. This instance is so big that the straightforward iterative implemetation uses an infeasible amount of time and space. So you will have to be creative to compute an optimal solution. One idea is to go back to a recursive implementation, solving subproblems — and, of course, caching the results to avoid redundant work — only on an “as needed” basis. Also, be sure to think about appropriate data structures for storing and looking up solutions to subproblems. In the box below, type in the value of the optimal solution. ADVICE: If you’re not getting the correct answer, try debugging your algorithm using some small test cases. And then post them to the discussion forum! Final Exam (1 attempt per 24 hours)Info and FAQ for final exam 10 min Unlike the problem sets, you have only ONE attempt per 24 hours. Thus DO NOT START THE EXAM until you are ready. There are 10 questions, and each is worth 1 point. Many are of the “select all that apply” type, rather than the strict multiple choice format that is more common on the problem sets. You will receive partial credit for each option that you leave correctly checked or correctly unchecked. (So if you mark 4 out of the 5 options for a problem correctly, you’ll receive 0.8 out of the 1 point.) You need 7 points total to pass the exam. All questions are about material covered in the required (i.e., non-optional) videos. Roughly half of the questions follow fairly directly from the lecture material (assuming that you understand it thoroughly). Roughly a quarter of the questions are variations on problem set questions. Roughly a quarter of the questions demand more thought, for example by asking you to consider whether certain results from lecture do or do not extend to more general situations. Good luck! Quiz: Final Exam 10 questionsQUIZFinal Exam10 questionsTo Pass70% or higherAttempts1 every 1 dayDeadlineAugust 13, 11:59 PM PDT 1 point1.Consider a connected undirected graph with distinct edge costs. Which of the following are true? [Check all that apply.] Suppose the edge e is not the cheapest edge that crosses the cut (A,B). Then e does not belong to any minimum spanning tree. Suppose the edge e is the most expensive edge contained in the cycle C. Then e does not belong to any minimum spanning tree. Suppose the edge e is the cheapest edge that crosses the cut (A,B). Then e belongs to every minimum spanning tree. The minimum spanning tree is unique.correct 1 point2.You are given a connected undirected graph G with distinct edge costs, in adjacency list representation. You are also given the edges of a minimum spanning tree T of G. This question asks how quickly you can recompute the MST if we change the cost of a single edge. Which of the following are true? [RECALL: It is not known how to deterministically compute an MST from scratch in O(m) time, where m is the number of edges of G.] [Check all that apply.] Suppose e$\notin$T and we decrease the cost of e. Then, the new MST can be recomputed in O(m) deterministic time. Suppose e$\notin$T and we increase the cost of e. Then, the new MST can be recomputed in O(m) deterministic time. Suppose e∈T and we decrease the cost of e. Then, the new MST can be recomputed in O(m) deterministic time. Suppose e∈T and we increase the cost of e. Then, the new MST can be recomputed in O(m) deterministic time.correct 1 point3.Which of the following graph algorithms can be sped up using the heap data structure? Dijkstra’s single-source shortest-path algorithm (from Part 2). Prim’s minimum-spanning tree algorithm. Our dynamic programming algorithm for computing a maximum-weight independent set of a path graph. Kruskal’s minimum-spanning tree algorithm.correct 1 point4.Which of the following problems reduce, in a straightforward way, to the minimum spanning tree problem? [Check all that apply.] The minimum bottleneck spanning tree problem. That is, among all spanning trees of a connected graph with edge costs, compute one with the minimum-possible maximum edge cost. The maximum-cost spanning tree problem. That is, among all spanning trees of a connected graph with edge costs, compute one with the maximum-possible sum of edge costs. Given a connected undirected graph G=(V,E) with positive edge costs, compute a minimum-cost set F$\subseteq$E such that the graph (V,E−F) is acyclic. The single-source shortest-path problem.correct 1 point5.Recall the greedy clustering algorithm from lecture and the max-spacing objective function. Which of the following are true? [Check all that apply.] Suppose the greedy algorithm produces a k-clustering with spacing S. Then, if x,y are two points in a common cluster of this k-clustering, the distance between x and y is at most S. If the greedy algorithm produces a k-clustering with spacing S, then every other k-clustering has spacing at most S. This greedy clustering algorithm can be viewed as Prim’s minimum spanning tree algorithm, stopped early. If the greedy algorithm produces a k-clustering with spacing S, then the distance between every pair of points chosen by the greedy algorithm (one pair per iteration) is at most S.correct 1 point6.We are given as input a set of n jobs, where job j has a processing time pj and a deadline dj. Recall the definition of completion times Cj from the video lectures. Given a schedule (i.e., an ordering of the jobs), we define the lateness lj of job j as the amount of time Cj−dj after its deadline that the job completes, or as 0 if Cj≤dj. Our goal is to minimize the total lateness, ∑jlj. Which of the following greedy rules produces an ordering that minimizes the total lateness? You can assume that all processing times and deadlines are distinct. WARNING: This is similar to but not identical to a problem from Problem Set #1 (the objective function is different). Schedule the requests in increasing order of processing time pj None of the other options are correct Schedule the requests in increasing order of deadline dj Schedule the requests in increasing order of the product dj$\cdot$pjcorrect 1 point7.Consider an alphabet with five letters, {a,b,c,d,e}, and suppose we know the frequencies fa=0.28, fb=0.27, fc=0.2, fd=0.15, and fe=0.1. What is the expected number of bits used by Huffman’s coding scheme to encode a 1000-letter document? 2230 2250 2520correct 24501 point8.Which of the following extensions of the Knapsack problem can be solved in time polynomial in n, the number of items, and M, the largest number that appears in the input? [Check all that apply.] You are given n items with positive integer values and sizes, as usual, and m positive integer capacities, W1,W2,…,Wm . These denote the capacities of m different Knapsacks, where m could be as large as Θ(n). The problem is to pack items into these knapsacks to maximize the total value of the packed items. You are not allowed to split a single item between two of the knapsacks. You are given n items with positive integer values and sizes, and a positive integer capacity W, as usual. The problem is to compute the max-value set of items with total size exactly W. If no such set exists, the algorithm should correctly detect that fact. You are given n items with positive integer values and sizes, as usual, and two positive integer capacities, W1 and W2. The problem is to pack items into these two knapsacks (of capacities W1 and W2) to maximize the total value of the packed items. You are not allowed to split a single item between the two knapsacks. You are given n items with positive integer values and sizes, and a positive integer capacity W, as usual. You are also given a budget k≤n on the number of items that you can use in a feasible solution. The problem is to compute the max-value set of at most k items with total size at most W.correct 1 point9.The following problems all take as input two strings X and Y, of length m and n, over some alphabet Σ. Which of them can be solved in O(mn) time? [Check all that apply.] Compute the length of a longest common subsequence of X and Y. (Recall a subsequence need not be consecutive. For example, the longest common subsequence of “abcdef” and “afebcd” is “abcd”.) Compute the length of a longest common substring of X and Y. (A substring is a consecutive subsequence of a string. So “bcd” is a substring of “abcdef”, whereas “bdf” is not.) Consider the following variation of sequence alignment. Instead of a single gap penalty αgap, you’re given two numbers a and b. The penalty of inserting k gaps in a row is now defined as ak+b, rather than kαgap. Other penalties (for matching two non-gaps) are defined as before. The goal is to compute the minimum-possible penalty of an alignment under this new cost model. Assume that X and Y have the same length n. Does there exist a permutation f, mapping each i∈{1,2,…,n} to a distinct f(i)∈{1,2,…,n}, such that Xi=Yf(i) for every i=1,2,…,n?correct 1 point10.Consider an instance of the optimal binary search tree problem with 7 keys (say 1,2,3,4,5,6,7 in sorted order) and frequencies w1=.2,w2=.05,w3=.17,w4=.1,w5=.2,w6=.03,w7=.25. What is the minimum-possible average search time of a binary search tree with these keys? 2.18 2.33 2.29 2.23correct Shortest Paths Revisited, NP-Complete Problems and What To Do About ThemCourse can be found hereMy certificate can be found hereLecture slides can be found here Week 1Week 2Week 3Week 4]]></content>
      <categories>
        <category>Coursera</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Stanford</tag>
        <tag>Coursera</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Courses]]></title>
    <url>%2F2017%2F07%2F17%2FCoursera%20Courses%2F</url>
    <content type="text"><![CDATA[Stay stuned… Course University Number Completed Notebook Python for Everybody Specialization University of Michigan: 5 0 None Applied Data Science with Python Specialization University of Michigan: 5 0 None Data Science Specialization Johns Hopkins University: 10 0 None Data Visualization with Tableau Specialization UC, Davis: 5 0 None Machine Learning Stanford 1 1 here Machine Learning Specialization UW 4 4 here Algorithms Specialization Stanford 4 3 here Data Mining Specialization UIUC 6 2 here Probabilistic Graphical Models Specialization Stanford 3 0 None Data Structures and Algorithms Specialization UCSanDiego 6 0 None Data Analysis and Interpretation Specialization Wesleyan University 5 0 None Recommender Systems Specialization University of Minnesota 5 0 None Deep Learning Specialization deeplearning.ai 5 3 here Cloud Computing Specialization UIUC 6 0 None Fundamentals of Computing Specialization Rice University 7 1 here Data Science at Scale Specialization UW 4 0 None Data Science Math Skills Duke University 1 0 None Excel to MySQL: Analytic Techniques for Business Specialization Duke 5 0 None Learn to Program: Crafting Quality Code University of Toronto 1 0 None Neural Networks for Machine Learning University of Toronto 1 0 None 程序设计与算法 Specialization Peking University 7 1 here Cisco Networking Basics Specialization cisco 5 0 None Introduction to TCP/IP Yonsei University 1 0 None Introduction to Scripting in Python Specialization Rice 4 0 None Big Data for Data Engineers Specialization Yandex 5 0 None Advanced Machine Learning Specialization HSE 7 1 None SQL for Data Science UC Davis 1 0 None Game Theory Stanford 1 0 None Game Theory II: Advanced Applications Stanford 1 0 None Robotics Specialization UP 6 0 None Control of Mobile Robots GIT 1 0 None 0 None 0 None 0 None 0 None]]></content>
      <tags>
        <tag>Coursera</tag>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud651 DA with R Notebook]]></title>
    <url>%2F2017%2F06%2F22%2FUdacity%20ud651%20DA%20with%20R%20Notebook%2F</url>
    <content type="text"><![CDATA[on the waystay stuned…course can be found here]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud170 Intro to DA Notebook]]></title>
    <url>%2F2017%2F06%2F20%2FUdacity%20ud170%20Intro%20to%20DA%20Notebook%2F</url>
    <content type="text"><![CDATA[Final ProjectThis course can be found in udacity ud170. Data Analysis ProcessSetting Up Your SystemOtherwise, you can find the free course here. Intro to CSVsIf you’d like to learn more about data wrangling, check out the Udacity course Data Wrangling with MongoDB. CSVs in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/22sQCo6ovH0.mp4 12345678910111213141516171819202122232425262728import unicodecsvenrollments_filename = &apos;/datasets/ud170/udacity-students/enrollments.csv&apos;## Longer version of code (replaced with shorter, equivalent version below)# enrollments = []# f = open(enrollments_filename, &apos;rb&apos;)# reader = unicodecsv.DictReader(f)# for row in reader:# enrollments.append(row)# f.close()with open(enrollments_filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) enrollments = list(reader) ### Write code similar to the above to load the engagement### and submission data. The data is stored in files with### the given filenames. Then print the first row of each### table to make sure that your code works. You can use the### &quot;Test Run&quot; button to see the output of your code.engagement_filename = &apos;/datasets/ud170/udacity-students/daily_engagement.csv&apos;submissions_filename = &apos;/datasets/ud170/udacity-students/project_submissions.csv&apos; daily_engagement = None # Replace this with your codeproject_submissions = None # Replace this with your code 123456789with open(engagement_filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) daily_engagement = list(reader) print daily_engagement[0]with open(submissions_filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) project_submissions = list(reader)print project_submissions[0] Python’s csv ModuleThis page contains documentation for Python’s csv module. Instead of csv, you’ll be using unicodecsv in this course. unicodecsv works exactly the same as csv, but it comes with Anaconda and has support for unicode. The csv documentation page is still the best way to learn how to use the unicodecsv library, since the two libraries work exactly the same way. Iterators in PythonThis page explains the difference between iterators and lists in Python, and how to use iterators. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Fixing Data Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7NSYtdVrlRE.mp4 Questions about Student Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AO8vSyAtfV4.mp4 Investigating the DataNow you’ve started the data wrangling process by loading the data and making sure it’s in a good format. The next step is to investigate a bit and see if there are any inconsistencies or problems in the data that you’ll need to clean up. For each of the three files you’ve loaded, find the total number of rows in the csv and the number of unique students. To find the number of unique students in each table, you might want to try creating a set of the account keys. Again, in case you’re not finished with your local setup, you can complete this exercise in the Udacity code editor. You’ll need to run the next exercise locally, though, so if you haven’t finished setting up, you should do that now.1234567891011121314151617181920212223import unicodecsvdef read_csv(filename): with open(filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) return list(reader)enrollments = read_csv(&apos;/datasets/ud170/udacity-students/enrollments.csv&apos;)daily_engagement = read_csv(&apos;/datasets/ud170/udacity-students/daily_engagement.csv&apos;)project_submissions = read_csv(&apos;/datasets/ud170/udacity-students/project_submissions.csv&apos;) ### For each of these three tables, find the number of rows in the table and### the number of unique students in the table. To find the number of unique### students, you might want to create a set of the account keys in each table.enrollment_num_rows = 0 # Replace this with your codeenrollment_num_unique_students = 0 # Replace this with your codeengagement_num_rows = 0 # Replace this with your codeengagement_num_unique_students = 0 # Replace this with your codesubmission_num_rows = 0 # Replace this with your codesubmission_num_unique_students = 0 # Replace this with your code 123456789101112131415161718192021222324252627def unique_num(data): unique_data = set() for element in data: if &apos;acct&apos; in element: element[&apos;account_key&apos;] = element[&apos;acct&apos;] del element[&apos;acct&apos;] unique_data.add(element[&apos;account_key&apos;]) return len(unique_data)print enrollments[0]enrollment_num_rows = len(enrollments) # Replace this with your codeenrollment_num_unique_students = unique_num(enrollments) # Replace this with your codeprint enrollment_num_rowsprint enrollment_num_unique_students print daily_engagement[0]engagement_num_rows = len(daily_engagement) # Replace this with your codeprint engagement_num_rowsengagement_num_unique_students = unique_num(daily_engagement) # Replace this with your codeprint engagement_num_unique_studentsprint project_submissions[0]submission_num_rows = len(project_submissions) # Replace this with your codesubmission_num_unique_students = unique_num(project_submissions) # Replace this with your codeprint submission_num_rowsprint submission_num_unique_students Problems in the DataRemoving an Element from a DictionaryIf you’re not sure how to remove an element from a dictionary, this post might be helpful. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Updated Code for Previous ExerciseAfter running the above code, Caroline also shows rewriting the solution from the previous exercise to the following code:1234567891011121314def get_unique_students(data): unique_students = set() for data_point in data: unique_students.add(data_point[&apos;account_key&apos;]) return unique_studentslen(enrollments)unique_enrolled_students = get_unique_students(enrollments)len(unique_enrolled_students)len(daily_engagement)unique_engagement_students = get_unique_students(daily_engagement)len(unique_engagement_students)len(project_submissions)unique_project_submitters = get_unique_students(project_submissions)len(unique_project_submitters) Missing Engagement RecordsPrinting a Single RowThis page describes how to use Python’s break statement, which might be helpful for printing only a single problem record. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Checking for More Problem RecordsTracking Down the Remaining ProblemsRefining the QuestionExploratory Data AnalysisIf you’d like to learn more about the exploratory phase of the data analysis process, check out the Udacity course Data Analysis with R. SolutionsDAND students click here for solution code IPND students: Look at the end of this lesson for Quiz Solutions Getting Data from First Weekhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/adqc5fF5B8Y.mp4https://classroom.udacity.com/courses/ud170/lessons/5430778793/concepts/53961386350923 Note that paid students may have canceled from other courses before paying, and the suggested solution will retain records from these other enrollments. Indulge CuriosityExploring Student EngagementDebugging Data Analysis CodeLessons Completed in First WeekNumber of Visits in the First Weekhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5GYA5j1fqBU.mp4https://classroom.udacity.com/courses/ud170/lessons/5430778793/concepts/53961386450923 Splitting out Passing StudentsQuiz: Comparing the Two Student GroupsQuiz: Making HistogramsVisualizing dataEven though you know the mean, standard deviation, maximum, and minimum of various metrics, there are a lot of other facts about each metric that would be nice to know. Are more values close to the minimum or the maximum? What is the median? And so on. Instead of printing out more statistics, at this point it makes sense to visualize the data using a histogram. Making histograms in PythonTo make a histogram in Python, you can use the matplotlib library, which comes with Anaconda. The following code will make a histogram of an example list of data points called data.12345data = [1, 2, 1, 3, 3, 1, 4, 2]%matplotlib inlineimport matplotlib.pyplot as pltplt.hist(data) The line %matplotlib inline is specifically for IPython notebook, and causes your plots to appear in your notebook rather than a new window. If you are not using IPython notebook, you should not include this line, and instead you should add the line plt.show() at the bottom to show the plot in a new window. Making histograms of student dataNow use this method to make a histogram of each of the three metrics we looked at for both students who pass the subway project and students who don’t. That is, you should create 6 histograms. Do any of the metrics have histograms with very different shapes for students who pass the subway project vs. those who don’t? You can also create histograms of the metrics you explored on your own if you’d like. Are your Results Just Noise?StatisticsIf you’d like to learn more about statistics, which you can use to rigorously determine how likely it is that your results are due to chance, check out the Udacity courses Intro to Descriptive Statistics and Intro to Inferential Statistics. Correlation Does Not Imply CausationCheese and Bedsheet TanglingTo see the plot shown in the video, as well as many other amusing or strange correlations, check out this website. A/B TestingTo learn more about using online experiments to determine whether one change causes another, take the Udacity course A/B Testing. Predicting Based on Many FeaturesMachine LearningTo learn more about using machine learning to automatically make predictions, take the Udacity course Intro to Machine Learning. CommunicationQuiz: Improving Plots and Sharing FindingsAdding labels and titlesIn matplotlib, you can add axis labels using plt.xlabel(&quot;Label for x axis&quot;) and plt.ylabel(&quot;Label for y axis&quot;). For histograms, you usually only need an x-axis label, but for other plot types a y-axis label may also be needed. You can also add a title using plt.title(&quot;Title of plot&quot;). Making plots look nicer with seabornYou can automatically make matplotlib plots look nicer using the seaborn library. This library is not automatically included with Anaconda, but Anaconda includes something called a package manager to make it easier to add new libraries. The package manager is called conda, and to use it, you should open the Command Prompt (on a PC) or terminal (on Mac or Linux), and type the command conda install seaborn. If you are using a different Python installation than Anaconda, you may have a different package manager. The most common ones are pip and easy_install, and you can use them with the commands pip install seaborn or easy_install seaborn respectively. Once you have installed seaborn, you can import it anywhere in your code using the line import seaborn as sns. Then any plot you make afterwards will automatically look better. Give it a try! If you’re wondering why the abbreviation for seaborn is sns, it’s because seaborn was named after the character Samuel Norman Seaborn from the show The West Wing, and sns are his initials. The seaborn package also includes some extra functions you can use to make complex plots that would be difficult in matplotlib. We won’t be covering those in this course, but if you’d like to see what functions seaborn has available, you can look through the documentation. Adding extra arguments to your plotYou’ll also frequently want to add some arguments to your plot to tune how it looks. You can see what arguments are available on the documentation page for the hist function. One common argument to pass is the bins argument, which sets the number of bins used by your histogram. For example, plt.hist(data, bins=20) would make sure your histogram has 20 bins. Improving one of your plotsUse these techniques to improve at least one of the plots you made earlier. Sharing your findingsFinally, decide which of the discoveries you made this lesson you would most want to communicate to someone else, and write a forum post sharing your findings. Data Analysis and Related TermsConclusionL1_Solution_Code.ipynb Quiz SolutionsCSVs in Python12345678910import unicodecsvdef read_csv(filename): with open(filename, &apos;rb&apos;) as f: reader = unicodecsv.DictReader(f) return list(reader)enrollments = read_csv(&apos;enrollments.csv&apos;)daily_engagement = read_csv(&apos;daily_engagement.csv&apos;)project_submissions = read_csv(&apos;project_submissions.csv&apos;) Investigating the Data1234567891011121314151617181920len(enrollments)unique_enrolled_students = set()for enrollment in enrollments: unique_enrolled_students.add(enrollment[&apos;account_key&apos;])len(unique_enrolled_students)len(daily_engagement)unique_engagement_students = set()for engagement_record in daily_engagement: unique_engagement_students.add(engagement_record[&apos;acct&apos;])len(unique_engagement_students)len(project_submissions)unique_project_submitters = set()for submission in project_submissions: unique_project_submitters.add(submission[&apos;account_key&apos;])len(unique_project_submitters) Problems in the Data123for engagement_record in daily_engagement: engagement_record[&apos;account_key&apos;] = engagement_record[&apos;acct&apos;] del[engagement_record[&apos;acct&apos;]] Missing engagement records12345for enrollment in enrollments: student = enrollment[&apos;account_key&apos;] if student not in unique_engagement_students: print enrollment break Checking for more problem records1234567num_problem_students = 0for enrollment in enrollments: student = enrollment[&apos;account_key&apos;] if (student not in unique_engagement_students and enrollment[&apos;join_date&apos;] != enrollment[&apos;cancel_date&apos;]): print enrollment num_problem_students += 1 num_problem_students Refining the Question12345678910paid_students = &#123;&#125;for enrollment in non_udacity_enrollments: if (not enrollment[&apos;is_canceled&apos;] or enrollment[&apos;days_to_cancel&apos;] &gt; 7): account_key = enrollment[&apos;account_key&apos;] enrollment_date = enrollment[&apos;join_date&apos;] if (account_key not in paid_students or enrollment_date &gt; paid_students[account_key]): paid_students[account_key] = enrollment_datelen(paid_students) Note that if you switch the order of the second if statement like so if (enrollment_date &gt; paid_students[account_key] oraccount_key not in paid_students)you will most likely get an error. Why do you think that is? Check out this Stackoverflow discussion to find out more: http://stackoverflow.com/questions/13960657/does-python-evaluate-ifs-conditions-lazily Getting Data from First Week1234567891011121314151617181920212223242526272829def within_one_week(join_date, engagement_date): time_delta = engagement_date - join_date return time_delta.days &lt; 7def remove_free_trial_cancels(data): new_data = [] for data_point in data: if data_point[&apos;account_key&apos;] in paid_students: new_data.append(data_point) return new_datapaid_enrollments = remove_free_trial_cancels(non_udacity_enrollments)paid_engagement = remove_free_trial_cancels(non_udacity_engagement)paid_submissions = remove_free_trial_cancels(non_udacity_submissions)print len(paid_enrollments)print len(paid_engagement)print len(paid_submissions)paid_engagement_in_first_week = []for engagement_record in paid_engagement: account_key = engagement_record[&apos;account_key&apos;] join_date = paid_students[account_key] engagement_record_date = engagement_record[&apos;utc_date&apos;] if within_one_week(join_date, engagement_record_date): paid_engagement_in_first_week.append(engagement_record)len(paid_engagement_in_first_week) Debugging Data Analysis CodeHere is the code Caroline shows in the solution video:12345678910111213student_with_max_minutes = Nonemax_minutes = 0for student, total_minutes in total_minutes_by_account.items(): if total_minutes &gt; max_minutes: max_minutes = total_minutes student_with_max_minutes = studentmax_minutesfor engagement_record in paid_engagement_in_first_week: if engagement_record[&apos;account_key&apos;] == student_with_max_minutes: print engagement_record Alternatively, you can find the account key with the maximum minutes using this shorthand notation:1max(total_minutes_by_account.items(), key=lambda pair: pair[1]) Fixing Bug in within_one_week()She also updated the code for the within_one_week function to the following:123def within_one_week(join_date, engagement_date): time_delta = engagement_date - join_date return time_delta.days &gt;= 0 and time_delta.days &lt; 7 Lessons Completed in First WeekFirst, Caroline refactors the given code to analyze total minutes spent in the first week into the following:123456789101112131415161718192021222324252627282930313233from collections import defaultdictdef group_data(data, key_name): grouped_data = defaultdict(list) for data_point in data: key = data_point[key_name] grouped_data[key].append(data_point) return grouped_dataengagement_by_account = group_data(paid_engagement_in_first_week, &apos;account_key&apos;)def sum_grouped_items(grouped_data, field_name): summed_data = &#123;&#125; for key, data_points in grouped_data.items(): total = 0 for data_point in data_points: total += data_point[field_name] summed_data[key] = total return summed_datatotal_minutes_by_account = sum_grouped_items(engagement_by_account, &apos;total_minutes_visited&apos;)import numpy as npdef describe_data(data): print &apos;Mean:&apos;, np.mean(data) print &apos;Standard deviation:&apos;, np.std(data) print &apos;Minimum:&apos;, np.min(data) print &apos;Maximum:&apos;, np.max(data)describe_data(total_minutes_by_account.values()) Then she called the functions she created to analyze the lessons completed in the first week as follows:123lessons_completed_by_account = sum_grouped_items(engagement_by_account, &apos;lessons_completed&apos;)describe_data(lessons_completed_by_account.values()) Number of Visits in the First WeekHere is the code Caroline shows in the solution video. First she ran this code to create the has_visited field:12345for engagement_record in paid_engagement: if engagement_record[&apos;num_courses_visited&apos;] &gt; 0: engagement_record[&apos;has_visited&apos;] = 1 else: engagement_record[&apos;has_visited&apos;] = 0 Then, after recreating the engagement_by_account dictionary with the updated data, she ran the following code to analyze days visited in the first week:123days_visited_by_account = sum_grouped_items(engagement_by_account, &apos;has_visited&apos;)describe_data(days_visited_by_account.values()) Splitting out Passing StudentsHere is the code Caroline shows in the solution video:12345678910111213141516171819202122232425subway_project_lesson_keys = [&apos;746169184&apos;, &apos;3176718735&apos;]pass_subway_project = set()for submission in paid_submissions: project = submission[&apos;lesson_key&apos;] rating = submission[&apos;assigned_rating&apos;] if ((project in subway_project_lesson_keys) and (rating == &apos;PASSED&apos; or rating == &apos;DISTINCTION&apos;)): pass_subway_project.add(submission[&apos;account_key&apos;])len(pass_subway_project)passing_engagement = []non_passing_engagement = []for engagement_record in paid_engagement_in_first_week: if engagement_record[&apos;account_key&apos;] in pass_subway_project: passing_engagement.append(engagement_record) else: non_passing_engagement.append(engagement_record)print len(passing_engagement)print len(non_passing_engagement) Comparing the Two Student GroupsHere is the code Caroline shows in the solution video:12345678910111213141516171819202122232425262728293031323334353637383940414243444546passing_engagement_by_account = group_data(passing_engagement, &apos;account_key&apos;)non_passing_engagement_by_account = group_data(non_passing_engagement, &apos;account_key&apos;)print &apos;non-passing students:&apos;non_passing_minutes = sum_grouped_items( non_passing_engagement_by_account, &apos;total_minutes_visited&apos;)describe_data(non_passing_minutes.values())​print &apos;passing students:&apos;passing_minutes = sum_grouped_items( passing_engagement_by_account, &apos;total_minutes_visited&apos;)describe_data(passing_minutes.values())print &apos;non-passing students:&apos;non_passing_lessons = sum_grouped_items( non_passing_engagement_by_account, &apos;lessons_completed&apos;)describe_data(non_passing_lessons.values())print &apos;passing students:&apos;passing_lessons = sum_grouped_items( passing_engagement_by_account, &apos;lessons_completed&apos;)describe_data(passing_lessons.values())print &apos;non-passing students:&apos;non_passing_visits = sum_grouped_items( non_passing_engagement_by_account, &apos;has_visited&apos;)describe_data(non_passing_visits.values())print &apos;passing students:&apos;passing_visits = sum_grouped_items( passing_engagement_by_account, &apos;has_visited&apos;)describe_data(passing_visits.values()) Making HistogramsHere is the code Caroline shows in the solution video:123456789101112%pylab inlineimport matplotlib.pyplot as pltimport numpy as np# Summarize the given datadef describe_data(data): print &apos;Mean:&apos;, np.mean(data) print &apos;Standard deviation:&apos;, np.std(data) print &apos;Minimum:&apos;, np.min(data) print &apos;Maximum:&apos;, np.max(data) plt.hist(data) Fixing the Number of BinsTo change how many bins are shown for each plot, try using the bins argument to the hist function. You can find documentation for the hist function and the arguments it takes here. Improving Plots and Sharing FindingsHere is the code Caroline shows in the solution video:1234567891011import seaborn as snsplt.hist(non_passing_visits.values(), bins=8)plt.xlabel(&apos;Number of days&apos;)plt.title(&apos;Distribution of classroom visits in the first week &apos; + &apos;for students who do not pass the subway project&apos;)plt.hist(passing_visits.values(), bins=8)plt.xlabel(&apos;Number of days&apos;)plt.title(&apos;Distribution of classroom visits in the first week &apos; + &apos;for students who pass the subway project&apos;) Quiz: Survey Says!Numpy and Pandas for 1D DataIntroductionQuiz: Gapminder DataGapminder dataThe data in this lesson was obtained from the site gapminder.org. The variables included are: Aged 15+ Employment Rate (%)Life Expectancy (years)GDP/capita (US$, inflation adjusted)Primary school completion (% of boys)Primary school completion (% of girls)You can also obtain the data to anlayze on your own from the Downloadables section. One-Dimensional Data in NumPy and PandasQuiz: NumPy Arrays Pandas Numpy Series Array similarity and difference between numpy array and python list similarity difference for loop numpy array have the same type 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import numpy as np# First 20 countries with employment datacountries = np.array([ &apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;, &apos;Bosnia and Herzegovina&apos;])# Employment data in 2007 for those 20 countriesemployment = np.array([ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076])# Change False to True for each block of code to see what it does# Accessing elementsif False: print countries[0] print countries[3]# Slicingif False: print countries[0:3] print countries[:3] print countries[17:] print countries[:]# Element typesif False: print countries.dtype print employment.dtype print np.array([0, 1, 2, 3]).dtype print np.array([1.0, 1.5, 2.0, 2.5]).dtype print np.array([True, False, True]).dtype print np.array([&apos;AL&apos;, &apos;AK&apos;, &apos;AZ&apos;, &apos;AR&apos;, &apos;CA&apos;]).dtype# Loopingif False: for country in countries: print &apos;Examining country &#123;&#125;&apos;.format(country) for i in range(len(countries)): country = countries[i] country_employment = employment[i] print &apos;Country &#123;&#125; has employment &#123;&#125;&apos;.format(country, country_employment)# Numpy functionsif False: print employment.mean() print employment.std() print employment.max() print employment.sum()def max_employment(countries, employment): &apos;&apos;&apos; Fill in this function to return the name of the country with the highest employment in the given employment data, and the employment in that country. &apos;&apos;&apos; max_country = None # Replace this with your code max_value = None # Replace this with your code return (max_country, max_value) solution12345678910def max_employment(countries, employment): &apos;&apos;&apos; Fill in this function to return the name of the country with the highest employment in the given employment data, and the employment in that country. &apos;&apos;&apos; max_country = countries[employment.argmax()] # Replace this with your code max_value = employment.max() # Replace this with your code return (max_country, max_value) argmax() return the position of max() Quiz: Vectorized Operations+ operation:python | numpy—|—list concatenation | vector addition Quiz: Multiplying by a ScalarQuiz: Calculate Overall Completion RateBitwise OperationsSee this article for more information about bitwise operations. In NumPy, a &amp; b performs a bitwise and of a and b. This is not necessarily the same as a logical and, if you wanted to see if matching terms in two integer vectors were non-zero. However, if a and b are both arrays of booleans, rather than integers, bitwise and and logical and are the same thing. If you want to perform a logical and on integer vectors, then you can use the NumPy function np.logical_and(a, b) or convert them into boolean vectors first. Similarly, a | b performs a bitwise or, and ~a performs a bitwise not. However, if your arrays contain booleans, these will be the same as performing logical or and logical not. NumPy also has similar functions for performing these logical operations on integer-valued arrays. For the quiz, assume that the number of males and females are equal i.e. we can take a simple average to get an overall completion rate. In the solution, we may want to / 2. instead of just / 2. This is because in Python 2, dividing an integer by another integer (2) drops fractions, so if our inputs are also integers, we may end up losing information. If we divide by a float (2.) then we will definitely retain decimal values. Erratum: The output of cell [3] in the solution video is incorrect: it appears that the male variable has not been set to the proper value set in cell [2]. All values except for the first will be different. The correct output in cell Out[3]: should instead start with: array([ 192.83205, 205.28855, 202.82258, 186.63257, 206.91115, 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import numpy as np# Change False to True for each block of code to see what it does# Arithmetic operations between 2 NumPy arraysif False: a = np.array([1, 2, 3, 4]) b = np.array([1, 2, 1, 2]) print a + b print a - b print a * b print a / b print a ** b # Arithmetic operations between a NumPy array and a single numberif False: a = np.array([1, 2, 3, 4]) b = 2 print a + b print a - b print a * b print a / b print a ** b # Logical operations with NumPy arraysif False: a = np.array([True, True, False, False]) b = np.array([True, False, True, False]) print a &amp; b print a | b print ~a print a &amp; True print a &amp; False print a | True print a | False # Comparison operations between 2 NumPy Arraysif False: a = np.array([1, 2, 3, 4, 5]) b = np.array([5, 4, 3, 2, 1]) print a &gt; b print a &gt;= b print a &lt; b print a &lt;= b print a == b print a != b # Comparison operations between a NumPy array and a single numberif False: a = np.array([1, 2, 3, 4]) b = 2 print a &gt; b print a &gt;= b print a &lt; b print a &lt;= b print a == b print a != b # First 20 countries with school completion datacountries = np.array([ &apos;Algeria&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Aruba&apos;, &apos;Austria&apos;,&apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Bolivia&apos;, &apos;Botswana&apos;, &apos;Brunei&apos;, &apos;Bulgaria&apos;, &apos;Burkina Faso&apos;, &apos;Burundi&apos;, &apos;Cambodia&apos;, &apos;Cameroon&apos;, &apos;Cape Verde&apos;])# Female school completion rate in 2007 for those 20 countriesfemale_completion = np.array([ 97.35583, 104.62379, 103.02998, 95.14321, 103.69019, 98.49185, 100.88828, 95.43974, 92.11484, 91.54804, 95.98029, 98.22902, 96.12179, 119.28105, 97.84627, 29.07386, 38.41644, 90.70509, 51.7478 , 95.45072])# Male school completion rate in 2007 for those 20 countriesmale_completion = np.array([ 95.47622, 100.66476, 99.7926 , 91.48936, 103.22096, 97.80458, 103.81398, 88.11736, 93.55611, 87.76347, 102.45714, 98.73953, 92.22388, 115.3892 , 98.70502, 37.00692, 45.39401, 91.22084, 62.42028, 90.66958])def overall_completion_rate(female_completion, male_completion): &apos;&apos;&apos; Fill in this function to return a NumPy array containing the overall school completion rate for each country. The arguments are NumPy arrays giving the female and male completion of each country in the same order. &apos;&apos;&apos; return None solution12345678def overall_completion_rate(female_completion, male_completion): &apos;&apos;&apos; Fill in this function to return a NumPy array containing the overall school completion rate for each country. The arguments are NumPy arrays giving the female and male completion of each country in the same order. &apos;&apos;&apos; return (female_completion + male_completion ) /2. Quiz: Standardizing Dataquiz123456789101112131415161718192021222324252627282930313233343536import numpy as np# First 20 countries with employment datacountries = np.array([ &apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;, &apos;Bosnia and Herzegovina&apos;])# Employment data in 2007 for those 20 countriesemployment = np.array([ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076])# Change this country name to change what country will be printed when you# click &quot;Test Run&quot;. Your function will be called to determine the standardized# score for this country for each of the given 5 Gapminder variables in 2007.# The possible country names are available in the Downloadables section.country_name = &apos;United States&apos;def standardize_data(values): &apos;&apos;&apos; Fill in this function to return a standardized version of the given values, which will be in a NumPy array. Each value should be translated into the number of standard deviations that value is away from the mean of the data. (A positive number indicates a value higher than the mean, and a negative number indicates a value lower than the mean.) &apos;&apos;&apos; return None solution12def standardize_data(values): return (values - values.mean()) / values.std() Quiz: NumPy Index Arraysquiz1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import numpy as np# Change False to True for each block of code to see what it does# Using index arraysif False: a = np.array([1, 2, 3, 4]) b = np.array([True, True, False, False]) print a[b] print a[np.array([True, False, True, False])] # Creating the index array using vectorized operationsif False: a = np.array([1, 2, 3, 2, 1]) b = (a &gt;= 2) print a[b] print a[a &gt;= 2] # Creating the index array using vectorized operations on another arrayif False: a = np.array([1, 2, 3, 4, 5]) b = np.array([1, 2, 3, 2, 1]) print b == 2 print a[b == 2]def mean_time_for_paid_students(time_spent, days_to_cancel): &apos;&apos;&apos; Fill in this function to calculate the mean time spent in the classroom for students who stayed enrolled at least (greater than or equal to) 7 days. Unlike in Lesson 1, you can assume that days_to_cancel will contain only integers (there are no students who have not canceled yet). The arguments are NumPy arrays. time_spent contains the amount of time spent in the classroom for each student, and days_to_cancel contains the number of days until each student cancel. The data is given in the same order in both arrays. &apos;&apos;&apos; return None# Time spent in the classroom in the first week for 20 studentstime_spent = np.array([ 12.89697233, 0. , 64.55043217, 0. , 24.2315615 , 39.991625 , 0. , 0. , 147.20683783, 0. , 0. , 0. , 45.18261617, 157.60454283, 133.2434615 , 52.85000767, 0. , 54.9204785 , 26.78142417, 0.])# Days to cancel for 20 studentsdays_to_cancel = np.array([ 4, 5, 37, 3, 12, 4, 35, 38, 5, 37, 3, 3, 68, 38, 98, 2, 249, 2, 127, 35]) sloution12def mean_time_for_paid_students(time_spent, days_to_cancel): return time_spent[days_to_cancel &gt;=7].mean() Quiz: + vs. +=notice12345import numpy as npa = np.array([1,2,3,4])b = aa += np.array([1,1,1,])print b array([2,3,4,5])12345import numpy as npa = np.array([1,2,3,4])b = aa = a + np.array([1,1,1,])print b array([1,2,3,4]) Quiz: In-Place vs. Not In-Placenotice12345import numpy as npa = np.array([1,2,3,4])slice = a[:3]slice[0] = 100print a array([100,2,3,4]) Quiz: Pandas Seriesquiz12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import pandas as pdcountries = [&apos;Albania&apos;, &apos;Algeria&apos;, &apos;Andorra&apos;, &apos;Angola&apos;, &apos;Antigua and Barbuda&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;]life_expectancy_values = [74.7, 75. , 83.4, 57.6, 74.6, 75.4, 72.3, 81.5, 80.2, 70.3, 72.1, 76.4, 68.1, 75.2, 69.8, 79.4, 70.8, 62.7, 67.3, 70.6]gdp_values = [ 1681.61390973, 2155.48523109, 21495.80508273, 562.98768478, 13495.1274663 , 9388.68852258, 1424.19056199, 24765.54890176, 27036.48733192, 1945.63754911, 21721.61840978, 13373.21993972, 483.97086804, 9783.98417323, 2253.46411147, 25034.66692293, 3680.91642923, 366.04496652, 1175.92638695, 1132.21387981]# Life expectancy and gdp data in 2007 for 20 countrieslife_expectancy = pd.Series(life_expectancy_values)gdp = pd.Series(gdp_values)# Change False to True for each block of code to see what it does# Accessing elements and slicingif False: print life_expectancy[0] print gdp[3:6] # Loopingif False: for country_life_expectancy in life_expectancy: print &apos;Examining life expectancy &#123;&#125;&apos;.format(country_life_expectancy) # Pandas functionsif False: print life_expectancy.mean() print life_expectancy.std() print gdp.max() print gdp.sum()# Vectorized operations and index arraysif False: a = pd.Series([1, 2, 3, 4]) b = pd.Series([1, 2, 1, 2]) print a + b print a * 2 print a &gt;= 3 print a[a &gt;= 3] def variable_correlation(variable1, variable2): &apos;&apos;&apos; Fill in this function to calculate the number of data points for which the directions of variable1 and variable2 relative to the mean are the same, and the number of data points for which they are different. Direction here means whether each value is above or below its mean. You can classify cases where the value is equal to the mean for one or both variables however you like. Each argument will be a Pandas series. For example, if the inputs were pd.Series([1, 2, 3, 4]) and pd.Series([4, 5, 6, 7]), then the output would be (4, 0). This is because 1 and 4 are both below their means, 2 and 5 are both below, 3 and 6 are both above, and 4 and 7 are both above. On the other hand, if the inputs were pd.Series([1, 2, 3, 4]) and pd.Series([7, 6, 5, 4]), then the output would be (0, 4). This is because 1 is below its mean but 7 is above its mean, and so on. &apos;&apos;&apos; num_same_direction = None # Replace this with your code num_different_direction = None # Replace this with your code return (num_same_direction, num_different_direction) solution12345678910def variable_correlation(variable1, variable2): both_above = (variable1 &gt; variable1.mean()) &amp; \ (variable2 &gt; variable2.mean()) both_below = (variable1 &lt; variable1.mean()) &amp; \ (variable2 &lt; variable2.mean()) is_same_direction = both_above | both_below num_same_direction = is_same_direction.sum() # Replace this with your code num_different_direction = len(variable1) - num_same_direction # Replace this with your code return (num_same_direction, num_different_direction) Quiz: Series Indexess.describe() s.loc[INDEX] s.iloc[0]Pandas idxmax()Note: The argmax() function mentioned in the videos has been realiased to idxmax(), and returns the index of the first maximally-valued element. You can find documentation for the idxmax() function in Pandas here.quiz123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdcountries = [ &apos;Afghanistan&apos;, &apos;Albania&apos;, &apos;Algeria&apos;, &apos;Angola&apos;, &apos;Argentina&apos;, &apos;Armenia&apos;, &apos;Australia&apos;, &apos;Austria&apos;, &apos;Azerbaijan&apos;, &apos;Bahamas&apos;, &apos;Bahrain&apos;, &apos;Bangladesh&apos;, &apos;Barbados&apos;, &apos;Belarus&apos;, &apos;Belgium&apos;, &apos;Belize&apos;, &apos;Benin&apos;, &apos;Bhutan&apos;, &apos;Bolivia&apos;, &apos;Bosnia and Herzegovina&apos;,]employment_values = [ 55.70000076, 51.40000153, 50.5 , 75.69999695, 58.40000153, 40.09999847, 61.5 , 57.09999847, 60.90000153, 66.59999847, 60.40000153, 68.09999847, 66.90000153, 53.40000153, 48.59999847, 56.79999924, 71.59999847, 58.40000153, 70.40000153, 41.20000076,]# Employment data in 2007 for 20 countriesemployment = pd.Series(employment_values, index=countries)def max_employment(employment): &apos;&apos;&apos; Fill in this function to return the name of the country with the highest employment in the given employment data, and the employment in that country. The input will be a Pandas series where the values are employment and the index is country names. Try using the Pandas idxmax() function. Documention can be found here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.idxmax.html &apos;&apos;&apos; max_country = None # Replace this with your code max_value = None # Replace this with your code return (max_country, max_value) solution12345def max_employment(employment): max_country = employment.idxmax() # Replace this with your code max_value = employment.loc[max_country] # Replace this with your code return (max_country, max_value) Quiz: Vectorized Operations and Series Indexesquiz123456789101112131415161718192021222324252627import pandas as pd# Change False to True for each block of code to see what it does# Addition when indexes are the sameif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) print s1 + s2# Indexes have same elements in a different orderif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;b&apos;, &apos;d&apos;, &apos;a&apos;, &apos;c&apos;]) print s1 + s2# Indexes overlap, but do not have exactly the same elementsif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;]) print s1 + s2# Indexes do not overlapif False: s1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) s2 = pd.Series([10, 20, 30, 40], index=[&apos;e&apos;, &apos;f&apos;, &apos;g&apos;, &apos;h&apos;]) print s1 + s2 Quiz: Filling Missing ValuesRemember that Jupyter notebooks will just print out the results of the last expression run in a code cell as though a print expression was run. If you want to save the results of your operations for later, remember to assign the results to a variable or, for some Pandas functions like .dropna(), use inplace = True to modify the starting object without needing to reassign it.quiz123456789import pandas as pds1 = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])s2 = pd.Series([10, 20, 30, 40], index=[&apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;])# Try to write code that will add the 2 previous series together,# but treating missing values from either series as 0. The result# when printed out should be similar to the following line:# print pd.Series([1, 2, 13, 24, 30, 40], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;]) solution1s1.add(s2, fill_value=0) Quiz: Pandas Series apply()Note: The grader will execute your finished reverse_names(names) function on some test names Series when you submit your answer. Make sure that this function returns another Series with the transformed names. split()You can find documentation for Python’s split() function here.quiz12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import pandas as pd# Change False to True to see what the following block of code does# Example pandas apply() usage (although this could have been done# without apply() using vectorized operations)if False: s = pd.Series([1, 2, 3, 4, 5]) def add_one(x): return x + 1 print s.apply(add_one)names = pd.Series([ &apos;Andre Agassi&apos;, &apos;Barry Bonds&apos;, &apos;Christopher Columbus&apos;, &apos;Daniel Defoe&apos;, &apos;Emilio Estevez&apos;, &apos;Fred Flintstone&apos;, &apos;Greta Garbo&apos;, &apos;Humbert Humbert&apos;, &apos;Ivan Ilych&apos;, &apos;James Joyce&apos;, &apos;Keira Knightley&apos;, &apos;Lois Lane&apos;, &apos;Mike Myers&apos;, &apos;Nick Nolte&apos;, &apos;Ozzy Osbourne&apos;, &apos;Pablo Picasso&apos;, &apos;Quirinus Quirrell&apos;, &apos;Rachael Ray&apos;, &apos;Susan Sarandon&apos;, &apos;Tina Turner&apos;, &apos;Ugueth Urbina&apos;, &apos;Vince Vaughn&apos;, &apos;Woodrow Wilson&apos;, &apos;Yoji Yamada&apos;, &apos;Zinedine Zidane&apos;])def reverse_names(names): &apos;&apos;&apos; Fill in this function to return a new series where each name in the input series has been transformed from the format &quot;Firstname Lastname&quot; to &quot;Lastname, FirstName&quot;. Try to use the Pandas apply() function rather than a loop. &apos;&apos;&apos; return None solution12345678910111213141516171819202122def reverse_name(name): name = name.split(&quot; &quot;) print &quot;name: &quot; print name first_name = name[1] last_name = name[0] return first_name + &apos;, &apos; + last_nameprint &quot;reverse_name: &quot;print reverse_name(names.iloc[0])def reverse_names(names): &apos;&apos;&apos; Fill in this function to return a new series where each name in the input series has been transformed from the format &quot;Firstname Lastname&quot; to &quot;Lastname, FirstName&quot;. Try to use the Pandas apply() function rather than a loop. &apos;&apos;&apos; return names.apply(reverse_name)print &quot;reverse: &quot;reverse_names(names) Quiz: Plotting in PandasIf the variable data is a NumPy array or a Pandas Series, just like if it is a list, the code12import matplotlib.pyplot as pltplt.hist(data) will create a histogram of the data. Pandas also has built-in plotting that uses matplotlib behind the scenes, so if data is a Series, you can create a histogram using data.hist(). There’s no difference between these two in this case, but sometimes the Pandas wrapper can be more convenient. For example, you can make a line plot of a series using data.plot(). The index of the Series will be used for the x-axis and the values for the y-axis. In the following quiz, we’ve created Series containing the various variables we’ve been looking at this lesson. Pick a country you’re interested in, and make a plot of each variable over time. The Udacity editor will only show one plot each time you click “Test Run”, so you can look at multiple plots by clicking “Test Run” multiple times. If you’re running plotting code locally, you may need to add the line plt.show() depending on your setup.quiz12345678910111213141516171819202122232425262728import pandas as pdimport seaborn as sns# The following code reads all the Gapminder data into Pandas DataFrames. You&apos;ll# learn about DataFrames next lesson.path = &apos;/datasets/ud170/gapminder/&apos;employment = pd.read_csv(path + &apos;employment_above_15.csv&apos;, index_col=&apos;Country&apos;)female_completion = pd.read_csv(path + &apos;female_completion_rate.csv&apos;, index_col=&apos;Country&apos;)male_completion = pd.read_csv(path + &apos;male_completion_rate.csv&apos;, index_col=&apos;Country&apos;)life_expectancy = pd.read_csv(path + &apos;life_expectancy.csv&apos;, index_col=&apos;Country&apos;)gdp = pd.read_csv(path + &apos;gdp_per_capita.csv&apos;, index_col=&apos;Country&apos;)# The following code creates a Pandas Series for each variable for the United States.# You can change the string &apos;United States&apos; to a country of your choice.employment_us = employment.loc[&apos;United States&apos;]female_completion_us = female_completion.loc[&apos;United States&apos;]male_completion_us = male_completion.loc[&apos;United States&apos;]life_expectancy_us = life_expectancy.loc[&apos;United States&apos;]gdp_us = gdp.loc[&apos;United States&apos;]# Uncomment the following line of code to see the available country names# print employment.index.values# Use the Series defined above to create a plot of each variable over time for# the country of your choice. You will only be able to display one plot at a time# with each &quot;Test Run&quot;. solution12345# employment_us.plot()# female_completion_us.plot()# male_completion_us.plot()# life_expectancy_us.plot()gdp_us.plot() ConclusionNumpy and Pandas for 2D DataIntroductionQuiz: Subway DataQuiz: Two-Dimensional NumPy Arrayspython: list of listsnumpy: 2D arraypandas: DataFrameThis page describes the memory layout of 2D NumPy arrays.quiz123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as np# Subway ridership for 5 stations on 10 different daysridership = np.array([ [ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]])# Change False to True for each block of code to see what it does# Accessing elementsif False: print ridership[1, 3] print ridership[1:3, 3:5] print ridership[1, :] # Vectorized operations on rows or columnsif False: print ridership[0, :] + ridership[1, :] print ridership[:, 0] + ridership[:, 1] # Vectorized operations on entire arraysif False: a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) b = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]) print a + bdef mean_riders_for_max_station(ridership): &apos;&apos;&apos; Fill in this function to find the station with the maximum riders on the first day, then return the mean riders per day for that station. Also return the mean ridership overall for comparsion. Hint: NumPy&apos;s argmax() function might be useful: http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html &apos;&apos;&apos; overall_mean = None # Replace this with your code mean_for_max = None # Replace this with your code return (overall_mean, mean_for_max) solution12345678def mean_riders_for_max_station(ridership): overall_mean = ridership.mean() # Replace this with your code station = ridership[0,:].argmax() mean_for_max = ridership[:,station].mean() # Replace this with your code return (overall_mean, mean_for_max) Quiz: NumPy Axisaxis = 0 column1 rowquiz123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as np# Change False to True for this block of code to see what it does# NumPy axis argumentif False: a = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) print a.sum() print a.sum(axis=0) print a.sum(axis=1) # Subway ridership for 5 stations on 10 different daysridership = np.array([ [ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]])def min_and_max_riders_per_day(ridership): &apos;&apos;&apos; Fill in this function. First, for each subway station, calculate the mean ridership per day. Then, out of all the subway stations, return the maximum and minimum of these values. That is, find the maximum mean-ridership-per-day and the minimum mean-ridership-per-day for any subway station. &apos;&apos;&apos; max_daily_ridership = None # Replace this with your code min_daily_ridership = None # Replace this with your code return (max_daily_ridership, min_daily_ridership) solution12345def min_and_max_riders_per_day(ridership): max_daily_ridership = ridership.mean(axis=0).max() # Replace this with your code min_daily_ridership = ridership.mean(axis=0).min() # Replace this with your code return (max_daily_ridership, min_daily_ridership) NumPy and Pandas Data typesQuiz: Accessing Elements of a DataFramequiz12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import pandas as pd# Subway ridership for 5 stations on 10 different daysridership_df = pd.DataFrame( data=[[ 0, 0, 2, 5, 0], [1478, 3877, 3674, 2328, 2539], [1613, 4088, 3991, 6461, 2691], [1560, 3392, 3826, 4787, 2613], [1608, 4802, 3932, 4477, 2705], [1576, 3933, 3909, 4979, 2685], [ 95, 229, 255, 496, 201], [ 2, 0, 1, 27, 0], [1438, 3785, 3589, 4174, 2215], [1342, 4043, 4009, 4665, 3033]], index=[&apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-06-11&apos;, &apos;05-07-11&apos;, &apos;05-08-11&apos;, &apos;05-09-11&apos;, &apos;05-10-11&apos;], columns=[&apos;R003&apos;, &apos;R004&apos;, &apos;R005&apos;, &apos;R006&apos;, &apos;R007&apos;])# Change False to True for each block of code to see what it does# DataFrame creationif False: # You can create a DataFrame out of a dictionary mapping column names to values df_1 = pd.DataFrame(&#123;&apos;A&apos;: [0, 1, 2], &apos;B&apos;: [3, 4, 5]&#125;) print df_1 # You can also use a list of lists or a 2D NumPy array df_2 = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;]) print df_2 # Accessing elementsif False: print ridership_df.iloc[0] print ridership_df.loc[&apos;05-05-11&apos;] print ridership_df[&apos;R003&apos;] print ridership_df.iloc[1, 3] # Accessing multiple rowsif False: print ridership_df.iloc[1:4] # Accessing multiple columnsif False: print ridership_df[[&apos;R003&apos;, &apos;R005&apos;]] # Pandas axisif False: df = pd.DataFrame(&#123;&apos;A&apos;: [0, 1, 2], &apos;B&apos;: [3, 4, 5]&#125;) print df.sum() print df.sum(axis=1) print df.values.sum() def mean_riders_for_max_station(ridership): &apos;&apos;&apos; Fill in this function to find the station with the maximum riders on the first day, then return the mean riders per day for that station. Also return the mean ridership overall for comparsion. This is the same as a previous exercise, but this time the input is a Pandas DataFrame rather than a 2D NumPy array. &apos;&apos;&apos; overall_mean = None # Replace this with your code mean_for_max = None # Replace this with your code return (overall_mean, mean_for_max) solution123456def mean_riders_for_max_station(ridership): station = ridership.iloc[0].argmax() overall_mean = ridership.values.mean() # Replace this with your code mean_for_max = ridership[station].mean() # Replace this with your code return (overall_mean, mean_for_max) Loading Data into a DataFrameQuiz: Calculating CorrelationUnderstand and Interpreting CorrelationsThis page contains some scatterplots of variables with different values of correlation.This page lets you use a slider to change the correlation and see how the data might look.Pearson’s r only measures linear correlation! This image shows some different linear and non-linear relationships and what Pearson’s r will be for those relationships. Corrected vs. Uncorrected Standard DeviationBy default, Pandas’ std() function computes the standard deviation using Bessel’s correction. Calling std(ddof=0) ensures that Bessel’s correction will not be used. Previous ExerciseThe exercise where you used a simple heuristic to estimate correlation was the “Pandas Series” exercise in the previous lesson, “NumPy and Pandas for 1D Data”. Pearson’s r in NumPyNumPy’s corrcoef() function can be used to calculate Pearson’s r, also known as the correlation coefficient.quiz123456789101112131415161718192021222324252627import pandas as pdfilename = &apos;/datasets/ud170/subway/nyc_subway_weather.csv&apos;subway_df = pd.read_csv(filename)def correlation(x, y): &apos;&apos;&apos; Fill in this function to compute the correlation between the two input variables. Each input is either a NumPy array or a Pandas Series. correlation = average of (x in standard units) times (y in standard units) Remember to pass the argument &quot;ddof=0&quot; to the Pandas std() function! &apos;&apos;&apos; return Noneentries = subway_df[&apos;ENTRIESn_hourly&apos;]cum_entries = subway_df[&apos;ENTRIESn&apos;]rain = subway_df[&apos;meanprecipi&apos;]temp = subway_df[&apos;meantempi&apos;]print correlation(entries, rain)print correlation(entries, temp)print correlation(rain, temp)print correlation(entries, cum_entries) solution1234def correlation(x, y): std_x = (x - x.mean()) / x.std(ddof=0) std_y = (y - y.mean()) / y.std(ddof=0) return ( std_x * std_y ).mean() Pandas Axis Names12axis=0 axis=1axis=&apos;index&apos; axis=&apos;columns&apos; Quiz: DataFrame Vectorized OperationsPandas shift()Documentation for the Pandas shift() function is here. If you’re still not sure how the function works, try it out and see! Alternative SolutionAs an alternative to using vectorized operations, you could also use the code return entries_and_exits.diff() to calculate the answer in a single step.quiz123456789101112131415161718192021222324252627282930313233343536373839404142import pandas as pd# Examples of vectorized operations on DataFrames:# Change False to True for each block of code to see what it does# Adding DataFrames with the column namesif False: df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;) df2 = pd.DataFrame(&#123;&apos;a&apos;: [10, 20, 30], &apos;b&apos;: [40, 50, 60], &apos;c&apos;: [70, 80, 90]&#125;) print df1 + df2 # Adding DataFrames with overlapping column names if False: df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;) df2 = pd.DataFrame(&#123;&apos;d&apos;: [10, 20, 30], &apos;c&apos;: [40, 50, 60], &apos;b&apos;: [70, 80, 90]&#125;) print df1 + df2# Adding DataFrames with overlapping row indexesif False: df1 = pd.DataFrame(&#123;&apos;a&apos;: [1, 2, 3], &apos;b&apos;: [4, 5, 6], &apos;c&apos;: [7, 8, 9]&#125;, index=[&apos;row1&apos;, &apos;row2&apos;, &apos;row3&apos;]) df2 = pd.DataFrame(&#123;&apos;a&apos;: [10, 20, 30], &apos;b&apos;: [40, 50, 60], &apos;c&apos;: [70, 80, 90]&#125;, index=[&apos;row4&apos;, &apos;row3&apos;, &apos;row2&apos;]) print df1 + df2# --- Quiz ---# Cumulative entries and exits for one station for a few hours.entries_and_exits = pd.DataFrame(&#123; &apos;ENTRIESn&apos;: [3144312, 3144335, 3144353, 3144424, 3144594, 3144808, 3144895, 3144905, 3144941, 3145094], &apos;EXITSn&apos;: [1088151, 1088159, 1088177, 1088231, 1088275, 1088317, 1088328, 1088331, 1088420, 1088753]&#125;)def get_hourly_entries_and_exits(entries_and_exits): &apos;&apos;&apos; Fill in this function to take a DataFrame with cumulative entries and exits (entries in the first column, exits in the second) and return a DataFrame with hourly entries and exits (entries in the first column, exits in the second). &apos;&apos;&apos; return None solution12def get_hourly_entries_and_exits(entries_and_exits): return entries_and_exits - entries_and_exits.shift(1) Quiz: DataFrame applymap()Note: The grader will execute your finished convert_grades(grades) function on some test grades DataFrames when you submit your answer. Make sure that this function returns a DataFrame with the converted grades. ​Hint​: You may need to define a helper function to use with .applymap().quiz1234567891011121314151617181920212223242526272829303132333435363738import pandas as pd# Change False to True for this block of code to see what it does# DataFrame applymap()if False: df = pd.DataFrame(&#123; &apos;a&apos;: [1, 2, 3], &apos;b&apos;: [10, 20, 30], &apos;c&apos;: [5, 10, 15] &#125;) def add_one(x): return x + 1 print df.applymap(add_one) grades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;]) def convert_grades(grades): &apos;&apos;&apos; Fill in this function to convert the given DataFrame of numerical grades to letter grades. Return a new DataFrame with the converted grade. The conversion rule is: 90-100 -&gt; A 80-89 -&gt; B 70-79 -&gt; C 60-69 -&gt; D 0-59 -&gt; F &apos;&apos;&apos; return None solution123456789101112131415def convert(grades): if (grades &gt;= 90) &amp; (grades &lt;= 100): grades = &apos;A&apos; elif (grades &gt;= 80) &amp; (grades &lt;= 89): grades = &apos;B&apos; elif (grades &gt;= 70) &amp; (grades &lt;= 79): grades = &apos;C&apos; elif (grades &gt;= 60) &amp; (grades &lt;= 69): grades = &apos;D&apos; else: grades = &apos;F&apos; return grades def convert_grades(grades): return grades.applymap(convert) Quiz: DataFrame apply()Note: In order to get the proper computations, we should actually be setting the value of the “ddof” parameter to 0 in the .std() function. Note that the type of standard deviation calculated by default is different between numpy’s .std() and pandas’ .std() functions. By default, numpy calculates a population standard deviation, with “ddof = 0”. On the other hand, pandas calculates a sample standard deviation, with “ddof = 1”. If we know all of the scores, then we have a population - so to standardize using pandas, we need to set “ddof = 0”..apply() used to convert columns(default) to columns and convert rows(with axis) to rows.applymap() used to elementsquiz123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdgrades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;])# Change False to True for this block of code to see what it does# DataFrame apply()if False: def convert_grades_curve(exam_grades): # Pandas has a bult-in function that will perform this calculation # This will give the bottom 0% to 10% of students the grade &apos;F&apos;, # 10% to 20% the grade &apos;D&apos;, and so on. You can read more about # the qcut() function here: # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html return pd.qcut(exam_grades, [0, 0.1, 0.2, 0.5, 0.8, 1], labels=[&apos;F&apos;, &apos;D&apos;, &apos;C&apos;, &apos;B&apos;, &apos;A&apos;]) # qcut() operates on a list, array, or Series. This is the # result of running the function on a single column of the # DataFrame. print convert_grades_curve(grades_df[&apos;exam1&apos;]) # qcut() does not work on DataFrames, but we can use apply() # to call the function on each column separately print grades_df.apply(convert_grades_curve) def standardize(df): &apos;&apos;&apos; Fill in this function to standardize each column of the given DataFrame. To standardize a variable, convert each value to the number of standard deviations it is above or below the mean. &apos;&apos;&apos; return None solution12345def std_col(col): return (col - col.mean()) / col.std(ddof=0) def standardize(df): return df.apply(std_col) Quiz: DataFrame apply() Use Case 2.apply() convert columns to elementdf.apply(np.max):=df.max()quiz12345678910111213141516171819202122import numpy as npimport pandas as pddf = pd.DataFrame(&#123; &apos;a&apos;: [4, 5, 3, 1, 2], &apos;b&apos;: [20, 10, 40, 50, 30], &apos;c&apos;: [25, 20, 5, 15, 10]&#125;)# Change False to True for this block of code to see what it does# DataFrame apply() - use case 2if False: print df.apply(np.mean) print df.apply(np.max) def second_largest(df): &apos;&apos;&apos; Fill in this function to return the second-largest value of each column of the input DataFrame. &apos;&apos;&apos; return None solution123456def second_max(col): sorted_col = col.sort_values(ascending=False) return sorted_col.iloc[1] def second_largest(df): return df.apply(second_max) Quiz: Adding a DataFrame to a Seriescode1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import pandas as pd# Change False to True for each block of code to see what it does# Adding a Series to a square DataFrameif False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding a Series to a one-row DataFrame if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123;0: [10], 1: [20], 2: [30], 3: [40]&#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s# Adding a Series to a one-column DataFrameif False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123;0: [10, 20, 30, 40]&#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding when DataFrame column names match Series indexif False: s = pd.Series([1, 2, 3, 4], index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;]) df = pd.DataFrame(&#123; &apos;a&apos;: [10, 20, 30, 40], &apos;b&apos;: [50, 60, 70, 80], &apos;c&apos;: [90, 100, 110, 120], &apos;d&apos;: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding when DataFrame column names don&apos;t match Series indexif False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; &apos;a&apos;: [10, 20, 30, 40], &apos;b&apos;: [50, 60, 70, 80], &apos;c&apos;: [90, 100, 110, 120], &apos;d&apos;: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s Quiz: Standardizing Each Column Againquiz123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import pandas as pd# Adding using +if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df + s # Adding with axis=&apos;index&apos;if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df.add(s, axis=&apos;index&apos;) # The functions sub(), mul(), and div() work similarly to add() # Adding with axis=&apos;columns&apos;if False: s = pd.Series([1, 2, 3, 4]) df = pd.DataFrame(&#123; 0: [10, 20, 30, 40], 1: [50, 60, 70, 80], 2: [90, 100, 110, 120], 3: [130, 140, 150, 160] &#125;) print df print &apos;&apos; # Create a blank line between outputs print df.add(s, axis=&apos;columns&apos;) # The functions sub(), mul(), and div() work similarly to add() grades_df = pd.DataFrame( data=&#123;&apos;exam1&apos;: [43, 81, 78, 75, 89, 70, 91, 65, 98, 87], &apos;exam2&apos;: [24, 63, 56, 56, 67, 51, 79, 46, 72, 60]&#125;, index=[&apos;Andre&apos;, &apos;Barry&apos;, &apos;Chris&apos;, &apos;Dan&apos;, &apos;Emilio&apos;, &apos;Fred&apos;, &apos;Greta&apos;, &apos;Humbert&apos;, &apos;Ivan&apos;, &apos;James&apos;])def standardize(df): &apos;&apos;&apos; Fill in this function to standardize each column of the given DataFrame. To standardize a variable, convert each value to the number of standard deviations it is above or below the mean. This time, try to use vectorized operations instead of apply(). You should get the same results as you did before. &apos;&apos;&apos; return Nonedef standardize_rows(df): &apos;&apos;&apos; Optional: Fill in this function to standardize each row of the given DataFrame. Again, try not to use apply(). This one is more challenging than standardizing each column! &apos;&apos;&apos; return None solution123456def standardize(df): return (df - df.mean()) / df.std(ddof=0)def standardize_rows(df): mean_diff = df.sub(df.mean(axis=&apos;columns&apos;), axis=&apos;index&apos;) return mean_diff.div(df.std(ddof=0, axis=&apos;columns&apos;), axis=&apos;index&apos;) Quiz: Pandas groupby()code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport seaborn as snsvalues = np.array([1, 3, 2, 4, 1, 6, 4])example_df = pd.DataFrame(&#123; &apos;value&apos;: values, &apos;even&apos;: values % 2 == 0, &apos;above_three&apos;: values &gt; 3 &#125;, index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;])# Change False to True for each block of code to see what it does# Examine DataFrameif False: print example_df # Examine groupsif False: grouped_data = example_df.groupby(&apos;even&apos;) # The groups attribute is a dictionary mapping keys to lists of row indexes print grouped_data.groups # Group by multiple columnsif False: grouped_data = example_df.groupby([&apos;even&apos;, &apos;above_three&apos;]) print grouped_data.groups # Get sum of each groupif False: grouped_data = example_df.groupby(&apos;even&apos;) print grouped_data.sum() # Limit columns in resultif False: grouped_data = example_df.groupby(&apos;even&apos;) # You can take one or more columns from the result DataFrame print grouped_data.sum()[&apos;value&apos;] print &apos;\n&apos; # Blank line to separate results # You can also take a subset of columns from the grouped data before # collapsing to a DataFrame. In this case, the result is the same. print grouped_data[&apos;value&apos;].sum() filename = &apos;/datasets/ud170/subway/nyc_subway_weather.csv&apos;subway_df = pd.read_csv(filename)### Write code here to group the subway data by a variable of your choice, then### either print out the mean ridership within each group or create a plot. Quiz: Calculating Hourly Entries and ExitsIn the quiz where you calculated hourly entries and exits, you did so for a single set of cumulative entries. However, in the original data, there was a separate set of numbers for each station. Thus, to correctly calculate the hourly entries and exits, it was necessary to group by station and day, then calculate the hourly entries and exits within each day. Write a function to do that. You should use the apply() function to call the function you wrote previously. You should also make sure you restrict your grouped data to just the entries and exits columns, since your function may cause an error if it is called on non-numerical data types. If you would like to learn more about using groupby() in Pandas, this page contains more details. Note: You will not be able to reproduce the ENTRIESn_hourly and EXITSn_hourly columns in the full dataset using this method. When creating the dataset, we did extra processing to remove erroneous values. quizTo clarify the structure of the data, the original data recorded the cumulative number of entries on each station at four-hour intervals. For the quiz, you just need to look at the differences between consecutive measurements on each station: by computing “hourly entries”, we just mean recording the number of new tallies between each recording period as a contrast to “cumulative entries”.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport pandas as pdvalues = np.array([1, 3, 2, 4, 1, 6, 4])example_df = pd.DataFrame(&#123; &apos;value&apos;: values, &apos;even&apos;: values % 2 == 0, &apos;above_three&apos;: values &gt; 3 &#125;, index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;])# Change False to True for each block of code to see what it does# Standardize each groupif False: def standardize(xs): return (xs - xs.mean()) / xs.std() grouped_data = example_df.groupby(&apos;even&apos;) print grouped_data[&apos;value&apos;].apply(standardize) # Find second largest value in each groupif False: def second_largest(xs): sorted_xs = xs.sort(inplace=False, ascending=False) return sorted_xs.iloc[1] grouped_data = example_df.groupby(&apos;even&apos;) print grouped_data[&apos;value&apos;].apply(second_largest)# --- Quiz ---# DataFrame with cumulative entries and exits for multiple stationsridership_df = pd.DataFrame(&#123; &apos;UNIT&apos;: [&apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;, &apos;R079&apos;, &apos;R051&apos;], &apos;TIMEn&apos;: [&apos;00:00:00&apos;, &apos;02:00:00&apos;, &apos;04:00:00&apos;, &apos;06:00:00&apos;, &apos;08:00:00&apos;, &apos;10:00:00&apos;, &apos;12:00:00&apos;, &apos;14:00:00&apos;, &apos;16:00:00&apos;], &apos;ENTRIESn&apos;: [3144312, 8936644, 3144335, 8936658, 3144353, 8936687, 3144424, 8936819, 3144594], &apos;EXITSn&apos;: [1088151, 13755385, 1088159, 13755393, 1088177, 13755598, 1088231, 13756191, 1088275]&#125;)def get_hourly_entries_and_exits(entries_and_exits): &apos;&apos;&apos; Fill in this function to take a DataFrame with cumulative entries and exits and return a DataFrame with hourly entries and exits. The hourly entries and exits should be calculated separately for each station (the &apos;UNIT&apos; column). Hint: Take a look at the `get_hourly_entries_and_exits()` function you wrote in a previous quiz, DataFrame Vectorized Operations. If you copy it here and rename it, you can use it and the `.apply()` function to help solve this problem. &apos;&apos;&apos; return None solution123456def hourly_entries_and_exits(entries_and_exits): return entries_and_exits - entries_and_exits.shift(1) def get_hourly_entries_and_exits(entries_and_exits): grouped_data = entries_and_exits.groupby(&apos;UNIT&apos;) return grouped_data[[&apos;ENTRIESn&apos;, &apos;EXITSn&apos;]].apply(hourly_entries_and_exits) Quiz: Combining Pandas DataFramesIn the merged table on the right, the join dates in the third and fourth rows should be 5/19 and 5/11, reflecting the account key mapping in the enrollments table.quiz1234567891011121314151617181920212223242526272829303132333435363738394041import pandas as pdsubway_df = pd.DataFrame(&#123; &apos;UNIT&apos;: [&apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R003&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;, &apos;R004&apos;], &apos;DATEn&apos;: [&apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;], &apos;hour&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;ENTRIESn&apos;: [ 4388333, 4388348, 4389885, 4391507, 4393043, 14656120, 14656174, 14660126, 14664247, 14668301], &apos;EXITSn&apos;: [ 2911002, 2911036, 2912127, 2913223, 2914284, 14451774, 14451851, 14454734, 14457780, 14460818], &apos;latitude&apos;: [ 40.689945, 40.689945, 40.689945, 40.689945, 40.689945, 40.69132 , 40.69132 , 40.69132 , 40.69132 , 40.69132 ], &apos;longitude&apos;: [-73.872564, -73.872564, -73.872564, -73.872564, -73.872564, -73.867135, -73.867135, -73.867135, -73.867135, -73.867135]&#125;)weather_df = pd.DataFrame(&#123; &apos;DATEn&apos;: [&apos;05-01-11&apos;, &apos;05-01-11&apos;, &apos;05-02-11&apos;, &apos;05-02-11&apos;, &apos;05-03-11&apos;, &apos;05-03-11&apos;, &apos;05-04-11&apos;, &apos;05-04-11&apos;, &apos;05-05-11&apos;, &apos;05-05-11&apos;], &apos;hour&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;latitude&apos;: [ 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 , 40.689945, 40.69132 ], &apos;longitude&apos;: [-73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135, -73.872564, -73.867135], &apos;pressurei&apos;: [ 30.24, 30.24, 30.32, 30.32, 30.14, 30.14, 29.98, 29.98, 30.01, 30.01], &apos;fog&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;rain&apos;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], &apos;tempi&apos;: [ 52. , 52. , 48.9, 48.9, 54. , 54. , 57.2, 57.2, 48.9, 48.9], &apos;wspdi&apos;: [ 8.1, 8.1, 6.9, 6.9, 3.5, 3.5, 15. , 15. , 15. , 15. ]&#125;)def combine_dfs(subway_df, weather_df): &apos;&apos;&apos; Fill in this function to take 2 DataFrames, one with subway data and one with weather data, and return a single dataframe with one row for each date, hour, and location. Only include times and locations that have both subway data and weather data available. &apos;&apos;&apos; return None solution12def combine_dfs(subway_df, weather_df): return subway_df.merge(weather_df, on=[&apos;DATEn&apos;, &apos;hour&apos;, &apos;latitude&apos;, &apos;longitude&apos;], how=&apos;inner&apos;) Quiz: Plotting for DataFramesJust like Pandas Series, DataFrames also have a plot() method. If df is a DataFrame, then df.plot() will produce a line plot with a different colored line for each variable in the DataFrame. This can be a convenient way to get a quick look at your data, especially for small DataFrames, but for more complicated plots you will usually want to use matplotlib directly. In the following quiz, create a plot of your choice showing something interesting about the New York subway data. For example, you might create: Histograms of subway ridership on both days with rain and days without rainA scatterplot of subway stations with latitude and longitude as the x and y axes and ridership as the bubble sizeIf you choose this option, you may wish to use the as_index=False argument to groupby(). There is example code in the following quiz.A scatterplot with subway ridership on one axis and precipitation or temperature on the otherIf you’re not sure how to make the plot you want, try searching on Google or take a look at the matplotlib documentation. Once you’ve created a plot you’re happy with, share what you’ve found on the forums!quiz1234567891011121314151617181920212223242526272829303132import matplotlib.pyplot as pltimport numpy as npimport pandas as pdimport seaborn as snsvalues = np.array([1, 3, 2, 4, 1, 6, 4])example_df = pd.DataFrame(&#123; &apos;value&apos;: values, &apos;even&apos;: values % 2 == 0, &apos;above_three&apos;: values &gt; 3 &#125;, index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;, &apos;g&apos;])# Change False to True for this block of code to see what it does# groupby() without as_indexif False: first_even = example_df.groupby(&apos;even&apos;).first() print first_even print first_even[&apos;even&apos;] # Causes an error. &apos;even&apos; is no longer a column in the DataFrame # groupby() with as_index=Falseif False: first_even = example_df.groupby(&apos;even&apos;, as_index=False).first() print first_even print first_even[&apos;even&apos;] # Now &apos;even&apos; is still a column in the DataFramefilename = &apos;/datasets/ud170/subway/nyc_subway_weather.csv&apos;subway_df = pd.read_csv(filename)## Make a plot of your choice here showing something interesting about the subway data.## Matplotlib documentation here: http://matplotlib.org/api/pyplot_api.html## Once you&apos;ve got something you&apos;re happy with, share it on the forums! solution123location = subway_df.groupby([&apos;latitude&apos;, &apos;longitude&apos;], as_index=False).mean()scale = location[&apos;ENTRIESn_hourly&apos;] / location[&apos;ENTRIESn_hourly&apos;].std(ddof=0)plt.scatter(location[&apos;latitude&apos;], location[&apos;longitude&apos;], s=scale) Three-Dimensional DataThree-Dimensional DataNow that you’ve worked with one-dimensional and two-dimensional data, you might be wondering how to work with three or more dimensions. 3D data in NumPyNumPy arrays can have arbitrarily many dimensions. Just like you can create a 1D array from a list, and a 2D array from a list of lists, you can create a 3D array from a list of lists of lists, and so on. For example, the following code would create a 3D array:1234a = np.array([ [[&apos;A1a&apos;, &apos;A1b&apos;, &apos;A1c&apos;], [&apos;A2a&apos;, &apos;A2b&apos;, &apos;A2c&apos;]], [[&apos;B1a&apos;, &apos;B1b&apos;, &apos;B1c&apos;], [&apos;B2a&apos;, &apos;B2b&apos;, &apos;B2c&apos;]]]) 3D data in PandasPandas has a data structure called a Panel, which is similar to a DataFrame or a Series, but for 3D data. If you would like, you can learn more about Panels here. ConclusionFinal Project: Investigate a Dataset]]></content>
      <categories>
        <category>Udacity</category>
        <category>Data Analysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Udacity</tag>
        <tag>Data Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity DAND Notebook]]></title>
    <url>%2F2017%2F06%2F20%2FUdacity%20DAND%20Notebook%2F</url>
    <content type="text"><![CDATA[Stay stunedPreview https://cn.udacity.com/course/statistics--st095https://cn.udacity.com/course/intro-to-descriptive-statistics--ud827https://cn.udacity.com/course/intro-to-inferential-statistics--ud201https://classroom.udacity.com/courses/ud1111https://www.udacity.com/course/intro-to-data-analysis--ud170https://cn.udacity.com/course/data-visualization-in-tableau--ud1006https://www.udacity.com/course/ab-testing--ud257https://www.udacity.com/course/ab-testing--ud979 https://cn.udacity.com/course/data-wrangling-with-mongodb--ud032https://cn.udacity.com/course/data-analysis-with-r--ud651https://cn.udacity.com/course/data-visualization-and-d3js--ud507]]></content>
      <tags>
        <tag>Data Analyst</tag>
        <tag>NanoDegree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud1111 Anaconda and Jupyter Notebooks notebook]]></title>
    <url>%2F2017%2F06%2F20%2FUdacity%20ud1111%20Anaconda%20and%20Jupyter%20Notebooks%20notebook%2F</url>
    <content type="text"><![CDATA[completed in 2017-06-20course can be found here LessonAnacondaManaging environmentsManaging environmentsAs I mentioned before, conda can be used to create environments to isolate your projects. To create an environment, use conda create -n env_name list of packages in your terminal. Here -n env_name sets the name of your environment (-n for name) and list of packages is the list of packages you want installed in the environment. For example, to create an environment named my_env and install numpy in it, type conda create -n my_env numpy. When creating an environment, you can specify which version of Python to install in the environment. This is useful when you’re working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like conda create -n py3 python=3 or conda create -n py2 python=2. I actually have both of these environments on my personal computer. I use them as general environments not tied to any specific project, but rather for general work with each Python version easily accessible. These commands will install the most recent version of Python 3 and 2, respectively. To install a specific version, use conda create -n py python=3.3 for Python 3.3. Entering an environmentOnce you have an environment created, use source activate my_env to enter it on OSX/Linux. On Windows, use activate my_env. When you’re in the environment, you’ll see the environment name in the terminal prompt. Something like (my_env) ~ $. The environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with conda list. Installing packages in the environment is the same as before: conda install package_name. Only this time, the specific packages you install will only be available when you’re in the environment. To leave the environment, type source deactivate (on OSX/Linux). On Windows, use deactivate. More environment actionsSaving and loading environmentsA really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with conda env export &gt; environment.yaml. The first part conda env export writes out all the packages in the environment, including the Python version. Above you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command, &gt; environment.yaml writes the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project. To create an environment from an environment file use conda env create -f environment.yaml. This will create a new environment with the same name listed in environment.yaml. Listing environmentsIf you forget what your environments are named (happens to me sometimes), use conda env list to list out all the environments you’ve created. You should see a list of environments, there will be an asterisk next to the environment you’re currently in. The default environment, the environment used when you aren’t in one, is called root. Removing environmentsIf there are environments you don’t use anymore, conda env remove -n env_name will remove the specified environment (here, named env_name). Best practicesUsing environmentsOne thing that’s helped me tremendously is having separate environments for Python 2 and Python 3. I used conda create -n py2 python=2 and conda create -n py3 python=3 to create two separate environments, py2 and py3. Now I have a general use environment for each Python version. In each of those environments, I’ve installed most of the standard data science packages (numpy, scipy, pandas, etc.) I’ve also found it useful to create environments for each project I’m working on. It works great for non-data related projects too like web apps with Flask. For example, I have an environment for my personal blog using Pelican. Sharing environmentsWhen sharing your code on GitHub, it’s good practice to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. I also usually include a pip requirements.txt file using pip freeze (learn more here) for people not using conda. More to learnTo learn more about conda and how it fits in the Python ecosystem, check out this article by Jake Vanderplas: Conda myths and misconceptions. And here’s the conda documentation you can reference later. On Python versions at UdacityMost Nanodegree programs at Udacity will be (or are already) using Python 3 almost exclusively. Why we’re switching to Python 3 Jupyter is switching to Python 3 only Python 2.7 is being retired Python 3.6 has great features such as formatted strings At this point, there are enough new features in Python 3 that it doesn’t make much sense to stick with Python 2 unless you’re working with old code. All new Python code should be written for version 3. The main breakage between Python 2 and 3For the most part, Python 2 code will work with Python 3. Of course, most new features introduced with Python 3 versions won’t be backwards compatible. The place where your Python 2 code will fail most often is the print statement. For most of Python’s history including Python 2, printing was done like so:12print &quot;Hello&quot;, &quot;world!&quot;&gt; Hello world! This was changed in Python 3 to a function.12print(&quot;Hello&quot;, &quot;world!&quot;)&gt; Hello world! The print function was back-ported to Python 2 in version 2.6 through the __future__ module:1234# In Python 2.6+from __future__ import print_functionprint(&quot;Hello&quot;, &quot;world!&quot;)&gt; Hello world! The print statement doesn’t work in Python 3. If you want to print something and have it work in both Python versions, you’ll need to import print_function in your Python 2 code. Note for students in the Data Analyst Nanodegree programCurrently, most of the materials for this Nanodegree program are still guaranteed to work only for Python 2.7. You can quickly set up an environment for the current DAND program by opening the Resources tab and downloading an appropriate YAML file. Note for students in the Machine Learning Engineer Nanodegree programCurrently, Machine Learning Engineer Nanodegree requires Python 2.7 to finish all the projects. Jupyter NotebooksWhat are Jupyter notebooks?Markdown cellsso if you don’t have experience with LaTeX please read this primer on using it to create math expressions. Magic keywordshttps://classroom.udacity.com/courses/ud1111/lessons/b15ba0a2-015d-4c5a-87ae-9efba2cabb43/concepts/256cdd36-17d4-442a-a033-7c64ce83f7f8 here’s the list of all available magic commands. Converting notebooksFor example, to convert a notebook to an HTML file, in your terminal use jupyter nbconvert --to html notebook.ipynb As always, learn more about nbconvert from the documentation. Creating a slideshowYou can see an example of a slideshow here Slides are full slides that you move through left to right. Sub-slides show up in the slideshow by pressing up or down. Fragments are hidden at first, then appear with a button press. You can skip cells in the slideshow with Skip and Notes leaves the cell as speaker notes. Running the slideshowTo create the slideshow from the notebook file, you’ll need to use nbconvert: jupyter nbconvert notebook.ipynb --to slides This just converts the notebook to the necessary files for the slideshow, but you need to serve it with an HTTP server to actually see the presentation. To convert it and immediately see it, use jupyter nbconvert notebook.ipynb --to slides --post serve This will open up the slideshow in your browser so you can present it.]]></content>
      <categories>
        <category>Udacity</category>
        <category>Anaconda and Jupyter Notebooks</category>
      </categories>
      <tags>
        <tag>jupyter notebook</tag>
        <tag>Udacity</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Daily Easy English Expression Notebook]]></title>
    <url>%2F2017%2F06%2F09%2FDaily%20Easy%20English%20Expression%20Notebook%2F</url>
    <content type="text"><![CDATA[on the way SourceFollow this video in YouTube Content How are you doing ha yuh doen What do you do for a living wha d yuh do fere living My knee went out my knee wen nout s n l cancel d t th What’s up this weekend wassup thi sweekend s cancel t]]></content>
      <tags>
        <tag>Speaking</tag>
        <tag>Listening</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity FEND Notebook]]></title>
    <url>%2F2017%2F06%2F04%2FUdacity%20FEND%20Notebook%2F</url>
    <content type="text"><![CDATA[FEND Completed in 2016 Article to Mockuporiginal siteAnimal Trading Cardsoriginal siteBuild a Portfolio Siteoriginal siteOnline Resumeoriginal siteClassic Arcade Game Cloneoriginal siteWebsite Optimizationoriginal siteNeighborhood Maporiginal siteFeed Reader Testingoriginal site]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>NanoDegree</tag>
        <tag>Front End</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera S Machine Learning Notebook]]></title>
    <url>%2F2017%2F05%2F31%2FCoursera%20S%20Machine%20Learning%20Notebook%2F</url>
    <content type="text"><![CDATA[For wrapping up and resume writingvideoLecture notesProgramming assignment 1 Week 1 Introduction &amp; Linear Regression with One VariableThis method looks at every example in the entire training set on every step, and is called batch gradient descent. Model and Cost FunctionCost Functionhttps://www.coursera.org/learn/machine-learning/lecture/rkTp3/cost-function Cost Functionhttps://www.coursera.org/learn/machine-learning/supplement/nhzyF/cost-function Cost Function - Intuition Ihttps://www.coursera.org/learn/machine-learning/lecture/N09c6/cost-function-intuition-i Cost Function - Intuition Ihttps://www.coursera.org/learn/machine-learning/supplement/u3qF5/cost-function-intuition-i Parameter LearningGradient Descenthttps://www.coursera.org/learn/machine-learning/lecture/8SpIM/gradient-descent Gradient Descenthttps://www.coursera.org/learn/machine-learning/supplement/2GnUg/gradient-descent Gradient Descent Intuitionhttps://www.coursera.org/learn/machine-learning/supplement/QKEdR/gradient-descent-intuition Gradient Descent For Linear Regressionhttps://www.coursera.org/learn/machine-learning/supplement/U90DX/gradient-descent-for-linear-regression Week 2Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:$x_i :=\frac{x_i - \mu_i}{s_i}$Where $μ_i$ is the average of all the values for feature (i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation. Linear Regression with Multiple VariablesEnvironment Setup InstructionsSetting Up Your Programming Assignment Environment8 minInstalling MATLAB3 minInstalling Octave on Windows3 minInstalling Octave on Mac OS X (10.10 Yosemite and 10.9 Mavericks and Later)10 minInstalling Octave on Mac OS X (10.8 Mountain Lion and Earlier)3 minInstalling Octave on GNU/Linux7 minMore Octave/MATLAB resources 10 minhttps://www.coursera.org/learn/machine-learning/supplement/Mlf3e/more-octave-matlab-resources Multivariate Linear RegressionMultiple Features8 minMultiple Features3 minGradient Descent for Multiple Variables5 minGradient Descent For Multiple Variables2 minGradient Descent in Practice I - Feature Scaling8 minGradient Descent in Practice I - Feature Scaling3 minGradient Descent in Practice II - Learning Rate8 minGradient Descent in Practice II - Learning Rate4 minFeatures and Polynomial Regression7 minFeatures and Polynomial Regression3 minComputing Parameters AnalyticallyNormal Equation16 minNormal Equation3 minNormal Equation Noninvertibility5 minNormal Equation Noninvertibility2 minSubmitting Programming AssignmentsWorking on and Submitting Programming Assignments3 minProgramming tips from Mentors10 minReviewLecture Slides20 minQuiz: Linear Regression with Multiple Variables5 questionsOctave/Matlab TutorialOctave/Matlab TutorialBasic Operations13 minMoving Data Around16 minComputing on Data13 minPlotting Data9 minControl Statements: for, while, if statement12 minVectorization13 minReviewLecture Slides10 minThe course has ended. Assignments may not be resubmitted.Quiz: Octave/Matlab Tutorial5 questionsProgramming Assignment: Linear Regression3hWeek 3Logistic RegressionClassification and RepresentationClassification8 minClassification2 minHypothesis Representation7 minHypothesis Representation3 minDecision Boundary14 minDecision Boundary3 minLogistic Regression ModelCost Function10 minCost Function3 minSimplified Cost Function and Gradient Descent10 minSimplified Cost Function and Gradient Descent3 minAdvanced Optimization14 minAdvanced Optimization3 minMulticlass ClassificationMulticlass Classification: One-vs-all6 minMulticlass Classification: One-vs-all3 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/QEYX8/lecture-slides Quiz: Logistic Regression 5 questionsRegularizationSolving the Problem of OverfittingThe Problem of Overfitting9 minThe Problem of Overfitting3 minCost Function10 minCost Function3 minRegularized Linear Regression10 minRegularized Linear Regression3 minRegularized Logistic Regression8 minRegularized Logistic Regression3 minReviewLecture Slides10 minThe course has ended. Assignments may not be resubmitted.Quiz: Regularization5 questionsProgramming Assignment: Logistic Regression3hWeek 4Neural Networks: RepresentationMotivationsNon-linear Hypotheses9 minNeurons and the Brain7 minNeural NetworksModel Representation I12 minModel Representation I6 minModel Representation II11 minModel Representation II6 minApplicationsExamples and Intuitions I7 minExamples and Intuitions I2 minExamples and Intuitions II10 minExamples and Intuitions II3 minMulticlass Classification3 minMulticlass Classification3 minReviewLecture Slides10 minThe course has ended. Assignments may not be resubmitted.Quiz: Neural Networks: Representation5 questionsProgramming Assignment: Multi-class Classification and Neural Networks3hWeek 5Neural Networks: LearningCost Function and BackpropagationCost Function6 minCost Function4 minBackpropagation Algorithm11 minBackpropagation Algorithm10 minBackpropagation Intuition12 minBackpropagation Intuition4 minBackpropagation in PracticeImplementation Note: Unrolling Parameters7 minImplementation Note: Unrolling Parameters3 minGradient Checking11 minGradient Checking3 minRandom Initialization6 minRandom Initialization3 minPutting It Together13 minPutting It Together4 minApplication of Neural NetworksAutonomous Driving6 minReviewLecture Slides10 minhttps://www.coursera.org/learn/machine-learning/supplement/FklyY/lecture-slides The course has ended. Assignments may not be resubmitted.Quiz: Neural Networks: Learning5 questionsProgramming Assignment: Neural Network Learning3hWeek 6Advice for Applying Machine LearningEvaluating a Learning AlgorithmDeciding What to Try Next5 minEvaluating a Hypothesis7 minEvaluating a Hypothesis4 minModel Selection and Train/Validation/Test Sets12 minModel Selection and Train/Validation/Test Sets3 minBias vs. VarianceDiagnosing Bias vs. Variance7 minDiagnosing Bias vs. Variance3 minRegularization and Bias/Variance11 minRegularization and Bias/Variance3 minLearning Curves11 minLearning Curves3 minDeciding What to Do Next Revisited6 minDeciding What to do Next Revisited3 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/7BHrF/lecture-slides The course has ended. Assignments may not be resubmitted.Quiz: Advice for Applying Machine Learning5 questionsProgramming Assignment: Regularized Linear Regression and Bias/Variance3hMachine Learning System DesignBuilding a Spam ClassifierPrioritizing What to Work On9 minPrioritizing What to Work On3 minError Analysis13 minError Analysis3 minHandling Skewed DataError Metrics for Skewed Classes11 minTrading Off Precision and Recall14 minUsing Large Data SetsData For Machine Learning11 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/gFC7y/lecture-slides Quiz: Machine Learning System Design5 questionsWeek 7Support Vector MachinesLarge Margin ClassificationOptimization Objective14 minLarge Margin Intuition10 minMathematics Behind Large Margin Classification19 minKernelsKernels I15 minKernels II15 minSVMs in PracticeUsing An SVM21 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/pSe2X/lecture-slides The course has ended. Assignments may not be resubmitted.Quiz: Support Vector Machines5 questionsProgramming Assignment: Support Vector Machines3hWeek 8Unsupervised LearningClusteringUnsupervised Learning: Introduction3 minK-Means Algorithm12 minOptimization Objective7 minRandom Initialization7 minChoosing the Number of Clusters8 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/hFF7A/lecture-slides Quiz: Unsupervised Learning5 questionsDimensionality ReductionMotivationMotivation I: Data Compression10 minMotivation II: Visualization5 minPrincipal Component AnalysisPrincipal Component Analysis Problem Formulation9 minPrincipal Component Analysis Algorithm15 minApplying PCAReconstruction from Compressed Representation3 minChoosing the Number of Principal Components10 minAdvice for Applying PCA12 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/SCJi4/lecture-slides The course has ended. Assignments may not be resubmitted.Quiz: Principal Component Analysis5 questionsProgramming Assignment: K-Means Clustering and PCA 3hWeek 9Anomaly DetectionDensity EstimationProblem Motivation7 minGaussian Distribution10 minAlgorithm12 minBuilding an Anomaly Detection SystemDeveloping and Evaluating an Anomaly Detection System13 minAnomaly Detection vs. Supervised Learning7 minChoosing What Features to Use12 minMultivariate Gaussian Distribution (Optional)Multivariate Gaussian Distribution13 minAnomaly Detection using the Multivariate Gaussian Distribution14 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/pB0Jq/lecture-slides Quiz: Anomaly Detection5 questionsRecommender SystemsPredicting Movie RatingsProblem Formulation7 minContent Based Recommendations14 minCollaborative FilteringCollaborative Filtering10 minCollaborative Filtering Algorithm8 minLow Rank Matrix FactorizationVectorization: Low Rank Matrix Factorization8 minImplementational Detail: Mean Normalization8 minReviewLecture Slides10 minThe course has ended. Assignments may not be resubmitted.Quiz: Recommender Systems5 questionsProgramming Assignment: Anomaly Detection and Recommender Systems 3hWeek 10Large Scale Machine LearningGradient Descent with Large DatasetsLearning With Large Datasets5 minStochastic Gradient Descent13 minMini-Batch Gradient Descent6 minStochastic Gradient Descent Convergence11 minAdvanced TopicsOnline Learning12 minMap Reduce and Data Parallelism14 minReviewLecture Slides 10 minQuiz: Large Scale Machine Learning5 questionsWeek 11Application Example: Photo OCRPhoto OCRProblem Description and Pipeline7 minSliding Windows14 minGetting Lots of Data and Artificial Data16 minCeiling Analysis: What Part of the Pipeline to Work on Next13 minReviewLecture Slides 10 minhttps://www.coursera.org/learn/machine-learning/supplement/Q32e6/lecture-slides The course has ended. Assignments may not be resubmitted.Quiz: Application: Photo OCR5 questionsConclusionSummary and Thank You4 min]]></content>
      <tags>
        <tag>Stanford</tag>
        <tag>Machine Learning</tag>
        <tag>Coursera</tag>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter Notebook Note]]></title>
    <url>%2F2017%2F05%2F25%2FJupyter%20Notebook%20Note%2F</url>
    <content type="text"><![CDATA[stay stuned… 05-25-2017activate python278 conda install notebook ipykernel ipython kernel install --user cd G:\Udacity\MLND\P1 Model Evaluation and Validation\program files jupyter notebook boston_housing.ipynb delete python278conda env remove -n python278 show conda envs infoconda info --envs conda create -n py27 python=2.7 anaconda 1234activate py27conda listconda install numpy pandas matplotlib jupyter notebookdeactivate To uninstall, useconda remove package_name. To update a package conda update package_name. If you want to update all packages in an environment, which is often useful, use conda update --all If you don’t know the exact name of the package you’re looking for, you can try searching with conda search search_term.]]></content>
      <tags>
        <tag>jupyter notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera UIUC Data Mining notebook]]></title>
    <url>%2F2017%2F05%2F06%2FCoursera%20UIUC%20Data%20Mining%20Notebook%2F</url>
    <content type="text"><![CDATA[stay stuned…Text Mining and AnalyticsCourse can be found hereMy certificate can be found hereLecture slides can be found here Data VisualizationCourse can be found here Lecture slides can be found here About this course: Learn the general concepts of data mining along with basic methodologies and applications. Then dive into one subfield in data mining: pattern discovery. Learn in-depth concepts, methods, and applications of pattern discovery in data mining. We will also introduce methods for pattern-based classification and some interesting applications of pattern discovery. This course provides you the opportunity to learn skills and content to practice and engage in scalable pattern discovery methods on massive transactional data, discuss pattern evaluation measures, and study methods for mining diverse kinds of patterns, sequential patterns, and sub-graph patterns. Week 1 Course OrientationAbout the CourseWelcome to Data VisualizationOverviewIn this module, you will become familiar with the course, your instructor, your classmates, and our learning environment. TimeThis module should take approximately 2 hours of dedicated time to complete. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Read and review the Syllabus and About the Discussion Forums pages. 30 minutes Complete the Orientation Quiz 15 minutes Update your Coursera Profile and check out the Social Media page 15 minutes Post on the Meet and Greet Discussion Board 1 hour Goals and ObjectivesThe goal of the orientation module is to familiarize you with the course structure and the online learning environment. The orientation also helps you obtain the technical skills required for the course. After this module, you should be able to: Recall important information about this course. Know your classmates. Recall how discussion forums operate in the course SyllabusCourse DescriptionData Visualization is a course that teaches how to create visualizations that effectively communicate the meaning behind data to an observer through visual perception. We will learn how a computer displays information using computer graphics, and how the human perceives that information visually. We will also study the forms of data, including quantitative and non-quantitative data, and how they are properly mapped to the elements of a visualization to be perceived well by the observer. We will briefly overview some design elements for effective visualization, though we will not focus on the visual design needed to make attractive and artistic visualizations. This course does not require computer programming, but computer programming can be used to complete the exercises. The course will conclude with the integration of visualization into database and data-mining systems to provide support for decision making, and the effective construction of a visualization dashboard. I am continually looking to improve this course and may encounter some issues requiring us to make changes sooner rather than later. As such, this syllabus is subject to change. I appreciate your input and ask that you have patience as we make adjustments to this course. Course Goals and ObjectivesUpon successful completion of this course, you will be able to: Describe how 2-D and 3-D computer graphics are used to visualize data. Describe how an observer perceives and processes information from a visual display. Utilize a wide vocabulary of visualization methods and how best to apply them to different kinds of data. Decide which design styles and colors work best for different visualization situations. Visualize data when it is not numerical. Use techniques for visualizing databases and data mining to help visually sort through massive datasets. Analyze tasks and build visualization dashboards to provide data to support making a decision. Textbook and ReadingsThere is no required textbook for this class. However the following textbooks may be helpful. Visualization Analysis and Design by Tamara MunznerInformation Visualization: Perception for Design (3rd Edition) by Colin Ware Course OutlineThe course consists of 4 weekly modules that focus on the following. Week 1: The Computer and the Human Key Concepts: Introduction to visualization Using computer graphics to display data The model human processor and Fitts’s law Human visual perception and cognitionWeek 2: Visualization of Numerical Data Key Concepts: Different kinds of visualizations and how best to apply them to data Basic charts such as bar charts and scatter plots More advanced visualization techniques, such as streamgraphs and parallel coordinates Some elements of design and color usageWeek 3: Visualization of Non-Numerical Data Key Concepts: Graphs, networks, and hierarchies Layout of relational and hierarchical data, such as treemaps Methods for visualizing high-dimensional data, such as principal component analysis and multidimensional scalingWeek 4: The Visualization Dashboard Key Concepts: Visualizing large datasets Visualization of databases and data mining results Visual analytics for decision support Task analysis Visualization dashboards Elements of This CourseThe course is comprised of the following elements: Lecture videos. In each module the concepts you need to know will be presented through a collection of short video lectures. You may stream these videos for playback within the browser by clicking on their titles or download the videos. You may also download the slides that go along with the videos. Quizzes. Week 1 and Week 4 will include a for-credit quiz. You will be allowed 3 attempts at the quiz per every 8 hours. Each attempt may present a different selection of questions to you. Your best score will be used when calculating your final score in the class. There is no time limit on how long you take to complete each attempt at the quiz. Programming assignments. There are two required programming assignments for the class. The first programming assignment is to create a visualization of numerical data, and the second programming assignment is to create a visualization of non-numerical data (e.g., a network or a hierarchy). For each assignment, sample data will be provided, but you are encouraged to find your own data. Your goal will be to present that data in a visualization that helps the observer to better understand what the data represents. The programming assignments will be peer graded based on rubrics that measure how well the course’s methods have been applied to the visualization of the data. Information About LecturesThe lectures in this course contain the most important information you need to know. The following resources accompany each video: The play button will open the video up in your browser window and stream the lecture to you. The duration of the video (in hours-minutes-seconds format) is also listed. Within the player that appears, you can activate subtitles. English subtitles are available for all videos. All video lectures have a discussion forum dedicated to them. This is a great place to discuss any questions you have about the content of the video or to share your ideas and responses to the video. Discussion ForumsThe discussion forums are a key element of this course. Be sure to read more about the discussion forums and how you can make the most of them in this class. How to Pass the CourseTo qualify for a Course Certificate, simply start verifying your coursework at the beginning of the course, get an 80% or higher on all quizzes and assignments combined, and pay the fee. Coursera Financial Aid is available to offset the registration cost for learners with demonstrated economic needs. If you have questions about Course Certificates, please see the help topics here. Also note that this course is in the Data Mining Specialization offered by the University of Illinois at Urbana-Champaign. By earning a Course Certificate in this course, you are on your way toward earning a Specialization Certificate in Data Mining. You may also choose to pre-pay for the entire Specialization, at a discount. See more information about Specialization payments here. If you choose not to pay the fee, you can still audit the course. You will still be able to view all videos, submit practice quizzes, and view required assessments. Auditing does not include the option to submit required assessments. As such, you will not be able to earn a grade or a Course Certificate. Getting and Giving HelpYou can get/give help via the following means: Use the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic’s page within the Learner Help Center. Use the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and Community Mentors will monitor this forum and respond to issues.Note: Due to the large number of learners enrolled in this course, I am not able to answer emails sent directly to my account. Rather, all questions should be reported as described above. About the Discussion ForumsExpectationsWith the large number of learners in this course, no one is expected to read every post made within the discussion forums. Rather, read those that seem interesting to you and reply when you can further the conversation. Above all, you are expected to remain civil and treat all other learners with respect. Failure to do so will result in your removal from the course. Helpful ToolsSearching for Posts You can search for a specific key words by using the search box at the top of any individual forum or the All Course Discussions page. It may be helpful to see if your topic or question has already been covered before posting. If so, you can reply to that thread instead of starting a new one. You can also click on someone’s name in the forums (including your own) to see all of the posts that have been made by that person. Upvoting Posts When you view any post, you will see an Upvote button under each post. Click the button to draw attention to thoughtful, interesting, or helpful posts. In the course feedback forums, upvoting will ensure that important questions get addressed. The upvoting system is the best way elevate the best posts to be seen by other learners and University of Illinois staff. Reporting Inappropriate Posts Please report any posts that are abusive, offensive, that infringe upon copyright, or that otherwise violate Coursera’s Honor Code by using the Report this option found under the menu arrow to the right of each post. Following If you find a particular thread to be interesting to you, click the Follow button under the original post of that thread page to receive email notifications when new posts are made. Improving Your PostsThe forums are your chance to interact with thousands of like-minded individuals on these topics. Getting their attention is one way to do well in this course. In any social interaction, certain rules of etiquette are expected and contribute to more enjoyable and productive communication. The following are tips for interacting in this course via the forums, adapted from guidelines originally compiled by AHA! and Chuq Von Rospach &amp; Gene Spafford: Search the other posts to see if your topic is already covered. If it is, reply to that thread instead of starting a new one. Post in the most appropriate forum for your topic, and do not post the same thing in multiple forums. Use a meaningful title for your thread. Be civil. If you disagree, explain your position with respect and refrain from any and all personal attacks. Stay on topic. In particular, don’t change the subject in the middle of an existing thread – just start a new topic. Make sure you’re understood, even by non-native English speakers. Try to write full sentences, and avoid text-message abbreviations or slang. Be careful when you use humor and sarcasm as these messages are easy to misinterpret. If asking a question, provide as much information as possible, what you’ve already considered, what you’ve already read, etc. Cite appropriate references when using someone else’s ideas, thoughts, or words. Do not use a forum to promote your product, service, or business.Do not post personal information about other posters in the forum. Ignore spammers and report them.For more details, refer to Coursera’s Code of Conduct. Updating Your ProfileOptionally, please consider updating your profile, which can also be accessed by clicking the Profile link in the menu that appears when you click on the arrow next to your initials at the top-right corner of this screen. When people find you in the forums, they can click on your name to view your complete profile and get to know you more. Social MediaLearning takes place not only through direct instruction in excellent courses but also through quality interaction with your peers. Research suggests that creating and/or joining a learning community around your interests and passions helps you stay motivated to learn. You are encouraged to leverage social media platforms to connect with thousands of your peers from across the world. Learn from others, network, share what you learn, create study groups, discuss interesting course topics as well as off-topic ideas, and share your own perspectives about this subject. You can even set up or get notified about physical meetups on these forums so that you don’t miss out any opportunity to interact face-to-face with those who share your interests. Remember that the more active these communities are, the more value they will bring to all. We are also looking for participants in the course to take a leading role in keeping these communities active and of value to all of us, so do join your preferred platform and share! Connect to your classmates via one or more of the following means: Google+ Facebook Twitter Use #ILLINOISdatavisualization More InformationIf you find another social media page or community related to this course, feel free to share it in the discussion forums. You may also be interested in liking the following: University of Illinois at Urbana-Champaign on Facebook @Illinois_Alma on Twitter Coursera on Facebook @Coursera on Twitter NOTE: Do not post links to copyrighted materials in the Coursera discussion forums or on social networks. Doing so is a violation of the Coursera Terms of Service. ResourcesData Visualization Resources Below are some links and other resources for tools for scientific visualization. The projects that you will be working on in Week 2 and Week 3 do not require computer programming, and you can use any of the tools below, including the free data visualization websites. Thanks to UIUC Prof. Karrie Karahalios for many of the links. None of these links are in any particular order, so feel free to jump around. Online Data Visualization Websites DataHero Plotly Number Picture Polychart Juice Analytics Weave Datavisual Silk Zoomdata (via the cloud platforms) RAW Datawrapper Software Tableau SAP Lumira (including a free Personal Edition version) Microsoft Excel (or any other spreadsheet that includes charts) ClearStory BeyondCore Mathematica MATLAB MatPlotLib (if you are comfortable programming Python) R Programming Language ggplot2 Some Data Sources Google Public Data Explorer Data from the World Bank Orientation Activities[1 point]1.I am required to read a textbook for this course. False True[1 point]2.Which of the following activities are required to pass the course? Check all that apply. Forum assignments Programming assignments Quizzes Reading assignments[1 point]3.The following tools will help me use the discussion forums: “Searching” for your question or a topic before you post a new thread. “Following” any forums that are particularly interesting to me. “Upvoting” posts that are thoughtful, interesting, or helpful.[1 point]4.If I have a problem in the course I should: Call the instructor Drop the class Report it to the Learner Help Center (if the problem is technical) or to the Content Issues forum (if the problem is an error in the course materials). Email the instructor[1 point]5.This course includes __ weekly modules. 2 4 6 8 Week 1 InformationWeek 1 OverviewWeek 1: The Computer and the Human OverviewDuring this week’s module, you will explore what data visualization is, as well as various types of data visualization. You’ll also begin to understand how computers display 2-D and 3-D shapes, as well as draw some simple 2-D shapes of your own. In addition to this, you will learn about how humans perceive, learn, and reason about information. TimeThis module should take approximately 5-6 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 1 Video Lectures 2 hours Read the How the Programming Assignments Work Page 1-2 hours Week 1 Quiz 30 minutes Week 1 Discussion Initial Post 30 minutes Week 1 Discussion Response Posts 30 minutes Goals and ObjectivesUpon successful completion of this module, you will be able to: Understand what data visualization is and the different kinds of data visualization. Understand how computer graphics are used to display shapes in 2-D and 3-D, and draw simple 2-D shapes on a web page using Scalable Vector Graphics Understand how we perceive, learn, and reason about information. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. What is data visualization and how is it used? How does the computer display information? How does the user perceive information? Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Interactive visualization, presentation visualization, and interactive storytelling Scalable Vector Graphics and the difference between how graphics’ shapes are described versus how they are displayed. Photorealistic rendering and non-photorealistic rendering The Model Human Processor and Fitts’s law Lateral inhibition Tips for SuccessTo do well this week, I recommend that you do the following: Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week. When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better. It’s always a good idea to refer to the video lectures and chapter readings we’ve read during this week and reference them in your responses. When appropriate, critique the information presented. Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes! Getting and Giving HelpYou can get/give help via the following means: Use the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic’s page within the Learner Help Center. Use the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues Week 1 Introductionvideo0:08 So what is data visualization? Well, data is stored on a computer. But we want to work with the data, and so we need to get the data somehow out of the computer and into our heads. So we can think about the data. So we can understand the data. So we can gain insight into what the data represents. 0:27 And so the best way we’ve found to do that is using visual communication. And so data visualization is a high bandwidth connection between the computer and the human brain that transmits as efficiently as possible the data from the computer and inserts it into our brains. 0:47 And so here’s a sample system, a typical visualization system, that I’ve got diagrammed here. We have a scanner here that’s going to acquire data. In this case it’s going to use a laser beam in order to scan. The shape of this graduate student’s head, and then that data gets processed by the computer into geometry and color, and then it gets displayed on this display screen.1:14At this point we set up this visualization, this data visualization channel that’s going to take this data from the display screen. And inserted it into my brain through this perceptual channel. So, we get this visualization operation where I’m looking at the data, and it’s going through my perceptual system, going into my human memory and then I can start to think about it. In this case, there’s some strange things happening with the laser scanning here at the top of the students head. And I want to try to understand why that’s happening. 1:47 And so data visualization isn’t just about seeing data. It’s about understanding data and being able to make decisions based on the data. And so another example of how data visualization has been useful. Is when I discovered just as a university professor. 2:05 So, students apply for graduate school at Computer Science at the University of Illinois, and we have a webpage that they use to select faculty they might want to work with. And so we say, pick out the three faculty members you’d like to work with, and then we have drop down menus with all the faculty. And students pick the faculty they want to work with, and then the students 2:29 having made that choice, that information is entered into a database. And for each student applicant, we can see the faculty they might be interested in working with and we can start to get an idea for each faculty member what students they would want to pick from. 2:46 So, I wanted to plot to see how uniform this was across the faculty, and so I used a data visualization tool in order to do this and I came up with a standard bar chart. And, horizontally, these are basically the different faculty members. There’s some faculty members are in our department, and some are affiliated with our department, and some get more students interested in working with them, and some get fewer students working with them, and so horizontally I’ve got the a faculty members here and vertically I’m plotting the number of student that want to work with that faculty member. And so we’ve got some faculty members that are very popular and some that aren’t very popular and again, some of the one’s that aren’t very popular are faculty that are affiliated with other departments. And students usually apply to work with them through other departments. 3:37 And I’ve eliminated the faculty names here, just to avoid any embarrassment. So we’ve got this list and then I colour code it, the bar charts. This is a stacked bar chart. I’ve colour coded the bar charts based on blue, if the faculty member was a students first choice. Orange if the faculty member was the students second choice, and green if the faculty member was the students third choice. 4:04 And you might see something here, I certainly saw it when I first plotted this data, and that was that students were picking people in the first half of the alphabet. This is in alphabetical order. Students are picking faculty from the first part of the alphabet first, and they’re picking people from the second half of the alphabet mostly last. And then the middle part of the alphabet, or I should say, they’re picking second people uniformly across the alphabet and what I discovered was that from this visualization, was that students were picking faculty in alphabetical order. 4:46 And having made that discovery from that visualization, I was able to work back into the system and figure out why students were picking faculty in alphabetical order. And the reason was we had three pull down lists with all the faculty. And so the students would go down the list and find the first faculty member they were interested in and click there. They’d go down the list and pick the second faculty they were interested in and they would select that, and then they would find the third faculty member they were interested in and click that, and then that would go into the database, but when we were looking at the database, in order to evaluate what students might want to work with us. We were assuming students were picking the faculty they wanted to work with first, as the first choice, and their second choice was second, and their third choice was third. When in fact, they were just finding three faculty members and picking them when they appeared in the list. And so in this case data visualization helped us make a decision that we wouldn’t have been aware of otherwise. And so that’s a hallmark of data visualization is to provide 5:53 insight into the data, and be able to use that insight in order 5:58to make further decisions, further hypotheses to explore further.6:04And so that’s the kind of insight that I want to provide and the tools that I want to provide, to students in this course. And so we’ll work through basically four weeks of materials on data visualization. The first week, we’re just going to understand the platform for data visualization. Basically, how a computer displays data, and then how the user perceives and processes it. 6:31And so we’ll spend most of the first week just studying the computer and studying the human. The second week, we’ll introduce a few simple charts, bar charts and so on. And more importantly, we’ll look at the kinds of data that we want to display. That we want to visualize, and we’ll make sure. That we’re using the proper chart for the proper kinds of data. And that the proper kinds of data are being mapped to the correct elements of that chart. And then we’ll also talk about how to design charts effectively to convey that data. In week three, we’ll look at how to layout and display other data, non numerical data, relational data. When you’re data isn’t necessarily going to be a number that will give you a height and a bar chart, but is basically a relation that says item A is related to item B. 7:26How do we display that? How do we visualize that? How do we gain insight into that kind of data? So we spend week three on more looking at abstract data as opposed to numerical data. And then week four, we’ll talk about visualizing data bases, how to visualize the results of data mining. And how to analyze the way we work, so the visualization can provide us the insight into how do we make a decision. And so we’ll talk about how to make a visualization dashboard that gives you the information you need, in order to make an effective decision, and an informed decision. 8:06So I think it’s useful to talk about what we won’t be able to cover in these four weeks. We’re going to talk about abstract data visualization. I’m not going to be able to talk in much detail about specific tools used for visualization, or visualization tools for scientific visualization or medical visualization, business data, business intelligence systems and so on. There’s a number of tutorials. And manuals for using specific packages and we just won’t have time to go into any one of them, but i’ll try to provide examples across those different applications. 8:46I’ll be able to provide, provide a few design principles, but I won’t be able to cover a full course on visual design for presentation and so there are good courses on that. 8:59That you might take a look at, but, we’ll just be looking at some very minimal design principles that are most effective for data visualization.9:10And this will be an introduction to the topics of data visualization. And data visualization can be a very deep subject. And I won’t be able to get into the details of visualization. Will be at sort of a high level of the different areas of visualization. But focusing on the important topics of visualization to make any visualization successful, and to work well. I’ll try to provide pointers to tools, but again, not tutorials on how to use those tools, and there’ll be a couple of assignments in 9:50the course, and programming can be used to complete those assignments and I encourage you to program if you know how to do these assignments, but it’s not required and I’ll provide existing tools that you can use that you don’t need to write a program in order to use to visualize your data in order to complete these assignments and in general to be able to visualize data. And finally, I’d like to thank a few people that have been very helpful with putting this course together. Eric Shaffer is a lecturer here at the University of Illinois. And he’s talked in one of the most recent scientific visualization classes we had and he’s provided some materials for me for this class from that. I’ve very grateful for his help on that. Karrie Karahalios is also a professor of Computer Science here at the University of Illinois and she’s taught social visualization classes, especially visualizing, kind of relational data in social situations and she’s a world renowned expert on that area and has provided me with a lot of resources and website links for tools to help with a variety of visualization methods. Mike Gleicher is a professor, a friend of mine at the University of Wisconsin. We just completed a university wide data visualization course and had some nice material there that he provided me that I’ve also been able to use in this course. Donna Cox is a good friend of mine from the NCSA. Who taught me a lot of what I know about data visualization. And many other presentation visualization examples I’ll be showing are her work from the National Center for Supercomputing Applications here at the University of Illinois. And finally, Pat Hanrahan, professor at Stanford, has been very helpful and provided a lot of good advice and resources for data visualization, and I owe him my gratitude as well. [MUSIC] How the Programming Assignments WorkHow the Programming Assignments WorkThe Data Visualization course includes two programming assignments, one during Week 2 and another during Week 3. These programming assignments give you the opportunity to apply your understanding of data visualization to a visualization project. For Programming Assignment 1 (during Week 2), you will create a visualization of quantitative data (data involving numbers), whereas for Programming Assignment 2 (during Week 3), you will create a visualization of non-quantitative data, such as visualizing a network. Using a programming language is not required to complete either project, but it is allowed. Your programming assignments will be peer graded. During Weeks 2 and 3, I will detail what you are to submit and where you are to submit it. The programming assignments that you submit will then be graded by four or more of your peers. At the same time, you will grade programming assignments submitted by four or more of your peers. Your final score for each graded programming assignment will be the median of all the scores your work receives. You are required to grade fairly. While we do not manually review every score that is given to a peer, we do scan for trends within the data that suggest further investigation when someone is not giving a thoughtful score. Unfair grading is a violation of the Coursera Terms of Service and may constitute dismissal from the class and disqualification from earning a Course Certificate. Peer grading is an important part of the ecosystem that makes a course with this many participants successful. Keep in mind, however, that you are not only helping others when you grade a peer’s assignment, but you also benefit from this process. Carefully reviewing a peer’s assignment gives you insights into what others are doing and may spark ideas for your own assignments. Furthermore, when you receive feedback from your peers, you will likely gain ideas of how to improve your own assignments. Instructions for Week 1The first week is best spent preparing an environment for you to complete these two projects, and so we have a project milestone to set up your data visualization environment. This project milestone is not graded, but it will be very helpful for you to make progress on your project. Delaying work on your project will result in a lot more work for you in the next modules. If you plan to use computer programming to complete your programming assignments, then you should set up a programming environment that enables you to plot data and display graphics in an image, video, or other medium that can be made available through a webpage URL to others for grading. If you do not plan to use computer programming, then you should select a software/web platform that you will use to construct your data visualization. A good starting point for these software and online web data visualizations is available on the Data Visualization Resources page. If you are aware of additional tools for data visualization, please share them on the Programming Assignment 1 or Programming Assignment 2 forums. You should also consider what data you would like to visualize. Is there anything in particular about which you might be curious and would like to study? Or is there a subject with which you are already familiar but about which you would like to educate your colleagues? This is an opportunity to peruse the many data repositories freely available on the internet, and to use data visualization to learn and educate. The Data Visualization Resources page includes some links to get you started in your search for interesting data. If you are aware of additional sources of data, or come across an interesting datastore, please share these on the Programming Assignment 1 or Programming Assignment 2 forums. I want to encourage you to be as creative as possible with the programming assignments. I’ve been amazed at the work my students have submitted in my past visualization classes, and I’m really looking forward to seeing what you come up with. Lesson 1-1: Introduction1.1.1. Some Books on Data Visualizationvideo 1.1.2. Overview of Visualizationvideo Lesson 1-2: Graphics, Drawing and Photorealism1.2.1. 2-D Graphicsvideo SVG-examplevideowebsite 1.2.2. 2-D Drawingvideo 1.2.3. 3-D Graphicsvideo 1.2.4. Photorealismvideo 1.2.5. Non-Photorealismvideo Lesson 1-3: Humans and Visualizations1.3.1. The Humanvideo 1.3.2. Memoryvideo 1.3.3. Reasoningvideo 1.3.4. The Human Retinavideo 1.3.5. Perceiving Two Dimensionsvideo 1.3.6. Perceiving Perspectivevideo Week 1 ActivitiesQuiz with 10/10 points (100%)[1 point]1.Which kind of visualization would you use to try to answer a personal question about your data? Presentation visualizationInteractive visualizationInteractive storytelling[1 point]2.Which kind of visualization would you use to share a discovery about your data with your colleagues in a slide show?Interactive visualizationPresentation visualizationInteractive storytelling[1 point]3.Which kind of visualization would you use to create a web page that allows viewers to see a visualization of data that you prepared, but also allows the viewer to further investigate the data?Interactive storytellingPresentation visualizationInteractive visualization[1 point]4.In what order does a data visualization graphics pipeline process information?Pixel processing, then vertex processing, then rasterizationVertex processing, then pixel processing, then rasterizationPixel processing, then rasterization, then vertex processingRasterization, then pixel processing, then vertex processingRasterization, then vertex processing, then pixel processingVertex processing, then rasterization, then pixel processing[1 point]5.How many items can human working memory (short-term memory) typically hold?3–7 items30–70 items300–700 items[1 point]6.A light gray box drawn on top of a dark gray background will make the light gray box appear __.BrighterDarkerThe same as it appears on a white background[1 point]7.When visualizing data, you should keep your eyes focused on one point for the entire duration of the visualization.False, because your visual system will play tricks on your perception of the data.True, because your visual system will better detect any changes to datapoints during the visualization.[1 point]8.On which of these colors does the human eye have the most difficulty focusing?YellowBlueRedGreen[1 point]9.Which one of the 3-D depth cues below indicates surface orientation?StereopsisShadowingIlluminationOcclusion[1 point]10.Given a plot of life expectancy based on country and birth year, you look up your country and birth year, find the displayed life expectancy, and conclude you will probably live that long. This is an example of _.Inductive reasoningDeductive reasoningSubductive reasoningAbductive reasoning Discussion PromptOverviewBe sure you have read the About the Discussion Forums page before beginning this assignment. Time EstimateYou should spend approximately 1 hour to prepare, perhaps search the web, formulate your thoughts, submit your initial post, and read and respond to your classmates’ posts. Option 1 – Good Visualization Background Review the definition of data visualization and the differences between interactive visualization, presentation visualization, and online storytelling Question What web page URL would you point people to as a good example of presentation visualization or interactive storytelling? Which is it and why is it a good example of this type? What about the web page impresses you the most? How does the web page help you understand the data and gain insight into what the data represents? Option 2 – Optical Illusion Background Review your notes on visual perception and the various factors that can lead to the misinterpretation of an image. (In particular, review your lecture notes from or revisit the lectures “The Human Retina,” “Perceiving Two Dimensions,” and “Perceiving Perspective.”) Question What is a favorite optical illusion of yours? Can you provide a pointer to an image or video demonstrating it? Or better yet, can you reproduce your own version of it? Why is this an optical illusion? What elements of the human visual perceptual system are causing the effect of the illusion? RequirementsCompose an initial post that meets the following requirements: Includes 300-500 words. Succinctly summarizes your thoughts. Uses accepted standard American or British English (e.g., capitalization, punctuation, and spelling). Submission Note: This is an optional activity. All participants submit an initial post in the forum for this activity. Simply type your response in the box below. All participants should also review the posts of their classmates. Click the title of any post to review it and any replies already submitted. All participants should respond to at least 2 posts of their classmates. Click the Reply to Thread link next to any post to compose a reply.Participation is optional Week 2: Visualization of Numerical DataWeek 2 InformationWeek 2 Overview 10 minOverviewDuring this week’s module, you will learn how to appropriately select chart time and assign data to chart elements, all while learning how to visualize data in the most effective way possible. You’ll also learn how to plot data variables using higher dimensional visualization methods, and apply principles of design and color to make your visualizations compelling, engaging, and effective. TimeThis module should take approximately 4-5 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold):— | —Activity | Estimated Time RequiredWeek 2 Video Lectures | 2 hoursProgramming Assignment 1 submission | 2-3 hours Goals and ObjectivesUpon successful completion of this module, you will be able to: Select an appropriate chart time and assign data to appropriate chart elements to visualize data effectively. Understand basic charts and how their elements imply certain characteristics of the data they display. Plot more data variables using higher dimensional visualization methods including glyphs, parallel coordinates, and streamgraphs. Apply principles of design and color to make a data visualization more compelling, engaging, and effective. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. How can I apply these techniques to the data I encounter in my own work? Additional Readings and ResourcesDesign: The Visual Display of Quantitative Information by Edward Tufte Envisioning Information by Edward Tufte Visual Explanations: Images and Quantities, Evidence and Narrative by Edward Tufte Color: Information Visualization: Perception for Design by Colin Ware Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Data variables: nominal, ordinal, and quantitative; discrete v. continuous; dependent v. independent The perceptual accuracy of how different chart elements represent data variables How glyphs represent multiple dimensions of individual data items, how parallel coordinates plot data over many dimensions, and how streamgraphs improve on stacked bar charts Chartjunk, the data-ink ratio, and other design rules Hue, saturation, value, and other ways of thinking about color Tips for SuccessTo do well this week, I recommend that you do the following: Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week. When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better. It’s always a good idea to refer to the video lectures and chapter readings we’ve read during this week and reference them in your responses. When appropriate, critique the information presented. Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes! Getting and Giving HelpYou can get/give help via the following means: Use the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic’s page within the Learner Help Center. Use the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues Week 2 Introduction 2 minvideo Lesson 2-1: Data, Mapping, and Charts2.1.1. Data 7 minvideotranscript 2.1.2. Mapping 9 minvideo 2.1.3. Charts 9 minvideo Lesson 2-2: Glyphs, Parallel Coordinates, and Stacked Graphs2.2.1. Glyphs (Part 1) 4 minvideo 2.2.1. Glyphs (Part 2) 6 minvideo 2.2.2. Parallel Coordinates 8 minvideo 2.2.3. Stacked Graphs (Part 1) 5 minvideo 2.2.3. Stacked Graphs (Part 2) 6 minvideo http://www.visualisingdata.com/2010/08/making-sense-of-streamgraphs/ Lesson 2-3: Tufte’s Design Rules and Using Color2.3.1. Tufte’s Design Rules 12 minvideo 2.3.2. Using Color 11 minvideohttp://colorbrewer2.org/ Week 2 ActivitiesProgramming Assignment 1: Visualize Data Using a Chart 10 minhttps://www.coursera.org/learn/datavisualization/supplement/m6KuV/programming-assignment-1-visualize-data-using-a-chart OverviewThis assignment will give you a chance to explore the topics covered in Week 2 of the course by visualizing some data as a chart. The data set we provided deals with world temperatures and comes from NASA. Alternatively you can use any data that you would like to explore. You are not required to use D3.js, but if you would like to, we have provided some helpful resources that will make it easier for you to create a visualization. You are welcome to use the additional resources, especially if you do not want to program to complete this project. GoalsThe goal of this assignment is to give you some experience with handling and deciding how to visualize some data and for you to get a feel for the various aspects of the visualization. This assignment will also help you to analyze other visualizations and assess their effectiveness. Time EstimationThis is not a tricky assignment, but the amount of time that it takes will vary based on the tools you use and the amount of customization you would like to do in your chart. Instructions Take the data from the GISTEMP site, specifically the data from “Table Data: Global and Hemispheric Monthly Means and Zonal Annual Means.” Alternatively you can use any data that you would like to explore instead. Parse the data to a suitable format for the tools that you are using – we have provided two files (in JS, TXT, and CSV formats) that contain the data used to make the visualizations here, which is a subset of the data on the GISTEMP site. Visualize the data in any meaningful way you wish, keeping in mind the requirements of the Programming Assignment 1 Rubric. Click below to download the .zip file for this programming assignment.Programming Assignment 1 Data New.zipIf you’re interested, you can also download the original data by clicking below. Programming Assignment Data - GISTEMP Original.zipIf you need them, there are free-to-use websites that you can use to write and execute D3 code: Runnable (see also D3.js for examples) VIDA Log of Changes to the DataWe realize that we have updated the data provided from our side multiple times, so this a quick log of the changes: We updated the .zip by adding the CSV and TXT files, in response to a request. There was a formatting issue (there were commas in incorrect places) in the files that caused confusion. The formatting was changed to become clearer. It was pointed out that the data from the GISTEMP site did not match the data we had provided on our side. This was because there had been an update to the GISTEMP data, due to a bug which they had found, since the time we created the .zip on our side. We overlooked this update, which is why the data had differed. A small formatting change was also made for clarity. In the second DATA2 files, there are no longer spaces in the longitudinal demarcations. From “24N -90N,” it is now “24N-90N,” and likewise. Submission You must upload an image (or a URL to a website displaying an image) of your visualization for peer evaluation. In addition to your visualizations, please include a paragraph that helps explain your submission. Here are a few questions that your paragraph could answer: What are your x- and y-axes? Did you use a subset of the data? If so, what was it? Are there any particular aspects of your visualization to which you would like to bring attention? What do you think the data and your visualization shows? Submit Programming Assignment 1 EvaluationYour peers will use the Programming Assignment 1 Rubric to evaluate your submissions. You will also evaluate four of your peers’ assignments after you have submitted your assignment. This assignment is worth 15 points. Evaluate Programming Assignment 1 Q &amp; APlease post any issues or questions about this assignment to the Programming Assignment 1 Help Forum. Sample SubmissionsSample Submission 1:Graph Submission: Explanation:This is a sample submission of a visualization of the data from the GISTEMP site. Every line corresponds to a decade, with every point on the line being a year. The y-axis is the deviation from the 1951–1980 mean in 0.01 degrees. The x-axis then goes from the start to end of the decade. When hovering over any of the dots, there is a tooltip that displays that year along with the particular deviation for that year. The resulting graph shows a clear increasing mean temperature over the decades. Grading Rubric – Sample 1 Appropriate Chart Selection and Variables 5 points – The data is well represented by the assignment of data to variables. Design of the Chart 4 points – It is hard to follow crossing lines. Note: The Contest part of the rubric is up to you. Sample Comments from Instructor:The easiest fix to the first chart submission would be to use color (hue) to differentiate the decades (here shown as a nominal or ordinal variable indicating which line corresponds to which decade). Since the lines cross each other so often, it may be a little hard to follow one line closely. It would be a good idea to have each line colored differently to better distinguish between them. Sample Submission 2:Graph Submission: Explanation:This graph visualizes the GISTEMP data for the Globe and the North and South Hemispheres through all the given years. The blue line describes the data for the Northern Hemisphere, the red for the South Hemisphere, and the black line is for the Globe. The resulting graph shows an increasing mean global temperature over the years. Grading Rubric – Sample 2: Appropriate Chart Selection and Variables 5 points – The data is well represented by the assignment of data to variables. Design of the Chart 4 points – The lack of a legend makes the explanation necessary and reduces the direct effect of the chart. Note: The Contest part of the rubric is up to you. Sample Comments from Instructor:The graph would probably be better with a legend on the graph itself so it would be easier to tell what each line describes. Additional Resources for D3These Sample Submissions used D3.js for the graphs. You are not required to use D3.js, but if you would like to, the following are some helpful resources that will make it easier for you to create a visualization: D3 Tutorials and examples:D3 Tutorial Book D3 – Making a Line Graph Simple D3 Line Graph D3 – Line Graph Programming Assignment 1 Rubric 10 minCriteriaAppropriate Chart Selection and VariablesDid you select the appropriate chart and use the correct chart elements to visualize the nominal, ordinal, discrete, and continuous variables, as described in lecture 2.1.3? Continuous data variables should be assigned to continuous chart elements (e.g., lines between data points), whereas discrete variables should be assigned to discrete chart elements (e.g., separate bars). Furthermore, the assignment of variables to elements should follow the priorities in lecture 2.1.2. Design of the ChartDoes the chart effectively display the data, based on the design rules in lecture 2.3.1? ContestHow interesting is the result? Does this represent an interesting choice of data and/or an interesting way to display the data? For example, was a streamgraph used instead of an ordinary bar chart? Grading Criteria Poor (1–2 points) Fair (3 points) Good (4 points) Great (5 points) Appropriate Chart Selection and Variables Chart is indecipherable or significantly misleading because of poor chart type or assignment of variables to elements Major problem(s) with chart selection or assignment of elements to variables Minor problem(s) with chart selection or assignment of elements to variables Chart selection is appropriate for data and its elements properly assigned to appropriate data variables Design of the Chart No apparent attention paid to design Evidence that several of the design rules should have been followed but were not Evidence that one of the design rules should have been followed but was not Attention paid to all design rules Contest Misleading Boring Not boring Interesting Peer-graded Assignment: Programming Assignment 12hDue May 21, 11:59 PM PDT InstructionsBefore submitting your visualization image, make sure you review the full instructions page. Submit your assignment on the My submission tab. Enter your answers directly in the spaces provided in the My submission tab. You may save a draft of your work as you go, and you can come back later to continue working on your draft. When you are finished working, click the Preview button, verify your identity, and then click Submit for review to submit the assignment.Review criterialessBefore submitting your visualization image, make sure you review the Programming Assignment 1 Rubric page. You are required to evaluate the submissions of at least FOUR of your peers based on the instructions and rubric provided. You may begin giving feedback to other students as soon as you submit your assignment; click the Review classmates tab to begin. Feel free to provide additional reviews beyond the four required! My submissionmy submission Review Your Peers: Programming Assignment 1Due May 24, 11:59 PM PDTReady to review your classmates?Reviewing another person’s work is a valuable way to learn and providing quality feedback is a useful skill to master. Here are some tips to help you provide the most helpful, accurate, and consistent feedback possible: Remember, English isn’t everyone’s native language. Focus primarily on how well the work meets the grading criteria. When writing critical feedback, use a constructive and polite tone.Everyone enrolled in the course must review at least 4 other submissions to ensure everyone receives a grade; however, many learners complete more to help their classmates who are still waiting. Ready to get started? Discussion Prompt: Programming Assignment 1 Help Forum5 minWeek 3: Visualization of Non-Numerical DataJohn C. Hart In this week’s module, you will learn how to visualize graphs that depict relationships between data items. You’ll also plot data using coordinates that are not specifically provided by the data set. Week 3 InformationWeek 3 Overview 10 minOverviewDuring this week’s module, you will learn how to visualize graphs that depict relationships between data items. You’ll also plot data using coordinates that are not specifically provided by the data set. TimeThis module should take approximately 6-7 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 3 Video Lectures 2 hours Programming Assignment 1 Peer Evaluation 2 hours Programming Assignment 2 submission 2-3 hours Goals and ObjectivesUpon successful completion of this module, you will be able to: Visualize graphs that depict relationships between data items. Layout data using coordinates that are not explicitly provided by the data. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. Is there any data you encounter that is difficult to plot because it lacks appropriate coordinates? Is it because the data has too many dimensions? Or is it because the interesting details are in the data points themselves? Or is it the relationship between data points? Additional Readings and ResourcesDesign: Graphviz – This page provides tools for graph visualization you can use to complete Programming Assignment 2 without any actual programming if necessary. Graph Drawing: Algorithms for the Visualization of Graphs – A good but older book on algorithms for graph visualization. Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Graphs, especially different kinds of graphs (e.g., directed v. undirected), ways to represent graphs (e.g., adjacency matrix), and the associated terms (e.g., node degree) NOTE: Don’t worry if you do not have experience with linear algebra and some of the other concepts used in the slides. Terms like linear system, eigenvector, and optimization describe the tools we use in visualization. While I am trying to provide the details of those tools to the students who may be interested in them, understanding those details is not going to be required to complete the course, and we will provide implementations of those tools that you can simply use as a black box if needed. In any event, it will be good for everyone to be exposed to the math and computation needed to produce good coordinates to plot data when it does not already contain good coordinates. Social networks and centrality metrics Treemaps, word clouds, cartograms, and choropleths Tips for SuccessTo do well this week, I recommend that you do the following: Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week. When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better. It’s always a good idea to refer to the video lectures and chapter readings we’ve read during this week and reference them in your responses. When appropriate, critique the information presented.Take notes while you read the materials and watch the lectures for this week. - By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes! Getting and Giving HelpYou can get/give help via the following means: Use the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic’s page within the Learner Help Center. Use the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and Community Mentors will monitor this forum and respond to issues. Week 3 Introduction 1 minvideo Lesson 3-1: Graphs, Graph Visualization, and Tree Maps3.1.1. Graphs and Networks 8 minvideo 3.1.2. Embedding Planar Graphs 11 minhttps://www.coursera.org/learn/datavisualization/lecture/OZYbt/3-1-2-embedding-planar-graphs 3.1.3. Graph Visualization 13 minhttps://www.coursera.org/learn/datavisualization/lecture/7N1GE/3-1-3-graph-visualization 3.1.4. Tree Maps 9 minhttps://www.coursera.org/learn/datavisualization/lecture/9Pj4n/3-1-4-tree-maps Lesson 3-2: Principal Component Analysis and Multidimensional Scaling3.2.1. Principal Component Analysis 8 minhttps://www.coursera.org/learn/datavisualization/lecture/lvgIt/3-2-1-principal-component-analysis 3.2.2. Multidimensional Scaling 6 minhttps://www.coursera.org/learn/datavisualization/lecture/6ZQop/3-2-2-multidimensional-scaling Lesson 3-3: Packing3.3.1. Packing 12 minhttps://www.coursera.org/learn/datavisualization/lecture/wZ6DH/3-3-1-packinghttp://www.mappinghacks.com/data/ Week 3 ActivitiesProgramming Assignment 2: Visualize Network Data 10 minhttps://www.coursera.org/learn/datavisualization/supplement/TTbrU/programming-assignment-2-visualize-network-data OverviewThis assignment is meant to give you the opportunity to do non-coordinate data visualization, specifically using a network of your choosing. GoalsNetwork data lends itself easily to graph visualization. This assignment will allow you to explore the means of network visualization that you have encountered in the course, to decide how best to visualize data, and to analyze other network visualizations. Time EstimationThe time this assignment takes will vary greatly and depends a lot on your choice of data set, on the tools you use, and on the customization of your data. Try to leave yourself at least three hours to devote to this assignment. Instructions Find some network data that you think is suitable and that you would like to visualize. Here are some sites that provide links to a wide variety of different graph/network datasets: Stanford Large Network Dataset Collection UCI Network Data Repository Choose a visualization platform and parse the data into a format suitable for the tools you will use. For non-programmers, there are downloadable programs for creating graph visualizations at Graphviz. The program “neato,” which creates a layout for an undirected graph based on multidimensional scaling, is a good place to start. 1. The main challenge with using these tools is converting the graph data into the input text file format used by the tool, and understanding (and experimenting with) the various tool settings. For programmers, there are graph visualization tools available in D3 for JavaScript, such as force-directed graphs, treemaps, collision detection, and a nice graph drawing tutorial. Feel free to use any other libraries or languages as well.Visualize the data in a meaningful way, keeping in mind the requirements of the rubric. Submission You must upload an image of your visualization for peer evaluation. In addition to your visualization, please include a paragraph that helps explain your submission. A few questions that your paragraph could answer include: What is the data set that you chose? Why? Did you use a subset of the data? If so, what was it? Are there any particular aspects of your visualization to which you would like to bring attention? What do you think the data and your visualization show? Submit Programming Assignment 2 EvaluationYour peers will use the Programming Assignment 2 Rubric to evaluate your submissions. You will also evaluate four of your peers’ assignments after you have submitted your assignment. This assignment is worth 15 points. Evaluate Programming Assignment 2 Q &amp; APlease post any issues or questions about this assignment to the Programming Assignment 2 Help Forum. Sample SubmissionsSample Submission 1: Explanation: This is a graph visualization representing the most commonly occurring adjectives and nouns in the novel David Copperfield by Charles Dickens. The nodes are both adjectives and nouns, and the edges connect words that appear in an adjacent position in a book. Hovering over any node brings up the data associated with that node. The size of the node is determined by the number of edges it has, and the positions are determined by a force-directed layout algorithm. Grading Rubric Proximate Layout 5 points – Related data is connected properly, and the varying sizes helps to properly separate data. Design of the Visualization 4 points – Only one problem with the use of color. Note: The Contest part of the rubric is up to you. Sample Comments from Instructor: Even though there are many edge overlaps, this is a large non-planar graph. The items are well distributed and the edges are easy to follow visually through the overlaps. The frequency of words is nicely mapped to the area. The colors are assigned randomly. While the random colors help differentiate the nodes, there is no differentiation between adjectives and nouns, and so the visualization misses the opportunity to easily incorporate this dimension in its use of color at the nodes. Sample Submission 2: One way to create a graph from ordinary data is to create an edge between items and the categories they belong to. For example, this is a map of the CS faculty at UIUC. We had a list of faculty and the areas of CS they worked in. For example: … Han: database, mining Hart: graphics … Zhai: database, mining, bio, ML Using a dot file (for the “dot” layout program available from Graphviz) that assigned each faculty member and each research area to a node, we then made edges from each faculty member to the research areas they had listed. The result was the attached map of the CS department that clustered faculty by the areas they worked in. Grading Rubric Proximate Layout 4 points – Unnecessary and distracting overlaps of the edges and nodes Design of the Visualization 4 points – No differentiation between faculty and areas. Note: The Contest part of the rubric is up to you. Sample Comments from Instructor: Even though the instructor made this graph, it is far from perfect. While the edges do not overlap each other, the edges do overlap the nodes, which makes it difficult to see. For a graph with this few number of elements, one could adjust the placement by hand, perhaps using the free vector graphics editing utility “inkscape.” Also, the graph does not differentiate between faculty and areas, but could with node shapes and/or colors. Programming Assignment 2 Rubric 10 minProximate Layout How well are related items placed near each other? Do the edges cross or do items overlap when perhaps they do not need to? Are the crossings distracting? Design of the Visualization Does the visualization effectively utilize the assignment of variables to elements and design of a visualization described in Week 2? Contest How interesting is the result? Does this represent an interesting choice of data and/or an interesting way to display the data? Grading Criteria Poor (1–2 points) Fair (3 points) Good (4 points) Great (5 points) Proximate Layout Relationship between items cannot be discerned because of poor layout. Major problems with the layout, leading to many long edges and/or overlaps that distract from the data. Minor problems with the layout, resulting in long edges or unnecessary overlaps in objects or edges. Related items are placed near each other and intersections of visualization elements are not unnecessarily distracting. Design of the Visualization Relationship between items cannot be discerned because of poor element and/or design choices. Major problems with some elements and/or design choices that interfere with the display of the data. Minor problems with some elements and/or design choices that distract from the display of the data. Visualization effectively uses elements and design to display the data. Contest Misleading Boring Not boring Interesting Peer-graded Assignment: Programming Assignment 2 2hDue May 28, 11:59 PM PDT Install graphviz with pythonhttps://goo.gl/8Fz9PYopen the command window with Win + R and type pip install graphvizit appears1234567891011121314151617181920212223242526Collecting graphvizC:\Python27\lib\site-packages\pip\_vendor\requests\packages\urllib3\util\ssl_.py:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For moreinformation, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning. SNIMissingWarningC:\Python27\lib\site-packages\pip\_vendor\requests\packages\urllib3\util\ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning. InsecurePlatformWarning Downloading graphviz-0.7-py2.py3-none-any.whlInstalling collected packages: graphvizSuccessfully installed graphviz-0.7C:\Python27\lib\site-packages\pip\_vendor\requests\packages\urllib3\util\ssl_.py:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning. InsecurePlatformWarning using gvedit.exe d3 display with local server download gml2json.py from this website download graph data from UCI data open cmd in your file directory and type python gml2json.py dolphins.gml &gt;&gt; dolphinsdata.json to save information to dolphinsdata.json file download index.html and .json file from Force-Directed Graph and modify to your style open cmd in your file directory and type python -m SimpleHTTPServer and you can see your svg in http://localhost:8000/index.html D3 display with website log in this website runnable.com play it in this website my account http://code.runnable.com/u/SSQ Review Your Peers: Programming Assignment 2Due May 31, 11:59 PM PDT Discussion Prompt: Programming Assignment 2 Help Forum 5 minhttps://www.coursera.org/learn/datavisualization/discussions/weeks/3/threads/Mq4-EDvHEeew7Q7A7m3f-g Week 4: The Visualization DashboardIn this week’s module, you will start to put together everything you’ve learned by designing your own visualization system for large datasets and dashboards. You’ll create and interpret the visualization you created from your data set, and you’ll also apply techniques from user-interface design to create an effective visualization system. Week 4 InformationWeek 4 Overview 10 minOverviewDuring this week’s module, you will begin to design a visualization system for a large data set. After creating an initial overview of the data set, you’ll search for interesting elements in that data set using techniques such as zooming and filtering. Finally, you’ll apply techniques you learned from user-interface design to create an effective visualization system. TimeThis module should take approximately 5 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe lessons and assignments for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 4 Video Lectures 2 hours Programming Assignment 2 Peer Evaluation 2 hours Week 4 Quiz 1 hour Goals and ObjectivesUpon successful completion of this module, you will be able to: Design a visualization system for large datasets and dashboards to support informed decision making. Create an initial overview of a large dataset, and then find interesting elements through zooming, filtering, and requesting details when needed. Use visualization as a method for forming effective queries that reveal structure in a database. Apply techniques from user-interface design to create effective visualization systems. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. How would I use these techniques to design a visualization system or a dashboard for my own datasets? For the data cube lessons, consider how the operations would be applied to a very simple database. For example, an image can be stored as a database of pixels, with records of the form (x,y,b) where x,y are the pixel coordinates and b is the pixel’s brightness. How would the various data cube visualization methods work on that kind of data? Additional Readings and Resources Visualization Analysis and Design by Tamara Munzner (includes some tasks) Interaction Design: Beyond Human-Computer Interaction by Jenny Preece et al. (My lecture content on user interface design are based on an older version of this textbook.) Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Information visualization and visual analytics Focus+Context OLAP, data cubes, and other database concepts Requirements specification and other system design stages Synthesizability and other user interface concepts Tips for SuccessTo do well this week, I recommend that you do the following: Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week. When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better. It’s always a good idea to refer to the video lectures and chapter readings we’ve read during this week and reference them in your responses. When appropriate, critique the information presented. Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes! Getting and Giving HelpYou can get/give help via the following means: Use the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic’s page within the Learner Help Center. Use the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and Community Mentors will monitor this forum and respond to issues. Week 4 Introduction 55 sechttps://www.coursera.org/learn/datavisualization/lecture/ubdoC/week-4-introduction Lesson 4-1: Visualization Systems and Database Visualization4.1.1. Visualization Systems 3 minhttps://www.coursera.org/learn/datavisualization/lecture/7fsLR/4-1-1-visualization-systems 4.1.2. The Information Visualization Mantra: Part 1 9 minhttps://www.coursera.org/learn/datavisualization/lecture/BdRy4/4-1-2-the-information-visualization-mantra-part-1 4.1.2. The Information Visualization Mantra: Part 2 9 minhttps://www.coursera.org/learn/datavisualization/lecture/Vdwv3/4-1-2-the-information-visualization-mantra-part-2 4.1.2. The Information Visualization Mantra: Part 3 5 minhttps://www.coursera.org/learn/datavisualization/lecture/eTQR1/4-1-2-the-information-visualization-mantra-part-3 4.1.3. Database Visualization Part: 1 12 minhttps://www.coursera.org/learn/datavisualization/lecture/vtMpA/4-1-3-database-visualization-part-1 4.1.3. Database Visualization Part: 2 8 minhttps://www.coursera.org/learn/datavisualization/lecture/5gXTf/4-1-3-database-visualization-part-2 4.1.3. Database Visualization Part: 3 9 minhttps://www.coursera.org/learn/datavisualization/lecture/k9fP2/4-1-3-database-visualization-part-3 Lesson 4-2: Visualization System Design4.2.1. Visualization System Design 14 minhttps://www.coursera.org/learn/datavisualization/lecture/Svnib/4-2-1-visualization-system-design Week 4 ActivitiesQuiz: Week 4 Quiz8 questionsDue June 4, 11:59 PM PDTQUIZWeek 4 Quiz8 questionsTo Pass80% or higherAttempts3 every 8 hoursDeadlineJune 4, 11:59 PM PDT 1 point1.When creating an overview visualization of a large dataset, it is most important to: Display only an important subset of the datapoints so as to not overwhelm the user Display all of the data using a simple representation and axes that spread the data out as much as possible Pack as many details as possible into the display to be as efficient and informative as possible1 point2.The process of zooming on the data plotted in a visualization, where the zoomed region then fills the entire display: Is an important part of Schneiderman’s information visualization mantra Provides focus on the zoomed portion of the data Is actually a filtering operation on the display coordinates of the plotted data All of the above None of the above1 point3.The goal of the filtering step of the information visualization mantra is to: Remove outliers Display a subset Smooth noisy data1 point4.Which of the following benefits of a fisheye lens is LEAST important for data visualization? It allows zooming without obscuring the unzoomed data. It supports focus on detail along with the context of that detail amid the rest of the dataset. It makes the data appear more interesting.1 point5.Suppose we have a dataset representing an image consisting of pixel records of the form (x,y,b) where x and y are the spatial coordinates of the pixel and b is the brightness of the pixel. Then, which of the following provides the best histogram of the data? A subdivision of the image’s (x,y) coordinates into regions, plotting an average pixel brightness for each region A plot of the average pixel brightness over the y axis, of all pixels that share the same x coordinate A subdivision of the image’s brightness values into ranges, plotting the count of the pixels whose brightness is in each range1 point6.Suppose we have a dataset representing an image consisting of pixel records of the form (x,y,b) where x and y are the spatial coordinates of the pixel and b is the brightness of the pixel. Then, which of the following represents a “rollup” of the x and y dimensions of this dataset? The average brightness of the image The position of the center of the image None of the above1 point7.Suppose we have a dataset representing an image consisting of pixel records of the form (x,y,b) where x and y are the spatial coordinates of the pixel and b is the brightness of the pixel. Which axis definition would we NOT use if we wanted to plot the pixel brightness at a unique axis location for every pixel in the image? An axis formed by nesting the x dimension under the y dimension An axis formed by the product of the x dimension and the y dimension An axis formed by the concatenation of the x dimension and the y dimension1 point8.When designing a dashboard visualization, what should the primary concern be? That the dashboard visualization presents all of the data necessary to make an informed decision That the dashboard visualization engages the user to motivate further study of the data That the dashboard provide a simple overview of all the data, without any distracting details View my Certification Text Retrieval and Search EnginesCourse can be found hereLecture slides can be found hereAbout this course: Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media such as blog articles, forum posts, product reviews, and tweets. Text data are unique in that they are usually generated directly by humans rather than a computer system or sensors, and are thus especially valuable for discovering knowledge about people’s opinions and preferences, in addition to many other kinds of knowledge that we encode in text. This course will cover search engine technologies, which play an important role in any data mining applications involving text data for two reasons. First, while the raw data may be large for any particular problem, it is often a relatively small subset of the data that are relevant, and a search engine is an essential tool for quickly discovering a small subset of relevant text data in a large text collection. Second, search engines are needed to help analysts interpret any patterns discovered in the data by allowing them to examine the relevant original text data to make sense of any discovered pattern. You will learn the basic concepts, principles, and the major techniques in text retrieval, which is the underlying science of search engines. resources ChengXiang Zhai Welcome to Text Retrieval and Search Engines! You’re joining thousands of learners currently enrolled in the course. I’m excited to have you in the class and look forward to your contributions to the learning community. To begin, I recommend taking a few minutes to explore the course site. Review the material we’ll cover each week, and preview the assignments you’ll need to complete to pass the course. Click Discussions to see forums where you can discuss the course material with fellow students taking the class. If you have questions about course content, please post them in the forums to get help from others in the course community. For technical problems with the Coursera platform, visit the Learner Help Center. Good luck as you get started, and I hope you enjoy the course! OrientationYou will become familiar with the course, your classmates, and our learning environment. The orientation will also help you obtain the technical skills required for the course. About the CourseWelcome to Text Retrieval and Search Engines! 10 minOverviewIn this module, you will become familiar with the course, your instructor, your classmates, and our learning environment. TimeThis orientation should take approximately 3 hours to complete. Goals and ObjectivesThe goal of the orientation module is to familiarize you with the course structure and the online learning environment. The orientation also helps you obtain the technical skills required for the course. After this module, you should be able to: Recall important information about this course. Get to know your classmates. Be familiar with how discussion forums operate in the course. Instructional ActivitiesBelow is a list of the activities and assignments you must complete in this module. Click on the name of each activity for more detailed instructions. Activity Estimated Time Required Watch the Welcome Video and Course Introduction Video 15 minutes Read and review the Syllabus and About the Discussion Forums pages 30 minutes Complete the Orientation Quiz 15 minutes Complete the course Pre-Quiz 30 minutes Update your Coursera Profile and check out the Social Media page 15 minutes Post on the Meet and Greet Discussion Board 1 hour Syllabus 10 minCourse DescriptionRecent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media such as blog articles, forum posts, product reviews, and tweets. Text data are unique in that they are usually generated directly by humans rather than a computer system or sensors, and are thus especially valuable for discovering knowledge about people’s opinions and preferences, in addition to many other kinds of knowledge that we encode in text. This course will cover search engine technologies, which play an important role in any data mining applications involving text data for two reasons. First, while the raw data may be large for any particular problem, it is often a relatively small subset of the data that are relevant, and a search engine is an essential tool for quickly discovering a small subset of relevant text data in a large text collection. Second, search engines are needed to help analysts interpret any patterns discovered in the data by allowing them to examine the relevant original text data to make sense of any discovered pattern. You will learn the basic concepts, principles, and the major techniques in text retrieval, which is the underlying science of search engines. Course Goals and ObjectivesBy the end the course, you will be able to do the following: Explain many basic concepts and multiple major algorithms in text retrieval and search engines. Explain how search engines and recommender systems work and how to quantitatively evaluate a search engine. Create a test collection, run text retrieval experiments, and experiment with ideas for improving a search engine (if you complete the programming assignment). Course OutlineThe course consists of 6 weekly modules. Please note: There are no required readings for this course. All readings listed below are optional and are primarily from the textbook C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Week 1Key Concepts: Part of Speech tagging, syntactic analysis, semantic analysis, and ambiguity “Bag of words” representation Push, pull, querying, browsing Probability ranking principle Relevance Vector space model Dot product Bit vector representation Recommended Readings: N. J. Belkin and W. B. Croft. 1992. Information filtering and information retrieval: Two sides of the same coin? Commun. ACM 35, 12 (Dec. 1992), 29-38. C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapters 1 - 6. Week 2Key Concepts: Term frequency (TF) Document frequency (DF) and inverse document frequency (IDF) TF transformation Pivoted length normalization BM25 Inverted index and postings Binary coding, unary coding, gamma-coding, and d-gap Zipf’s law Recommended Readings: C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapter 6 - Section 6.3, and Chapter 8. Ian H. Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition. Morgan Kaufmann, 1999. Week 3Key Concepts: Cranfield evaluation methodology Precision and recall Average precision, mean average precision (MAP), and geometric mean average precision (gMAP) Reciprocal rank and mean reciprocal rank F-measure Normalized Discounted Cumulative Gain (nDCG) Statistical significance test Recommended Readings: Mark Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247-375. C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapter 9 Week 4Key Concepts: p(R=1|q,d), query likelihood, and p(q|d) Statistical and unigram language models Maximum likelihood estimate Background, collection, and document language models Smoothing of unigram language models Relation between query likelihood and TF-IDF weighting Linear interpolation (i.e., Jelinek-Mercer) smoothing Dirichlet Prior smoothing Recommended Readings: C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapter 6 - Section 6.4 Week 5Key Concepts: Relevance feedback Pseudo-relevance feedback Implicit feedback Rocchio feedback Kullback-Leiber divergence (KL-divergence) retrieval function Mixture language model Scalability and efficiency Spams Crawler, focused crawling, and incremental crawling Google File System (GFS) MapReduce Link analysis and anchor text PageRank and HITS Recommended Readings: C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapters 7 &amp; 10 Week 6Key Concepts: Learning to rank, features, and logistic regression Content-based filtering Collaborative filtering Beta-Gamma threshold learning Linear utility User profile Exploration-exploitation tradeoff Memory-based collaborative filtering Cold start Recommended Readings: C. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapters 10 - Section 10.4, Chapter 11. Elements of This CourseThe course is comprised of the following elements: Lecture videos. Each week your instructor will teach you the concepts you need to know through a collection of short video lectures. You may either stream these videos for playback within the browser by clicking on their titles, or you can download each video for later offline playback by clicking the download icon. The videos in each week is usually total 1.5 to 2 hours, but you generally need to spend at least the same amount of time digesting the content in the video. The actual amount of time needed to digest the content would naturally vary according to your background. Quizzes. Each week will include one for-credit quiz. Your cumulative score will be used when calculating your final score in the class. There is no time limit on how long you take to complete each quiz, and you will be allowed 3 attempts at the quiz every 8 hours. The deadline for all quizzes is the last day of the course. The quiz in each week should take less than 1 hour to finish, assuming you have mastered the materials to be tested in the quiz, and you should make sure that you have mastered the materials before you attempt to work on the quizzes. You should use the practices to help you understand and master the materials. Programming assignments. The programming assignments for this course are optional, but they provide an opportunity for you to practice your programming skills and apply what you’ve learned in the course. The programming assignment is optional and you can budget about 2 hours each week to work on it if you plan to finish it; you may need to budget more time for this if you are not familiar with C++ programming. Information about LecturesThe lectures in this course contain the most important information you need to know. You can access these lectures in each week’s lesson section. The following resources accompany each video: The play button will open the video up in your browser window and stream the lecture to you. The duration of the video (in hours-minutes-seconds format) is also listed. Some lectures may include in-video questions described above. Within the player that appears, you can click the CC button to activate closed captions. English captions are available for all videos. All video lectures have a discussion forum dedicated to them. This is a great place to discuss any questions you have about the content of the video or to share your ideas and responses to the video. Discussion ForumsThe discussion forums are a key element of this course. Be sure to read more about the discussion forums and how you can make the most of them in this class. How to Pass the CourseI am continually looking to improve this course and may encounter some issues requiring us to make changes sooner rather than later. As such, this syllabus is subject to change. I appreciate your input and ask that you have patience as we make adjustments to this course. I also recognize that this is no ordinary course. You may have different perspectives and different goals for this course than some of your peers, or that I could have anticipated. Therefore, I want to empower you to customize this course to meet your needs. To qualify for a Course Certificate, simply start verifying your coursework at the beginning of the course, get an 70% or higher on all quizzes and assignments combined, and pay the fee. Coursera Financial Aid is available to offset the registration cost for learners with demonstrated economic needs. If you have questions about Course Certificates, please see the help topics here. Also note that this course is in the Data Mining Specialization offered by the University of Illinois at Urbana-Champaign. By earning a Course Certificate in this course, you are on your way toward earning a Specialization Certificate in Data Mining. You may also choose to pre-pay for the entire Specialization, at a discount. See more information about specialization payments here. If you choose not to pay the fee, you can still audit the course. You will still be able to view all videos, submit practice quizzes, and view required assessments. Auditing does not include the option to submit required assessments. As such, you will not be able to earn a grade or a Course Certificate. Getting and Giving HelpYou can get/give help via the following means: Use the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic’s page within the Learner Help Center. Use the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and Community Mentors will monitor this forum and respond to issues. About the Discussion Forums 10 minExpectationsWith the large number of learners in this course, no one is expected to read every post made within the discussion forums. Rather, read those that seem interesting to you and reply when you can further the conversation. Above all, you are expected to remain civil and treat all other learners with respect. Failure to do so will result in your removal from the course. Helpful ToolsSearching for Posts You can search for a specific key words by using the search box at the top of any individual forum or the All Course Discussions page. It may be helpful to see if your topic or question has already been covered before posting. If so, you can reply to that thread instead of starting a new one. You can also click on someone’s name in the forums (including your own) to see all of the posts that have been made by that person. Upvoting Posts When you view any post, you will see an Upvote button under each post. Click the button to draw attention to thoughtful, interesting, or helpful posts. In the course feedback forums, upvoting will ensure that important questions get addressed. The upvoting system is the best way elevate the best posts to be seen by other learners and University of Illinois staff. Reporting Inappropriate Posts Please report any posts that are abusive, offensive, that infringe upon copyright, or that otherwise violate Coursera’s Honor Code by using the Report this option found under the menu arrow to the right of each post. Following If you find a particular thread to be interesting to you, click the Follow button under the original post of that thread page to receive email notifications when new posts are made. Improving Your PostsThe forums are your chance to interact with thousands of like-minded individuals on these topics. Getting their attention is one way to do well in this course. In any social interaction, certain rules of etiquette are expected and contribute to more enjoyable and productive communication. The following are tips for interacting in this course via the forums, adapted from guidelines originally compiled by AHA! and Chuq Von Rospach &amp; Gene Spafford: Search the other posts to see if your topic is already covered. If it is, reply to that thread instead of starting a new one. Post in the most appropriate forum for your topic, and do not post the same thing in multiple forums. Use a meaningful title for your thread. Be civil. If you disagree, explain your position with respect and refrain from any and all personal attacks. Stay on topic. In particular, don’t change the subject in the middle of an existing thread – just start a new topic. Make sure you’re understood, even by non-native English speakers. Try to write full sentences, and avoid text-message abbreviations or slang. Be careful when you use humor and sarcasm as these messages are easy to misinterpret. If asking a question, provide as much information as possible, what you’ve already considered, what you’ve already read, etc. Cite appropriate references when using someone else’s ideas, thoughts, or words. Do not use a forum to promote your product, service, or business.Do not post personal information about other posters in the forum. Ignore spammers and report them.For more details, refer to Coursera’s Code of Conduct. Updating your Profile 10 minOptionally, please consider updating your profile, which can also be accessed by clicking the Profile link in the menu that appears when you click on the arrow next to your initials at the top-right corner of this screen. When people find you in the forums, they can click on your name to view your complete profile and get to know you more. Social Media 10 minFor some people, social media helps them get even more connected with the people and topics of interest to them. This page is to help you get connected to this course via social media. You are not required to participate in social media in any way. This information is presented merely for those who want to engage with the course in this way. We have provided it to give you an opportunity to further connect with your classmates, learn what others are saying about the class, create study groups, discuss interesting course topics as well as off-topic ideas, and share your own perspectives about this subject. Connect to your classmates via one or more of the following means: Google+ Facebook Twitter - #ILLINOIStextretrieval Google Maps Where Are We From?Google Map Instructions Want to find out where everyone else taking this course lives? Check out this Google Map. If you haven’t done so already, consider adding a pushpin indicating where you live. No need to be too specific, but put a pin on the town or even country in which you live. To do so: First, you must be logged into your Google account (e.g., a service such as Gmail). Then, go to the map. Next, search for your general location via the search box at the top. Next, click on the pushpin that drops in your general location. If there is no box when you click on the pushpin, then likely you were not logged into Google before clicking the map link. Click Add to map. The pushpin will turn red when it’s added to the map. No need to specify your name or anything. Just click the OK button. That’s it! More InformationIf you find another social media page or community related to this course, feel free to share it in the discussion forums. You may also be interested in liking or following: University of Illinois at Urbana-Champaign on Facebook @Illinois_Alma on Twitter Coursera on Facebook @Coursera on Twitter NOTE: Do not post links to copyrighted materials in the Coursera discussion forums or on social networks. Doing so is a violation of the Coursera Terms of Service and Facebook Statement of Rights and Responsibilities. Course Errata 10 minFor updates on changes made to the course, please see the Course Errata website. Orientation ActivitiesCourse Welcome Video 3 minhttps://www.coursera.org/learn/text-retrieval/lecture/04Py5/course-welcome-video Course Introduction Video 11 minhttps://www.coursera.org/learn/text-retrieval/lecture/4nB2z/course-introduction-video Quiz: Orientation Quiz 5 questions1 point1.This course lasts for ___ weeks. 4 8 10 6 1 point2.I am required to read a textbook for this course. False True 1 point3.Which of the following activities are required each week to get credit for the course? Check all that apply. Forum Assignments Quizzes Programming Assignments Reading Assignments 1 point4.The following tools will help me use the discussion forums: “Searching” for your question or a topic before you post a new thread. “Following” any forums that are particularly interesting to me. “Voting-up” posts that are thoughtful, interesting, or helpful 1 point5.If I have a problem in the course I should: Drop the class Call the instructor Report it to the Learner Help Center (if the problem is technical) or to the Content Issues forum (if the problem is an error in the course materials). Email the instructor Practice Quiz: Pre-Quiz 13 questionsPRACTICE QUIZPre-Quiz13 questionsTo Pass70% or higherDeadlineMay 28, 11:59 PM PDT 1 point1.Probability &amp; Statistics: Rolling a 6-faced die, what’s the probability of seeing a “6”? 1/6 1/3 1 1/21 point2.Probability &amp; Statistics: Rolling a 6-faced die, what’s the probability of seeing an even number? 1/3 1/2 1/6 11 point3.Probability &amp; Statistics: Rolling a 6-faced die, given that the number is even, what’s the probability that we’ve got a “6”? 1/2 1/6 1 1/31 point4.Probability &amp; Statistics: Rolling two independent 6-faced dice, what’s the probability that both dice show the same number? 1/2 1/3 1/6 11 point5.Linear algebra: What’s the value of 2 * x, where x =[1.0,2.0,3.0]? [2.0, 2.0, 3.0] [1.0, 2.0, 3.0] [2.0, 4.0, 6.0]1 point6.Linear algebra: If x=[1,2,3] and y=[1,−2,2], what’s the dot product x and y? 1 4 3 21 point7.Linear algebra: What is the result of multiplying matrix M=[1221] by a vector xT=[1,1], [3,3] [4,4] [1,1] [2,2]1 point8.Basic algebra: if x and y are both positive numbers, and x &gt; y, then what can we say about log(x) and log(y)? log(x) &lt; log(y) This can’t be determined log(x) &gt; log(y)1 point9.Basic algebra: Which of the following is true? log(x-y) = log(x) - log(y) log(xy) = log(x) + log(y) log (x+y) = log(x) + log(y)1 point10.Basic algebra: Which of the following is true (where exp(x) is the exponential function with e as the base)? exp(x+y) = ln(x)*ln(y) ln(x+y) = exp(x) + exp(y) exp(ln (x)) = x1 point11.Basic Computer Science: Which of the following operations occur in a computer program faster? Reading a 32-bit integer from the memory Reading a 32-bit integer from the hard disk.1 point12.Basic Computer Science: Which of the following statements is true about data structures? A linked list is the best for supporting direct access to any element in the list A linked list to store k integers would require more storage space than an array to store the same k integers.CorrectCorrect! Linked lists also require storing header information. Hash table is faster for sequential access to the elements than a linked list1 point13.Basic Computer Science: What’s the value of the binary code 1011? 10 9 11 12 Week 1During this week’s lessons, you will learn of natural language processing techniques, which are the foundation for all kinds of text-processing applications, the concept of a retrieval model, and the basic idea of the vector space model. Week 1 InformationWeek 1 Overview 10 minDuring this week’s lessons, you will learn the overall course design, an overview of natural language processing techniques, which are the foundation for all kinds of text-processing applications, the concept of a retrieval model, and the basic idea of the vector space model. TimeThis module should take approximately 3 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with required assignments in bold): Activity Estimated Time Required Week 1 Video Lectures 2 hours Week 1 Graded Quiz 1 hour Goals and ObjectivesAfter you actively engage in the learning experiences in this module, you should be able to: Explain some basic concepts in natural language processing, text information access. Explain why text retrieval is often defined as a ranking problem. Explain the basic idea of the vector space retrieval model and how to instantiate it with the simplest bit-vector representation. Guiding QuestionsDevelop your answers to the following guiding questions while watching the video lectures throughout the week. What does a computer have to do in order to understand a natural language sentence? Lexical analysis (part-of-speech tagging); Syntactic analysis (Parsing); Pragmatic analysis (speech act) What is ambiguity?•Word-level ambiguity: E.g.,–“design” can be a noun or a verb (Ambiguous POS)–“root” has multiple meanings (Ambiguous sense)•Syntactic ambiguity: E.g.,–“natural language processing” (Modification)–“A man saw a boy with a telescope.” (PP Attachment)•Anaphora resolution: “John persuaded Bill to buy a TV for himself.” (himself = John or Bill?)•Presupposition: “He has quit smoking.” implies that he smoked before. Why is natural language processing (NLP) difficult for computers? –Ambiguity is a “killer”!–Common sense reasoning is pre-required What is bag-of-words representation? Why do modern search engines use this simple representation of text? In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. the search task is not all that difficult. What are the two modes of text information access? Which mode does a web search engine such as Google support? Pull vs. Push pull When is browsing more useful than querying to help a user find relevant information? Works well when the user wants to explore information, doesn’t know what keywords to use, or can’t conveniently enter a query Why is a text retrieval task defined as a ranking task? Document selection(absolute relevance), Document ranking(relative relevance) Problems of Document Selection•The classifier is unlikely accurate–“Over-constrained” query  no relevant documents to return–“Under-constrained” query  over delivery–Hard to find the right position between these two extremes•Even if it is accurate, all relevant documents are not equally relevant (relevance is a matter of degree!)–Prioritization is needed•Thus, ranking is generally preferred What is a retrieval model? formalization of relevance (give a computational definition of relevance) •Similarity-based models: f(q,d) = similarity(q,d)–Vector space model•Probabilistic models: f(d,q) = p(R=1|d,q), where R {0,1}–Classic probabilistic model $\rightarrow$ BM25–Language model $\rightarrow$ Query Likelihood–Divergence-from-randomness model $\rightarrow$ PL2•Probabilistic inference model: f(q,d) = p(dq)•Axiomatic model: f(q,d) must satisfy a set of constraints•These different models tend to result in similar ranking functions involving similar variables What are the two assumptions made by the Probability Ranking Principle? –The utility of a document (to a user) is independent of the utility of any other document–A user would browse the results sequentially What is the Vector Space Retrieval Model? How does it work? Similarity-based models Represent a doc/query by a term vector Term: basic concept. e.g.,word or phrase Each term defines one dimension N terms define an N-dimensional space Query vector: q=($x_1$, …$x_N$), $x_i\in R$ is query term weight Doc vector: d=($y_1$, …$y_N$), $y_j\in R$ is doc term weight relevance(q,d) $\propto$ similarity(q,d) =f(q,d) How do we define the dimensions of the Vector Space Model? What does “bag of words” representation mean? Represent a doc/query by a term vector Term: basic concept. e.g.,word or phrase Each term defines one dimension N terms define an N-dimensional space In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. What does the retrieval function intuitively capture when we instantiate a vector space model with bag of words representation and bit representation for documents and queries? word presence/absence Additional Readings and ResourcesThe following readings are optional: N. J. Belkin and W. B. Croft. 1992. Information filtering and information retrieval: Two sides of the same coin? Commun. ACM 35, 12 (Dec. 1992), 29-38. C. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapters 1-6. Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Part of speech tagging, syntactic analysis, semantic analysis, and ambiguity “Bag of words” representation Push, pull, querying, browsing Probability ranking principle Relevance Vector space model Dot product Bag of words representation Bit vector representation Tips for SuccessTo do well this week, I recommend that you do the following: Review the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week. When possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better. It’s always a good idea to refer to the video lectures and chapter readings we’ve read during this week and reference them in your responses. When appropriate, critique the information presented. Take notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes! Getting and Giving HelpYou can get/give help via the following means: Use the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic’s page within the Learner Help Center. Use the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and Community Mentors will monitor this forum and respond to issues. Week 1 LessonsLesson 1.1: Natural Language Content Analysis 21 min What is Natural Language Processing (NLP)? State of the Art in NLP NLP for Text Retrieval https://www.coursera.org/learn/text-retrieval/lecture/rLpwp/lesson-1-1-natural-language-content-analysis Lesson 1.2: Text Access 9 minModes: Push Pull Querying Browsing https://www.coursera.org/learn/text-retrieval/lecture/OvxTu/lesson-1-2-text-access Lesson 1.3: Text Retrieval Problem 26 min What is Text Retrieval? Text Retrieval vs. Database Retrieval Document Selection vs. Document Ranking https://www.coursera.org/learn/text-retrieval/lecture/CXoWB/lesson-1-3-text-retrieval-problem Lesson 1.4: Overview of Text Retrieval Methods 10 min Term Frequency(TF) and Document Frequency(DF) https://www.coursera.org/learn/text-retrieval/lecture/gxXq6/lesson-1-4-overview-of-text-retrieval-methods Lesson 1.5: Vector Space Model - Basic Idea 9 min Represent a doc/query by a term vector Term: basic concept. e.g.,word or phrase Each term defines one dimension N terms define an N-dimensional space Query vector: q=($x_1$, …$x_N$), $x_i\in R$ is query term weight Doc vector: d=($y_1$, …$y_N$), $y_j\in R$ is doc term weight https://www.coursera.org/learn/text-retrieval/lecture/o8WNd/lesson-1-5-vector-space-model-basic-idea Lesson 1.6: Vector Space Retrieval Model - Simplest Instantiation 17 min Dimension: word Vector=0-1 bit vector(word presence/absence) Similarity=dot product f(q,d) = number of distinct query words mached in d https://www.coursera.org/learn/text-retrieval/lecture/dM6kh/lesson-1-6-vector-space-retrieval-model-simplest-instantiation Week 1 ActivitiesPractice Quiz: Week 1 Practice Quiz 10 questionsPRACTICE QUIZWeek 1 Practice Quiz10 questionsTo Pass 70% or higherDeadline June 11, 11:59 PM PDT 1 point1.Consider the instantiation of the vector space model where documents and queries are represented as term frequency vectors. Assume we have the following query and two documents:Q = “future of online education”D1 = “Coursera is shaping the future of online education; online education is affordable.”D2 = “In the future, online education will dominate.”Let V(X) = [c1 c2 c3 c4] represent a part of the term frequency vector for document or query X, where c1, c2, c3, and c4 are the term weights corresponding to “future,” “of,” “online,” and “education,” respectively.Which of the following is true? V(Q) = [1 1 1 1] V(D1) = [1 1 2 2] V(D2) = [1 1 1 1] V(Q) = [1 1 1 1] V(D1) = [1 1 2 2] V(D2) = [1 0 1 1] V(Q) = [1 1 1 1] V(D1) = [1 1 1 1] V(D2) = [1 0 1 1]1 point2.Consider the same scenario as in Question 1, with the dot product as the similarity measure. Which of the following is true? Sim(Q,D1) = 4 Sim(Q,D2) = 3 Sim(Q,D1) = 6 Sim(Q,D2) = 4 Sim(Q,D1) = 6 Sim(Q,D2) = 31 point3.Which is NOT the reason that ranking is preferred over document selection? Not all relevant documents are equally relevant. The boundary between relevant vs. not relevant documents is hard to define. Users prefer to browse results in a sequential manner. Ranking is computationally easier than selection.1 point4.Which of the following application scenarios in text retrieval relies LESS on NLP? Compare homework submissions from students to check if plagiarism occurs Use query in English to retrieve documents in French1 point5.What information about text is lost using “bag of words” representation? Number of unique words in document Word ordering Word occurrence counts Phrases formed by multiple words Document length1 point6.Select the following applications that provide “push” information access. Google Maps (maps.google.com) Netflix (netflix.com) Bing (bing.com) Spotify mobile (iOS/Android app)1 point7.Which of the following factors make text retrieval more difficult than database retrieval? Unstructured/free text query and data Ambiguity in text Subjective judgement of relevance and empirical evaluation involving humans1 point8.Stop words, such as “the,” “is,” “at,” “which,” and “on” can be identified with TF-IDF: Low TF, high IDF High TF, high IDF High TF, low IDF Low TF, low IDF1 point9.How many syntactic structures can you identified in sentence “A man saw a boy with a telescope”? 1 2 31 point10.In VSM model, which of the following similarity/distance measure would be affected by document length? L2 distance: $||v_1−v_2||_2$ Cosine similarity: $\cos(v_1,v_2)$ Quiz: Week 1 Quiz10 questionsQUIZWeek 1 Quiz10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineJune 11, 11:59 PM PDT 1 point1.The sentence “A man saw a boy with a telescope” is syntactically ambiguous and has two distinct syntactic structures. True False1 point2.Which of the following is false? Recommender systems are based on the text push mode. Search engines rely on the text push mode. Querying and browsing are both examples of the text pull mode. Browsing is suitable when the user doesn’t know what keywords to use.1 point3.Consider the instantiation of the vector space model where documents and queries are represented as bit vectors. Assume we have the following query and two documents: Q = “healthy diet plans” D1 = “healthy plans for weight loss. Check out other healthy plans” D2 = “the presidential candidate plans to change the educational system.” Let V(X) = [b1 b2 b3] represent a part of the bit vector for document or query X, where b1, b2, and b3 are the bits corresponding to “healthy,” “diet,” and “plans,” respectively. Which of the following is true? V(Q) = [1 1 1] V(D1) = [1 0 1] V(D2) = [0 0 1] V(Q) = [1 1 1] V(D1) = [1 1 1] V(D2) = [0 0 1] V(Q) = [1 1 1] V(D1) = [1 1 1] V(D2) = [0 0 0] V(Q) = [1 1 1] V(D1) = [2 0 2] V(D2) = [0 0 1]1 point4.Consider the same scenario as in Question 3, with dot product as the similarity measure. Which of the following is true? Sim(Q,D1) = 3 Sim(Q,D2) = 0 Sim(Q,D1) = 3 Sim(Q,D2) = 1 Sim(Q,D1) = 2 Sim(Q,D2) = 1 Sim(Q,D1) = 4 Sim(Q,D2) = 11 point5.In the “simplest” VSM instantiation, if instead of using 0-1 bit vectors but we use the word count instead, when we concatenate each document by itself, will the ranking list still remain the same? False True1 point6.In Text Retrieval problem for N distinct documents, select statements below that are correct? If use document selection, the number of outcomes is $2^N$ The numbers of outcome for document ranking and selection are the same If use document ranking, the number of outcomes is N! Document selection is preferred as there is no need to determine document absolute relevance1 point7.Suppose we compute the term vector for a baseball sports news article in a collection of general news articles using TF weighting only. Which of the following do you expect to have the highest weight? baseball the computer1 point8.Assume the same scenario as in Question 7, but with TF-IDF weighting. Which of the following words do you expect to have the highest weight in this case? the baseball computer1 point9.Consider the following retrieval formula: Where c(w, D) is the count of word w in document D, dl is the document length, avdl is the average document length of the collection, N is the total number of documents in the collection, and df (w) is the number of documents containing word w. In view of TF, IDF weighting, and document length normalization, which part is missing or does not work appropriately? TF IDF Document length normalization1 point10.In VSM model, which of the following will be a better way to measure similarity/distance? Cosine similarity: $\cos(v_1,v_2)$ L2 distance: $||v_1−v_2||_2$ Week 2In this week’s lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e., a search engine), including how to build an inverted index and how to score documents quickly for a query. Week 2 InformationWeek 2 Overview 10 minhttps://www.coursera.org/learn/text-retrieval/supplement/okACu/week-2-overview During this week’s lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e., a search engine), including how to build an inverted index and how to score documents quickly for a query. TimeThis module should take approximately 3 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 2 Video Lectures 2 hours Week 2 Graded Quiz 1 hour Goals and ObjectivesAfter you actively engage in the learning experiences in this module, you should be able to: Explain what TF-IDF weighting is and why TF transformation and document length normalization are necessary for the design of an effective ranking function. Explain what an inverted index is and how to construct it for a large set of text documents that do not fit into the memory. Explain how variable-length encoding can be used to compress integers and how unary coding and gamma-coding work. Explain how scoring of documents in response to a query can be done quickly by using an inverted index. Explain Zipf’s law. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. What are some different ways to place a document as a vector in the vector space? Improved Vector Placement: Term Frequency Vectorxi = count of word Wi in query; yi = count of word Wi in doc Further Improvement of Vector Placement: Adding Inverse Document Frequency (IDF)xi = count of word Wi in query; yi = c(Wi ,d) *IDF(Wi) What is term frequency (TF)? count of word Wi in query or doc What is TF transformation? c(w,d) $\rightarrow$ TF(w,d); it is used to–capture the intuition of “diminishing return” from higher TF–avoid dominance by one single term over all others such as BM25 Transformation–has an upper bound–is robust and effective What is document frequency (DF)? total number of docs containing word W (Doc Frequency) What is inverse document frequency (IDF)? IDF Weighting: Penalizing Popular TermsIDF(W) = log[(M+1)/k]M: total number of docs in collectionk: total number of docs containing word W (Doc Frequency) What is TF-IDF weighting? $f(q,d) = \sum_{i=1}^{N}x_i y_i = \sum_{w\in q \cap d}^{}c(w,q) c(w,d)log\frac{M+1}{df(w)}$ Why do we need to penalize long documents in text retrieval? •Penalize a long doc with a doc length normalizer–Long doc has a better chance to match any query–Need to avoid over-penalization•A document is long because–it uses more words $\rightarrow$ more penalization–it has more contents $\rightarrow$ less penalization What is pivoted document length normalization? average doc length as “pivot”–Normalizer = 1 if |d| =average doc length (avdl)$$normalizer = 1 - b + b\frac{|d|}{avdl}, b \in [0,1]$$ What are the main ideas behind the retrieval function BM25?TF-IDF weighting, TF transformation, Pivoted length normalizerAddress the problem of over penalization of long documents by BM25 What is the typical architecture of a text retrieval system? Offline: doc $\rightarrow$ tokenizer $\rightarrow$ index $\rightarrow$ indexerOnline: queryOffline &amp; Online: judgments $\rightarrow$ feedbackindexer + query + feedback $\rightarrow$ scorer $\rightarrow$ result What is an inverted index? Indexing = Convert documents to data structures that enable fast search (precomputing as much as we can)Inverted index is the dominating indexing method for supporting basic search algorithms Why is it desirable to compress an inverted index? •The main difficulty is to build a huge index with limited memory•Memory-based methods: not usable for large collections How can we create an inverted index when the collection of documents doesnot fit into the memory? •TF compression: Binary code, unary code, $\gamma$-code, $\delta$-code,•Doc ID compression: –“d-gap” (store difference): d1, d2-d1, d3-d2,… How can we leverage an inverted index to score documents quickly? •Caching (e.g., query results, list of inverted index)•Keep only the most promising accumulators•Scaling up to the Web-scale? (need parallel processing) Additional Readings and ResourcesThe following readings are optional: C. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapter 6 - Section 6.3, and Chapter 8. Ian H. Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition. Morgan Kaufmann, 1999. Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Term frequency (TF) Document frequency (DF) and inverse document frequency (IDF) TF transformation Pivoted length normalization BM25 Inverted index and postings Binary coding, unary coding, gamma-coding, and d-gap Zipf’s law Week 2 LessonsStart LessonLesson 2.1: Vector Space Model - Improved Instantiation 16 min Improved VSM – Dimension = word – Vector = TF-IDF weight vector – Similarity = dot product – Working better than the simplest VSM – Still having problems TF = term / length of docDF = docs containing term / entire collectionIDF = log[(entire collection +1) / number of docs containing term]https://www.coursera.org/learn/text-retrieval/lecture/7jqJI/lesson-2-1-vector-space-model-improved-instantiationcourse error: Vector = IDF weight vector Lesson 2.2: TF Transformation 9 min Sublinear TF Transformation is needed to – capture the intuition of “diminishing return” from higher TF – avoid dominance by one single term over all others BM25 Transformation – has an upper bound – is robust and effective Ranking function with BM25 TF (k &gt;=0) $f(q,d) = \sum_{i=1}^{N}x_i y_i = \sum_{w\in q \cap d}^{}c(w,q) \frac{(k+1)c(w,d)}{c(w,d)+k}log\frac{M+1}{df(w)}$https://www.coursera.org/learn/text-retrieval/lecture/W0NZe/lesson-2-2-tf-transformation Lesson 2.3: Doc Length Normalization 18 minRelevance(q,d) = similarity(q,d) Query and documents are represented as vectors Heuristic design of ranking function Major term weighting heuristics – TF weighting and transformation – IDF weighting – Document length normalization BM25 and Pivoted normalization seem to be mosteffective https://www.coursera.org/learn/text-retrieval/lecture/RnXhr/lesson-2-3-doc-length-normalization Lesson 2.4: Implementation of TR Systems 21 minhttps://www.coursera.org/learn/text-retrieval/lecture/2Cbq9/lesson-2-4-implementation-of-tr-systems Lesson 2.5: System Implementation - Inverted Index Construction 18 minhttps://www.coursera.org/learn/text-retrieval/lecture/PgzsP/lesson-2-5-system-implementation-inverted-index-construction Lesson 2.6: System Implementation - Fast Search 17 minSome Text Retrieval Toolkits Lucene: http://lucene.apache.org/ Lemur/Indri: http://www.lemurproject.org/ Terrier: http://terrier.org/ MeTA: http://meta-toolkit.github.io/meta/ More can be found at http://timan.cs.uiuc.edu/resources Inverted index and its construction – Preprocess data as much as we can – Compression when appropriate Fast search using inverted index – Exploit inverted index to accumulate scores for documents matching a query term – Exploit Zipf’s law to avoid touching many documents not matching any query term – Can support a wide range of ranking algorithms Great potential for further scaling up using distributed filesystem, parallel processing, and caching https://www.coursera.org/learn/text-retrieval/lecture/QKK7y/lesson-2-6-system-implementation-fast-search Week 2 ActivitiesPractice Quiz: Week 2 Practice Quiz 10 questionsWeek 2 Practice Quiz10 questionsTo Pass70% or higherDeadlineJune 18, 11:59 PM PDT 1 point1.Let w1, w2, and w3 represent three words in the dictionary of an inverted index. Suppose we have the following document frequency distribution: Word Document Frequency w1 1 w2 5 w3 10 Assume that each posting entry of document ID and term frequency takes exactly the same disk space. Which word’s postings list will occupy the largest disk space? w2 w1 w31 point2.Assume we have the same scenario as in Question 1. If we enter a query Q= “w1 w2 w3” then the maximum possible number of accumulators needed to score all the matching documents is: 10 16 1 51 point3.Assume that the d-gap between two documents is equal to 9. If you want to compress this d-gap with a gamma code, what will be the binary representation of the code? 1110010 1110011 1110001 11100001 point4.Why is TF transformation needed? To capture the intuition of “diminishing return” from higher TF So that computation is more efficient1 point5.What is the upperbound for BM25 transformation? k+1 k-1 k1 point6.Do we always want to penalize a long document? Yes No1 point7.Which is true about pivoted length normalization? It always rewards. It has both a penalization and reward effect. It always penalizes.1 point8.Is word segmentation on Chinese easier than English? No Yes1 point9.What is NOT the advantage for using inverted index? Inverted index can map words of the same meaning into one slot. It can search for documents that contains both “A” and “B” efficiently. It is more efficient than sequentially scanning docs.1 point10.What does Zipf’s law tell you? Words are evenly distributed. There are many words that have a small probability. There are only a few words that have a small probability. Quiz: Week 2 Quiz 12 questionsDue June 18, 11:59 PM PDTQUIZWeek 2 Quiz12 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineJune 18, 11:59 PM PDT 1 point1.Let w1, w2, and w3 represent three words in the dictionary of an inverted index. Suppose we have the following document frequency distribution: Word Document Frequency w1 1000 w2 100 w3 10 Assume that each posting entry of document ID and term frequency takes exactly the same disk space. Which word, if removed from the inverted index, will save the most disk space? We cannot tell from the given information. w3 w1 w21 point2.Assume we have the same scenario as in Question 1. If we enter the query Q= “w1 w2” then the minimum possible number of accumulators needed to score all the matching documents is: 100 1100 1000 101 point3.The gamma code for the term frequency of a certain document is 1110010. What is the term frequency of the document? 11 12 10 91 point4.When using an inverted index for scoring documents for queries, a shorter query always uses fewer score accumulators than a longer query. False True1 point5.What is the advantage of tokenization (normalize and stemming) before index? Extracts words as lexical units from strings of text Improves performance by mapping words with similar meanings into the same indexing term Reduces the number of terms (size of vocabulary)1 point6.What can’t an inverted index alone do for fast search? Search document contains “A” and “B” Search document contains “A” or “B” Retrieve documents that are relevant to the query1 point7.If Zipf’s law does not hold, will an inverted index be much faster or slower? Faster Slower1 point8.In BM25, the TF after transformation has upper bound k +1 k 11 point9.Which of the following are weighing heuristics for the vector space model? TF weighting and transformation Document length normalization IDF weighting1 point10.Which of the following integer compression has equal-length coding? Unary Binary γ-code1 point11.Consider the following retrieval formula: Where c(w, D) is the count of word w in document D, dl is the document length, avdl is the average document length of the collection, N is the total number of documents in the collection, and df (w) is the number of documents containing word w. In view of TF, IDF weighting, and document length normalization, which part is missing or does not work appropriately? TF IDF Document length normalization1 point12.Suppose we compute the term vector for a baseball sports news article in a collection of general news articles using TF-IDF weighting. Which of the following words do you expect to have the highest weight in this case? baseball computer the Week 3ChengXiang Zhai In this week’s lessons, you will learn how to evaluate an information retrieval system (a search engine), including the basic measures for evaluating a set of retrieved results and the major measures for evaluating a ranked list, including the average precision (AP) and the normalized discounted cumulative gain (nDCG), and practical issues in evaluation, including statistical significance testing and pooling. Week 3 InformationWeek 3 Overview 10 minhttps://www.coursera.org/learn/text-retrieval/supplement/0wyle/week-3-overview TimeThis module should take approximately 3-6 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 3 Video Lectures 2 hours Week 3 Graded Quiz 1 hour Optional Programming Assignment 1 3 hours Goals and ObjectivesAfter you actively engage in the learning experiences in this module, you should be able to: Explain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system. Explain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1. Explain how to evaluate a ranked list of documents. Explain how to compute and plot a precision-recall curve. Explain how to compute average precision and mean average precision (MAP). Explain how to evaluate a ranked list with multi-level relevance judgments. Explain how to compute normalized discounted cumulative gain. Explain why it is important to perform statistical significance tests. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. Why is evaluation so critical for research and application development in text retrieval? •Reason 1: Assess the actual utility of a TR system–Measures should reflect the utility to users in a real application–Usually done through user studies (interactive IR evaluation)•Reason 2: Compare different systems and methods–Measures only need to be correlated with the utility to actual users, thus don’t have to accurately reflect the exact utility to users–Usually done through test collections (test set IR evaluation) How does the Cranfield evaluation methodology work? •Idea: Build reusable test collections &amp; define measures–A sample collection of documents (simulate real document collection)–A sample set of queries/topics (simulate user queries)–Relevance judgments (ideally made by users who formulated the queries)  Ideal ranked list–Measures to quantify how well a system’s result matches the ideal ranked list•A test collection can then be reused many times to compare different systems How do we evaluate a set of retrieved documents? Precision and Recall;Combine Precision and Recall: F-Measure How do you compute precision, recall, and F1? $precision = \frac{a}{a+c}$ $recall = \frac{a}{a+b}$ $F_{\beta} = \frac{(\beta^2+1)P*R}{\beta^2P+R}$a: Relevant Retrieved, b: Relevant Rejected, c: Irrelevant Retrieved How do we evaluate a ranked list of search results? Precision-Recall curve •Average Precision:–The average of precision at every cutoff where a new relevant document is retrieved–Normalizer = the total # of relevant docs in collection–Sensitive to the rank of each relevant document•Mean Average Precisions (MAP)–MAP = arithmetic mean of average precision over a set of queries–gMAP = geometric mean of average precision over a set of queries How do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)? –The average of precision at every cutoff where a new relevant document is retrieved–Normalizer = the total # of relevant docs in collection–Sensitive to the rank of each relevant document arithmetic mean of average precision over a set of queries geometric mean of average precision over a set of queries What is mean reciprocal rank? •When there’s only one relevant document in the collection (e.g., known item search)–Average Precision = Reciprocal Rank = 1/r, where r is the rank position of the single relevant doc–Mean Average Precision $\rightarrow$ Mean Reciprocal Rank Why is MAP more appropriate than precision at k documents when comparing two retrieval methods? •Precision-Recall curve characterizes the overall accuracy of a ranked list•The actual utility of a ranked list depends on how many top-ranked results a user would examine•Average Precision is the standard measure for comparing two ranking methods–Combines precision and recall–Sensitive to the rank of every relevant document Why is precision at k documents more meaningful than average precision from a user’s perspective? precision at k documents is more intuitive How can we evaluate a ranked list of search results using multi-level relevance judgments? Normalized Discounted Cumulative Gain (nDCG) How do you compute normalized discounted cumulative gain (nDCG)? Relevance level: r=1 (non-relevant) , 2 (marginally relevant), 3 (very relevant) doc gain cumulative gain Discounted Cumulative Gain D1 3 3 3 D2 2 3+2 3+2/log 2 D3 1 3+2+1 3+2/log 2+1/log 3 D4 1 3+2+1+1 D5 3 D6 1 D7 1 D8 2 D9 1 DCG@10 = 3+2/log 2+1/log 3 +…+ 1/log 10IdealDCG@10 = 3+3/log 2+3/log 3 +…+ 3/log 9+ 2/log 10Assume: there are 9 documents rated “3” in total in the collection$Normalized DCG @10= \frac{DCG@10}{idealDCG@10}$ Why is normalization necessary in nDCG? Does MAP need a similar normalization? Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems? different topics same scale No How sure can you be that an observed difference doesn’t simply result from the particular queries you chose? Additional Readings and Resources Mark Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247-375. C. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapter 9 Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Cranfield evaluation methodology Precision and recall Average precision, mean average precision (MAP), and geometric mean average precision (gMAP) Reciprocal rank and mean reciprocal rank F-measure Normalized discounted cumulative Gain (nDCG) Statistical significance test Week 3 LessonsLesson 3.1: Evaluation of TR Systems 10 minhttps://www.coursera.org/learn/text-retrieval/lecture/YSvkh/lesson-3-1-evaluation-of-tr-systems The Cranfield Evaluation Methodology– A sample collection of documents (simulate real document collection)– A sample set of queries/topics (simulate user queries)– Relevance judgments (ideally made by users who formulated the queries)  Idealranked list– Measures to quantify how well a system’s result matches the ideal ranked list Lesson 3.2: Evaluation of TR Systems - Basic Measures 12 minhttps://www.coursera.org/learn/text-retrieval/lecture/VMh3Z/lesson-3-2-evaluation-of-tr-systems-basic-measures Precision: are the retrieved results all relevant?• Recall: have all the relevant documents been retrieved?• F measure combines Precision and Recall• Tradeoff between Precision and Recall depends on theuser’s search task Lesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1 15 minhttps://www.coursera.org/learn/text-retrieval/lecture/rU7LT/lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1 PR curvesaverage precision Lesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2 10 minhttps://www.coursera.org/learn/text-retrieval/lecture/8Q2Tw/lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2 Mean Average Precisions (MAP)Mean Reciprocal RankAverage Precision is the standard measure for comparingtwo ranking methods– Combines precision and recall– Sensitive to the rank of every relevant document Lesson 3.5: Evaluation of TR Systems - Multi-Level Judgements 10 minhttps://www.coursera.org/learn/text-retrieval/lecture/uGa00/lesson-3-5-evaluation-of-tr-systems-multi-level-judgements Main idea of nDCG@k documents– Measure the total utility of the top k documents to auser– Utility of a lowly ranked document is discounted– Normalized to ensure comparability across queries Lesson 3.6: Evaluation of TR Systems - Practical Issues 15 minhttps://www.coursera.org/learn/text-retrieval/lecture/thRNy/lesson-3-6-evaluation-of-tr-systems-practical-issues Week 3 ActivitiesPractice Quiz: Week 3 Practice Quiz 10 questionsPRACTICE QUIZWeek 3 Practice Quiz10 questionsTo Pass70% or higherDeadlineJune 25, 11:59 PM PDT 1 point1.Suppose a query has a total of 5 relevant documents in a collection of 100 documents. System A and System B have each retrieved 10 documents, and the relevance status of the ranked lists is shown below: System A: [+ + - - - - - - - -] System B: [- + - - + - - - - +] where the leftmost entry corresponds to the highest ranked document, and the rightmost entry corresponds to the lowest ranked document. A “+” indicates a relevant document and a “-” corresponds to a non-relevant one. For example, the top ranked document retrieved by System A is relevant, whereas the top ranked document retrieved by B is non-relevant. What is the precision at 10 documents of both systems? P(A) = 2/10 P(B) = 3/10 P(A) = 2/100 P(B) = 3/100 P(A) = 2/5 P(B) = 3/5 P(A) = 8/100 P(B) = 7/1001 point2.Assume the same scenario as in Question 1. What is the recall of both systems? R(A) = 2/5 R(B) = 3/5 R(A) = 2/100 R(B) = 3/100 R(A) = 2/10 R(B) = 3/10 R(A) = 8/100 R(B) = 7/1001 point3.Assume the same scenario as in Question 1. What is the average precision of both systems?(1/1 + 2/2 + 0+0+0)/5 = 2/5(1/2+2/5+3/10+0+0)/5=6/25 AP(A) = 2/5 AP(B) = 6/25 AP(A) = 2/10 AP(B) = 3/25 AP(A) = 2/100 AP(B) = 3/250 AP(A) = 3/10 AP(B) = 9/201 point4.Assume you have two retrieval systems X and Y. If X has a higher MAP (mean average precision), can Y have a higher gMAP (geometric mean average precision)?MAP_x &gt; MAP_yMAP_x &gt;= gMAP_x Yes No1 point5.If system A has higher precision at k document than system B for any number of k, does it mean A also has higher recall than B at any position? Yes No1 point6.F-measure contains a parameter that weighs between precision and recall. For an automatic system that filters tweets in the search for possible communication between terrorists about attack plans on US soil, when evaluating the system’s performance with F-measure, should we use a higher parameter or lower? Lower Higher1 point7.What is nDCG capable of but not DCG? Compare two systems performed on a set of queries Compare two systems performed on one single query Work with relevance judgement that is multi-level Work with relevance judgement that is binary-level (relevant vs. not relevant)1 point8.Why is pooling useful? So that we don’t need to judge every document So that we don’t need humans to do judgement So that all documents can be judged efficiently1 point9.Which of the following is not correct? Precision@10docs is easy for users to interpret. MAP and nDCG are good for comparing ranking algorithms. DCG is better than nDCG as its value is within [0, 1].1 point10.If in PR (precision, recall) curves, curve A is above B for all recall, what can you say? A is better than B. B is better than A. There is no clear conclusion about A vs. B. A is as good as B. Quiz: Week 3 Quiz 10 questionsDue June 25, 11:59 PM PDTQUIZWeek 3 Quiz10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineJune 25, 11:59 PM PDT 1 point1.Suppose a query has a total of 4 relevant documents in the collection. System A and System B have each retrieved 10 documents, and the relevance status of the ranked lists is shown below: System A: [- + - - - - - - - -] System B: [+ + - - - - - - - -] where the leftmost entry corresponds to the highest ranked document, and the rightmost entry corresponds to the lowest ranked document. A “+” indicates a relevant document and a “-” corresponds to a non-relevant one. For example, the top ranked document retrieved by System A is non-relevant, whereas the top ranked document retrieved by B is relevant. What is the precision at 10 documents of both systems? P(A) = 1/40 P(B)= 2/40 P(A) = 1/10 P(B)= 2/10 P(A) = 1/4 P(B)= 2/4 P(A) = 9/10 P(B)= 8/101 point2.Assume the same scenario as in Question 1. What is the recall of both systems? R(A) = 1/40 R(B)= 2/40 R(A) = 1/10 R(B)= 2/10 R(A) = 1/4 R(B)= 2/4 R(A) = 9/10 R(B)= 8/101 point3.Assume the same scenario as in Question 1. What is the average precision of both systems?(1/2)/4(1/1 + 2/2 )/4 AP(A) = 7/20 AP(B) = 7/10 AP(A) = 1/10 AP(B) = 1/5 AP(A) = 1/8 AP(B) = 1/2 AP(A) = 1/20 AP(B) = 1/51 point4.Assume you have two retrieval systems X and Y. For a specific query, system X has a higher precision at 10 documents compared to Y. Can system Y have a higher average precision on the same query? Yes No1 point5.Can a retrieval system have an F1 score of 0.75 and a precision of 0.5? No Yes1 point6.For any ranked list of search results, precision at 10 documents is always higher than precision at 20 documents. False True1 point7.What can you say about the precision-recall (PR) curve? It is always monotonically decreasing. The ideal system should have the PR curve as a horizontal line. It is always monotonically increasing.1 point8.Which is correct about average precision? It does not show the difference between ranks of relevant documents. It combines precision and recall.1 point9.Which of the following is NOT true about Cranfield evaluation methodology? It does not involve humans to make relevance judgments. It simulates user queries. It simulates real document collections.1 point10.Which of following is wrong about nDCG@k? It discounts only top ranked documents. It has a range between 0 and 1. It can be used to compare across queries. Optional Honors ContentHonors Track Programming AssignmentProgramming Assignments Overview 10 minOverviewFor the programming assignments, you are going to use MeTA, a C++ data sciences toolkit that supports a variety of tasks such as text retrieval, natural language processing, and machine learning. The main focus in this course will be on how to use and extend the text retrieval methods implemented in MeTA. The assignments assume a very basic understanding of C++ and elementary data structures. That being said, even if your experience is in languages other than C++, you are highly encouraged to go through the assignments, as the focus will be on exploring text retrieval concepts, not on testing your knowledge of the syntax. This assignment will explore the main text processing techniques implemented in MeTA and then guide you through implementing a major component of a search engine. It will also involve a competition where you can freely create and optimize your own search engine. Although the assignment starts in Week 3, we encourage you to install the toolkit as soon as possible. If you have questions about the installation process, use the Programming Assignments Forum. This is a great place to ask questions and also help your fellow classmates. Software InstallationThere are two software prerequisites you must install in order to complete the assignments: MeTA Python 2.7 (used only for uploading your assignment, not for coding) To install the MeTA tool, please follow this page. Currently, most major OS including OS X, Ubuntu (and other Linux distributions), and Windows are supported. windows setup guidefollow this website something to note: git clone https://github.com/meta-toolkit/meta.git or quick download zip files to C:\msys64\home\SSQ\meta1234567SSQ@SSQ-PC MINGW64 ~$ git clone https://github.com/meta-toolkit/meta.git正克隆到 &apos;meta&apos;...remote: Counting objects: 33329, done.remote: Total 33329 (delta 0), reused 0 (delta 0), pack-reused 33329接收对象中: 100% (33329/33329), 30.84 MiB | 23.00 KiB/s, 完成.处理 delta 中: 100% (25323/25323), 完成. 123456&apos;deps/bandit&apos;（https://github.com/joakimkarlsson/bandit.git）&apos;deps/cpptoml&apos;（https://github.com/skystrife/cpptoml.git）&apos;deps/findicu&apos;（https://github.com/julp/FindICU.cmake.git）&apos;deps/libsvm-modules&apos;（https://github.com/meta-toolkit/meta-libsvm.git） &apos;deps/meta-cmake&apos;（https://github.com/meta-toolkit/meta-cmake.git） &apos;deps/meta-stlsoft&apos;（https://github.com/meta-toolkit/meta-stlsoft.git） or quick download zip files to C:\msys64\home\SSQ\meta\deps after typing make and hit enter1234567891011121314151617181920212223242526Scanning dependencies of target meta-io[ 3%] Building CXX object src/io/CMakeFiles/meta-io.dir/filesystem.cpp.obj[ 4%] Building CXX object src/io/CMakeFiles/meta-io.dir/gzstream.cpp.obj[ 4%] Building CXX object src/io/CMakeFiles/meta-io.dir/libsvm_parser.cpp.obj[ 4%] Building CXX object src/io/CMakeFiles/meta-io.dir/mmap_file.cpp.obj[ 4%] Building C object src/io/CMakeFiles/meta-io.dir/mman-win32/mman.c.objC:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c: In function &apos;__map_mman_error&apos;:C:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c:12:56: warning: unused parameter &apos;deferr&apos; [-Wunused-parameter] static int __map_mman_error(const DWORD err, const int deferr) ^~~~~~C:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c: In function &apos;mmap&apos;:C:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c:58:18: warning: unused parameter &apos;addr&apos; [-Wunused-parameter] void* mmap(void *addr, size_t len, int prot, int flags, int fildes, off_t off) ^~~~C:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c: In function &apos;munmap&apos;:C:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c:129:31: warning: unused parameter &apos;len&apos; [-Wunused-parameter] int munmap(void *addr, size_t len) ^~~C:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c: In function &apos;msync&apos;:C:/msys64/home/SSQ/meta/src/io/mman-win32/mman.c:152:39: warning: unused parameter &apos;flags&apos; [-Wunused-parameter] int msync(void *addr, size_t len, int flags) ^~~~~[ 5%] Building CXX object src/io/CMakeFiles/meta-io.dir/xzstream.cpp.obj[ 5%] Linking CXX static library ../../lib/libmeta-io.a[ 5%] Built target meta-ioScanning dependencies of target meta-filters Programming Assignment: Programming Assignment 13hDue June 25, 11:59 PM PDT MeTA OverviewBefore starting the assignment, you are highly encouraged to read MeTA’s system overview to gain a high level understanding of how MeTA operates. Throughout the assignments, if you want to know more about a certain class or function, you can use the search toolbar in MeTA’s documentation, which provides a brief explanation of the different modules. If you have questions about the programming assignment, use the Programming Assignments Forum. This is a great place to ask questions and also help your fellow classmates. Downloading the AssignmentIn what follows, we assume that you have already installed MeTA and the directory meta is located in ~/Desktop/. Download Assignment_1.tar.gz (below) and extract it into the parent directory of meta. For example, if meta is in ~/Desktop/, then you should extract the assignment in ~/Desktop/In the terminal, change the directory to Assignment_1 and run the bash script Setup.sh. This can be done using the following two commands:12cd Assignment_1/./setup.sh Now using MeTA as a library, to compile:123cd Assignment_1/buildcmake .. -G &quot;MSYS Makefiles&quot; -DCMAKE_BUILD_TYPE=Releasemake -j8 Download Assignment_1.tar.gz:Assignment_1.tar.gz Warm Up!In this section you will start by exploring some of the basic text processing techniques such as tokenization, stemming, and stopword removal. These techniques are usually applied to the corpus prior to the indexing stage. You will also perform part-of-speech tagging on a text document. Tokenization is the process of segmenting a stream of words (such as a document) into a sequence of units called tokens. Loosely speaking, when tokenization is performed on the word level, the tokens will be the words in the document. To perform tokenization, MeTA relies on the default segmentation boundaries between words and sentences defined by the Unicode Standard (see Unicode Standard Annex #29 for more information). Typically, after tokenization is performed, a number of text filtering techniques are applied on the tokens in order to perform more effective and efficient retrieval. These techniques include: Stopword removal: Stopwords are frequent words that are not informative for the task at hand. For most retrieval applications, words such as “in,” “or,” “have,” and “the” are not useful for identifying the relevant documents, and thus, they are discarded before the indexing stage. This considerably helps in reducing the size of the inverted index since stopwords occur very frequently and tend to have large postings lists. MeTA uses a list of stopwords saved in /meta/data/lemur-stopwords.txt. Feel free to open the file and have a look at some of the popular stopwords. Conversion to lower case: For most applications, converting all letters to lower case can help in boosting the retrieval performance. The intuition here is that uppercase and lowercase forms of words usually refer to the same concept and should not be treated as orthogonal dimensions. However, this conversion can lead to inaccuracies in certain situations. For example, a proper noun like “CAT” (a construction company) will have the same representation as the common noun “cat.” Stemming: It is the process of converting words back to their original stems or roots. For example, the words “retrieve,” “retrieval,” and “retrieving” will all be mapped to the same root “retrieve.” This can prevent different forms of the same word from being treated as orthogonal concepts. MeTA uses a very well known stemmer called the Porter2 English Stemmer; see English (Porter 2) stemming algorithm for more information on how this stemmer operates.Now you are ready to apply these concepts using MeTA! Task 1: Stopword Removal (5 pts)In this task you will run tokenization and stopword removal on a document. Go to Assignment_1/build/Assignment1/. You should find a text document called doc.txt. Open doc.txt and have a look at its content. The document contains the description found on the main page of this course. In the terminal, perform the following:1234cd Assignment_1/buildcmake .. -G &quot;MSYS Makefiles&quot; -DCMAKE_BUILD_TYPE=Releasemake -j8./analyze ../config.toml Assignment1/doc.txt --stop After running the above, if everything is setup correctly, then you should see:123$ ./analyze ../config.toml Assignment1/doc.txt --stopRunning stopword removal -&gt; file saved as Assignment1/doc.stops.txt Open the output file Assignment_1/build/Assignment1/doc.stops.txt and you should see how tokenization and stopword removal have been applied. In the 2nd command above, you have executed a compiled program called analyze and passed to it a configuration file called config.toml, the document’s path, and the parameter “stop,” which tells the program to remove stopwords. The analyze is a demo program that supports multiple text analysis functions. Passing config.toml to programs in MeTA is a default practice as it gives the program all the required parameters to run. Open Assignment_1/config.toml and examine it. You should see a list of configuration parameters, including the directory of the list of stopwords (which should appear in the first line). If you have a new list of stopwords, you can simply point MeTA to it (but do not change the list for this assignment). After finishing Task 1, you should submit the output file using the submission script. In the shell, execute:12cd Assignment_1/build/Assignment1wc &lt; doc.stops.txt | sed &apos;s/^\s*//g&apos; &gt; doc.stops.txt.wc Submit the file docs.stops.txt.wc to Task 1. In the My submission tab, click + Create submission. Then click on Task 1: Stopword Removal and upload your file. Task 2: Stemming (10 pts)Now you will perform tokenization and stemming on the same document doc.txt. The analyze program you ran in Task 1 provides several other text analysis functionalities, including stemming. For the different functions implemented in analyze, open Assignment_1/analyze.cpp and have a look. In addition, simply run ./analyze and you should see:12345678910111213$ ./analyzeUsage: ./analyze config.toml file.txt [OPTION]where [OPTION] is one or more of: --stem perform stemming on each word --stop remove stopwords --stopstem remove stopwords and perform stemming --pos annotate words with POS tags --pos-replace replace words with their POS tags --parse create grammatical parse trees from file content --freq-unigram sort and count unigram words --freq-bigram sort and count bigram words --freq-trigram sort and count trigram words --all run all options Your task is to perform stemming on doc.txt using the analyze program. After running stemming, you should see a new file named doc.stems.txt in output directory Assignment_1/build/Assignment1/. Examine the file and see how words are mapped back to their stems. Now do the word count again and1wc &lt; doc.stems.txt | sed &apos;s/^\s*//g&apos; &gt; doc.stems.txt.wc Submit your output file doc.stems.txt.wc. In the My submission tab, click + Create submission. Upload your file to Task 2: Stemming. solution open mingw64.exe run cd Assignment_1/build and ./analyze to check the analyze.exe function. run ./analyze ../config.toml Assignment1/doc.txt --stem to generate stemmed text file 12Running stemming algorithm -&gt; file saved as Assignment1/doc.stems.txt cd Assignment1 run wc &lt; doc.stems.txt | sed &#39;s/^\s*//g&#39; &gt; doc.stems.txt.wc submit Task 3: Part-of-Speech Tagging (10 pts)As you have learned in the lecture on natural language processing, part-of-speech tagging is the process of assigning parts of speech (such as verb, noun, adjective, adverb, etc.) to the words in a given text. Your task is to do POS tagging (without replacement) on doc.txt using the analyze program. After executing the program, examine the output in doc.pos-tagged.txt. You should see how a POS tag is assigned to each word after the underscore. Note that the tags are abbreviated; for example, the tag “NN” stands for a noun. For a list of commonly used tags and their meanings see the UPenn Treebank list. For a more comprehensive covering of POS tagging, you can check MeTA’s POS Tagging Tutorial. Don’t forget to submit your output file after running the following!wc &lt; doc.pos-tagged.txt | sed &#39;s/^\s*//g&#39; &gt; doc.pos-tagged.txt.wc solution open mingw64.exe run cd Assignment_1/build run ./analyze ../config.toml Assignment1/doc.txt --pos to generate POS tagging (without replacement) text file 123Running POS-tagging with replace = falseLoading tagging model -&gt; file saved as Assignment1/doc.pos-tagged.txt cd Assignment1 run wc &lt; doc.pos-tagged.txt | sed &#39;s/^\s*//g&#39; &gt; doc.pos-tagged.txt.wc submit Task 3.5: Writing a New Function (15 pts)Now that you have explored some of the basic text analysis techniques in analyze, you will add a new function to analyze that does stopword removal followed by stemming on the same text document. MeTA implements the different text analysis techniques such as stemming and stopword removal as filters that can be chained together. See MeTA’s filters Reference for a list of the different supported filters. To implement your new function, edit Assignment_1/analyze.cpp. Examine and try to understand how the functions stem and stop are performing tokenization and filtering. Find the function:12void stopstem(const std::string&amp; file, const cpptoml::table&amp; config) Your task is to complete the function in order to perform the required task. In the place where we wrote:12// Insert the line required to do stopword removal here// Insert the line required to do stemming here (using Porter2 Stemmer) You should only add the stopword removal filter and the Porter2 stemming filter. (2 Lines in total) Do not add any other filters. Since source code is changed, you should recompile after writing your code:123cd Assignment_1/buildcmake .. -DCMAKE_BUILD_TYPE=Release; make -j8./analyze ../config.toml Assignment1/doc.txt --stopstem and we still ask you to run1wc &lt; doc.stopstem.txt | sed &apos;s/^\s*//g&apos; &gt; doc.stopstem.txt.wc and submit your result. solution add codes in analyze.cpp 1234// Insert the line required to do stopword removal herestream = make_unique&lt;filters::list_filter&gt;(std::move(stream), *stopwords);// Insert the line required to do stemming here (using Porter2 Stemmer)stream = make_unique&lt;filters::porter2_filter&gt;(std::move(stream)); run 123cd Assignment_1/buildcmake .. -G &quot;MSYS Makefiles&quot; -DCMAKE_BUILD_TYPE=Releasemake -j8 it appears123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101[ 97%] Built target relevance-judgementsC:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:45:9: error: &apos;float pl2_ranker::score_one(const meta::index::score_data&amp;)&apos; marked &apos;override&apos;, but does not override float score_one(const index::score_data&amp;) override; ^~~~~~~~~C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp: In member function &apos;float pl2_ranker::score_one(const meta::index::score_data&amp;)&apos;:C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:69:9: warning: unused variable &apos;doc_len&apos; [-Wunused-variable] float doc_len = sd.idx.doc_size(sd.d_id); // Current document length ^~~~~~~C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:70:9: warning: unused variable &apos;avg_dl&apos; [-Wunused-variable] float avg_dl = sd.avg_dl; // Average document length in the corpus ^~~~~~C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:71:9: warning: unused variable &apos;tf&apos; [-Wunused-variable] float tf = sd.doc_term_count; // Raw term count in the document ^~C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:72:9: warning: unused variable &apos;pi&apos; [-Wunused-variable] float pi = 3.14; // Use this for pi - Do NOT use other values ^~C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:73:9: warning: unused variable &apos;lambda&apos; [-Wunused-variable] float lambda = lambda_; // pl2&apos;s parameter ^~~~~~C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:74:9: warning: unused variable &apos;c&apos; [-Wunused-variable] float c = c_; // pl2&apos;s parameter ^C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp: In function &apos;void pl2_tune(const std::shared_ptr&lt;meta::index::cached_index&lt;meta::index::inverted_index, meta::caching::default_dblru_cache&gt; &gt;&amp;, std::vector&lt;meta::corpus::document&gt;&amp;, meta::index::ir_eval&amp;, float&amp;, float&amp;)&apos;:C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:127:48: error: no matching function for call to &apos;meta::index::ir_eval::avg_p(std::vector&lt;meta::index::search_result&gt;&amp;, meta::doc_id, int)&apos; eval.avg_p(ranking, (*query).id(), 1000); // eval.avg_p stores the ^In file included from C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:14:0:C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:91:12: note: candidate: double meta::index::ir_eval::avg_p(const result_type&amp;, meta::query_id, uint64_t) double avg_p(const result_type&amp; results, query_id q_id, ^~~~~C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:91:12: note: no known conversion for argument 2 from &apos;meta::doc_id &#123;aka meta::util::numerical_identifier&lt;meta::doc_id_tag, long long unsigned int&gt;&#125;&apos; to &apos;meta::query_id &#123;aka meta::util::numerical_identifier&lt;meta::query_id_tag, long long unsigned int&gt;&#125;&apos;C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp: In function &apos;int main(int, char**)&apos;:C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:241:43: error: no matching function for call to &apos;meta::index::ir_eval::avg_p(std::vector&lt;meta::index::search_result&gt;&amp;, meta::doc_id, int)&apos; eval.avg_p(ranking, query.id(), 1000); ^In file included from C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:14:0:C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:91:12: note: candidate: double meta::index::ir_eval::avg_p(const result_type&amp;, meta::query_id, uint64_t) double avg_p(const result_type&amp; results, query_id q_id, ^~~~~C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:91:12: note: no known conversion for argument 2 from &apos;meta::doc_id &#123;aka meta::util::numerical_identifier&lt;meta::doc_id_tag, long long unsigned int&gt;&#125;&apos; to &apos;meta::query_id &#123;aka meta::util::numerical_identifier&lt;meta::query_id_tag, long long unsigned int&gt;&#125;&apos;C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp: In lambda function:C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:264:58: error: no matching function for call to &apos;meta::index::ir_eval::precision(std::vector&lt;meta::index::search_result&gt;&amp;, meta::doc_id, int)&apos; &lt;&lt; eval.precision(ranking, query.id(), 10) &lt;&lt; std::endl; ^In file included from C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:14:0:C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:48:12: note: candidate: double meta::index::ir_eval::precision(const result_type&amp;, meta::query_id, uint64_t) const double precision(const result_type&amp; results, query_id q_id, ^~~~~~~~~C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:48:12: note: no known conversion for argument 2 from &apos;meta::doc_id &#123;aka meta::util::numerical_identifier&lt;meta::doc_id_tag, long long unsigned int&gt;&#125;&apos; to &apos;meta::query_id &#123;aka meta::util::numerical_identifier&lt;meta::query_id_tag, long long unsigned int&gt;&#125;&apos;C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:268:43: error: no matching function for call to &apos;meta::index::ir_eval::avg_p(std::vector&lt;meta::index::search_result&gt;&amp;, meta::doc_id, int)&apos; eval.avg_p(ranking, query.id(), 1000); ^In file included from C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:14:0:C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:91:12: note: candidate: double meta::index::ir_eval::avg_p(const result_type&amp;, meta::query_id, uint64_t) double avg_p(const result_type&amp; results, query_id q_id, ^~~~~C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:91:12: note: no known conversion for argument 2 from &apos;meta::doc_id &#123;aka meta::util::numerical_identifier&lt;meta::doc_id_tag, long long unsigned int&gt;&#125;&apos; to &apos;meta::query_id &#123;aka meta::util::numerical_identifier&lt;meta::query_id_tag, long long unsigned int&gt;&#125;&apos;In file included from C:/msys64/mingw64/include/c++/6.3.0/bits/locale_conv.h:41:0, from C:/msys64/mingw64/include/c++/6.3.0/locale:43, from C:/msys64/mingw64/include/c++/6.3.0/iomanip:43, from C:/msys64/home/SSQ/meta/deps/cpptoml/include/cpptoml.h:15, from C:/msys64/home/SSQ/meta/include/meta/corpus/metadata.h:17, from C:/msys64/home/SSQ/meta/include/meta/corpus/document.h:18, from C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:13:C:/msys64/mingw64/include/c++/6.3.0/bits/unique_ptr.h: In instantiation of &apos;typename std::_MakeUniq&lt;_Tp&gt;::__single_object std::make_unique(_Args&amp;&amp; ...) [with _Tp = pl2_ranker; _Args = &#123;&#125;; typename std::_MakeUniq&lt;_Tp&gt;::__single_object = std::unique_ptr&lt;pl2_ranker, std::default_delete&lt;pl2_ranker&gt; &gt;]&apos;:C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:110:41: required from hereC:/msys64/mingw64/include/c++/6.3.0/bits/unique_ptr.h:791:30: error: invalid new-expression of abstract class type &apos;pl2_ranker&apos; &#123; return unique_ptr&lt;_Tp&gt;(new _Tp(std::forward&lt;_Args&gt;(__args)...)); &#125; ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:26:7: note: because the following virtual functions are pure within &apos;pl2_ranker&apos;: class pl2_ranker : public index::ranker &#123; ^~~~~~~~~~In file included from C:/msys64/home/SSQ/meta/include/meta/index/eval/ir_eval.h:19:0, from C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:14:C:/msys64/home/SSQ/meta/include/meta/index/ranker/ranker.h:220:40: note: virtual std::vector&lt;meta::index::search_result&gt; meta::index::ranker::rank(meta::index::ranker_context&amp;, uint64_t, const filter_function_type&amp;) virtual std::vector&lt;search_result&gt; rank(ranker_context&amp; ctx, ^~~~In file included from C:/msys64/mingw64/include/c++/6.3.0/bits/locale_conv.h:41:0, from C:/msys64/mingw64/include/c++/6.3.0/locale:43, from C:/msys64/mingw64/include/c++/6.3.0/iomanip:43, from C:/msys64/home/SSQ/meta/deps/cpptoml/include/cpptoml.h:15, from C:/msys64/home/SSQ/meta/include/meta/corpus/metadata.h:17, from C:/msys64/home/SSQ/meta/include/meta/corpus/document.h:18, from C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:13:C:/msys64/mingw64/include/c++/6.3.0/bits/unique_ptr.h: In instantiation of &apos;typename std::_MakeUniq&lt;_Tp&gt;::__single_object std::make_unique(_Args&amp;&amp; ...) [with _Tp = pl2_ranker; _Args = &#123;const double&amp;, const double&amp;&#125;; typename std::_MakeUniq&lt;_Tp&gt;::__single_object = std::unique_ptr&lt;pl2_ranker, std::default_delete&lt;pl2_ranker&gt; &gt;]&apos;:C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:164:53: required from hereC:/msys64/mingw64/include/c++/6.3.0/bits/unique_ptr.h:791:30: error: invalid new-expression of abstract class type &apos;pl2_ranker&apos; &#123; return unique_ptr&lt;_Tp&gt;(new _Tp(std::forward&lt;_Args&gt;(__args)...)); &#125; ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~C:/msys64/mingw64/include/c++/6.3.0/bits/unique_ptr.h: In instantiation of &apos;typename std::_MakeUniq&lt;_Tp&gt;::__single_object std::make_unique(_Args&amp;&amp; ...) [with _Tp = pl2_ranker; _Args = &#123;std::basic_istream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;&#125;; typename std::_MakeUniq&lt;_Tp&gt;::__single_object = std::unique_ptr&lt;pl2_ranker, std::default_delete&lt;pl2_ranker&gt; &gt;]&apos;:C:/msys64/home/SSQ/meta/include/meta/index/ranker/ranker_factory.h:174:31: required from &apos;std::unique_ptr&lt;meta::index::ranker, std::default_delete&lt;meta::index::ranker&gt; &gt; meta::index::load_ranker(std::istream&amp;) [with Ranker = pl2_ranker; std::istream = std::basic_istream&lt;char&gt;]&apos;C:/msys64/home/SSQ/meta/include/meta/index/ranker/ranker_factory.h:188:5: required from &apos;void meta::index::register_ranker() [with Ranker = pl2_ranker]&apos;C:/msys64/home/SSQ/Assignment_1/ranking-experiment.cpp:170:38: required from hereC:/msys64/mingw64/include/c++/6.3.0/bits/unique_ptr.h:791:30: error: invalid new-expression of abstract class type &apos;pl2_ranker&apos;make[2]: *** [CMakeFiles/ranking-experiment.dir/build.make:63：CMakeFiles/ranking-experiment.dir/ranking-experiment.cpp.obj] 错误 1make[1]: *** [CMakeFiles/Makefile2:138：CMakeFiles/ranking-experiment.dir/all] 错误 2make[1]: *** 正在等待未完成的任务....[ 97%] Linking CXX executable analyze.exe[ 97%] Built target analyzemake: *** [Makefile:130：all] 错误 2 run ./analyze ../config.toml Assignment1/doc.txt --stopstemit appears 12Running stopword removal and stemming -&gt; file saved as Assignment1/doc.stopstem.txt run cd Assignment1 run wc &lt; doc.stopstem.txt | sed &#39;s/^\s*//g&#39; &gt; doc.stopstem.txt.wc submit Build the Search EngineIn this section, you will perform and experiment with indexing and retrieval on a special dataset called the MOOCs dataset. Exploring the DatasetThe MOOCs dataset contains the descriptions found on the webpages of around 23,000 MOOCs (Massive Open Online Courses). You will start by exploring the dataset. Navigate to Assignment_1/moocs. This directory contains the files that describe the dataset. moocs.dat contains the content of the webpages of all the MOOCs; each MOOC’s main page occupies exactly one line in the file. Feel free to open the file to get an idea about the contents, but be wary that it is a large file and might take some time to load. metadata.dat contains the names and the URLs of the MOOCs. The entry on line x in metadata.dat corresponds to the MOOC on line x in moocs.dat. moocs-queries.txt contains a set of queries that you will use to evaluate the effectiveness of your search engine. moocs-qrels.txt contains the relevance judgments corresponding to the queries in moocs-queries.txt. Each line in moocs-qrels.txt has the following format: (querynum documentID 1). This means that the document represented by documentID is a relevant document for the query whose number is querynum. The relevance judgments in moocs-qrels.txt were created by human assessors who ran the queries and chose the relevant documents. Later on in the assignment, you are going to use these judgments to quantify the performance of your search engine. IndexingNow that you have an idea about tokenization, text filtering, and the dataset to be used, you will proceed to index the MOOCs dataset. In this process, an inverted index will be created. As you have seen in the lectures, the inverted index is a data structure that supports efficient retrieval of documents by allowing the lookup of all the documents that contain a specific term. Before you proceed to index the dataset, we encourage you to open config.toml and examine the settings that we have already configured. For instance, the snippet shown below tells the indexer where to find the MOOCs dataset, specifies that it is a line corpus (i.e., each document is on one line), and defines the name of the inverted index to be created. The forward index will not be used in this assignment.123456query-judgements = &quot;../../meta/data/moocs/moocs-qrels.txt&quot; querypath = &quot;../../meta/data/moocs/&quot; corpus = &quot;line.toml&quot; dataset = &quot;moocs&quot; forward-index = &quot;moocs-fwd&quot; inverted-index = &quot;moocs-inv&quot; You should also have a look at another important snippet:1234[[analyzers]]method = &quot;ngram-word&quot;ngram = 1filter = &quot;default-unigram-chain&quot; The settings under the analyzers tag control how tokenization and filtering are performed, prior to creating the inverted index. The tokenizer will segment each document into unigrams. MeTA uses its default filtering chain, which performs a couple of predefined filters including lower case conversion, length filtering (which discards tokens whose length is not within a certain range), and stemming. To read more about modifying META’s default tokenization and filtering behavior see MeTA’s analyzers and filters page. To index the dataset, in the Assignment_1/build directory run:1../../meta/build/index ../config.toml if it occurs error index name missing from configuration fileadd index = &quot;moocs-inv&quot; in your config.toml file after inverted-index = &quot;moocs-inv&quot; 123456789101112 &gt; Counting lines in file: [=================================] 100% ETA 00:00:001498050577: [info] Creating index: moocs-inv/inv (C:/msys64/home/SSQ/meta/src/index/inverted_index.cpp:119) &gt; Tokenizing Docs: [========================================] 100% ETA 00:00:00 &gt; Merging: [================================================] 100% ETA 00:00:001498050585: [info] Created uncompressed postings file moocs-inv/inv/postings.index (9.120000 MB) (C:/msys64/home/SSQ/meta/src/index/inverted_index.cpp:148) &gt; Compressing postings: [===================================] 100% ETA 00:00:001498050586: [info] Created compressed postings file (7.620000 MB) (C:/msys64/home/SSQ/meta/src/index/inverted_index.cpp:279)1498050586: [info] Done creating index: moocs-inv/inv (C:/msys64/home/SSQ/meta/src/index/inverted_index.cpp:166)Number of documents: 17972Avg Doc Length: 358.238Unique Terms: 160615Index generation took: 11.217 seconds (index program is in the meta/build directory under root). You should see something like below:1234567$ ./meta/build/index ../config.toml1463154868: [info] Loading index from disk: moocs-inv (root/meta/src /index/inverted_index.cpp:178)Number of documents: 23566Avg Doc Length: 383.014Unique Terms: 193614Index generation took: 0.007 seconds This will start by performing tokenization and applying the text filters defined in config.toml; it then creates the inverted index and places it in/meta/build/moocs-inv. When the program finishes execution, you should get a summary of the indexed corpus as above SearchingAfter creating the inverted index, you can efficiently search the MOOCs dataset. The ranking function to be used in retrieving documents is defined in config.toml. Open config.toml and look for the ranker tag. Under this tag, you will see that the default ranker is “bm25” along with values assigned to its three parameters. To test your search engine, you can use the interactive-search program provided with MeTA:12cd Assignment_1/build../../meta/build/interactive-search ../config.toml Enter any query you want, and the top results should show up instantaneously. Feel free to experiment with different queries to explore the contents of the dataset.12345678910111213141516171498051690: [info] Loading index from disk: moocs-inv/inv (C:/msys64/home/SSQ/meta/src/index/inverted_index.cpp:171)Enter a query, or blank to quit.&gt; machine learningShowing top 5 results (3ms)1. [none] (score = 8.174631, docid = 2146)2. [none] (score = 7.979603, docid = 12388)3. [none] (score = 7.951803, docid = 4087)4. [none] (score = 7.932771, docid = 7194)5. [none] (score = 7.900622, docid = 10)&gt; text miningShowing top 5 results (1ms)1. [none] (score = 11.931686, docid = 11892)2. [none] (score = 11.278910, docid = 11890)3. [none] (score = 10.591638, docid = 10424)4. [none] (score = 10.041771, docid = 1646)5. [none] (score = 9.718287, docid = 5603) When you finish, enter a blank query to exit. In what follows you will evaluate the performance of the search engine using different ranking functions and parameters. The main evaluation measure to be used is MAP (mean average precision), which is the arithmetic mean of the average precision of all the queries being used for evaluation. We have provided you with a program called ranking-experiment which evaluates the MAP of any retrieval function you specify in config.toml. Task 4: BM25 (5 pts)Evaluate the performance of bm25 with default parameters (i.e., do not change config.toml) by running:12cd Assignment_1/build./ranking-experiment ../config.toml task4 You should see a list of the top results corresponding to the different test queries, the precision at 10 for each query, and the final MAP value. At the same time, we create a output file for you to submit: /Assignment_1/build/Assignment1/task4.txt Submit task4.txt. Task 5: BM25 with tuned parameters (10 pts)Change the parameter b of bm25 in config.toml to 0.7512345ranker] method = &quot;bm25&quot; k1 = 1.2 b = 0.75 k3 = 500 and run again:12cd Assignment_1/build./ranking-experiment ../config.toml task5 and submit your /Assignment_1/build/Assignment1/task5.txt Week 4In this week’s lessons, you will learn probabilistic retrieval models and statistical language models, particularly the detail of the query likelihood retrieval function with two specific smoothing methods, and how the query likelihood retrieval function is connected with the retrieval heuristics used in the vector space model. Week 4 InformationWeek 4 Overview 10 minTimeThis module should take approximately 3 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 4 Video Lectures 2 hours Week 4 Graded Quiz 1 hour Goals and ObjectivesAfter you actively engage in the learning experiences in this module, you should be able to: Explain how to interpret p(R=1|q,d) and estimate it based on a large set of collected relevance judgments (or clickthrough information) about query q and document d.https://www.coursera.org/learn/text-retrieval/lecture/nkg5n/lesson-4-1-probabilistic-retrieval-model-basic-idea Explain how to interpret the conditional probability p(q|d) used for scoring documents in the query likelihood retrieval function. Explain what a statistical language model and a unigram language model are. Explain how to compute the maximum likelihood estimate of a unigram language model. Explain how to use unigram language models to discover semantically related words. Compute p(q|d) based on a given document language model p(w|d). Explain what smoothing does. Show that query likelihood retrieval function implements TF-IDF weighting if we smooth the document language model p(w|d) using the collection language model p(w|C) as a reference language model. Compute the estimate of p(w|d) using Jelinek-Mercer (JM) smoothing and Dirichlet Prior smoothing, respectively. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. Given a table of relevance judgments in the form of three columns (query, document, and binary relevance judgments), how can we estimate p(R=1|q,d)? https://www.coursera.org/learn/text-retrieval/lecture/nkg5n/lesson-4-1-probabilistic-retrieval-model-basic-idea =$\frac{𝑐𝑜𝑢𝑛𝑡(𝑞, 𝑑, 𝑅 = 1)}{𝑐𝑜𝑢𝑛𝑡(𝑞, 𝑑)}$ How should we interpret the query likelihood conditional probability p(q|d)? $q=w_1 w_2 … w_n. p(q,d) = p(w_1|d)\cdotp(w_2|d)…p(w_n|d)$ What is a statistical language model? What is a unigram language model? How many parameters are there in a unigram language model? A probability distribution over word sequences. word distribution. $\frac{c(w,d)}{|d|}$ How do we compute the maximum likelihood estimate of the unigram language model (based on a text sample)? $\frac{c(w,d)}{|d|}$ What is a background language model? What is a collection language model? What is a document language model? p(w|B) p(w|C) p(w|d) Why do we need to smooth a document language model in the query likelihood retrieval model? What would happen if we don’t do smoothing? let the unseen word have probability. probability of unseen word would be 0. When we smooth a document language model using a collection language model as a reference language model, what is the probability assigned to an unseen word in a document? $\alpha_d p(p|d)$ How can we prove that the query likelihood retrieval function implements TF-IDF weighting if we use a collection language model smoothing? $p_{seen}(W_i|d)$ refers to TF weighting, $\alpha_d p(p|d)$ refers to IDF weighting. How does linear interpolation (Jelinek-Mercer) smoothing work? What is the formula? $p(w,d) = (1-\lambda)\frac{c(w,d)}{|d|}+\lambda p(w,d)$, $\lambda \in [0,1]$ How does Dirichlet prior smoothing work? What is the formula? $p(w,d) = \frac{c(w,d)+\mu p(w|C)}{|d|+\mu} = \frac{|d|}{|d|+\mu}\frac{c(w,d)}{|d|}+\frac{\mu}{|d|+\mu}p(w,d)$, $\mu\in[0,\infty)$ What are the similarities and differences between Jelinek-Mercer smoothing and Dirichlet prior smoothing? • Two smoothing methods– Jelinek-Mercer: Fixed coefficient linear interpolation– Dirichlet Prior: Adding pseudo counts; adaptive interpolation• Both lead to state of the art retrieval functions withassumptions clearly articulated (less heuristic)– Also implementing TF-IDF weighting and doc lengthnormalization– Has precisely one (smoothing) parameter Additional Readings and ResourcesC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapter 6 - Section 6.4 Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. p(R=1|q,d) ; query likelihood, p(q|d) Statistical and unigram language models Maximum likelihood estimate Background, collection, and document language models Smoothing of unigram language models Relation between query likelihood and TF-IDF weighting Linear interpolation (i.e., Jelinek-Mercer) smoothing Dirichlet Prior smoothing Week 4 LessonsLesson 4.1: Probabilistic Retrieval Model - Basic Idea 12 minhttps://www.coursera.org/learn/text-retrieval/lecture/nkg5n/lesson-4-1-probabilistic-retrieval-model-basic-idea Lesson 4.2: Statistical Language Model 17 minhttps://www.coursera.org/learn/text-retrieval/lecture/kv4Aj/lesson-4-2-statistical-language-model• What is a Language Model?• Unigram Language Model• Uses of a Language ModelLanguage Model = probability distribution over text• Unigram Language Model = word distribution• Uses of a Language Model– Representing topics– Discovering word associations Lesson 4.3: Query Likelihood Retrieval Function 12 minhttps://www.coursera.org/learn/text-retrieval/lecture/BWexZ/lesson-4-3-query-likelihood-retrieval-function Lesson 4.4: Statistical Language Model - Part 1 12 minhttps://www.coursera.org/learn/text-retrieval/lecture/f4CYl/lesson-4-4-statistical-language-model-part-1 Lesson 4.5: Statistical Language Model - Part 2 9 minhttps://www.coursera.org/learn/text-retrieval/lecture/hI1vE/lesson-4-5-statistical-language-model-part-2 Lesson 4.6: Smoothing Methods - Part 1 9 minhttps://www.coursera.org/learn/text-retrieval/lecture/kM6Ie/lesson-4-6-smoothing-methods-part-1 Lesson 4.7: Smoothing Methods - Part 2 13 minhttps://www.coursera.org/learn/text-retrieval/lecture/gxNMo/lesson-4-7-smoothing-methods-part-2 Week 4 ActivitiesPractice Quiz: Week 4 Practice Quiz 10 questionsPRACTICE QUIZWeek 4 Practice Quiz10 questionsTo Pass70% or higherDeadlineJuly 2, 11:59 PM PDT 1 point1.You are given a vocabulary composed of only three words: “text,” “mining,” and “research.” Below are the probabilities of two of these three words given by a unigram language model: Word Probability text 0.4 mining 0.2 What is the probability of generating the phrase “text mining research” using this unigram language model? 0.032The probability of “research” is P(“research”) = 1 – ( P(“text”)+P(“mining”) ) = 1 – (0.4 + 0.2) = 0.4. The probability of generating the given phrase P(“text mining research”) = P(“text”) x P(“mining”) x P(“research”) = 0.4 x 0.2 x 0.4 = 0.032. 0.4 0 0.081 point2.You are given the query Q= “food safety” and two documents: D1 = “food quality regulations” D2 = “food safety measures” Assume you are using the maximum likelihood estimator without smoothing to calculate the probabilities of words in documents (i.e., the estimated p(w|D) is the relative frequency of word w in the document D). Based on the unigram query likelihood model, which of the following choices is correct? P(Q|D1) = 1/3 P(Q|D2) = 0 P(Q|D1) = 1/2 P(Q|D2) = 1/2 P(Q|D1) = 0 P(Q|D2) = 1/9 P(Q|D1) = 1/3 P(Q|D2) = 1/91 point3.Probability smoothing avoids assigning zero probabilities to unseen words in documents. True False1 point4.Assume you are given two scoring functions: S1(Q,D)=P(Q|D)S2(Q,D)=logP(Q|D)For the same query and corpus, S1 and S2 will give the same ranked list of documents. True False1 point5.Assume you are using linear interpolation (Jelinek-Mercer) smoothing to estimate the probabilities of words in a certain document. What happens to the smoothed probability of the word when the parameter λ is decreased? It becomes closer to the maximum likelihood estimate of the probability derived from the document. It becomes closer to the probability of the word in the collection language model. It does not change.1 point6.Refer to the Rocchio feedback formula in the lectures. If you want to reduce the effect of the relevant documents in the updated query, which of the following should be done? Increase γ Reduce βThe weight assigned to the centroid of the relevant documents is directly proportional to β. Increase β Reduce γ1 point7.Let q be the original query vector, DR={P1,…,Pn} be the set of positive document vectors, and DN={N1,…,Nm} be the set of negative document vectors. Let q1 be the expanded query vector after applying Rocchio on DR and DN with positive parameter values α, β, and γ. Let q2 be the expanded query vector after applying Rocchio on DR and DN with the same values for α, β, but γ being set to zero. In which updated query do you expect stopwords to have higher weights? q2 q11 point8.In the query likelihood model, why is smoothing necessary? Without smoothing, if a term in the document is not in the query, then the likelihood becomes −∞. Without smoothing, if a term is neither in the query nor the document, then the likelihood becomes −∞. Without smoothing, if a term in the query is not in the document, then the likelihood becomes −∞.1 point9.Which of the following is NOT correct about the unigram model? The probability of generating the word sequence “A” “B” “C” is the same as generating “C” “B” “A.” The probability of generating the words A AND B is the product of the probability of generating A and the probability of generating B. The probability of generating the word A OR B is the sum of the probability of generating A and the probability of generating B.1 point10.Assume that β=1 is a good choice when performing relevance feedback using Rocchio’s method. What is a reasonable value of β to use when relying on pseudo feedback? 1 Less than 1When doing relevance feedback, the judgments are usually reliable since human assessors generate them after reading the queries and documents. However, in pseudo feedback, the top k documents retrieved by the system are “blindly” assumed to be relevant, which makes the judgments less reliable compared to relevance feedback. The reasonable choice is to lower the parameter β, which can be thought of as the degree of “confidence” in the documents being used as “positive” examples in feedback. More than 1 Quiz: Week 4 Quiz10 questions Due July 2, 11:59 PM PDTQUIZWeek 4 Quiz10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineJuly 2, 11:59 PM PDT 1 point1.Assume you are using a unigram language model to calculate the probabilities of phrases. Then, the probabilities of generating the phrases “study text mining” and “text mining study” are not equal, i.e., P(“study text mining”) ≠ P(“text mining study”). True False1 point2.You are given a vocabulary composed of only four words: “the,” “computer,” “science,” and “technology.” Below are the probabilities of three of these four words given by a unigram language model. Word Probability the 0.4 computer 0.2 science 0.3 What is the probability of generating the phrase “the technology” using this unigram language model? 0.0024 0.04 0.1 0.51 point3.You are given the query Q= “online courses” and two documents: D1 = “online courses search engine” D2 = “online education is affordable” Assume you are using the maximum likelihood estimator without smoothing to calculate the probabilities of words in documents (i.e., the estimated p(w|D) is the relative frequency of the word w in the document D). Based on the unigram query likelihood model, which of the following choices is correct? P(Q|D1) = 0 P(Q|D2) = 1/4 P(Q|D1) = 1/16 P(Q|D2) = 1/4 P(Q|D1) = 1/2 P(Q|D2) = 1/2 P(Q|D1) = 1/16 P(Q|D2) = 01 point4.Assume the same scenario as in Question 3, but using linear interpolation (Jelinek-Mercer) smoothing with λ=0.5. Furthermore, you are given the following probabilities of some of the words in the collection language model: Word P(w C) online 1/4 courses 1/4 education 1/8 Based on the unigram query likelihood model, which of the following choices is correct? $P(Q|D1) = \sum_{i}^N p(w_i|D1) = p(“online”|D1)\cdot p(“courses”|D1) = {0.5\frac{1}{4} + 0.5\frac{1}{4}}\cdot{0.5\frac{1}{4} + 0.5\frac{1}{4}} = 1/16$ $P(Q|D2) = \sum_{i}^N p(w_i|D2) = p(“online”|D2)\cdot p(“courses”|D2) = {0.5\frac{1}{4} + 0.5\frac{1}{4}}\cdot{0.5\frac{0}{4} + 0.5\frac{1}{4}} = 1/32$ P(Q|D1) = 1/32 P(Q|D2) = 1/32 P(Q|D1) = 1/16 P(Q|D2) = 0 P(Q|D1) = 1/16 P(Q|D2) = 1/32 P(Q|D1) = 1/16 P(Q|D2) = 1/161 point5.If word count for every term doubles in one document: If not using any smoothing, query likelihood would change for some queries. p(w|d) remains the same if using Jelinek-Mercer smoothing. p(w|d) remains the same if using Dirichlet-prior smoothing.1 point6.Assume you are using Dirichlet Prior smoothing to estimate the probabilities of words in a certain document. What happens to the smoothed probability of the word when the parameter μ is increased? It becomes closer to the probability of the word in the collection language model. It does not change. It becomes closer to the maximum likelihood estimate of the probability derived from the document. It tends to 1.1 point7.It is possible that pseudo feedback decreases the precision and recall of a certain retrieval system. False True1 point8.Refer to the Rocchio feedback formula in the lectures. If you want to eliminate the effect of non-relevant documents when doing feedback, which of the following parameters must be set to zero? γ and β γ α β1 point9.Let q be the original query vector, DR={P1,…,Pn} be the set of positive document vectors, and DN={N1,…,Nm} be the set of negative document vectors. Let q1 be the expanded query vector after applying Rocchio on DR and DN with positive parameter values α, β, and γ. Let q2 be the expanded query vector after applying Rocchio on DR and DN with the same values for α, β, but γ being set to zero. Which of the following is correct? q2 can have greater or equal weights to q1 for each dimension. q1 can have greater or equal weights to q2 for each dimension. q1 has strictly greater weights than q2 for each dimension. q2 has strictly greater weights than q1 for each dimension.1 point10.Which of the following is not true about the KL-divergence retrieval model? It represents both queries and documents as language models. It supports relevance feedback. It cannot be computed as efficiently as the query likelihood model. Week 5In this week’s lessons, you will learn feedback techniques in information retrieval, including the Rocchio feedback method for the vector space model, and a mixture model for feedback with language models. You will also learn how web search engines work, including web crawling, web indexing, and how links between web pages can be leveraged to score web pages. Week 5 InformationWeek 5 Overview 10 minTimeThis module should take approximately 3 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 5 Video Lectures 2 hours Week 5 Graded Quiz 1 hour Goals and ObjectivesAfter you actively engage in the learning experiences in this module, you should be able to: Explain the similarity and differences in the three different kinds of feedback, i.e., relevance feedback, pseudo-relevance feedback, and implicit feedback. Explain how the Rocchio feedback algorithm works. Explain how the Kullback-Leibler (KL) divergence retrieval function generalizes the query likelihood retrieval function. Explain the basic idea of using a mixture model for feedback. Explain some of the main general challenges in creating a web search engine. Explain what a web crawler is and what factors have to be considered when designing a web crawler. Explain the basic idea of Google File System (GFS). Explain the basic idea of MapReduce and how we can use it to build an inverted index in parallel. Explain how links on the web can be leveraged to improve search results. Explain how PageRank and HITS algorithms work. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. What is relevance feedback? What is pseudo-relevance feedback? What is implicit feedback? Users make explicit relevance judgments on the initial results(judgments are reliable, but users don’t want to make extra effort) Top-k initial results are simply assumed to be relevant(judgments aren’t reliable, but no user activity is required) User-clicked docs are assumed to be relevant; skipped ones non-relevant(judgments aren’t completely reliable, but no extra effort from users) How does Rocchio work? Why do we need to ensure that the original query terms have sufficiently large weights in feedback? Drag the query to the centroid of relevant of docments.$$\vec{q_m} = \alpha \vec{q} + \frac{\beta}{|D_r|}\sum_{\forall\vec{d_j} \in D_r}^{}{\vec{d_j}} - \frac{\gamma}{|D_n|}\sum_{\forall \vec{d_j} \in D_n}^{}{\vec{d_j}}$$ Avoid “over-fitting”. What is the KL-divergence retrieval function? How is it related to the query likelihood retrieval function? KL-divergence(cross entropy): $$f(q,d) = \sum_{w\in d,p(w|\theta_Q)&gt;0}{}{[p(w|\hat{\theta_Q})log\frac{P_{seen}(w|d)}{\alpha_d p(w|C)}] + log\alpha_d}$$ Query Likelihood: $$f(q,d) = \sum_{w_i \in d, w_i \in q}{}{c(w|q)[log\frac{P_{seen}(w|d)}{\alpha_d p(w|C)}] + n log\alpha_d}$$ What is the basic idea of the two-component mixture model for feedback? High probability of topic words when feedback is common and background words are rare. What are some of the general challenges in building a web search engine? ScalabilityHow to handle the size of the Web and ensure completeness of coverage?How to serve many user queries quickly? Low quality information and spams Dynamics of the WebNew pages are constantly created and some pages may be updated very quickly What is a crawler? How can we implement a simple crawler? crawler– Start with a set of “seed pages” in a priority queue– Fetch pages from the web– Parse fetched pages for hyperlinks; add them to the queue– Follow the hyperlinks in the queue how– Robustness (server failure, trap, etc.)– Crawling courtesy (server load balance, robot exclusion, etc.)– Handling file types (images, PDF files, etc.)– URL extensions (cgi script, internal references, etc.)– Recognize redundant pages (identical and duplicates)– Discover “hidden” URLs (e.g., truncating a long URL ) What is focused crawling? What is incremental crawling? Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers. The process of prioritizing and revisiting URLs is usually referred to as incremental crawling. What kind of pages should have a higher priority for recrawling in incremental crawling? What can we do if the inverted index doesn’t fit in any single machine? What’s the basic idea of the Google File System (GFS)? GFS is made up of several storage systems built from low-cost commodity hardware components. 一种软件定义存储把所有服务器上的存储设备“聚合起来”变成一个分布式的文件系统当一台服务器上插满硬盘也不能满足存储需求的时候，软件定义储存就诞生了对外展示为一个文件系统，底层是很多台机器的很多硬盘有一种是对外表现为一个硬盘，（可以再分区什么的）有一种是对外表现为一个文件系统（分布式的，可以多个主机同时操作）还有一种是需要写程序才能用的还有一种是同时具备以上功能gfs应该是第二种 How does MapReduce work? What are the two key functions that a programmer needs to implement when programming with a MapReduce framework? Software framework for parallel computation. Map and Reduce. How can we use MapReduce to build an inverted index in parallel? Parallel map via lots of documents and reduce according to the “key”. What is anchor text? Why is it useful for improving search accuracy? Anchor Text is the visible, clickable text in a hyperlink. In modern browsers, it is often blue and underlined, such as this link to the SSQ homepage. “Extra text”/summary for a doc What is a hub page? What is an authority page? A page has lots of outlinks A page has lots of inlinks What kind of web pages tend to receive high scores from PageRank? A page that is cited often How can we interpret PageRank from the perspective of a random surfer “walking” on the Web? Computing the probability of visiting every page. How exactly do you compute PageRank scores?Random surfing model: At any page,With prob. $\alpha$, randomly jumping to another pageWith prob. (1-$\alpha$), randomly picking a link to follow.p(di): PageRank score of di = average probability of visiting page di two forms $$p(d_j) = \sum_{i = 1}^{N}{[\frac{1}{N}\alpha + (1-\alpha)M_{ij}]p(d_i)}$$ or $$\bar{p} = {(\alpha I + (1 - \alpha) M)}^T \bar{p}$$M: transition matrixMij = probability of going from di to dj$\sum_{j=1}{N}{M_{ij}=1}$ How does the HITS algorithm work? Get the adjacency matrix Initial values: $a(d_i) = h(d_i) = 1$ Using $$h(d_i) = \sum_{d_j \in OUT(d_i)}{}{a(d_j)}$$ $$a(d_i) = \sum_{d_j \in IN(d_i)}{}{h(d_j)}$$ i.e. $$\bar{h} = A \bar{a}, \bar{a} = A^T \bar{h}, \bar{h} = AA^T\bar{h}, \bar{a} = A^TA\bar{a}$$ Iterate and normalize: $$\sum_{i}{}{a(d_i)^2} = \sum_{i}{}{h(d_i)^2} = 1$$ Additional Readings and Resources C. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapters 7 &amp; 10 Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Relevance feedback Pseudo-relevance feedback Implicit feedback Rocchio feedback Kullback-Leiber divergence (KL-divergence) retrieval function Mixture language model Scalability and efficiency Spams Crawler, focused crawling, and incremental crawling Google File System (GFS) MapReduce Link analysis and anchor text PageRank and HITS Week 5 LessonsLesson 5.1: Feedback in Text Retrieval6 minhttps://www.coursera.org/learn/text-retrieval/lecture/gw3fo/lesson-5-1-feedback-in-text-retrieval Relevance Feedback User Pseudo/Blind/Automatic Feedback Top k Implicit Feedback Clickthroughs Lesson 5.2: Feedback in Vector Space Model - Rocchio 12 minhttps://www.coursera.org/learn/text-retrieval/lecture/PyTkW/lesson-5-2-feedback-in-vector-space-model-rocchio Rocchio Feedback Lesson 5.3: Feedback in Text Retrieval - Feedback in LM 19 minhttps://www.coursera.org/learn/text-retrieval/lecture/M7ylk/lesson-5-3-feedback-in-text-retrieval-feedback-in-lm Kullback-Leibler (KL) Divergence Retrieval Model Generative Mixture Model Lesson 5.4: Web Search: Introduction &amp; Web Crawler 11 minhttps://www.coursera.org/learn/text-retrieval/lecture/qkTHD/lesson-5-4-web-search-introduction-web-crawler Web Search: Challenges &amp; Opportunities Basic Search Engine Technologies Major Crawling Strategies Web search is one of the most important applications of text retrieval Crawler is an essential component of Web search applications Lesson 5.5: Web Indexing 17 minhttps://www.coursera.org/learn/text-retrieval/lecture/lRm0I/lesson-5-5-web-indexing GFS Architecture MapReduce: A Framework for Parallel Programming Lesson 5.6: Link Analysis - Part 1 9 minhttps://www.coursera.org/learn/text-retrieval/lecture/nE8nq/lesson-5-6-link-analysis-part-1 anchor text hub and authority PageRank Lesson 5.7: Link Analysis - Part 2 17 minhttps://www.coursera.org/learn/text-retrieval/lecture/GUQ1Q/lesson-5-7-link-analysis-part-2 The PageRank Algorithm: : Capturing Page “Popularity” PageRank: Example Lesson 5.8: Link Analysis - Part 3 5 minhttps://www.coursera.org/learn/text-retrieval/lecture/d6INf/lesson-5-8-link-analysis-part-3The key idea of HITS (Hypertext-Induced Topic Search)– Good authorities are cited by good hubs– Good hubs point to good authorities– Iterative reinforcement…The HITS Algorithm: Capturing Authorities &amp; Hubs Week 5 ActivitiesPractice Quiz: Week 5 Practice Quiz 10 questionsPRACTICE QUIZWeek 5 Practice Quiz10 questionsTo Pass70% or higherDeadlineJuly 9, 11:59 PM PDTYou can still pass this quiz before the course ends. 1 point1.In PageRank, what is NOT the benefit of introducing random jumping? Otherwise for zero-outlink nodes will receive all the probability Otherwise PageRank will favor nodes with fewer incoming links Otherwise disconnected page always has zero probability1 point2.Modern web search engines often combine many features (e.g., content-based scores, link-based scores) to rank documents. True False1 point3.HITS and PageRank only use the inter-document links when calculating a document’s score, without considering the content of the document. True False1 point4.Can a crawler that only follows hyperlinks identify hidden pages that do not have any incoming links? Yes No1 point5.Which one is NOT a feature of incremental crawling? Target at a subset of pages (e.g., all pages about “automobiles’’) Target at frequently accessed pages Target at frequently updated pages Can learn from past experience (updated daily vs. monthly)1 point6.Which of the following sites will get a larger value in PageRank? A site that has many others linking to it but it does not link to others A site that outlinks to many others but none others link to it1 point7.Which of the following is NOT a reason that PageRank is efficient? Dimension of matrix M is usually large Matrix M is usually sparse Each iteration of PageRank is simple (matrix multiplication)1 point8.Which algorithm has more parameters than the other for running on the same dataset? PageRank HITS1 point9.If we analyze a large network in Facebook, which algorithms will find both zoombie users and celebrity users? PageRank HITS1 point10.Which of following feedback does not involve humans? Pseudo Implicit Relevance Quiz: Week 5 Quiz 10 questionsQUIZWeek 5 Quiz10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineJuly 9, 11:59 PM PDTYou can still pass this quiz before the course ends. 1 point1.Which of the following is not true about GFS? The GFS keeps multiple replicas of the same file chunk. The file data transfer happens directly between the GFS client and the GFS master. The file data transfer happens directly between the GFS client and the GFS chunkservers.1 point2.MapReduce allows parallelizing the creation of the inverted index. True False1 point3.In MapReduce, the Reduce function is called for each unique key of the output key-value pairs from the Map function. False True1 point4.Which of the following would cause a web page P to have a higher PageRank score? Add to another page Q a link that points to page P Add to page P a link that points to another page Q1 point5.Imagine if the web is fully connected with N pages such that for any pair of pages, P and Q, there exists a link from P to Q, then which of the following is true? At least one page will have a PageRank score larger than 1/N. All the pages will have a PageRank score of 1/N. At least one page will have a PageRank score smaller than 1/N.1 point6.Which of the following is/are the difference(s) between pseudo and implicit feedback? Both methods do not involve user activity. Implicit feedback assumes user-clicked documents are relevant. Pseudo feedback assumes top ranked documents are relevant.1 point7.What is true about Rocchio feedback? Negative examples are as important as positive examples. It works the best if you discard original query weights during feedback. It can be used for relevance feedback and also pseudo feedback. All words are used (no truncation) so that performance and efficiency can be guaranteed.1 point8.The advantages of incremental crawling are: It targets frequently accessed pages. It learns from past experience. It adds overhead. It targets frequently updated pages.1 point9.What is NOT the reason that PageRank works better than “citation counting”? It utilizes text information other than link information. PageRank has smoothing. PageRank considers “indirect citations.”1 point10.What is WRONG about HITS algorithm? The adjacency matrix is symmetric. It has two sets of parameters: a (authority) and h (hub). It’s an iterative algorithm. Week 6In this week’s lessons, you will learn how machine learning can be used to combine multiple scoring factors to optimize ranking of documents in web search (i.e., learning to rank), and learn techniques used in recommender systems (also called filtering systems), including content-based recommendation/filtering and collaborative filtering. You will also have a chance to review the entire course. Week 6 InformationWeek 6 Overview 10 minTimeThis module should take approximately 3-6 hours of dedicated time to complete, with its videos and assignments. ActivitiesThe activities for this module are listed below (with assignments in bold): Activity Estimated Time Required Week 6 Video Lectures 2 hours Week 6 Graded Quiz 1 hour Optional Programming Assignment 2 3 hours Goals and ObjectivesAfter you actively engage in the learning experiences in this module, you should be able to: Explain the basic idea of using machine learning to combine multiple features for ranking documents (i.e., learning to rank). Explain how we can extend a retrieval system to perform content-based information filtering (recommendation). Explain how we can use a linear utility function to evaluate an information filtering system. Explain the basic idea of collaborative filtering. Explain how the memory-based collaborative filtering algorithm works. Guiding QuestionsDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week. What’s the basic idea of learning to rank? General idea:– Given a query-doc pair (Q,D), define various kinds of features Xi(Q,D)– Examples of feature: the number of overlapping terms, BM25 score of Q and D, p(Q|D), PageRank of D, p(Q|Di), where Di may be anchor text or big font text, “does the URL contain ‘~’?”….– Hypothesize p(R=1|Q,D)=s(X1(Q,D),…,Xn(Q,D), $\lambda$) where $\lambda$ is aset of parameters– Learn $\lambda$ by fitting function s with training data, i.e., 3-tuples like (D, Q, 1) (D is relevant to Q) or (D,Q,0) (D is non-relevant to Q) How can logistic regression be used to combine multiple features for improving ranking accuracy of a search engine? Logistic Regression: Xi(Q,D) is feature; $\beta$’s are parameters; Estimate $\beta$’s by maximizing the likelihood of training data; Once $\beta$’s are known, we can take Xi(Q,D) computed based on a new query and a new document to generate a score for D w.r.t. Q. What is content-based information filtering? Look at what items U likes, and then check if X is similar. Item similarity =&gt; content-based filtering How can we use a linear utility function to evaluate a filtering system? How should we set the coefficients in such a linear utility function? Linear Utility = 3 #good - 2 #bad#good (#bad): number of good (bad) documents delivered to user It depends. How can we extend a retrieval system to perform content-based information filtering?• “Reuse” retrieval techniques to score documents• Use a score threshold for filtering decision• Learn to improve scoring with traditional feedback• New approaches to threshold setting and learning• Utility evaluation function to evaluate What is the exploration-exploitation tradeoff? The dilemma is between choosing what you know and getting something close to what you expect (‘exploitation’) and choosing something you aren’t sure about and possibly learning more (‘exploration’). How does the beta-gamma threshold learning algorithm work? Encourage exploration up to $\theta_{zero}$: $$\theta = \alpha \cdot \theta_{zero} + (1 - \alpha) \cdot \theta_{optimal}$$ The more examples,the less exploration(closer to optimal): $$\alpha = \beta + (1-\beta) \cdot e^{-N \cdot \gamma} , \beta, \gamma \in [0,1]$$ N =#training examples What is the basic idea of collaborative filtering? General idea– Given a user u, find similar users {u1, …, um}– Predict u’s preferences based on the preferences of u1, …, um– User similarity can be judged based on their similarity in preferences on a common set of items How does the memory-based collaborative filtering algorithm work? General ideas:– Xij: rating of object oj by user ui– ni: average rating of all objects by user ui–Normalized ratings: Vij = Xij – ni– Prediction of rating of object oj by user ua $$\hat{v_{aj}} = k \sum_{i=1}^{m}{w(a,i)v_{ij}}, \hat{x_{aj}} = \hat{v_{aj}}+n_a, k = 1/\sum_{i=1}^{m}{w(a,i)}$$– Specific approaches differ in w(a,i) – the distance/similarity between user ua and ui What is the “cold start” problem in collaborative filtering? little information about users at the beginning Additional Readings and ResourcesC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan &amp; Claypool Publishers, 2016. Chapters 10 - Section 10.4, Chapters 11 Key Phrases and ConceptsKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module. Learning to rank, features, and logistic regression Content-based filtering Collaborative filtering Beta-gamma threshold learning Linear utility User profile Exploration-exploitation tradeoff Memory-based collaborative filtering Cold start Week 6 LessonsLesson 6.1: Learning to Rank - Part 1 5 minhttps://www.coursera.org/learn/text-retrieval/lecture/mFYTD/lesson-6-1-learning-to-rank-part-1General idea of Learning to Rank Lesson 6.2: Learning to Rank - Part 2 10 minhttps://www.coursera.org/learn/text-retrieval/lecture/3d9fD/lesson-6-2-learning-to-rank-part-2Regression-Based ApproachesLogistic Regression: Xi(Q,D) is feature; $\beta$’s are parametersEstimate $\beta$’s by maximizing the likelihood of training dataOnce $\beta$’s are known, we can take Xi(Q,D) computed based on a new query and a new document to generate a score for D w.r.t. Q Lesson 6.3: Learning to Rank - Part 3 4 minhttps://www.coursera.org/learn/text-retrieval/lecture/h3Jru/lesson-6-3-learning-to-rank-part-3More Advanced Learning Algorithms Lesson 6.4: Future of Web Search 13 minhttps://www.coursera.org/learn/text-retrieval/lecture/kM78U/lesson-6-4-future-of-web-searchNext Generation Search EnginesThe Data-User-Service (DUS) TriangleMillions of Ways to Connect the DUS TriangleFuture Intelligent Information Systems Lesson 6.5: Recommender Systems: Content-Based Filtering - Part 1 12 minhttps://www.coursera.org/learn/text-retrieval/lecture/QORNe/lesson-6-5-recommender-systems-content-based-filtering-part-1Two Modes of Text Access: Pull vs. PushRecommender - Filtering SystemBasic Filtering Question: Will User U Like Item X?A Typical Content-Based Filtering SystemThree Basic Problems in Content-Based FilteringExtend a Retrieval System for Information FilteringA General Vector-Space Approach Lesson 6.6: Recommender Systems: Content-Based Filtering - Part 2 10 minhttps://www.coursera.org/learn/text-retrieval/lecture/7M0GD/lesson-6-6-recommender-systems-content-based-filtering-part-2Difficulties in Threshold LearningEmpirical Utility OptimizationBeta-Gamma Threshold Learning Lesson 6.7: Recommender Systems: Collaborative Filtering - Part 1 6 minhttps://www.coursera.org/learn/text-retrieval/lecture/cIFsU/lesson-6-7-recommender-systems-collaborative-filtering-part-1What is Collaborative Filtering (CF)?CF: AssumptionsThe Collaboration Filtering Problem Lesson 6.8: Recommender Systems: Collaborative Filtering - Part 2 12 minhttps://www.coursera.org/learn/text-retrieval/lecture/awVwS/lesson-6-8-recommender-systems-collaborative-filtering-part-2Memory-based ApproachesUser Similarity MeasuresImproving User Similarity Measures Lesson 6.9: Recommender Systems: Collaborative Filtering - Part 3 4 minhttps://www.coursera.org/learn/text-retrieval/lecture/tfXZ4/lesson-6-9-recommender-systems-collaborative-filtering-part-3Filtering/Recommendation is “easy”Filtering is “hard”Content-based vs. Collaborative filtering vs. HybridRecommendation can be combined with search :Push + PullMany advanced algorithms have been proposed to use more context information and advanced machine learning Lesson 6.10: Course Summary 9 minhttps://www.coursera.org/learn/text-retrieval/lecture/9CAed/lesson-6-10-course-summary Week 6 ActivitiesPractice Quiz: Week 6 Practice Quiz 10 questionsPRACTICE QUIZWeek 6 Practice Quiz10 questionsTo Pass70% or higherDeadlineJuly 16, 11:59 PM PDT 1 point1.In collaborative filtering, to measure user similarity using cosine, which one of the following is correct? It solves problem of missing values. It will not be biased by the user activity (less/more activity of users).1 point2.Which of the following tasks can be solved as a classification problem? Recommendation Ranking Spamming filtering1 point3.In recommendation/filtering, which heuristic in the following is NOT correct? If a user likes one item, then he/she will dislike other items not similar to the one liked. Similar users will like the same items. The same user will like similar items.1 point4.Comparing the logistic regression learning-to-rank method vs. the query likelihood model, which of the following is NOT true? The query likelihood method does not require time for training. The logistic regression method is better as it can take the query likelihood output as features. If trained until converged, the logistic regression will output the same solution (probability) as the query likelihood model as they are using the same relevance judgement.1 point5.GFS is a parallel programming framework that allows parallelized construction of the inverted index. False True1 point6.After obtaining the chunk’s handle and locations from the GFS master, the GFS client (application) obtains the actual file data directly from one of the GFS chunkservers. False True1 point7.What is the advantage of Learning to Rank over BM25? BM25 is much slower to train than Learning to Rank. Learning to Rank can combine many more features than BM25.1 point8.Can the regression based approach Learning to Rank utilize multi-grade relevance judgement (for example, not very relevant, not relevant, mediocre, relevant, very relevant)? No Yes1 point9.Which is/are the reason(s) learning-based algorithms became popular in text retrieval? There are more information needs such as removing spamming or diversified ranking. More features are available now for determining the results. More methods are available now for combining.1 point10.When a new user comes, which of the following will NOT help for recommendation? Ask user to first select a few items that he likes Ask user to provide a short description of himself Recommend user with random selected items5.Explanation: GFS is a distributed file system that aims at providing scalability and reliability in file storage.6.Explanation: The actual data transfer happens directly between the client and the chunkservers. The master can only send and receive control signals (not data files). Quiz: Week 6 Quiz 10 questionsQUIZWeek 6 Quiz10 questionsTo Pass70% or higherAttempts3 every 8 hoursDeadlineJuly 16, 11:59 PM PDT 1 point1.When using the learning to rank framework for combining multiple features into a ranking function, training data composed of queries and relevance judgments is needed to learn the model parameters. True False1 point2.Information filtering systems are more suitable to help users satisfy long-term information needs than short-term ad hoc information needs. False True1 point3.In content-based filtering, an item is recommended to a user based on whether other “similar” users like the item or not. True False1 point4.In recommendation systems, one uses Beta-Gamma threshold learning for trade-off between exploration and exploitation: $θ=α*θ_{zero}+(1−α)*θ_{optimal}$. Which of the following is true? α should be smaller for new users α should the same for all users no matter if they are new α should be larger for new users1 point5.Content-based filtering and collaborative filtering can be combined in a recommender system. True False1 point6.Which of the following is not an advantage of Learning to Rank? Directly optimize retrieval measures Is much faster than BM25 or language model when training Combining multiple sources of features1 point7.Recommendation is one type of Pull mode of information access. True False1 point8.In Netflix, if a user has watched a lot of thriller movies, then it recommends “Inception” and “The Silence of the Lambs” to the user. What is this an example of? This is content-based filtering. This is collaborative filtering.1 point9.In Spotify, if a user has indicated himself/herself as youth, then Spotify recommends songs that are most listened by users under 20 years old. What is this an example of? This is collaborative filtering. This is content-based filtering.1 point10.When adding social network information into recommendation systems, such as friends’ info and friends’ liked items, this can be used to help: Collaborative filtering Content-based filtering Optional Honors Content Honors Track Programming AssignmentProgramming Assignment: Programming Assignment 23hText Mining and AnalyticsCourse can be found hereMy certificate can be found hereLecture slides can be found here Pattern Discovery in Data MiningCourse can be found hereMy certificate can be found hereLecture slides can be found here Cluster Analysis in Data MiningCourse can be found hereMy certificate can be found hereLecture slides can be found here Data Mining ProjectCourse can be found hereMy certificate can be found hereLecture slides can be found here]]></content>
      <categories>
        <category>Coursera</category>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>Coursera</tag>
        <tag>Data Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity cs253 How to Build a Blog Notebook]]></title>
    <url>%2F2017%2F05%2F01%2FUdacity%20cs253%20How%20to%20Build%20a%20Blog%20Notebook%2F</url>
    <content type="text"><![CDATA[Components of the Webupdated in 2017-05-01no more updated Course can be found herehttps://classroom.udacity.com/courses/cs253/lessons/48737165/concepts/483298540923wikihttps://developer.mozilla.org/en-US/docs/Learn How the Web Workshttps://www.udacity.com/wiki/cs253/unit-1 Introduction to the Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AkjMCbSvTto.mp4 World Wide Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NBI9kXzMHS0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uPTMmyZB7tw.mp4 File Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ND8jAv7WrmU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VngVBqQYxVg.mp4 Components of the Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kzyfIiVZPJA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cVU7hYn-B8I.mp4 Best Browserhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/57kH7Yole2k.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eQDNuWOxaJ0.mp4 HTML Basicshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5Kjx-NOwcSc.mp4 Intro to HTML Tagshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VsxbuJWcxqA.mp4 Bold Tag&lt;b&gt;content&lt;/b&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/irJ9o1Uv6U8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JWld9DM-La4.mp4 Italics&lt;em&gt;content&lt;/em&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4H23hC78J_A.mp4Instructions: Make the phrase ‘HTML is reasonably straightforward’ italic. Note: the textbox should have default text in it for you to edit. In the meantime, it should look like this: HTML is &lt;b&gt;reasonably straightforward&lt;/b&gt; Your job is to make entire phase italicized by using the &lt;em&gt; tag.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1079BoYQUD8.mp4 Missing End Taghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OWjh74s_uT4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pKouF71QK_c.mp4 Making Links&lt;tag attr=&quot;value&quot;&gt;content&lt;/tag&gt; &lt;a href=&quot;www.reddit.com&quot;&gt;derp&lt;/a&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-0S9lBFHeo0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IjdvSD1tX8U.mp4Errata:URLs need the protocolThe URL in the link should be “http://udacity.com“ and not “udacity.com”. If the default text is missingThe textbox should have default text in it for you to edit. In the meantime, it should look like this: This website is my favorite Your job is to make the words my favorite a link to udacity.com. Adding Images&lt;img src=&quot;url&quot; alt=&quot;text&quot;&gt;https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Le8i8TtDleU.mp4 Whitespace&lt;br&gt; &lt;p&gt;content&lt;/p&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dEtVii1eYYY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/at-mlJ0KeMQ.mp4 Paragraph Taghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fSnKsMM6DRI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BH46s-hYLXg.mp4 Inline vs Blockinline: &lt;b&gt;&lt;/b&gt; &lt;em&gt;&lt;/em&gt; &lt;img src=&quot;&quot;&gt; block: &lt;p&gt;&lt;/p&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bOh9WjucNsA.mp4 Span and Divinline:&lt;span&gt;text&lt;/span&gt; block: &lt;div&gt;text&lt;/div&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rv1Oy-EduCk.mp4We now recommend using scratchpad.io instead of the HTML playground Steve refers to.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jCdjfORR7BI.mp4 Document Structure&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;b&gt;context&lt;/b&gt; &lt;/body&gt; &lt;/html&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VL7Wm0UzY6s.mp4 Introducing URLsUniform Resource LocatorFor those of you wondering what an IP address is, wikipedia has this to say: An Internet Protocol address (IP address) is a numerical label assigned to each device (e.g., computer, printer) participating in a computer network that uses the Internet Protocol for communication. An IP address serves two principal functions: host or network interface identification and location addressing. Its role has been characterized as follows: &quot;A name indicates what we seek. An address indicates where it is. A route indicates how to get there.&quot; The URL is the human readable locator which resolves to a numerical IP Address and represents, as Steve says, &quot;the location of the physical machine which has the document we want to fetch.&quot; An example IPv4 address looks like this: 172.16.254.1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yKKGg6ihUCs.mp4 Correct URLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kUMasWPRKE4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4_nLjYoN8ks.mp4 Query Parametershttp://example.com/foo?p=1&amp;q=neat name=value https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qv5XK91OhFo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e7N_4uaoNeI.mp4 Fragmentshttp://www.example.com/foo#fragment http://example.com/foo?p=1#fragment https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xq95EZdiOQc.mp4 Porthttp://localhost:8000/ port-default=80 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3P7e6R9LsRY.mp4The url got slightly cut off at the end. It should read: http://example.com:80/toys?p=foo#blah Note that urls are, in general, case-sensitive, as are most subsections of urls. Keep that in mind when you answer! The full url ishttp://example.com:80/toys?p=foo#blah https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8hiQyL6lcBs.mp4 Gethttp://www.example.com/foo request line: GET /foo HTTP/1.1 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8QjYUp3w5U0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rByBs2dt4dg.mp4 Most Common MethodGET POST https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/F9Fp-LtY7So.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pJNxDTa5uP0.mp4 Making Requestsrequest line: GET /foo?p=1 HTTP/1.1 Headers: Name: value Host: www.example.com User-Agent: chrome v.17 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5X7wcZuO5mU.mp4 User Agent Headerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oJ6MBvzgPcQ.mp4 Valid Headershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RdZDK0121kI.mp4Here is some information about headers that you might find useful.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/li-vh81yxKI.mp4 HTTP Responsesrequest: request-line: GET /foo HTTP/1.1 response: status-line: HTTP/1.1 200 Ok status code: 200 Ok 302 Found 404 Not found 500 Server Error https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/I4kUB17pTno.mp4 Response Headerstelnet www.udacity.com 80 GET / HTTP/1.0 Host: www.udacity.com https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ie0ONOZzNlw.mp4 NOTE:This example does not work properly at the moment, as http://www.udacity.com redirects to https://www.udacity.com. The former uses the protocol HTTP while the site now actually uses the protocol HTTPS. The extra “S” in HTTPS stands for “secure”, and you will hear more about it later. Opening A Terminal WindowMacMethod 1:Open Finder, then go to Applications -&gt; Utilities -&gt; Terminal Method 2:Open Spotlight, type ‘Terminal’, and the correct program should be the top result. LinuxHow you open the terminal varies depending on the specific distro and desktop environment. For Ubuntu, this page has a good introduction to the terminal and how to open it. WindowsOpen the Start Menu, then go to All Programs -&gt; Accessories -&gt; Command Prompt Running TelnetRunning telnet in Windows requires enabling it in “Windows Features” first. This page.aspx) has a good introduction to enabling the telnet client in Windows up to version 8. Windows 10 is similar but you can find “Windows Features” with the search box in the taskbar. Telnet should already be installed on Mac and Linux machines, so you should be able to copy the video exactly. Telnet Requesthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CIDBvFiccXE.mp4Since the video was recorded, the location header has changed to http://iana.org You’ll need to use telnet iana.org 80 and GET /domains/example HTTP/1.0 Host: iana.org instead of using domain example.com Since the video was recorded, the location header has changed to http://example.iana.org GET /domains/example HTTP/1.0 Host: iana.orghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ly95z-I1qmA.mp4 Web Applicationshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_E8qXBHI4cg.mp4 Dynamic Contenthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/543bDFHQKTs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4MlMQ5RJ4cc.mp4 Problem Set 1 - Creating Your First CiteGoogle App Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Sj_OmHLEdEA.mp4Check out the next lesson, “Problem Set 1 Help” for help getting Google App Engine running. In the current version of Google App Engine (1.9.1 - 2014-03-19), the command line argument to choose a port in dev_appserver.py is no longer “-p”. The argument is now “–port”.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PNmji8qNL0s.mp4tutorial in the forum tutorialhttps://console.cloud.google.com/home/dashboard?project=ssq-udacity-cs253 set pathhttp://docs.python-guide.org/en/latest/starting/install/win/ Welcome to Cloud Shell! Type &quot;help&quot; to get started. ssq6554@ssq-udacity-cs253:~$ TUTORIALDIR=~/src/ssq-udacity-cs253/ type: TUTORIALDIR=~/src/ssq-udacity-cs253/ python_gae_quickstart-2017-05-03-15-34 ssq6554@ssq-udacity-cs253:~$ git clone https://github.com/GoogleCloudPlatform/appengine-try-python-flask.git $TUTORIALDIR type: git clone https://github.com/GoogleCloudPlatform/appengine-try-python-flask.git $TUTORIALDIR Cloning into &apos;/home/ssq6554/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34&apos;... remote: Counting objects: 386, done. remote: Total 386 (delta 0), reused 0 (delta 0), pack-reused 386 Receiving objects: 100% (386/386), 776.84 KiB | 355.00 KiB/s, done. Resolving deltas: 100% (53/53), done. ssq6554@ssq-udacity-cs253:~$ cd $TUTORIALDIR type: cd $TUTORIALDIR ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ git checkout gcloud type: git checkout gcloud Branch gcloud set up to track remote branch gcloud from origin. Switched to a new branch &apos;gcloud&apos; ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ dev_appserver.py $PWD type: dev_appserver.py $PWD INFO 2017-05-03 07:45:50,733 devappserver2.py:692] Skipping SDK update check. WARNING 2017-05-03 07:45:50,973 simple_search_stub.py:1152] Could not read search indexes from /tmp/appengine.None.ssq6554/search_indexes INFO 2017-05-03 07:45:50,977 api_server.py:272] Starting API server at: http://0.0.0.0:48288 INFO 2017-05-03 07:45:50,980 dispatcher.py:205] Starting module &quot;default&quot; running at: http://0.0.0.0:8080 INFO 2017-05-03 07:45:50,981 admin_server.py:116] Starting admin server at: http://0.0.0.0:8000 INFO 2017-05-03 07:46:22,190 module.py:813] default: &quot;GET /?authuser=0 HTTP/1.0&quot; 200 12 ^CINFO 2017-05-03 07:46:39,143 shutdown.py:45] Shutting down. INFO 2017-05-03 07:46:39,144 api_server.py:863] Applying all pending transactions and saving the datastore INFO 2017-05-03 07:46:39,144 api_server.py:866] Saving search indexes ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ gcloud app deploy app.yaml --project ssq-udacity-cs253 type: gcloud app deploy app.yaml --project ssq-udacity-cs253 You are about to deploy the following services: - ssq-udacity-cs253/default/20170503t155355 (from [/home/ssq6554/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34/app.yaml]) Deploying to URL: [https://ssq-udacity-cs253.appspot.com] Do you want to continue (Y/n)? y type: y Beginning deployment of service [default]... Some files were skipped. Pass `--verbosity=info` to see which ones. You may also view the gcloud log file, found at remote: Counting objects: 386, done. [/tmp/tmp.QsMJk5uBEu/logs/2017.05.03/15.53.52.611133.log]. ╔════════════════════════════════════════════════════════════╗ ╠═ Uploading 77 files to Google Cloud Storage ═╣ ╚════════════════════════════════════════════════════════════╝ File upload done. Updating service [default]...done. Deployed service [default] to [https://ssq-udacity-cs253.appspot.com] You can stream logs from the command line by running: $ gcloud app logs tail -s default To view your application in the web browser run: $ gcloud app browse ssq6554@ssq-udacity-cs253:~/src/ssq-udacity-cs253/python_gae_quickstart-2017-05-03-15-34$ succeed in deployinghttp://ssq-udacity-cs253.appspot.com/ (success)Try locallytype: gcloud init gcloud components list Your current Cloud SDK version is: 153.0.0 The latest available version is: 153.0.0 +------------------------------------------------------------------------------- ------------------------------+ | Components | +---------------+------------------------------------------------------+-------- ------------------+-----------+ | Status | Name | ID | Size | +---------------+------------------------------------------------------+-------- ------------------+-----------+ | Not Installed | App Engine Go Extensions | app-eng ine-go | 48.4 MiB | | Not Installed | Bigtable Command Line Tool | cbt | 3.9 MiB | | Not Installed | Cloud Datalab Command Line Tool | datalab | &lt; 1 MiB | | Not Installed | Cloud Datastore Emulator | cloud-d atastore-emulator | 15.4 MiB | | Not Installed | Cloud Datastore Emulator (Legacy) | gcd-emu lator | 38.1 MiB | | Not Installed | Cloud Pub/Sub Emulator | pubsub- emulator | 21.0 MiB | | Not Installed | Emulator Reverse Proxy | emulato r-reverse-proxy | 14.5 MiB | | Not Installed | Google Container Registry&apos;s Docker credential helper | docker- credential-gcr | 3.3 MiB | | Not Installed | gcloud Alpha Commands | alpha | &lt; 1 MiB | | Not Installed | gcloud Beta Commands | beta | &lt; 1 MiB | | Not Installed | gcloud app Java Extensions | app-eng ine-java | 128.6 MiB | | Not Installed | gcloud app PHP Extensions (Windows) | app-eng ine-php-windows | 19.1 MiB | | Not Installed | gcloud app Python Extensions | app-eng ine-python | 6.1 MiB | | Not Installed | kubectl | kubectl | 14.9 MiB | | Installed | BigQuery Command Line Tool | bq | &lt; 1 MiB | | Installed | Cloud SDK Core Libraries | core | 5.9 MiB | | Installed | Cloud Storage Command Line Tool | gsutil | 2.9 MiB | | Installed | Default set of gcloud commands | gcloud | | +---------------+------------------------------------------------------+-------- ------------------+-----------+ To install or remove components at your current SDK version [153.0.0], run: $ gcloud components install COMPONENT_ID $ gcloud components remove COMPONENT_ID To update your SDK installation to the latest version [153.0.0], run: $ gcloud components update type:gcloud components install app-engine-python Restarting command: $ gcloud components install app-engine-python There appears a new cmd window Your current Cloud SDK version is: 153.0.0 Installing components from version: 153.0.0 +--------------------------------------------------+ | These components will be installed. | +------------------------------+---------+---------+ | Name | Version | Size | +------------------------------+---------+---------+ | gcloud app Python Extensions | 1.9.52 | 6.1 MiB | +------------------------------+---------+---------+ For the latest full release notes, please visit: https://cloud.google.com/sdk/release_notes Do you want to continue (Y/n)? type: y #============================================================# #= Creating update staging area =# #============================================================# #= Installing: gcloud app Python Extensions =# #============================================================# #= Creating backup and activating new installation =# #============================================================# Performing post processing steps...done. Update done! 请按任意键继续. . . open git bashtype: git clone https://github.com/GoogleCloudPlatform/python-docs-samplestype: cd G:/\Udacity/\cs253 Cloning into &apos;python-docs-samples&apos;... remote: Counting objects: 10835, done. remote: Compressing objects: 100% (161/161), done. remote: Total 10835 (delta 58), reused 1 (delta 1), pack-reused 10649 Receiving objects: 100% (10835/10835), 4.60 MiB | 190.00 KiB/s, done. Resolving deltas: 100% (5742/5742), done. type: cd python-docs-samples/\appengine/\standard/\hello_world type: dev_appserver.py .look in http://localhost:8080/ (failed)or open cmd Go to cloud.google.com Login and create a project Copy the project id and create a project in App Engine on your machine with same id Install Google Cloud SDK Open Google Cloud SDK shell Type &apos;gcloud init &apos; It will show ur gmail id OR ask u to log in Select the project you created in step 2 Type &apos;gcloud beta app create&apos; It will ask for region - choose the closest region Hit Deploy button in App Engine OR Type &apos;gcloud app deploy &apos; in the shell type: gcloud beta app create You do not currently have this command group installed. Using it requires the installation of components: [beta] Restarting command: $ gcloud components install beta Installing component in a new window. Please re-run this command when installation is complete. $ C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\..\lib\gc loud.py beta app create there opens a new window Your current Cloud SDK version is: 153.0.0 Installing components from version: 153.0.0 +---------------------------------------------+ | These components will be installed. | +----------------------+------------+---------+ | Name | Version | Size | +----------------------+------------+---------+ | gcloud Beta Commands | 2017.03.24 | &lt; 1 MiB | +----------------------+------------+---------+ For the latest full release notes, please visit: https://cloud.google.com/sdk/release_notes Do you want to continue (Y/n)? y #============================================================# #= Creating update staging area =# #============================================================# #= Installing: gcloud Beta Commands =# #============================================================# #= Creating backup and activating new installation =# #============================================================# Performing post processing steps...done. Update done! 请按任意键继续. . . type: gcloud beta app create ERROR: (gcloud.beta.app.create) The project [ssq-udacity-helloworld] already con tains an App Engine application in region [us-central]. You can deploy your app lication using `gcloud app deploy`. type: cd G:\Udacity\cs253\helloworld g:type: gcloud app deploy ERROR: gcloud crashed (UnicodeEncodeError): &apos;ascii&apos; codec can&apos;t encode character u&apos;\xad&apos; in position 22: ordinal not in range(128) If you would like to report this issue, please run the following command: gcloud feedback To check gcloud for common problems, please run the following command: gcloud info --run-diagnostics failedtype: C:\Users\SSQ&gt;python &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\dev_appserver.py&quot; G:\Udacity\cs253\python-docs-samples\appengine\standard\hello_world\app.yaml success (Fail)try locally againopen Google Cloud SDK Shelltype: python &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\bin\dev_appserver.py&quot; G:\Udacity\cs253\helloworld\app.yaml it appears: Updates are available for some Cloud SDK components. To install them, please run: $ gcloud components update Traceback (most recent call last): File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 103, in &lt;module&gt; _run_file(__file__, globals()) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 97, in _run_file execfile(_PATHS.script_file(script_name), globals_) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 899, in &lt;module&gt; main() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 895, in main dev_server.stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 775, in stop metrics.GetMetricsLogger().Stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\metrics.py&quot;, line 119, in St op total_run_time = int((Now() - self._start_time).total_seconds()) TypeError: unsupported operand type(s) for -: &apos;datetime.datetime&apos; and &apos;NoneType&apos; type: gcloud components updatethere’s a new window: Your current Cloud SDK version is: 153.0.0 You will be upgraded to version: 154.0.1 +-------------------------------------------------+ | These components will be updated. | +--------------------------+------------+---------+ | Name | Version | Size | +--------------------------+------------+---------+ | Cloud SDK Core Libraries | 2017.05.04 | 6.0 MiB | | gcloud cli dependencies | 2017.05.01 | 1.6 MiB | +--------------------------+------------+---------+ The following release notes are new in this upgrade. Please read carefully for information about new features, breaking changes, and bugs fixed. The latest full release notes can be viewed at: https://cloud.google.com/sdk/release_notes 154.0.1 (2017-05-04) Cloud SDK o Fixed issue in for gcloud init command. See https://issuetracker.google.com/37968909. 154.0.0 (2017-05-03) Cloud SDK o Added support for project creation during the gcloud init flow. Google Cloud Logging o BUG FIX: gcloud beta logging sinks update would remove any start time or end time from a sink o gcloud beta logging sinks describe now reports values of start_time, end_time and include_children o The --include-children flag is now available for gcloud beta logging sinks create to create sinks that apply to an organization or folder and also to all of its child projects and folders. Google Compute Engine o Workaround problems with alpha and beta versions of compute ssh command fail an attempt to use clouduseraccounts APIs. Google Container Engine o Promote --cluster-version from beta to GA in gcloud container clusters create. o &apos;--no-source&apos; flag for &apos;gcloud container builds submit&apos; allows builds with no source input. Google Cloud ML Engine o Added --config parameter to gcloud ml-engine versions create; this parameter allows specifying scaling settings for a version. Google Cloud Speech o The gcloud ml speech commands to recognize spoken words in recorded speech using the Cloud Speech API are now available in beta. Please run gcloud beta ml speech --help or visit https://cloud.google.com/speech/docs/ to learn more. Google App Engine o gcloud beta app deploy now attempts to use the Service Management API to enable the Appengine Flexible Environment API for Flexible deployments, if needed. Before deploying a Flexible app, please ensure that the Flexible Environment API is enabled on the app&apos;s project. o The new Node.js Runtime Builder pipeline will now be used to deploy apps when using gcloud beta app deploy. Google Cloud SQL o Promote gcloud sql operations to GA. The beta surface still remains and is identical. Google Cloud Source Repositories o Add a source repos describe command to describe a repository to the beta track. Do you want to continue (Y/n)? type: y #============================================================# #= Creating update staging area =# #============================================================# #= Uninstalling: Cloud SDK Core Libraries =# #============================================================# #= Uninstalling: gcloud cli dependencies =# #============================================================# #= Installing: Cloud SDK Core Libraries =# #============================================================# #= Installing: gcloud cli dependencies =# #============================================================# #= Creating backup and activating new installation =# #============================================================# Performing post processing steps...done. Update done! To revert your SDK to the previously installed version, you may run: $ gcloud components update --version 153.0.0 请按任意键继续. . . type: `` Traceback (most recent call last): File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 103, in &lt;module&gt; _run_file(__file__, globals()) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\dev_appserver.py&quot;, line 97, in _run_file execfile(_PATHS.script_file(script_name), globals_) File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 899, in &lt;module&gt; main() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 895, in main dev_server.stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\devappserver2.py&quot;, line 775, in stop metrics.GetMetricsLogger().Stop() File &quot;C:\Users\SSQ\AppData\Local\Google\Cloud SDK\google-cloud-sdk\platform\go ogle_appengine\google\appengine\tools\devappserver2\metrics.py&quot;, line 119, in St op total_run_time = int((Now() - self._start_time).total_seconds()) TypeError: unsupported operand type(s) for -: &apos;datetime.datetime&apos; and &apos;NoneType&apos; Deploy your app to the cloudcd to hello_world dir and type gcloud app deploy G:\Udacity\cs253\python-docs-samples\appengine\standard\hello_world&gt;gcloud app deploy You are about to deploy the following services: - ssq-udacity-cs253/default/20170503t210424 (from [G:\Udacity\cs253\python-docs -samples\appengine\standard\hello_world\app.yaml]) Deploying to URL: [https://ssq-udacity-cs253.appspot.com] Do you want to continue (Y/n)? y type y Beginning deployment of service [default]... Some files were skipped. Pass `--verbosity=info` to see which ones. You may also view the gcloud log file, found at [C:\Users\SSQ\AppData\Roaming\gcloud\logs\2017.05.03\21.04.21.920000.log]. #============================================================# #= Uploading 0 files to Google Cloud Storage =# #============================================================# File upload done. Updating service [default]...done. Deployed service [default] to [https://ssq-udacity-cs253.appspot.com] You can stream logs from the command line by running: $ gcloud app logs tail -s default To view your application in the web browser run: $ gcloud app browse type gcloud app browse G:\Udacity\cs253\python-docs-samples\appengine\standard\hello_world&gt;gcloud app browse Opening [https://ssq-udacity-cs253.appspot.com] in a new tab in your default bro wser. Problem Set 1 HelpIntrohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IFYmvf_RNT0.mp4 Install Google App Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/euox_iFW2eM.mp4 Install Google App EngineMake sure you are downloading Python 2 rather than Python 3, as this is what Google App Engine uses! Changes to Google App EngineThe Google App Engine system (now part of Google Cloud) has changed a lot since this course was created. You will need to create an app with a new, unique name using the Developer Console before you can deploy it. For more information, take a look at these guides written by Udacity mentor Steven Wooding: (Windows), (Mac and Linux) Installing Python on LinuxIf you’re on a Mac or Linux machine, open a terminal and type ‘python -v’ at the prompt. If it returns some information about Python, then you already have Python installed! If not, then Mac users can follow the instructions in the video, and Linux users can use their package managers instead. For Ubuntu or Debian, type ‘sudo apt-get install python’ into your terminal, then your password. For Fedora, type ‘sudo yum install python’ into your terminal, then your password. For other Linux distros, their should be documentation available for your specific package manager. Running Google App Engine on Mac OSUnfortunately Google no longer supports the GoogleAppengineLauncher program that Steve demonstrates in this video. You follow use the instructions below for Linux, or you can download the deprecated GoogleAppengineLauncher installer from the “Supporting Materials” list below. Warning: Use the GoogleAppengineLauncher program at your own peril! As Google no longer supports the tool it may stop working in the future. The gcloud command-line program is the currently supported tool. Running Google App Engine on LinuxTo run the Development Web Server locally, run: dev_appserver.py myapp Where myapp is the name you want your app to have. To upload your code to Google App Engine, run: appcfg.py update helloworld/ Where helloworld/ is the directory you’re running your web app from. Further help is available here and hereHave questions? Head to the forums for discussion with the Udacity Community. Supporting MaterialsWindows Installation Guide for App Engine.pdfGoogleAppEngineLauncher-1.9.40-OSXhttps://discussions.udacity.com/t/problem-set-1-seems-to-be-nothing-like-help-video-shows/215184/3 Office Hours 1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QKTIOnISsCw.mp4 Forms and Inputhttps://www.udacity.com/wiki/cs253/unit-2 Introductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3qgaev6_ZkY.mp4 Forms https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OnptThLQb_k.mp4 User Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qNdg3ZKoKnA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PmF0C6dWzFk.mp4 Naming Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IOkzztrmxbg.mp4how to implement play.html:https://discussions.udacity.com/t/unit2-this-is-fustrating/68505/2 Entering Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JDwulMznybc.mp4Try the HTML code &lt;form&gt; &lt;input name=&quot;q&quot;&gt; &lt;/form&gt; Note that if you chose the text disappears in addition to the the URL changes with my text, that you are also correct. However, we’re focused on the latter, and arguably more interesting, of these two events. But, you may always re-submit your answer to any quiz.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vUHabH7bsTc.mp4 Submitting Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/easPpLBGf2w.mp4 &lt;form&gt; &lt;input name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GMNVeltn0ms.mp4 The Action Attributehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UR-I9lkV3No.mp4 &lt;form action=&quot;http://www.google.com/search&quot;&gt; &lt;input name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7YawG07y9lg.mp4 URL Encodinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Z3udiqgW1VA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lSjOr6_LaMs.mp4 Hello Webapp Worldhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t9qqgCvHxgs.mp4 Content Typehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/a0k61hlrO9M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TGHZjKuxTB0.mp4 More Handlershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9624RoIt2pk.mp4 GET /testform?q=hello+world%21 HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Language: zh-CN,zh;q=0.8 Content-Type: ; charset=&quot;utf-8&quot; Cookie: _xsrf=2|69d62867|9814fd50905531e52e958841989ec6fb|1491708993; username-localhost-8890=&quot;2|1:0|10:1491841902|23:username-localhost-8890|44:NzI4YzlkMzc5MzIyNGQ5YTgxMWMzMDAyZDZiY2M4NTY=|69e8bf9e2ec4a47c684886256bda319f88bbbbe1cc4bf5c9193aa52c9449bab1&quot;; username-localhost-8891=&quot;2|1:0|10:1491842087|23:username-localhost-8891|44:YThkM2ViMzc0NmJjNDVhNGE4YWI4OTgwZGE5M2RlYzk=|cd0850025b097f89ae64b4bda7911166954a2384ecd7eb8a17991270ad4111a4&quot;; username-localhost-8889=&quot;2|1:0|10:1492581579|23:username-localhost-8889|44:NGM1MDZlMTUzNWE1NGUzNmFkYTNhMzRiMjdmOGYyNWY=|a15a04a3925f6c3954789ffbdb5b01f5bda3a0c4e9e47420ac1564c5302aaf9d&quot;; username-localhost-8888=&quot;2|1:0|10:1493640422|23:username-localhost-8888|44:NjYzZDYzYThhNmQwNDZiODk2ZTVlZDU0YjkxOTI5MWM=|fb58980d83c777e3d64786ba156f12ab1834a9c404151d3d5dacb943ca4e0616&quot;; Hm_lvt_3c8ad2ecdd2387b44044b1d7cd3536a9=1492997775,1493349600,1493631556,1493712288; _ga=GA1.1.1682530332.1487774100 Host: localhost:8080 Referer: http://localhost:8080/ Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36 X-Appengine-Country: ZZ https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6BpCLwYGwBA.mp4 The Method Attributehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eDotptZ1Nyo.mp4 405 Method Not Allowed The method POST is not allowed for this resource. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BOnkFj9uMkg.mp4 http://localhost:8080/testform hello world! Methods and Parametershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E_OiAN5VKOM.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot; action=&quot;/testform&quot;&gt; &lt;input name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.write(form) class TestHandler(webapp2.RequestHandler): def post(self): #q=self.request.get(&quot;q&quot;) #self.response.out.write(q) self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.out.write(self.request) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage), (&apos;/testform&apos;,TestHandler) ], debug=True) POST /testform HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Language: zh-CN,zh;q=0.8 Cache-Control: max-age=0 Content-Length: 16 Content-Type: application/x-www-form-urlencoded; charset=&quot;utf-8&quot; Content_Length: 16 Content_Type: application/x-www-form-urlencoded Cookie: _xsrf=2|69d62867|9814fd50905531e52e958841989ec6fb|1491708993; username-localhost-8890=&quot;2|1:0|10:1491841902|23:username-localhost-8890|44:NzI4YzlkMzc5MzIyNGQ5YTgxMWMzMDAyZDZiY2M4NTY=|69e8bf9e2ec4a47c684886256bda319f88bbbbe1cc4bf5c9193aa52c9449bab1&quot;; username-localhost-8891=&quot;2|1:0|10:1491842087|23:username-localhost-8891|44:YThkM2ViMzc0NmJjNDVhNGE4YWI4OTgwZGE5M2RlYzk=|cd0850025b097f89ae64b4bda7911166954a2384ecd7eb8a17991270ad4111a4&quot;; username-localhost-8889=&quot;2|1:0|10:1492581579|23:username-localhost-8889|44:NGM1MDZlMTUzNWE1NGUzNmFkYTNhMzRiMjdmOGYyNWY=|a15a04a3925f6c3954789ffbdb5b01f5bda3a0c4e9e47420ac1564c5302aaf9d&quot;; username-localhost-8888=&quot;2|1:0|10:1493640422|23:username-localhost-8888|44:NjYzZDYzYThhNmQwNDZiODk2ZTVlZDU0YjkxOTI5MWM=|fb58980d83c777e3d64786ba156f12ab1834a9c404151d3d5dacb943ca4e0616&quot;; Hm_lvt_3c8ad2ecdd2387b44044b1d7cd3536a9=1492997775,1493349600,1493631556,1493712288; _ga=GA1.1.1682530332.1487774100 Host: localhost:8080 Origin: http://localhost:8080 Referer: http://localhost:8080/ Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.96 Safari/537.36 X-Appengine-Country: ZZ q=hello+world%21 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9Hj83VRhOQY.mp4 Differences Between Get and Posthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UObINRj2EGY.mp4 Problems with Gethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cIliEo0zOwg.mp4 When to Use Get and Posthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CprytP12okM.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KYPi9loZE-M.mp4 Passwordshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xS71dGcER_s.mp4 &lt;form&gt; &lt;input type=&quot;password&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=hellohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oWvmNCuI47k.mp4 Checkboxeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bFK7ZIwnVik.mp4 &lt;form&gt; &lt;input type=&quot;checkbox&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html? Multiple Checkboxeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Td_6JT-1MQ0.mp4 &lt;form&gt; &lt;input type=&quot;checkbox&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;checkbox&quot; name=&quot;r&quot;&gt; &lt;input type=&quot;checkbox&quot; name=&quot;s&quot;&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=on&amp;r=onhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HEACkANsj9o.mp4 Radio Buttonshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/di_FzsEzyQk.mp4 &lt;form&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;r&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;s&quot;&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=on&amp;r=on Grouping Radio Buttonshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QKxqWDx2bcs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5GqK_VmlrAY.mp4 Radio Button Valueshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xPTL73OT5IY.mp4 &lt;form&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;q&quot;&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hTDenQDEvqU.mp4 Label Elementshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XnBMnDGQqtw.mp4 &lt;form&gt; &lt;label&gt; One &lt;input type=&quot;radio&quot; name=&quot;q&quot; value=&quot;one&quot;&gt; &lt;/label&gt; &lt;label&gt; Two &lt;input type=&quot;radio&quot; name=&quot;q&quot; value=&quot;two&quot;&gt; &lt;/label&gt; &lt;label&gt; Three &lt;input type=&quot;radio&quot; name=&quot;q&quot; value=&quot;three&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; Dropdownshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gVtTvYfc5NA.mp4 &lt;form&gt; &lt;select name=&quot;q&quot;&gt; &lt;option&gt;One&lt;/option&gt; &lt;option&gt;Two&lt;/option&gt; &lt;option&gt;Three&lt;/option&gt; &lt;/select&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; file:///G:/Udacity/cs253/L5%20Forms%20and%20Input/play.html?q=Twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/joUhCZw_zG0.mp4 Dropdowns and Valueshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/q2jmRGrAixc.mp4 The Number Onehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mRTBZNTmimA.mp4 &lt;form&gt; &lt;select name=&quot;q&quot;&gt; &lt;option value=&quot;1&quot;&gt;the number One&lt;/option&gt; &lt;option&gt;Two&lt;/option&gt; &lt;option&gt;Three&lt;/option&gt; &lt;/select&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zCBUF8oDJPE.mp4 Validationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uC8ny0rTzIE.mp4 What Is Your Birthday?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Gx16pZ7crqU.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.write(form) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) 405 Method Not Allowed The method POST is not allowed for this resource. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/psynonn163k.mp4 Handling Postshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NyIz4ht6NGI.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.out.write(form) def post(self): self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) Handling Bad Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fcrTA3_iHLY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/u7vPsxvUNMA.mp4 Valid Monthhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mOAcUVFqIO0.mp4 Hint: You may want to look into the string.capitalize() function In this answer video, Steve is using list comprehensions to create the dictionary of month abbreviations. If we were to translate this code: month_abbvs = dict((m[:3].lower(),m) for m in months) it would be: month_abbvs = {} for m in months: month_abbvs[m[:3].lower()] = m You can read more about list comprehensions here # ----------- # User Instructions # # Modify the valid_month() function to verify # whether the data a user enters is a valid # month. If the passed in parameter &apos;month&apos; # is not a valid month, return None. # If &apos;month&apos; is a valid month, then return # the name of the month with the first letter # capitalized. # months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] def valid_month(month): # print valid_month(&quot;january&quot;) # =&gt; &quot;January&quot; # print valid_month(&quot;January&quot;) # =&gt; &quot;January&quot; # print valid_month(&quot;foo&quot;) # =&gt; None # print valid_month(&quot;&quot;) # =&gt; None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/a2sLiEgBl9k.mp4 month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) Valid Dayhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jj44KNeNK3A.mp4 # ----------- # User Instructions # # Modify the valid_day() function to verify # whether the string a user enters is a valid # day. The valid_day() function takes as # input a String, and returns either a valid # Int or None. If the passed in String is # not a valid day, return None. # If it is a valid day, then return # the day as an Int, not a String. Don&apos;t # worry about months of different length. # Assume a day is valid if it is a number # between 1 and 31. # Be careful, the input can be any string # at all, you don&apos;t have any guarantees # that the user will input a sensible # day. # # Hint: The string function isdigit() might be helpful. def valid_day(day): # print valid_day(&apos;0&apos;) # =&gt; None # print valid_day(&apos;1&apos;) # =&gt; 1 # print valid_day(&apos;15&apos;) # =&gt; 15 # print valid_day(&apos;500&apos;) # =&gt; None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6CdFM4grsXc.mp4 def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day Valid Yearhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/g1f2nOw7a9M.mp4 # ----------- # User Instructions # # Modify the valid_year() function to verify # whether the string a user enters is a valid # year. If the passed in parameter &apos;year&apos; # is not a valid year, return None. # If &apos;year&apos; is a valid year, then return # the year as a number. Assume a year # is valid if it is a number between 1900 and # 2020. # def valid_year(year): #print valid_year(&apos;0&apos;) #=&gt; None #print valid_year(&apos;-11&apos;) #=&gt; None #print valid_year(&apos;1950&apos;) #=&gt; 1950 #print valid_year(&apos;2000&apos;) #=&gt; 2000 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YHEqWp6R0IA.mp4 def valid_year(year): if year and year.isdigit(): year=int(year) if year&gt;1900 and year&lt;=2020: return year Checking Validationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WbBDizcryiA.mp4 Responding Based On Verificationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3EgN3NxgH-Y.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot;&gt; &lt;/label&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; self.response.out.write(form) def post(self): user_month=valid_month(self.request.get(&apos;month&apos;)) user_day=valid_day(self.request.get(&apos;day&apos;)) user_year=valid_year(self.request.get(&apos;year&apos;)) if not (user_month and user_day and user_year): #self.response.out.write(user_month) self.response.out.write(form) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) String Substitutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/DYncK4Tuthc.mp4 # User Instructions # # Write a function &apos;sub1&apos; that, given a # string, embeds that string in # the string: # &quot;I think X is a perfectly normal thing to do in public.&quot; # where X is replaced by the given # string. # The function should return the new string. given_string = &quot;I think %s is a perfectly normal thing to do in public.&quot; def sub1(s): #print sub1(&quot;running&quot;) # =&gt; &quot;I think running is a perfectly normal thing to do in public.&quot; #print sub1(&quot;sleeping&quot;) # =&gt; &quot;I think sleeping is a perfectly normal thing to do in public.&quot; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0h0RYI_MGtU.mp4 def sub1(s): return given_string % s Substituting Multiple Stringshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ahNxN_Ly5sE.mp4 # User Instructions # # Write a function &apos;sub2&apos; that, given two # strings, embeds those strings in the string: # &quot;I think X and Y are perfectly normal things to do in public.&quot; # where X and Y are replaced by the given # strings. # The function should return the new string. given_string2 = &quot;I think %s and %s are perfectly normal things to do in public.&quot; def sub2(s1, s2): # print sub2(&quot;running&quot;, &quot;sleeping&quot;) # =&gt; &quot;I think running and sleeping are perfectly normal things to do in public.&quot; # print sub2(&quot;sleeping&quot;, &quot;running&quot;) # =&gt; &quot;I think sleeping and running are perfectly normal things to do in public.&quot; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K7mx3o3-X3Y.mp4 def sub2(s1, s2): return given_string2 %(s1,s2) Advanced String Substitutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4OXzeuhwM-E.mp4 # User Instructions # # Write a function &apos;sub_m&apos; that takes a # name and a nickname, and returns a # string of the following format: # &quot;I&apos;m NICKNAME. My real name is NAME, but my friends call me NICKNAME.&quot; # given_string2 = &quot;I&apos;m %(nickname)s. My real name is %(name)s, but my friends call me %(nickname)s.&quot; def sub_m(name, nickname): #print sub_m(&quot;Mike&quot;, &quot;Goose&quot;) # =&gt; &quot;I&apos;m Goose. My real name is Mike, but my friends call me Goose.&quot; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3iHZY-xJJUY.mp4 def sub_m(name, nickname): return given_string2 % {&quot;nickname&quot;:nickname,&quot;name&quot;:name} Substituting into Our Formhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/trxe2U-OplI.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error}) def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; #self.response.out.write(form) self.write_form() def post(self): user_month=valid_month(self.request.get(&apos;month&apos;)) user_day=valid_day(self.request.get(&apos;day&apos;)) user_year=valid_year(self.request.get(&apos;year&apos;)) if not (user_month and user_day and user_year): #self.response.out.write(user_month) #self.response.out.write(form) self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) Preserving User Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BpNhr-xc0Nw.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lwHWqkMijss.mp4 Problems with HTML Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/iWX-XylQsH8.mp4 import webapp2 form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot; value=&quot;%(month)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot; value=&quot;%(day)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot; value=&quot;%(year)s&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;, month=&quot;&quot;, day=&quot;&quot;, year=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error, &quot;month&quot;: month, &quot;day&quot;: day, &quot;year&quot;: year}) def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; #self.response.out.write(form) self.write_form() def post(self): user_month = self.request.get(&apos;month&apos;) user_day = self.request.get(&apos;day&apos;) user_year = self.request.get(&apos;year&apos;) month = valid_month(user_month) day = valid_day(user_day) year = valid_year(user_year) if not (month and day and year): #self.response.out.write(user_month) #self.response.out.write(form) self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;, user_month, user_day, user_year) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OpOOJcP-fY0.mp4 Handling HTML Inputhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K37Ldm4GSPo.mp4 HTML Escapinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/teNvqTRP5EY.mp4 Using HTML Escapinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Hp-z-SVf3fw.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2T-ubUA5xLk.mp4 Implementing HTML Escapinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NmCRWl-OZ60.mp4 # User Instructions # # Implement the function escape_html(s), which replaces # all instances of: # &gt; with &amp;gt; # &lt; with &amp;lt; # &quot; with &amp;quot; # &amp; with &amp;amp; # and returns the escaped string # Note that your browser will probably automatically # render your escaped text as the corresponding symbols, # but the grading script will still correctly evaluate it. # def escape_html(s): # print escape_html(&apos;&gt;&apos;) # print escape_html(&apos;&lt;&apos;) # print escape_html(&apos;&quot;&apos;) # print escape_html(&quot;&amp;&quot;) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4NKBjlnV530.mp4 def escape_html(s): for (i,o) in ((&quot;&amp;&quot;,&quot;&amp;amp;&quot;),(&quot;&gt;&quot;,&quot;&amp;gl;&quot;),(&quot;&lt;&quot;,&quot;&amp;lt;&quot;),(&apos;&quot;&apos;,&quot;&amp;quot;&quot;)): s=s.replace(i,o) return s or import cgi def escape_html(s): return cgi.escape(s, quote = True) Problems Reinventing the Wheelhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4YEcl5g3ADA.mp4 Current Limitationshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_lRKZMPh5R8.mp4 import webapp2 import cgi def escape_html(s): return cgi.escape(s, quote = True) form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot; value=&quot;%(month)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot; value=&quot;%(day)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot; value=&quot;%(year)s&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;, month=&quot;&quot;, day=&quot;&quot;, year=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error, &quot;month&quot;: escape_html(month), &quot;day&quot;: escape_html(day), &quot;year&quot;: escape_html(year)}) def get(self): #self.response.headers[&apos;Content-Type&apos;] = &apos;text/plain&apos; #self.response.out.write(form) self.write_form() def post(self): user_month = self.request.get(&apos;month&apos;) user_day = self.request.get(&apos;day&apos;) user_year = self.request.get(&apos;year&apos;) month = valid_month(user_month) day = valid_day(user_day) year = valid_year(user_year) if not (month and day and year): self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;, user_month, user_day, user_year) else: self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage) ], debug=True) Redirectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gjPdQ-ywbPM.mp4 Redirection Advantageshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/k84dleLQ-WI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eygchvjmP60.mp4 Implementing Redirectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/46HPScqA0CA.mp4 import webapp2 import cgi def escape_html(s): return cgi.escape(s, quote = True) form=&quot;&quot;&quot; &lt;form method=&quot;post&quot;&gt; What&apos;s your birthday? &lt;br&gt; &lt;label&gt; Month &lt;input type=&quot;text&quot; name=&quot;month&quot; value=&quot;%(month)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Day &lt;input type=&quot;text&quot; name=&quot;day&quot; value=&quot;%(day)s&quot;&gt; &lt;/label&gt; &lt;label&gt; Year &lt;input type=&quot;text&quot; name=&quot;year&quot; value=&quot;%(year)s&quot;&gt; &lt;/label&gt; &lt;div style=&quot;color: red&quot;&gt;%(error)s&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;input type=&quot;submit&quot;&gt; &lt;/form&gt; &quot;&quot;&quot; def valid_day(day): if day and day.isdigit(): day=int(day) if day&gt;0 and day&lt;=31: return day months = [&apos;January&apos;, &apos;February&apos;, &apos;March&apos;, &apos;April&apos;, &apos;May&apos;, &apos;June&apos;, &apos;July&apos;, &apos;August&apos;, &apos;September&apos;, &apos;October&apos;, &apos;November&apos;, &apos;December&apos;] month_abbvs = dict((m[:3].lower(),m) for m in months) def valid_month(month): if month: short_month=month[:3].lower() return month_abbvs.get(short_month) def valid_year(year): if(year and year.isdigit()): year = int(year) if(year &lt; 2020 and year &gt; 1880): return year class MainPage(webapp2.RequestHandler): def write_form(self, error=&quot;&quot;, month=&quot;&quot;, day=&quot;&quot;, year=&quot;&quot;): self.response.out.write(form %{&quot;error&quot;: error, &quot;month&quot;: escape_html(month), &quot;day&quot;: escape_html(day), &quot;year&quot;: escape_html(year)}) def get(self): self.write_form() def post(self): user_month = self.request.get(&apos;month&apos;) user_day = self.request.get(&apos;day&apos;) user_year = self.request.get(&apos;year&apos;) month = valid_month(user_month) day = valid_day(user_day) year = valid_year(user_year) if not (month and day and year): self.write_form(&quot;That doesn&apos;t look valid to me, friend.&quot;, user_month, user_day, user_year) else: self.redirect(&quot;/thanks&quot;) class ThanksHandler(webapp2.RequestHandler): def get(self): self.response.out.write(&quot;Thanks! That&apos;s a totally valid day!&quot;) app = webapp2.WSGIApplication([ (&apos;/&apos;, MainPage), (&apos;/thanks&apos;, ThanksHandler) ], debug=True) Problem Set 2Rot13https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uMGNwoFHfB4.mp4In order to be graded correctly for this homework, there are a few things to keep in mind. We’ll be grading your web app by POSTing to your form and retrieving the text that has been encoded with ROT13. There are a few main issues you need to keep in mind in order for this to work: The textarea form element where the user inputs the text to encode must be named ‘text’. In other words, you must have ‘textarea name=”text”‘ for us to post to. The form method must be POST, not GET. You must enter the full url into the supplied textbox above, including the path. For example, our example app is running at http://udacity-cs253.appspot.com/unit2/rot13, but if we instead only entered http://udacity-cs253.appspot.com/ then the grading script would not work. Don’t forget to escape your output! User Signuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SkSJp6IIZfw.mp4In order to be graded correctly for this homework, there are a few things to keep in mind. We’ll be grading your web app by posting to your form and then checking the HTTP Response we receive. There are a few main issues you need to keep in mind in order for this to work: The form elements where the user inputs their username, password, password again, and email address must be named “username”, “password”, “verify”, and “email”, respectively. The form method must be POST, not GET. Upon invalid user input, your web app should re-render the form for the user. Upon valid user input, your web app should redirect to a welcome page for the user. This page must include both “Welcome” and the user’s username. You must enter the full url into the supplied textbox above, including the path. For example, our example app is running at http://udacity-cs253.appspot.com/unit2/signup, but if we instead only entered http://udacity-cs253.appspot.com/ then the grading script would not work. Regular ExpressionsA regular expression is a handy tool for matching text to a pattern. The regular expressions that we’re using to validate you input are as follows: Username: “^[a-zA-Z0-9_-]{3,20}$” Password: “^.{3,20}$” Email: “^[\S]+@[\S]+.[\S]+$” Example code for validating a username is as follows: import re USER_RE = re.compile(r&quot;^[a-zA-Z0-9_-]{3,20}$&quot;) def valid_username(username): return USER_RE.match(username) More information on using regular expressions in Python can be found here NOTE: When you go off to make real applications that require form validation, remember that using regex to check an email address is not quite as simple as we make it seem here. See this Stack Overflow question for more on email validation. Rot13 Solutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NYzMnbSjOWA.mp4The solution files are here. In order to open them, Windows users will need an archive utility that opens tgz files, like 7zip. Linux and Mac users can use the built-in functionality of their archive managers. Barring that, Linux and Mac users can directly use the command: tar xf hw2.tgz User Signup SolutionThe solution files are here. In order to open them, Windows users will need an archive utility that opens tgz files, like 7zip. Linux and Mac users can use the built-in functionality of their archive managers. Barring that, Linux and Mac users can directly use the command: tar xf hw2.tgz Office Hours 2Question 1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0uBFVZRVnrQ.mp4Jinja2 Documentation Question 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5-UWdqZQOIw.mp4 Question 3https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YY7joMbML8k.mp4 Question 4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9vTJkEjXw6Q.mp4We now have a course for beginner Object Oriented Programming in Python: https://www.udacity.com/course/ud036 Question 5https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rLPeR3A3CnM.mp4 Lesson 8: Lesson 2a-TemplatesWriting a Basic Formhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ScXJ5au8q_w.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Bnh2EnFQdkw.mp4 Hidden Inputshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dfFiYpxh4js.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EHACuozhDQA.mp4http://localhost:8080/?food=steak&amp;food=eggs Shopping List Take 1https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h_EkzhX0D0I.mp412&quot;hello %s&quot; % &quot;SSQ&quot;&apos;hello SSQ&apos; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/87ieO3hrkDI.mp4 Introducing Templateshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nT0wOJ3pQMw.mp4jinja Template Refactorhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9HoEid5G4I4.mp4 Variable Substitutionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/D6q5TePkyo0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ppsNOLB7p4M.mp4 ErrataDon’t include quotation marks in your solution. For additional clarification on the different render methods, see this discussion thread.&lt;h2&gt;Hello, &lt;/h2&gt; Statement Syntaxhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/13P8ncVwL_o.mp412345&#123;% if name==&quot;steve&quot; %&#125; hello, steve!&#123;% else %&#125; who are you?&#123;% endif %&#125; Testing Statement Syntaxhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zI4rmjjICmU.mp4 Templates and Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4VYTtyHnKEs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/d-PAPeH0bd0.mp412345&#123;% if n==1 %&#125;n=1&#123;% else %&#125;n!=1&#123;% endif %&#125; 12345def get(self): n=self.request.get(&quot;n&quot;) if n: n=int(n) self.render(&quot;shopping_list.html&quot;, n=n) Templates and Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4VYTtyHnKEs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/d-PAPeH0bd0.mp412345&#123;% if n==1 %&#125;n=1&#123;% else %&#125;n!=1&#123;% endif %&#125; 12345def get(self): n=self.request.get(&quot;n&quot;) if n: n=int(n) self.render(&quot;shopping_list.html&quot;, n=n) For Loop Syntaxhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KzZwbKZweKA.mp4123&#123;% for statement %&#125; body&#123;% endfor %&#125; 12345&lt;ol&gt; &#123;% for x in range(1,n+1) %&#125; &lt;li&gt;&#123;&#123;x ** 2&#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ol&gt; FizzBuzzhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nxoQoSYJryc.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KhrAMbp2MRA.mp4123n=self.request.get(&quot;n&quot;)if n: n=int(n) 123456789101112131415&lt;ol&gt; &#123;% for i in range(1,n+1) %&#125; &lt;li&gt; &#123;% if i % 3 ==0 and i % 5 ==0 %&#125; fizzbuzz &#123;% elif i % 3 ==0 %&#125; fizz &#123;% elif i % 5 ==0 %&#125; buzz &#123;% else %&#125; &#123;&#123;i&#125;&#125; &#123;% endif %&#125; &lt;/li&gt; &#123;% endfor %&#125;&lt;/ol&gt; Shopping List Take 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BkqW2HKdL0M.mp4get rid of12345678910111213141516output = form_htmloutput_hidden=&quot;&quot;items=self.request.get_all(&quot;food&quot;)if items: output_items=&quot;&quot; for item in items: output_hidden+=hidden_html % item output_items+=item_html % item output_shopping = shopping_list_html % output_items output += output_shoppingoutput=output % output_hiddenself.write(output) and1234567891011121314hidden_html=&quot;&quot;&quot;&lt;input type=&quot;hidden&quot; name=&quot;food&quot; value=&quot;%s&quot;&gt;&quot;&quot;&quot;item_html=&quot;&lt;li&gt;%s&lt;/li&gt;&quot;shopping_list_html=&quot;&quot;&quot;&lt;br&gt;&lt;br&gt;&lt;h2&gt;Shopping List&lt;/h2&gt;&lt;ul&gt;%s&lt;/ul&gt;&quot;&quot;&quot; change following code in your shopping_list.html12345678910111213141516171819202122&lt;form&gt; &lt;h2&gt;Add a food&lt;/h2&gt; &lt;input type=&quot;text&quot; name=&quot;food&quot;&gt; &#123;% if items %&#125; &#123;% for item in items %&#125; &lt;input type=&quot;hidden&quot; name=&quot;food&quot; value=&quot;&#123;&#123;item&#125;&#125;&quot;&gt; &#123;% endfor %&#125; &#123;% endif %&#125; &lt;button&gt;Add&lt;/button&gt; &#123;% if items %&#125; &lt;br&gt; &lt;br&gt; &lt;h2&gt;Shopping List&lt;/h2&gt; &lt;ul&gt; &#123;% for item in items %&#125; &lt;li&gt;&#123;&#123;item&#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &#123;% endif %&#125;&lt;/form&gt; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9DaCBZImS54.mp4 self.request.get_all()self.request.get_all() is a function that returns a list of all values that belong to string that matches a key in our query parameter. For example, if we pass in these query parameters: mysite.com?food=chips&amp;food=fruit&amp;food=milk, then get_all() will return a list that contains the strings chips,food,milk Jinja2 TemplatesNote that the bracket and the percent sign needs to be next to each other such as:1&#123;% keyword %&#125; AND NOT1&#123; % keyword % &#125; Escaping Templateshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oQJCuxvH_VM.mp4(recommand)first way12jinja_env = jinja2.Environment(loader = jinja2.FileSystemLoader(template_dir), autoescape=True) except:&lt;li&gt;&lt;/li&gt;second way:&lt;li&gt;&lt;/li&gt; Helpful Tipshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4eXgr5beSkU.mp4 Template Inheritancehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/v81yoy1L1Bo.mp4 FizzBuzz Inheritancehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ls9qRh8LVts.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nmEgJJYhW-I.mp4 Conclusionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/O60XPmSy1OY.mp4 Lesson 9: Lesson 3-Databasesnote:https://www.udacity.com/wiki/CS253%20Unit%203 Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EK0lMxBKRRw.mp4 Databases Continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Y1av5nYgQKM.mp4 What Is a Database?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7yEjSZvZOAo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ngXxI8Q0WTg.mp4 Tableshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7ldYR0Uis0I.mp4 Implementing Tables in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/j-KIhN7LKJ8.mp41234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192from collections import namedtuple# make a basic Link classLink = namedtuple(&apos;Link&apos;, [&apos;id&apos;, &apos;submitter_id&apos;, &apos;submitted_time&apos;, &apos;votes&apos;, &apos;title&apos;, &apos;url&apos;])# list of Links to work withlinks = [ Link(0, 60398, 1334014208.0, 109, &quot;C overtakes Java as the No. 1 programming language in the TIOBE index.&quot;, &quot;http://pixelstech.net/article/index.php?id=1333969280&quot;), Link(1, 60254, 1333962645.0, 891, &quot;This explains why technical books are all ridiculously thick and overpriced&quot;, &quot;http://prog21.dadgum.com/65.html&quot;), Link(23, 62945, 1333894106.0, 351, &quot;Learn Haskell Fast and Hard&quot;, &quot;http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/&quot;), Link(2, 6084, 1333996166.0, 81, &quot;Announcing Yesod 1.0- a robust, developer friendly, high performance web framework for Haskell&quot;, &quot;http://www.yesodweb.com/blog/2012/04/announcing-yesod-1-0&quot;), Link(3, 30305, 1333968061.0, 270, &quot;TIL about the Lisp Curse&quot;, &quot;http://www.winestockwebdesign.com/Essays/Lisp_Curse.html&quot;), Link(4, 59008, 1334016506.0, 19, &quot;The Downfall of Imperative Programming. Functional Programming and the Multicore Revolution&quot;, &quot;http://fpcomplete.com/the-downfall-of-imperative-programming/&quot;), Link(5, 8712, 1333993676.0, 26, &quot;Open Source - Twitter Stock Market Game - &quot;, &quot;http://www.twitstreet.com/&quot;), Link(6, 48626, 1333975127.0, 63, &quot;First look: Qt 5 makes JavaScript a first-class citizen for app development&quot;, &quot;http://arstechnica.com/business/news/2012/04/an-in-depth-look-at-qt-5-making-javascript-a-first-class-citizen-for-native-cross-platform-developme.ars&quot;), Link(7, 30172, 1334017294.0, 5, &quot;Benchmark of Dictionary Structures&quot;, &quot;http://lh3lh3.users.sourceforge.net/udb.shtml&quot;), Link(8, 678, 1334014446.0, 7, &quot;If It&apos;s Not on Prod, It Doesn&apos;t Count: The Value of Frequent Releases&quot;, &quot;http://bits.shutterstock.com/?p=165&quot;), Link(9, 29168, 1334006443.0, 18, &quot;Language proposal: dave&quot;, &quot;http://davelang.github.com/&quot;), Link(17, 48626, 1334020271.0, 1, &quot;LispNYC and EmacsNYC meetup Tuesday Night: Large Scale Development with Elisp &quot;, &quot;http://www.meetup.com/LispNYC/events/47373722/&quot;), Link(101, 62443, 1334018620.0, 4, &quot;research!rsc: Zip Files All The Way Down&quot;, &quot;http://research.swtch.com/zip&quot;), Link(12, 10262, 1334018169.0, 5, &quot;The Tyranny of the Diff&quot;, &quot;http://michaelfeathers.typepad.com/michael_feathers_blog/2012/04/the-tyranny-of-the-diff.html&quot;), Link(13, 20831, 1333996529.0, 14, &quot;Understanding NIO.2 File Channels in Java 7&quot;, &quot;http://java.dzone.com/articles/understanding-nio2-file&quot;), Link(15, 62443, 1333900877.0, 1244, &quot;Why vector icons don&apos;t work&quot;, &quot;http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html&quot;), Link(14, 30650, 1334013659.0, 3, &quot;Python - Getting Data Into Graphite - Code Examples&quot;, &quot;http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html&quot;), Link(16, 15330, 1333985877.0, 9, &quot;Mozilla: The Web as the Platform and The Kilimanjaro Event&quot;, &quot;https://groups.google.com/forum/?fromgroups#!topic/mozilla.dev.planning/Y9v46wFeejA&quot;), Link(18, 62443, 1333939389.0, 104, &quot;github is making me feel stupid(er)&quot;, &quot;http://www.serpentine.com/blog/2012/04/08/github-is-making-me-feel-stupider/&quot;), Link(19, 6937, 1333949857.0, 39, &quot;BitC Retrospective: The Issues with Type Classes&quot;, &quot;http://www.bitc-lang.org/pipermail/bitc-dev/2012-April/003315.html&quot;), Link(20, 51067, 1333974585.0, 14, &quot;Object Oriented C: Class-like Structures&quot;, &quot;http://cecilsunkure.blogspot.com/2012/04/object-oriented-c-class-like-structures.html&quot;), Link(10, 23944, 1333943632.0, 188, &quot;The LOVE game framework version 0.8.0 has been released - with GLSL shader support!&quot;, &quot;https://love2d.org/forums/viewtopic.php?f=3&amp;amp;t=8750&quot;), Link(22, 39191, 1334005674.0, 11, &quot;An open letter to language designers: Please kill your sacred cows. (megarant)&quot;, &quot;http://joshondesign.com/2012/03/09/open-letter-language-designers&quot;), Link(21, 3777, 1333996565.0, 2, &quot;Developers guide to Garage48 hackatron&quot;, &quot;http://martingryner.com/developers-guide-to-garage48-hackatron/&quot;), Link(24, 48626, 1333934004.0, 17, &quot;An R programmer looks at Julia&quot;, &quot;http://www.r-bloggers.com/an-r-programmer-looks-at-julia/&quot;)]# links is a list of Link objects. Links have a handful of properties. For# example, a Link&apos;s number of votes can be accessed by link.votes if &quot;link&quot; is a# Link.# make the function query() return the number of votes for the link whose ID is# 15def query(): Here is more information on namedtuple used in this exercise.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0jE0n6deDpk.mp4 Queryinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2TFA879U_pc.mp4123456789def query(): submision=[] for l in links: if l.submitter_id==62443: submision.append(l) submision.sort(key=lambda x: x.submitted_time) return submision print query() https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aXwzyv11_4Q.mp4Here is more information on sorting and Lambda expressions in Python . Why Databases?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bGvBewhooJc.mp4 Types of Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/MORw_tCy42A.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Q3tsjEH9W-c.mp4 SQLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vgP5bfu0na4.mp4 Databases in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oleOUdXSfs0.mp4123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121from collections import namedtupleimport sqlite3# make a basic Link classLink = namedtuple(&apos;Link&apos;, [&apos;id&apos;, &apos;submitter_id&apos;, &apos;submitted_time&apos;, &apos;votes&apos;, &apos;title&apos;, &apos;url&apos;])# list of Links to work withlinks = [ Link(0, 60398, 1334014208.0, 109, &quot;C overtakes Java as the No. 1 programming language in the TIOBE index.&quot;, &quot;http://pixelstech.net/article/index.php?id=1333969280&quot;), Link(1, 60254, 1333962645.0, 891, &quot;This explains why technical books are all ridiculously thick and overpriced&quot;, &quot;http://prog21.dadgum.com/65.html&quot;), Link(23, 62945, 1333894106.0, 351, &quot;Learn Haskell Fast and Hard&quot;, &quot;http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/&quot;), Link(2, 6084, 1333996166.0, 81, &quot;Announcing Yesod 1.0- a robust, developer friendly, high performance web framework for Haskell&quot;, &quot;http://www.yesodweb.com/blog/2012/04/announcing-yesod-1-0&quot;), Link(3, 30305, 1333968061.0, 270, &quot;TIL about the Lisp Curse&quot;, &quot;http://www.winestockwebdesign.com/Essays/Lisp_Curse.html&quot;), Link(4, 59008, 1334016506.0, 19, &quot;The Downfall of Imperative Programming. Functional Programming and the Multicore Revolution&quot;, &quot;http://fpcomplete.com/the-downfall-of-imperative-programming/&quot;), Link(5, 8712, 1333993676.0, 26, &quot;Open Source - Twitter Stock Market Game - &quot;, &quot;http://www.twitstreet.com/&quot;), Link(6, 48626, 1333975127.0, 63, &quot;First look: Qt 5 makes JavaScript a first-class citizen for app development&quot;, &quot;http://arstechnica.com/business/news/2012/04/an-in-depth-look-at-qt-5-making-javascript-a-first-class-citizen-for-native-cross-platform-developme.ars&quot;), Link(7, 30172, 1334017294.0, 5, &quot;Benchmark of Dictionary Structures&quot;, &quot;http://lh3lh3.users.sourceforge.net/udb.shtml&quot;), Link(8, 678, 1334014446.0, 7, &quot;If It&apos;s Not on Prod, It Doesn&apos;t Count: The Value of Frequent Releases&quot;, &quot;http://bits.shutterstock.com/?p=165&quot;), Link(9, 29168, 1334006443.0, 18, &quot;Language proposal: dave&quot;, &quot;http://davelang.github.com/&quot;), Link(17, 48626, 1334020271.0, 1, &quot;LispNYC and EmacsNYC meetup Tuesday Night: Large Scale Development with Elisp &quot;, &quot;http://www.meetup.com/LispNYC/events/47373722/&quot;), Link(101, 62443, 1334018620.0, 4, &quot;research!rsc: Zip Files All The Way Down&quot;, &quot;http://research.swtch.com/zip&quot;), Link(12, 10262, 1334018169.0, 5, &quot;The Tyranny of the Diff&quot;, &quot;http://michaelfeathers.typepad.com/michael_feathers_blog/2012/04/the-tyranny-of-the-diff.html&quot;), Link(13, 20831, 1333996529.0, 14, &quot;Understanding NIO.2 File Channels in Java 7&quot;, &quot;http://java.dzone.com/articles/understanding-nio2-file&quot;), Link(15, 62443, 1333900877.0, 1244, &quot;Why vector icons don&apos;t work&quot;, &quot;http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html&quot;), Link(14, 30650, 1334013659.0, 3, &quot;Python - Getting Data Into Graphite - Code Examples&quot;, &quot;http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html&quot;), Link(16, 15330, 1333985877.0, 9, &quot;Mozilla: The Web as the Platform and The Kilimanjaro Event&quot;, &quot;https://groups.google.com/forum/?fromgroups#!topic/mozilla.dev.planning/Y9v46wFeejA&quot;), Link(18, 62443, 1333939389.0, 104, &quot;github is making me feel stupid(er)&quot;, &quot;http://www.serpentine.com/blog/2012/04/08/github-is-making-me-feel-stupider/&quot;), Link(19, 6937, 1333949857.0, 39, &quot;BitC Retrospective: The Issues with Type Classes&quot;, &quot;http://www.bitc-lang.org/pipermail/bitc-dev/2012-April/003315.html&quot;), Link(20, 51067, 1333974585.0, 14, &quot;Object Oriented C: Class-like Structures&quot;, &quot;http://cecilsunkure.blogspot.com/2012/04/object-oriented-c-class-like-structures.html&quot;), Link(10, 23944, 1333943632.0, 188, &quot;The LOVE game framework version 0.8.0 has been released - with GLSL shader support!&quot;, &quot;https://love2d.org/forums/viewtopic.php?f=3&amp;amp;t=8750&quot;), Link(22, 39191, 1334005674.0, 11, &quot;An open letter to language designers: Please kill your sacred cows. (megarant)&quot;, &quot;http://joshondesign.com/2012/03/09/open-letter-language-designers&quot;), Link(21, 3777, 1333996565.0, 2, &quot;Developers guide to Garage48 hackatron&quot;, &quot;http://martingryner.com/developers-guide-to-garage48-hackatron/&quot;), Link(24, 48626, 1333934004.0, 17, &quot;An R programmer looks at Julia&quot;, &quot;http://www.r-bloggers.com/an-r-programmer-looks-at-julia/&quot;)]# links is a list of Link objects. Links have a handful of properties. For# example, a Link&apos;s number of votes can be accessed by link.votes if &quot;link&quot; is a# Link.# make and populate a tabledb = sqlite3.connect(&apos;:memory:&apos;)db.execute(&apos;create table links &apos; + &apos;(id integer, submitter_id integer, submitted_time integer, &apos; + &apos;votes integer, title text, url text)&apos;)for l in links: db.execute(&apos;insert into links values (?, ?, ?, ?, ?, ?)&apos;, l)# db is an in-memory sqlite database that can respond to sql queries using the# execute() function.## For example. If you run## c = db.execute(&quot;select * from links&quot;)## c will be a &quot;cursor&quot; to the results of that query. You can use the fetchmany()# function on the cursor to convert that cursor into a list of results. These# results won&apos;t be Links; they&apos;ll be tuples, but they can be passed turned into# a Link.## For example, to print all the votes for all of the links, do this:## c = db.execute(&quot;select * from links&quot;)# for link_tuple in c:# link = Link(*link_tuple)# print link.votes## QUIZ - make the function query() return the number of votes the link with ID = 2 hasdef query(): c = db.execute(&quot;PUT YOUR SQL HERE&quot;) link = Link(*c.fetchone()) return link.votes https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/F35zP0xOMFM.mp4123456def query(): c = db.execute(&quot;select * from links where id=2&quot;) link = Link(*c.fetchone()) return link.votesprint query() More Advanced SQLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aWIoeUm-HVA.mp4 Advanced SQL in Pythonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/u2nQqxMfLE4.mp412345def query(): c = db.execute(&quot;select * from links where submitter_id=62443 AND votes &gt; 1000&quot;) link = Link(*c.fetchone()) return link.idprint query() or123456def query(): c = db.execute(&quot;select * from links where submitter_id=62443 AND votes &gt; 1000&quot;) for link_tuple in c: link = Link(*link_tuple) return link.idprint query() https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KsGqEVJsmO4.mp4 Order Byhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L55IfVfNvL8.mp4You can find out more information about list comprehensions in the Python documentation and a supplementary lesson found here123456789def query(): results=[] c=db.execute(&quot;select * from links where submitter_id=62443 ORDER BY submitted_time ASC&quot;) for link_tuple in c: link = Link(*link_tuple) print link results.append(link.id) return resultsprint query() better12345def query(): c=db.execute(&quot;select id from links where submitter_id=62443 order by submitted_time asc&quot;) results=[t[0] for t in c] return resultsprint query() https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kWvI1pvxYZ4.mp4 Joinshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KddpLKB5JYA.mp4 Indexeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kgC5ZwM9BS8.mp4 Querying Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dLAZvKNkZcs.mp412345678# QUIZ - implement the function link_by_id() that takes a link&apos;s ID and returns# the Link object itselfdef link_by_id(link_id): for l in links: if l.id==link_id: return lprint link_by_id(24) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ASrkP2-oBuM.mp4 Using Dictionaries As Indiceshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4c3leET2nzg.mp41234567# QUIZ - implement the function build_link_index() that creates a python dictionary# the maps a link&apos;s ID to the link itselfdef build_link_index(): result=&#123;&#125; for l in links: result[l.id]=l return result https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/snkHGcSRRsk.mp4 Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pq5cH9fx524.mp41234567891011121314def build_link_index(): index = &#123;&#125; for l in links: index[l.id] = l return indexlink_index = build_link_index()def link_by_id(link_id): return link_index.get(link_id)# QUIZ - implement the function add_new_link() that both adds a link to the # &quot;links&quot; list and updates the link_index dictionary. def add_new_link(link): https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/v-nktgcT2A4.mp412345678def add_new_link(link): links.append(link) link_index[link.id]=link l=Link(50,1,1,1,&apos;title&apos;,&apos;url&apos;)add_new_link(l)print links[-1]print link_by_id(50) output12Link(id=50, submitter_id=1, submitted_time=1, votes=1, title=&apos;title&apos;, url=&apos;url&apos;)Link(id=50, submitter_id=1, submitted_time=1, votes=1, title=&apos;title&apos;, url=&apos;url&apos;) Advantages of Indiceshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9CoBxepbSR4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/da2uj4wK10Y.mp4 Real-World Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/31XAKJmp0sk.mp4 Indices for Sortinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/REgL9DoFMdU.mp4 Another Real-World Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XkuT8x6Y94A.mp4 Scaling Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dkhOZOmV7Fo.mp4 note:method | replicate | shard— | — | —downside: | 1. doesn’t increase the write speed. 2. replication lag | 1. complex queries(range queries). 2. joins become difficult https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NTpHCHAD2tM.mp4 Growing Databaseshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cFFM_GCvrYs.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/R9T3hQ1axaw.mp4 ACIDhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/j-r6Fmzr4PM.mp4note:atumicityconsistencyisolationdurability Google App Engine Datastorehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/r-wl-VZhNXo.mp4 GQLhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SnaxUDryqaU.mp4note:GQL: all queries begin with SELECT * no joins all queries must be indexedSQL:run arbitrary queriesAutomatic Sharding and Replicationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i4Fgm_XQ10M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TmPtKXgjZwQ.mp4ASCII Chanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dtlAAUtkvgQ.mp4Getting Started on ASCII Chanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hiyj6fDFm3c.mp4Creating the Formhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yx4SvoTbiuI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gHsG6aYIaXo.mp4Textareahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/u5iR0i5KOMY.mp4Form Handlinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/odSexP9bScI.mp4Form Handling Continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/iB5cwOZK0JU.mp4Creating Entitieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dFX5Dp-v04s.mp4Datastore Typeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_opjCcw2Al0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/y4o26tQcAqc.mp4Creating Entities Continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gmgKRL1wiCk.mp4Working with Entitieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JUDj8fXp3eY.mp4Running Querieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h7LyH4cvYJo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cbpxkj6DGIg.mp4Stylinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1pvAum4H7-Y.mp4Lesson 10: Problem Set 3-Building a Basic BlogBasic Bloghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ysFzedoB3Rg.mp4]]></content>
      <categories>
        <category>Udacity</category>
        <category>How to Build a Blog</category>
      </categories>
      <tags>
        <tag>Udacity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 notebook]]></title>
    <url>%2F2017%2F04%2F14%2FCS229%2F</url>
    <content type="text"><![CDATA[update in 2017-04-14stay tuned… ML StanfordCS 229 Machine Learning Course MaterialsCourses Supervised learning. (7 classes) Supervised learning setup. LMS. Logistic regression. Perceptron. Exponential family. Generative learning algorithms. Gaussian discriminant analysis. Naive Bayes. Support vector machines. Model selection and feature selection. Ensemble methods: Bagging, boosting. Evaluating and debugging learning algorithms. Learning theory. (3 classes) Bias/variance tradeoff. Union and Chernoff/Hoeffding bounds. VC dimension. Worst case (online) learning. Practical advice on how to use learning algorithms. Unsupervised learning. (5 classes) Clustering. K-means. EM. Mixture of Gaussians. Factor analysis. PCA (Principal components analysis). ICA (Independent components analysis). Reinforcement learning and control. (4 classes) MDPs. Bellman equations. Value iteration and policy iteration. Linear quadratic regulation (LQR). LQG. Q-learning. Value function approximation. Policy search. Reinforce. POMDPs.]]></content>
      <tags>
        <tag>Stanford</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prepare for TOEFL]]></title>
    <url>%2F2017%2F04%2F13%2FPrepare%20for%20TOEFL%2F</url>
    <content type="text"><![CDATA[update in 2017-04-13stay tuned… cd g:hexohexo new post &quot;Prepare for TOEFL&quot; #TOEFL Strategies with English native-speaker MagooshTOEFLThe Format of the TOEFLHow to Study for the TOEFLTOEFL Study Plan (1 Month)TOEFL Listening Practice TestBest Free TOEFL ResourcesEstimating Your TOEFL ScoreTOEFL Study Plans and GuidesTwo Month TOEFL Study Plan edXTOEFL Test Preparation: The Insider’s Guide ETSTOEFL iBT® Quick Prep准备 TOEFL iBT® 考试Inside the TOEFL® Test: Speaking Questions 1&amp;2 A GUIDE TO UNDERSTANDING TOEFL IBT® SCORESSpeaking_RubricsWriting_Rubrics #清北托福备考联盟慕课——托福听力（超清） ETSx: TOEFLx TOEFL Test PreparationOverview of the CourseWelcome to TOEFL® Test Preparation: The Insider’s Guide. In this six-week course, we will explain the TOEFL test and take you through an in-depth look at all four sections of the TOEFL test: Reading, Listening, Speaking and Writing. Each week you will have the opportunity to learn more about the question types, scoring guidelines and resources to help you prepare for test day. We will give you test preparation tips and practice materials. You should expect to spend 2 to 4 hours per week to get the most out of this course. The course includes video lectures, discussion forums, weekly quizzes, practice tests and recommended readings. ETS instructors and staff will moderate the discussions and provide feedback where possible on a weekly basis. Please note: Passing this course is not a predictor of how well you might perform on the actual TOEFL iBT® test. Learning Objectives Gain a broad understanding of the four sections of the TOEFL test: Reading, - Listening, Speaking and Writing Acquire helpful tips to prepare you for the TOEFL test Improve your English language skills Learn how the TOEFL test is scored Learn how to use your TOEFL test scores for employment, school, visas, scholarships Find test prep resources for the TOEFL test Learn how to register for the TOEFL test Weekly TopicsWeek 1 – Welcome to TOEFL Test Preparation: The Insider’s Guide 1.1 Welcome 1.2 Survey 1.3 Course Information and Support 1.4 Around the World With the TOEFL Test 1.5 A Look Inside the TOEFL Test 1.6 Accommodations and Accessibility 1.7 Week 1 Quiz 1.8 This Week in Review and Getting Ready for Week 2 Week 2 – Reading Section 2.1 Challenges of Reading 2.2 About the Reading Section 2.3 Factual/Negative Factual Information Questions 2.4 Inference and Rhetorical Purpose Questions 2.5 Vocabulary Questions 2.6 Reference Questions 2.7 Sentence Simplification Questions 2.8 Insert Text Questions 2.9 Prose Summary and Fill in a Table Questions 2.10 How the Reading Section is Scored 2.11 Reading Practice Test 2.12 Practice Activities and Resources 2.13 This Week in Review and Getting Ready for Week 3 Week 3 – Listening Section 3.1 Challenges of Listening 3.2 About the Listening Section 3.3 Gist-Content and Gist-Purpose Questions 3.4 Detail Questions 3.5 Function Questions 3.6 Attitude Questions 3.7 Organization Questions 3.8 Connecting Content Questions 3.9 Inference Questions 3.10 How the Listening Section is Scored 3.11 Listening Practice Test 3.12 Practice Activities and Resources 3.13 This Week in Review and Getting Ready for Week 4 Week 4 – Speaking Section 4.1 Challenges of Speaking 4.2 About the Speaking Section 4.3 Independent Questions 1 and 2 4.4 Integrated Questions 3 and 5 4.5 Integrated Questions 4 and 6 4.6 How the Speaking Section is Scored 4.7 Speaking Practice Test 4.8 Review of Speaking Responses 4.9 Practice Activities and Resources 4.10 This Week in Review and Getting Ready for Week 5 Week 5 – Writing Section 5.1 Challenges of Writing 5.2 About the Writing Section 5.3 Integrated Writing Question 1 5.4 Independent Writing Question 2 5.5 How the Writing Section is Scored 5.6 Writing Practice Test 5.7 Review of Writing Responses 5.8 Practice Activities and Resources 5.9 This Week in Review and Final Steps for Week 6 Week 6 – About Test Day and Beyond 6.1 Putting It All Together: Preparing for Test Day 6.2 Live Event 6.3 The Test Center 6.4 How to Register 6.5 Receiving and Sending Scores 6.6 Week 6 Quiz 6.7 Official TOEFL Prep Resources 6.8 End-of-Course Survey 6.9 Share with a Friend 6.10 Go Pursue Your Dreams]]></content>
      <tags>
        <tag>TOEFL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Duet display between Win7 and Ipad Air 2]]></title>
    <url>%2F2017%2F03%2F04%2FDuet%20display%20between%20Win7%20and%20Ipad%20Air%202%2F</url>
    <content type="text"><![CDATA[updated 03-22-2017Fix error “”duet” 不再可用” Source videoFollow this video in YouTube. Quote from YouTubeLinks:Duet Display IPA: http://bit.ly/2jvH3WRCydia Impactor Link: http://bit.ly/1bv4PDK Download Duet IPADuet Display IPA, copy the follows url and download.http://down5.dailyuploads.net:182/d/zicch26joghlriinqndkca2paz7zppcgdlhoa6tbe2u4334fbte4z2h2/Duet%20Display%20[Duet%20Inc.]%20(v1.3.7%20v6%203GS%20Univ%20LP%20os80)-Widow.rc334d_902.ipa Download Cydia Impactor (Win7)From this Cydia Impactor website, I choose the windows version Download Duet (Windows OS)Install Duet in window 7, PC download here) Successful instance Fix errorhttp-win.cpp:158 Peer cetificate cannot be authenticated]]></content>
      <tags>
        <tag>Duet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud595 Linux Command Line Basics Notebook]]></title>
    <url>%2F2017%2F02%2F28%2FUdacity%20ud595%20Linux%20Command%20Line%20Basics%20Notebook%2F</url>
    <content type="text"><![CDATA[Linum Command Line BasicsShell Commandsstay tuned… Get Into the ShellWhat’s a Virtual MachineVirtual MachineA virtual machine (VM) is a computer program that simulates a computer. The VM software we’re using in this course is called VirtualBox. When you set up your virtual machine you installed Linux on the VM, making Linux the guest operating system. The operating system (OS) that’s installed directly on your physical computer is called the host OS. We use a virtual machine in this course to ensure that everyone is working in an identical environment with the correct programs installed, but there are many other reasons programmers use VMs. VMs isolate programming projects from everything else on a programmer’s computer. The programmer can configure the guest OS by installing programs and customizing settings without disrupting their day-to-day environment. VMs are also used to simulate the environment that software will be deployed to. Most developers use Windows or Mac OS, but often deploy their code to servers running Linux. Using a Linux VM lets programmers run code on their target platform, without leaving the comfort of their preferred host OS. VagrantVagrant is a program that makes VMs more convenient to use. For example when you ran vagrant up Vagrant created a VM, installed a guest OS, and configured the guest OS. Vagrant did all of this automatically by following instructions in the Vagrantfile. Automating this process saves time and ensures consistent results. Vagrant also makes it easy to edit files that are in the VM from programs installed on the host OS. We won’t use this feature in this class, but it’s very helpful in other Udacity courses and on the job. Command Line InterfaceSetting Up Your Own (Virtual) Linux BoxYour own Linux boxTo learn the Linux shell, you need a Linux machine to run it on. But we can’t really ship a new Linux computer to every one of you. So instead you will set up a Linux virtual machine (VM) on your own computer. You’ll be using the VirtualBox application to run the virtual machine, and the vagrant software to configure it. This virtual-machine setup is very similar to the ones you will use in later Udacity courses on the Linux platform. So when you get to those courses, you will not need to re-install this software. Setting the virtual machine up is not complicated, but it will take some time when your computer downloads the Linux OS. Follow the instructions below to set it up before proceeding on in this course. What’s a virtual machine?A virtual machine is a program that runs on your Windows or Mac computer, and that can run a different operating system inside it. In this case, you’ll be running an Ubuntu Linux server system. Install GitYou can skip this step if you are not running Windows, but many other courses use Git, so you may want to install it now. Download Git from git-scm.com. Install the version for your operating system. On Windows, Git will provide you with the Git Bash terminal program, which you will use to run and connect to your Linux VM. Find your terminal programTo take this course you will need to use a terminal program, which presents the shell user interface and lets you log in to your Linux VM. Windows: Use the Git Bash program, which is installed with Git (above).Mac OS X: Use the Terminal program, located in your Applications/Utilities folder.Linux: Use any terminal program (e.g. xterm or gnome-terminal). Install VirtualBoxVirtualBox is the software that actually runs the VM. You can download it from virtualbox.org, here. Install the platform package for your operating system. You do not need the extension pack or the SDK. You do not need to launch VirtualBox after installing it. Ubuntu 14.04 Note: If you are running Ubuntu 14.04, install VirtualBox using the Ubuntu Software Center, not the virtualbox.org web site. Due to a reported bug, installing VirtualBox from the site may uninstall other software you need. Install VagrantVagrant is the software that configures the VM and lets you share files between your host computer and the VM’s filesystem. You can download it from vagrantup.com. Install the version for your operating system. Windows Note: The Installer may ask you to grant network permissions to Vagrant or make a firewall exception. Be sure to allow this. Download the VM configuration fileMake a new folder to keep your workspace for this course. You might call it Shell, but whatever name you pick is OK. Keep track of what folder you created it in (for instance, Desktop). In the Supporting Materials section of this page, below, you’ll find a link to the configuration file, called Vagrantfile. Download this file into the new folder you just created. Run the virtual machine!Open your terminal program. Type this shell command and press Enter: cd Desktop/Shell(for me cd G:/Udacity/ud595/Shell) (If your new folder is called something other than “Shell”, or is located somewhere other than “Desktop”, change those.) Then start the VM by running the command vagrant up. This will make your system download the Linux OS and start up the virtual machine. Unfortunately, this will take a long time on most network connections. Fortunately, you only have to do it once, and the same Linux OS image will work for later Udacity courses too. Once it is done, run the command vagrant ssh. And you will be logged in to the virtual machine and ready to do the course exercises!The Udacity VM is the official shell for this class, but if your computer already has a Unix* shell you can use it if you prefer. Caveat: Your computer’s own shell may differ from the VM in unanticipated ways, and may not have all the programs installed which the VM provides. The recommended environment is the VM. if you’re running Linux or Mac OS X for instance Log In and Break Stuffexitvagrant ssh In the VM or out of the VM?We’ve set this course’s exercises up to work in the virtual machine (VM) that you set up using the vagrant program. If you get logged out of the VM, you may end up typing shell commands in to your regular operating system instead of to the Linux system that we’ve set up for the course. Some commands won’t work, and some files probably won’t be where the course expects them to be. Getting logged outIf you type the command exit into the shell, or if you type Control-D, you will see a message like this: logout Connection to 127.0.0.1 closed.This just means that you got logged out. After logging out, you won’t be in the VM any more. To get back into the VM, use the command vagrant ssh. If vagrant ssh doesn’t workIf you get a message like this: VM must be running to open SSH connection. Run vagrant up to start the virtual machine.This means that the VM program is not running, for instance because you rebooted your computer. This is just fine and it doesn’t mean you’ve lost any work. Just run vagrant up to bring the VM back up, then vagrant ssh to log in. This will not take as long as the first time you ran it, because it won’t need to download the Linux OS. If vagrant up doesn’t workIf you get a message like this: A Vagrant environment or target machine is required to run this command. Run `vagrant init` to create a new Vagrant environment. Or, get an ID of a target machine from `vagrant global-status` to run this command on. A final option is to change to a directory with a Vagrantfile and to try again. That means that vagrant can’t find the configuration file you downloaded. Go back to the instructions, check to be sure that you did step 5, and then do step 6 again. Multiple terminalsIf you open up more than one terminal window, only the one(s) that you ran vagrant ssh in will be connected to your Linux OS for this course. The others will be connected to your regular OS. (It’s actually really normal for Linux users to have to carefully keep track of which terminal windows are connected to which machines. Don’t panic. Just look for whether “vagrant” appears on the command line.) Commands That Worklscurl http://udacity.github.io/ud595-shell/stuff.zip -o things.zipls Try More Commandsdate expr 2+2 echo you rock uname hostname host udacity.com bash --version history Shell CommandsThe Linux Filesystem]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>VirtualBox</tag>
        <tag>Vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity ud256 Networking for Web Developers Notebook]]></title>
    <url>%2F2017%2F02%2F27%2FUdacity%20ud256%20Networking%20for%20Web%20Developers%20Notebook%2F</url>
    <content type="text"><![CDATA[Networking for Web DevelopersclassroomAim to build a VM.stay tuned… From Ping to HTTPIntro Linum Command Line Basics Configuring Linux Web Servers Designing RESTful APIs Setting Up For This CourseLocal VMInstall two pieces of software: VirtualBox, which you can get from this download page. Vagrant, which you can get from this download page. Use Git Bash mkdir networking cd networking vagrant init ubuntu/trusty64 vagrant upWhen it is complete, you can log into the Linux instance with vagrant ssh. .I have install it before so for me cd G:/Udacity/ud256/from\ ping\ to\ http/networking remove files in networking vagrant up(need about 6 hours to download) When it is complete, you can log into the Linux instance with vagrant ssh. If you log out of the Linux instance or close the terminal, the next time you want to use it you only need to run cd networking and vagrant ssh. Installing networking tools SSH into your Linux machine. Then take a moment to bring it up to date with any package updates: sudo apt-get update &amp;&amp; sudo apt-get upgrade Note:1W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/trusty-security/main/source/Sources Hash Sum mismatch You’ll be using several network utility programs in this course. Some of them may already be installed, but just to make sure, let’s install them all: sudo apt-get install netcat-openbsd tcpdump traceroute mtr]]></content>
      <tags>
        <tag>Udacity</tag>
        <tag>VirtualBox</tag>
        <tag>Vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTex]]></title>
    <url>%2F2017%2F02%2F23%2FLaTex%2F</url>
    <content type="text"><![CDATA[stay tuned… Original videos can be found in LaTeX Tutorials (featuring Texmaker) made by Michelle Krummel. Installation for windows 7 Go to LaTex Download MiKTeX and run Download TexMaker and run Tutorial 1In TexMaker, type1234567891011\documentclass[11pt]&#123;article&#125;\begin&#123;document&#125;This is my first LaTex document.Suppose we are given a rectangle with side lengths $(x+1)$ and $(x+3)$.Then the equation $A=x^2+4X+3$ represents the area of the rectangle.Suppose we are given a rectangle with side lengths $(x+1)$ and $(x+3)$.Then the equation $$A=x^2+4X+3$$ represents the area of the rectangle.\end&#123;document&#125; It shows:\documentclass[11pt]{article} \begin{document}This is my first LaTex document. Suppose we are given a rectangle with sidelengths $(x+1)$ and $(x+3)$.Then the equation $A=x^2+4X+3$ represents the area of the rectangle. Suppose we are given a rectangle with sidelengths $(x+1)$ and $(x+3)$.Then the equation $$A=x^2+4X+3$$ represents the area of the rectangle.\end{document} Tutorial 2In TexMaker, type1234567891011121314151617181920212223242526272829303132333435363738394041424344\documentclass[11pt]&#123;article&#125;\begin&#123;document&#125;superscripts: $$2x^3$$$$2x^&#123;34&#125;$$$$2x^&#123;3x+4&#125;$$$$2x^&#123;3x^4+5&#125;$$subscripts: $$x_1$$$$x_&#123;12&#125;$$$$&#123;x_1&#125;_2$$greek letters:$$\pi$$$$\alpha$$$$A=\pi r^2$$trig functions:$$y=\sin&#123;x&#125;$$log functions:$$\log_5&#123;x&#125;$$$$\ln&#123;x&#125;$$square roots:$$\sqrt&#123;2&#125;$$$$\sqrt[3]&#123;2&#125;$$$$\sqrt&#123;x^2+y^2&#125;$$$$\sqrt&#123;1+\sqrt&#123;x&#125;&#125;$$fractions:About $\displaystyle&#123;\frac&#123;2&#125;&#123;3&#125;&#125;$ of the glass is full.$$\frac&#123;x&#125;&#123;x^+x+1&#125;$$$$\frac&#123;\sqrt&#123;x+1&#125;&#125;&#123;\sqrt&#123;x-1&#125;&#125;$$$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;x&#125;&#125;$$$$\sqrt&#123;\frac&#123;x&#125;&#123;x^2+x+1&#125;&#125;$$\end&#123;document&#125; Note : In hexo, need to use 1234subscripts: $$x\_1$$$$x\_&#123;12&#125;$$$$&#123;x\_1&#125;\_2$$ It shows:\documentclass[11pt]{article} \begin{document} superscripts: $$2x^3$$$$2x^{34}$$$$2x^{3x+4}$$$$2x^{3x^4+5}$$ subscripts:$$x_1$$$$x_{12}$$$${x_1}_2$$ greek letters:$$\pi$$$$\alpha$$$$A=\pi r^2$$ trig functions:$$y=\sin{x}$$ log functions:$$\log_5{x}$$$$\ln{x}$$ square roots:$$\sqrt{2}$$$$\sqrt[3]{2}$$$$\sqrt{x^2+y^2}$$$$\sqrt{1+\sqrt{x}}$$ fractions: About $\displaystyle{\frac{2}{3}}$ of the glass is full. $$\frac{x}{x^+x+1}$$ $$\frac{\sqrt{x+1}}{\sqrt{x-1}}$$ $$\frac{1}{1+\frac{1}{x}}$$ $$\sqrt{\frac{x}{x^2+x+1}}$$\end{document} Paper formula$Z{normal}=[z{normal1}, z{normal2}…z{normal_n}]\in R^{n\times m}$ \begin{equation}\label{eq:xznormal}x{normal} = \frac{z{normali}-\bar{z}{normali}}{\sigma(z{normal_i})}\end{equation} $\bar{z}_{normali}$$z{normali}$$\sigma(z{normali})$$x{normal}$ \begin{equation}\label{eq:snormalk}s_{normalk} = W{normalk}\cdot x{normal_i}\end{equation} $W_{normalk}\in R^{k\times m}$$x{normal_i}\in R^{m\times 1}$ \begin{equation}\label{eq:snormale}s_{normale} = W{normale}\cdot x{normal_i}\end{equation} $W_{normal_e}\in R^{(d-k)\times m}$ $E{normal}$${I^2}{normal}$${I^2}_{normal_e}$ \begin{equation}\label{eq:inormal}{I^2}{normal}= s{normalk}^T\cdot s{normalk}\end{equation}\begin{equation}\label{eq:inormale}I{normale}^2= s{normale}^T\cdot s{normale}\end{equation}\begin{equation}\label{eq:xase}X{normal}= A{normal}\cdot S{normal}+E_{normal}\end{equation} $E_{normal}$ \begin{equation}\label{eq:enormal}E{normal}=T{normal}\cdot P{normal}^T+F{normal}\end{equation} $E{normal}=[e{normal1}, e{normal2}…e{normal_m}]\in R^{n\times m}$ $T{normal}^2$$Q{normal}$ \begin{equation}\label{eq:t2normal}T{normal}^2=e{normali}^T\cdot P{normal}\cdot \Lambda ^{-1}\cdot P{normal}^T\cdot e{normali}\end{equation}\begin{equation}\label{eq:qnormal}Q{normal}= r{normal}^T r{normal}\end{equation} $r{normal}=(I-P{normal}\cdot P{normal}^T )\cdot e{normal_i}\in R^{m\times 1}$$I\in R^{m\times m}$ \begin{equation}\label{eq:inputmatrix}X{normal}= [{I^2}{normal},I_{normale}^2,T{normal}^2,Q_{normal}]\end{equation} $Z{fault}=[z{fault1}, z{fault2}…z{faultm}]\in R^{n\times m}$$X{fault}=[x_{fault1}, x{fault2}…x{faultm}]\in R^{n\times m}$Calculate Statistics$y{normal}=0$ \begin{equation}\label{eq:ifault}{I^2}{fault}= s{faultk}^T\cdot s{faultk}\end{equation}\begin{equation}\label{eq:ifaulte}I{faulte}^2= s{faulte}^T\cdot s{faulte}\end{equation}\begin{equation}\label{eq:t2fault}T{fault}^2=e_{faulti}^T\cdot P{fault}\cdot \Lambda ^{-1}\cdot P{fault}^T\cdot e{faulti}\end{equation}\begin{equation}\label{eq:qfault}Q{fault}= r{fault}^T r{fault}\end{equation} \begin{equation}\label{eq:inputmatrixf}X{fault}= [{I^2}{fault},I_{faulte}^2,T{fault}^2,Q_{fault}]\end{equation} \begin{equation}\label{eq:inew}{I^2}{new}= s{newk}^T\cdot s{newk}\end{equation}\begin{equation}\label{eq:inewe}I{newe}^2= s{newe}^T\cdot s{newe}\end{equation}\begin{equation}\label{eq:t2new}T{new}^2=e_{newi}^T\cdot P\cdot \Lambda ^{-1}\cdot P^T\cdot e{newi}\end{equation}\begin{equation}\label{eq:qnew}Q{new}= r{new}^T r{new}\end{equation} $X{new}= [{I^2}{new},I_{newe}^2,T{new}^2,Q_{new}]$ \multicolumn{1}{c}{ICA-PCA-SVM} &amp; \multicolumn{1}{c}{PCA-RVM} &amp; \multicolumn{1}{c}{ICA-RVM} &amp; \multicolumn{1}{c}{Proposed Method} &amp; ICA-SVM &amp; ICA-PCA-SVM &amp; PCA-RVM &amp; ICA-RVM &amp; ICA-PCA-RVM]]></content>
      <tags>
        <tag>LaTex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity MLND Notebook]]></title>
    <url>%2F2017%2F02%2F06%2FUdacity%20MLND%20Notebook%2F</url>
    <content type="text"><![CDATA[Capstone Projectupdate in 2017-04-8https://classroom.udacity.com/nanodegrees/nd009/parts/5375cf82-14fe-422d-b737-7bc893e20a6dstay tuned… nd009syllabus [TOC] Welcome to the NanodegreeGet started with learning about your Nanodegree. Introduction to Decision Trees, Naive Bayes, Linear and Logistic Regression and Support Vector Machines. You can join the MLND student community by following this link and registering your email - https://mlnd-slack.udacity.com WELCOME TO THE NANODEGREEWelcome to MLNDhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HG5IYufgDAo.mp4 Program Readinesshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dc9CmcGTnr0.mp4 What is Machine Learning?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K45QM8Wi7BU.mp4 Machine Learning vs. Traditional Codinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_N2iIB_bLXA.mp4 Applications of Machine Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kIM5D_W6Mh8.mp4 Connections to GA Techhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/DysCmGKRpvs.mp4 Program Outlinehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/m0cIDrRWyLw.mp4 What is MLIntroduction to Machine Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bYeteZQrUcE.mp4 Decision Treeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1RonLycEJ34.mp4 Decision Trees QuizQUIZ QUESTION Between Gender and Age, which one seems more decisive for predicting what app will the users download? Gender Age Decision Trees Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h8zH47iFhCo.mp4 Naive Bayeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jsLkVYXmr3E.mp4 Naive Bayes QuizQUIZ QUESTION If an e-mail contains the word “cheap”, what is the probability of it being spam? 40% 60%80% Naive Bayes Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YKN-fjuZ1VU.mp4 Gradient Descenthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BEC0uH1fuGU.mp4 Linear Regression Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sf51L0RN6zc.mp4QUIZ QUESTION What’s the best estimate for the price of a house? 80k 120k 190kSUBMIT Linear Regression Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L5QBqYDNJn0.mp4 Logistic Regression Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wQXKdeVHTmc.mp4QUIZ QUESTION Does the student get Accepted? Yes NoSUBMIT Logistic Regression Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JuAJd9Qvs6U.mp4 Support Vector Machineshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Fwnjx0s_AIw.mp4 Support Vector Machines QuizQUIZ QUESTION Which one is a better line?The yellow line The blue lineSUBMIT Support Vector Machines Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JrUtTwfnsfM.mp4 Neural Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xFu1_2K2D2U.mp4 Kernel Methodhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/x0JqH6-Dhvw.mp4 Kernel Method QuizQUIZ QUESTION Which equation could come to our rescue? x+yxy x^2SUBMIT Kernel Method Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dRFd6HaAXys.mp4 Recap and Challengehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ecREasTrKu4.mp4 K-means Clusteringhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pv_i08zjpQw.mp4 Hierarchical Clusteringhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1PldDT8AwMA.mp4 Practice Project: Detect SpamPractice Project: Using Naive Bayes to detect spam.From time to time you will be encouraged to work on practice projects which are aimed at deepening your understanding of the concepts being taught. In this practice project, you will be implementing the Naive Bayes algorithm to detect spam text messages(as taught by Luis earlier in the lesson) from an open source dataset. Here is the notebook, the solutions are included. Summaryhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hJEuaOUu2yA.mp4 MLND Program OrientationBefore the Program Orientationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/73CdKtS-IwU.mp4 Introductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fxNSn63xFvA.mp4 Projects and Progresshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Z9ZLMQWsbsk.mp4 Career DevelopmentBeing enrolled in one of Udacity’s Nanodegree programs has many careers-based perks. Our goal is to help you take your learning from this program and apply it in the real world and in your career. As you venture through the Machine Learning Engineer Nanodegree program, you’ll have the opportunity to Update your resume through a peer-reviewed process using conventions that recruiters expect and get tips on how to best represent yourself to pass the “6 second screen”; Create a cover letter that portrays your soft and hard skills, and most importantly your passion for a particular job that you are interested in applying to; Get your GitHub and LinkedIn profiles reviewed through the lens of a recruiter or hiring manager, focusing on how your profile, projects, code, education and past experiences represent you as a potential candidate; Practice for a technical interview with a professional reviewer on a variety of topics; And more! You can also find career workshops that Udacity has hosted over the years, where you can gain a plethora of information to prepare you for your ventures into a career. Udacity also provides job placement opportunities with many of our industry partners. To take advantage of this opportunity, fill out the career section of your Udacity professional profile, so we know more about you and your career goals! If all else fails, you can always default to emailing the career team at career-support@udacity.com. Connecting with Your CommunityYour Nanodegree community will play a huge role in supporting you when you get stuck and in helping you deepen your learning. Getting to know your fellow students will also make your experience a lot more fun! To ask and answer questions, and to contribute to discussions, head to your program forum. You can get there by clicking the Discussion link in the classroom and in the Resources tab in your Udacity Home. You can search to see if someone has already asked a question related to yours, or you can make a new post if no one has. Chances are, someone else is wondering about the same thing you are, so don’t be shy! In addition, students may connect with one another through Slack, a team-oriented chat program. You can join the MLND Slack student community by following this link and registering your email. There are many content-related channels where you can speak with students about a particular concept, and even discuss your first week in the program using the #first-week-experience channel. In addition, you can talk with MLND graduates and alumni to get a live perspective on the program in the #ask-alumni channel! You can find the student-run community wiki here. Support from the Udacity TeamThe Udacity team is here to help you reach your Nanodegree program goals! You can interact with us in the following ways: Forums: Along with your student community, the Udacity team maintains a strong presence in the forum to help make sure your questions get answered and to connect you with other useful resources. 1-on-1 Appointments: If you get stuck working on a project in the program, our mentors are here to help! You can set up a half-hour appointment with a mentor available for the project at a time you choose to get assistance. Project Reviews: During the project submission process, your submissions will be reviewed by a qualified member of our support team, who will provide comments and helpful feedback on where your submission is strongest, and where your submission needs improvement. The reviews team will support your submissions all the way up to meeting specifications! By email: You can always contact the Machine Learning team with support-related questions using machine-support@udacity.com. Please make sure that you have exhausted all other options before doing so!Find out more about the support we offer using the Resources tab in your Udacity Nanodegree Home. How Does Project Submission Work?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jCJa_VP6qgg.mp4 Integrity and Mindsethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zCOr3O50gQM.mp4 How Do I Find Time for My Nanodegree?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/d-VfUw7wNEQ.mp4All calendar applications now let you set up a weekly reminder. I have included a screen capture below of how to set one up in Google Calendar. We recommend coming into the classroom at least twice a week. It is a best practice to set up at least one repeating weekly reminder to continue the Nanodegree program. Final Tipshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1ZVBvM54hQw.mp4 Wrapping Up the Program Orientationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xujb3Rqxuog.mp4You now have all the info you need to proceed on your Nanodegree journey! If you have any further questions, perhaps about payment or enrollment, read your Nanodegree Student Handbook for more details. Download Udacity’s mobile app to learn on the go! Remember to put in time consistently, engage with your community, take advantage of the resources available to you, and give us feedback throughout the program.We are so glad to have you with us! Return to your Udacity home to keep learning. Good luck! Professional Profile Career Development Workshops MLND Slack Forums MLND wiki (Optional) Exploratory ProjectSoftware Requirements windows + R and type pip install --user pandas jupyter, oops, error: Microsoft Visual C++ 9.0 is required. Get it from http://aka.ms/vcpython27 download and install successfully Starting the ProjectFirst try windows + R typecd &lt;path&gt;my&lt;path&gt;isG:\Udacity\MLND\machine-learning-master\projects\titanic_survival_exploration type&lt;path&gt; g: typebash jupyter notebook titanic_survival_exploration.ipynb,show &#39;bash&#39; 不是内部或外部命令，也不是可运行的程序 Failed Second try opengit bash cd&lt;path&gt;with/``G:/Udacity/MLND/machine-learning-master/projects/titanic_survival_exploration typebash jupyter notebook titanic_survival_exploration.ipynb failed Third try install Anaconda windows + R typecd &lt;path&gt;my&lt;path&gt;isG:\Udacity\MLND\machine-learning-master\projects\titanic_survival_exploration type&lt;path&gt;``g: typejupyter notebook titanic_survival_exploration.ipynb done Fourth try opengit bash cd&lt;path&gt;with/``G:/Udacity/MLND/machine-learning-master/projects/titanic_survival_exploration typejupyter notebook titanic_survival_exploration.ipynb done ipython notebookMarkdown Question 4(stay tuned): Pclass == 3 Career: OrientationThroughout your Nanodegree program, you will see Career Development Lessons and Projects that will help ensure you’re presenting your new skills best during your job search. In this short lesson, meet the Careers team and learn about the career resources available to you as a Nanodegree student. If you are a Nanodegree Plus student, Career Content and Career Development Projects are required for graduation. If you are enrolled in a standard Nanodegree program, Career Content and Career Development Projects are optional and do not affect your graduation. ORIENTATIONCareer Services Available to YouMeet the Careers Teamhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oR1IxPTTz0U.mp4 Resources Career Resource Center Career Services Get Hired Your Udacity ProfileConnect to Hiring Partners through your Udacity Professional ProfileIn addition to the Career Lessons and Projects you’ll find in your Nanodegree program, you have a Udacity Professional Profile linked in the left sidebar.Your Udacity Professional Profile features important, professional information about yourself. When you make your profile public, it becomes accessible to our Hiring Partners, as well as to recruiters and hiring managers who come to Udacity to hire skilled Nanodegree graduates. As you complete projects in your Nanodegree program, they will be automatically added to your Udacity Professional Profile to ensure you’re able to show employers the skills you’ve gained through the program. In order to differentiate yourself from other candidates, make sure to go in and customize those project cards. In addition to these projects, be sure to: Keep your profile updated with your basic info and job preferences, such as location Ensure you upload your latest resume Return regularly to your Profile to update your projects and ensure you’re showcasing your best work If you are looking for a job, make sure to keep your Udacity Professional Profile updated and visible to recruiters!EDIT YOUR PROFILE NOW ! Model Evaluation and ValidationApply statistical analysis tools to model observed data, and gauge how well your models perform. Project: Predicting Boston Housing Prices For most students, this project takes approximately 8 - 15 hours to complete (about 1 - 3 weeks). P1 Predicting Boston Housing Prices STATISTICAL ANALYSISIntro: Model Evaluation and ValidationIntro to Model Evaluation and Validationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cseqEWRDs5Q.mp4 Model Evaluation What You’ll Watchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jYZO17CeZDI.mp4 Model Evaluation What You’ll Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ZLOucNwuqCk.mp4 Course Outline - Fitting It All TogetherThe ultimate goal of Machine Learning is to have data models that can learn and improve over time. In essence machine learning is making inferences on data from previous examples. In this first section we review some basic statistics and numerical tools to manipulate &amp; process our data. Then we will move on to modeling data; reviewing different data types and seeing how they play out in the case of one specific dataset. The section ends by introducing the basic tool of a supervised learning algorithm. Next, we’ll see how to use our dataset for both training and testing data, and review various tools for how to evaluate how well an algorithm performs. Finally, we’ll look at the reasons that errors arise, and the relationship between adding more data and adding more complexity in getting good predictions. The last section ends by introducing cross validation, a powerful meta-tool for helping us use our tools correctly. Model Evaluation What You’ll Dohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kJCAuHjWiOA.mp4 PrerequisitesStatistics Review &amp; Supporting LibrariesIn this section we will go over some prerequisites for this course, review basic statistics concepts and problem sets, and finally teach you how to use some useful data analysis Python libraries to explore real-life datasets using the concepts you reviewed earlier. Prerequisiteshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ANDJ8i_deE.mp4Here are shortcuts to the prerequisite statistics courses: Udacity’s descriptive stats course [link] Udacity’s inferential stats course [link] You will also need to have just a little bit of git experience — enough to check out our code repository. If you’ve ever used git before, you should be fine. If this is truly your first time with git, once you get to the first mini-project, you may want to quickly look at the first lesson of Udacity’s git course. mode mean variance``standard deviation Bessel&#39;s Correction use n-1 instead of n sample SD Measures of Central TendencyIntroduction: Topics CoveredMeasures of Central TendencyIn this lesson, we will cover the following topics: Mean Median Mode This lesson is meant to be a refresher for those who have no statistics background and therefore if you are familiar with these concepts you may skip this lesson. Which Major?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mIzPoh_kqw4.mp4Quiz: Which Major?Enter your answers as a number with no commas or symbols ($). Enter the number in thousands (5 digits) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t0yIetl9ZxI.mp4 One Number to Describe Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6QfvhQ0En0E.mp4 Which Number to Choose?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7E7Czixpviw.mp4 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TSw2AAaKxBA.mp4 Mode of Datasethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/80BAbiEWsaY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HvOjcTFlVTI.mp4 Mode of Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/47JDwoDUxP8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/s9dHF4MMGx0.mp4 Mode - Negatively Skewed Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xBnhUJENAtk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oNRtSJvtkJc.mp4 Mode - Uniform Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TE2BZql64XY.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mFm0FfWHlXw.mp4 More than One Mode?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1GuHNqJNY2M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9bsmO7cKzPk.mp4 Mode of Categorical DataQuizThe data doesn’t need to be numeric to find a mode: we can also compute the mode for categorical data as well! On the next slide, you’ll be asked to find the mode of a categorical data set: the preferred M&amp;M flavor of 8,000 Udacity students.START QUIZ AnswerRemember, the mode occurs on the X-axis, so you are looking for whatever value has the highest frequency. The numbers 7,000 and 1,000 are the actual frequencies. The mode, itself, is “Plain.” More o’ Mode!https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wBduq7St2Ak.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/74mudn321tA.mp4 Find the Meanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/S4KbzIyEwV8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/El5cY2jlzuM.mp4 Procedure for Finding Meanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nRurXCTYxG4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lK2lDLdE6iA.mp4 Iterative Procedurehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/r2s9INGd-Ls.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NcWS_BvM3IU.mp4 Helpful Symbolshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/w4_8YCp-9fI.mp4 Properties of the Meanhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uSXJtEpwEVM.mp4Quiz: Properties Of The MeanPlease note: The last option “The mean will change if we add an extreme value to the dataset.” is not necessarily a property of the mean, more a behavioral tendency. But for the purposes of this quiz, you can mark it as a property https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AqlvTMZg6HY.mp4 Mean with Outlier What Can You Expect? UNC Requirement for Median Find the Median Median with Outlier Find Median with Outlier Measures of Center Order Measures of Center 1 Order Measures of Center 2 Use Measures of Center to CompareLink to poll: How many Facebook friends do you have?Link to poll results Mashable: Who is an Average Facebook User?Zeebly Social Me (optional but fun use of statistics) Udacians’ Facebook Friends - MeanLink to Udacians’ Facebook FriendsCopy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Please round your answer to two decimal places. Udacians’ Facebook Friends - MedianLink to Udacians’ Facebook Friends Copy and paste the data into your own spreadsheet to perform the calculations. From Google Drive (at the top of the page once you’re signed in to your Google account), click the button on the left that says “CREATE” and click “Spreadsheet.” Formula for Location of Median Wrap Up - Measures of CenterQuiz: Wrap Up - Measures Of CenterHere is a short doc outlining Mean, Median, and Mode. http://tinyurl.com/measureOfCenter Good Job! Variability of DataIntroduction: Topics CoveredIn this lesson, we will cover the following topics: Inter Quartile Range Outliers Standard Deviation Bessel’s Correction This lesson is meant to be a refresher for those who have no statistics background and therefore if you are familiar with these concepts you may skip this lesson. Social Networkers’ Salaries Should You Get an Account? What’s the Difference? Quantify Spread Does Range Change? Mark Z the Outlier Chop Off the Tails Where Is Q1? Q3 - Q1 IQR What Is an Outlier? Define Outlier Match Boxplots Mean Within IQR? Problem with IQR Measure Variability Calculate Mean Deviation from Mean Average Deviation Equation for Average Deviation Be Happy and Get Rid of Negatives Absolute Deviations Average Absolute Deviation Formula for Avg. Abs. Dev. Squared Deviations Sum of Squares Average Squared Deviation #### #### #### #### #### #### #### #### Numpy&amp;Pandas TutorialsNumpy and Pandas TutorialsNow that you reviewed some basic statistics, lets go over some Python libraries that allow you to explore data and process large datasets. Specifically we will go over numpy which will allow us to process large amount of numerical data and panda series and dataframes which allow us to store large datasets and extract information from them. Numpy Library Documentation: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html Pandas Library Documentation: http://pandas.pydata.org/pandas-docs/version/0.17.0/ We highly recommend going through this resource by Justin Johnson if you have not worked with Numpy before. Another great resource is the SciPy-lectures series on this topic. Numpyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/l_Tzjxfa_5g.mp4 Numpy PlaygroundHere’s the code import numpy as np &apos;&apos;&apos; The following code is to help you play with Numpy, which is a library that provides functions that are especially useful when you have to work with large arrays and matrices of numeric data, like doing matrix matrix multiplications. Also, Numpy is battle tested and optimized so that it runs fast, much faster than if you were working with Python lists directly. &apos;&apos;&apos; &apos;&apos;&apos; The array object class is the foundation of Numpy, and Numpy arrays are like lists in Python, except that every thing inside an array must be of the same type, like int or float. &apos;&apos;&apos; # Change False to True to see Numpy arrays in action if False: array = np.array([1, 4, 5, 8], float) print array print &quot;&quot; array = np.array([[1, 2, 3], [4, 5, 6]], float) # a 2D array/Matrix print array &apos;&apos;&apos; You can index, slice, and manipulate a Numpy array much like you would with a a Python list. &apos;&apos;&apos; # Change False to True to see array indexing and slicing in action if False: array = np.array([1, 4, 5, 8], float) print array print &quot;&quot; print array[1] print &quot;&quot; print array[:2] print &quot;&quot; array[1] = 5.0 print array[1] # Change False to True to see Matrix indexing and slicing in action if False: two_D_array = np.array([[1, 2, 3], [4, 5, 6]], float) print two_D_array print &quot;&quot; print two_D_array[1][1] print &quot;&quot; print two_D_array[1, :] print &quot;&quot; print two_D_array[:, 2] &apos;&apos;&apos; Here are some arithmetic operations that you can do with Numpy arrays &apos;&apos;&apos; # Change False to True to see Array arithmetics in action if False: array_1 = np.array([1, 2, 3], float) array_2 = np.array([5, 2, 6], float) print array_1 + array_2 print &quot;&quot; print array_1 - array_2 print &quot;&quot; print array_1 * array_2 # Change False to True to see Matrix arithmetics in action if False: array_1 = np.array([[1, 2], [3, 4]], float) array_2 = np.array([[5, 6], [7, 8]], float) print array_1 + array_2 print &quot;&quot; print array_1 - array_2 print &quot;&quot; print array_1 * array_2 &apos;&apos;&apos; In addition to the standard arthimetic operations, Numpy also has a range of other mathematical operations that you can apply to Numpy arrays, such as mean and dot product. Both of these functions will be useful in later programming quizzes. &apos;&apos;&apos; if True: array_1 = np.array([1, 2, 3], float) array_2 = np.array([[6], [7], [8]], float) print np.mean(array_1) print np.mean(array_2) print &quot;&quot; print np.dot(array_1, array_2) Pandashttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8ay7tX26YxE.mp4 Pandas Playground – SeriesHere’s the code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pandas as pd&apos;&apos;&apos;The following code is to help you play with the concept of Series in Pandas.You can think of Series as an one-dimensional object that is similar toan array, list, or column in a database. By default, it will assign anindex label to each item in the Series ranging from 0 to N, where N isthe number of items in the Series minus one.Please feel free to play around with the concept of Series and see what it does*This playground is inspired by Greg Reda&apos;s post on Intro to Pandas Data Structures:http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/&apos;&apos;&apos;# Change False to True to create a Series objectif False: series = pd.Series([&apos;Dave&apos;, &apos;Cheng-Han&apos;, &apos;Udacity&apos;, 42, -1789710578]) print series&apos;&apos;&apos;You can also manually assign indices to the items in the Series whencreating the series&apos;&apos;&apos;# Change False to True to see custom index in actionif False: series = pd.Series([&apos;Dave&apos;, &apos;Cheng-Han&apos;, 359, 9001], index=[&apos;Instructor&apos;, &apos;Curriculum Manager&apos;, &apos;Course Number&apos;, &apos;Power Level&apos;]) print series&apos;&apos;&apos;You can use index to select specific items from the Series&apos;&apos;&apos;# Change False to True to see Series indexing in actionif False: series = pd.Series([&apos;Dave&apos;, &apos;Cheng-Han&apos;, 359, 9001], index=[&apos;Instructor&apos;, &apos;Curriculum Manager&apos;, &apos;Course Number&apos;, &apos;Power Level&apos;]) print series[&apos;Instructor&apos;] print &quot;&quot; print series[[&apos;Instructor&apos;, &apos;Curriculum Manager&apos;, &apos;Course Number&apos;]]&apos;&apos;&apos;You can also use boolean operators to select specific items from the Series&apos;&apos;&apos;# Change False to True to see boolean indexing in actionif True: cuteness = pd.Series([1, 2, 3, 4, 5], index=[&apos;Cockroach&apos;, &apos;Fish&apos;, &apos;Mini Pig&apos;, &apos;Puppy&apos;, &apos;Kitten&apos;]) print cuteness &gt; 3 print &quot;&quot; print cuteness[cuteness &gt; 3] Pandas Playground – DataframeHere’s the code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import numpy as npimport pandas as pd&apos;&apos;&apos;The following code is to help you play with the concept of Dataframe in Pandas.You can think of a Dataframe as something with rows and columns. It issimilar to a spreadsheet, a database table, or R&apos;s data.frame object.*This playground is inspired by Greg Reda&apos;s post on Intro to Pandas Data Structures:http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/&apos;&apos;&apos;&apos;&apos;&apos;To create a dataframe, you can pass a dictionary of lists to the Dataframeconstructor:1) The key of the dictionary will be the column name2) The associating list will be the values within that column.&apos;&apos;&apos;# Change False to True to see Dataframes in actionif False: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football&apos;&apos;&apos;Pandas also has various functions that will help you understand some basicinformation about your data frame. Some of these functions are:1) dtypes: to get the datatype for each column2) describe: useful for seeing basic statistics of the dataframe&apos;s numerical columns3) head: displays the first five rows of the dataset4) tail: displays the last five rows of the dataset&apos;&apos;&apos;# Change False to True to see these functions in actionif True: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football.dtypes print &quot;&quot; print football.describe() print &quot;&quot; print football.head() print &quot;&quot; print football.tail() Create a DataFramehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hMxOuWJaVDA.mp4 Here is a link to the pandas documentation. Here’s also an excellent series of tutorials as IPython notebooks (Thank you to Dominique Luna for sharing!) Also note: you do not need to use pandas.Series, you can pass in python lists as the values in this case: olympic_medal_counts_df = DataFrame( {&apos;country_name&apos;: countries, &apos;gold&apos;: gold, &apos;silver&apos;: silver, &apos;bronze&apos;: bronze}) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from pandas import DataFrame, Series################## Syntax Reminder:## The following code would create a two-column pandas DataFrame# named df with columns labeled &apos;name&apos; and &apos;age&apos;:## people = [&apos;Sarah&apos;, &apos;Mike&apos;, &apos;Chrisna&apos;]# ages = [28, 32, 25]# df = DataFrame(&#123;&apos;name&apos; : Series(people),# &apos;age&apos; : Series(ages)&#125;)def create_dataframe(): &apos;&apos;&apos; Create a pandas dataframe called &apos;olympic_medal_counts_df&apos; containing the data from the table of 2014 Sochi winter olympics medal counts. The columns for this dataframe should be called &apos;country_name&apos;, &apos;gold&apos;, &apos;silver&apos;, and &apos;bronze&apos;. There is no need to specify row indexes for this dataframe (in this case, the rows will automatically be assigned numbered indexes). You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] # your code here olympic_medal_counts_df = DataFrame( &#123;&apos;country_name&apos;: countries, &apos;gold&apos;: gold, &apos;silver&apos;: silver, &apos;bronze&apos;: bronze&#125;) return olympic_medal_counts_df https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-6Zw3Y4iXRY.mp4 Dataframe Columnshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/G4gFryXnrr8.mp4 Pandas Playground - Indexing DataframesHere’s the code1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import pandas as pd&apos;&apos;&apos;You can think of a DataFrame as a group of Series that share an index.This makes it easy to select specific columns that you want from the DataFrame. Also a couple pointers:1) Selecting a single column from the DataFrame will return a Series2) Selecting multiple columns from the DataFrame will return a DataFrame*This playground is inspired by Greg Reda&apos;s post on Intro to Pandas Data Structures:http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/&apos;&apos;&apos;# Change False to True to see Series indexing in actionif False: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football[&apos;year&apos;] print &apos;&apos; print football.year # shorthand for football[&apos;year&apos;] print &apos;&apos; print football[[&apos;year&apos;, &apos;wins&apos;, &apos;losses&apos;]]&apos;&apos;&apos;Row selection can be done through multiple ways.Some of the basic and common methods are: 1) Slicing 2) An individual index (through the functions iloc or loc) 3) Boolean indexingYou can also combine multiple selection requirements through booleanoperators like &amp; (and) or | (or)&apos;&apos;&apos;# Change False to True to see boolean indexing in actionif True: data = &#123;&apos;year&apos;: [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], &apos;team&apos;: [&apos;Bears&apos;, &apos;Bears&apos;, &apos;Bears&apos;, &apos;Packers&apos;, &apos;Packers&apos;, &apos;Lions&apos;, &apos;Lions&apos;, &apos;Lions&apos;], &apos;wins&apos;: [11, 8, 10, 15, 11, 6, 10, 4], &apos;losses&apos;: [5, 8, 6, 1, 5, 10, 6, 12]&#125; football = pd.DataFrame(data) print football.iloc[[0]] print &quot;&quot; print football.loc[[0]] print &quot;&quot; print football[3:5] print &quot;&quot; print football[football.wins &gt; 10] print &quot;&quot; print football[(football.wins &gt; 10) &amp; (football.team == &quot;Packers&quot;)] Pandas Vectorized Methodshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hvSEZxcH9PM.mp4As a refresher on lambda, lambda functions are small inline functions that are defined on-the-fly in Python. lambda x: x&gt;= 1 will take an input x and return x&gt;=1, or a boolean that equals True or False. In this example, map() and applymap() create a new Series or DataFrame by applying the lambda function to each element. Note that map() can only be used on a Series to return a new Series and applymap() can only be used on a DataFrame to return a new DataFrame. For further reference, please refer to the official documentation on lambda: Lambda Function Average Bronze Medalshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AjniaYFfCeg.mp4You might find using “boolean indexing“ helpful for this problem. Here is a link to the pandas documentation. Here’s also an excellent series of tutorials as IPython notebooks (Thank you to Dominique Luna for sharing!) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pandas import DataFrame, Seriesimport numpydef avg_medal_count(): &apos;&apos;&apos; Compute the average number of bronze medals earned by countries who earned at least one gold medal. Save this to a variable named avg_bronze_at_least_one_gold. You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. HINT-1: You can retrieve all of the values of a Pandas column from a data frame, &quot;df&quot;, as follows: df[&apos;column_name&apos;] HINT-2: The numpy.mean function can accept as an argument a single Pandas column. For example, numpy.mean(df[&quot;col_name&quot;]) would return the mean of the values located in &quot;col_name&quot; of a dataframe df. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] olympic_medal_counts = &#123;&apos;country_name&apos;:Series(countries), &apos;gold&apos;: Series(gold), &apos;silver&apos;: Series(silver), &apos;bronze&apos;: Series(bronze)&#125; df = DataFrame(olympic_medal_counts) # YOUR CODE HERE #print df[df.gold&gt;=1] #print df[df.gold&gt;=1][&apos;bronze&apos;] #print (df[df.gold&gt;=1][&apos;bronze&apos;]).apply(numpy.mean) avg_bronze_at_least_one_gold = numpy.mean(df[df.gold&gt;=1][&apos;bronze&apos;]) return avg_bronze_at_least_one_gold https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kibtHtPgWvs.mp4 Average Gold, Silver, and Bronze Medalshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/19b96U6dLtY.mp412345678910111213141516171819202122232425262728293031323334353637383940import numpyfrom pandas import DataFrame, Seriesdef avg_medal_count(): &apos;&apos;&apos; Using the dataframe&apos;s apply method, create a new Series called avg_medal_count that indicates the average number of gold, silver, and bronze medals earned amongst countries who earned at least one medal of any kind at the 2014 Sochi olympics. Note that the countries list already only includes countries that have earned at least one medal. No additional filtering is necessary. You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] olympic_medal_counts = &#123;&apos;country_name&apos;:countries, &apos;gold&apos;: Series(gold), &apos;silver&apos;: Series(silver), &apos;bronze&apos;: Series(bronze)&#125; df = DataFrame(olympic_medal_counts) # YOUR CODE HERE #print df[(df.gold&gt;=1)|(df.silver&gt;=1)|(df.bronze&gt;=1)] #print df[(df.gold&gt;=1)|(df.silver&gt;=1)|(df.bronze&gt;=1)][&apos;bronze&apos;,&apos;silver&apos;,&apos;gold&apos;] print df[&apos;bronze&apos;,&apos;silver&apos;,&apos;gold&apos;].apply(numpy.mean) #return 0 #return avg_medal_count https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HbCmZuVp548.mp4 Matrix Multiplication and Numpy Dothttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yAgqlTfWc9E.mp4The second line in the green vector on the top line starting at 1:15 should read: “1 x 4 + 2 x 5”. This vector should also be a row vector (1 x 3 matrix) instead of a column vector (3 x 1 matrix). You can read more about numpy.dot or matrix multiplication with numpy below:http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html Olympics Medal Pointshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uKvbguVQYh4.mp412345678910111213141516171819202122232425262728293031323334353637383940import numpyfrom pandas import DataFrame, Seriesdef numpy_dot(): &apos;&apos;&apos; Imagine a point system in which each country is awarded 4 points for each gold medal, 2 points for each silver medal, and one point for each bronze medal. Using the numpy.dot function, create a new dataframe called &apos;olympic_points_df&apos; that includes: a) a column called &apos;country_name&apos; with the country name b) a column called &apos;points&apos; with the total number of points the country earned at the Sochi olympics. You do not need to call the function in your code when running it in the browser - the grader will do that automatically when you submit or test it. &apos;&apos;&apos; countries = [&apos;Russian Fed.&apos;, &apos;Norway&apos;, &apos;Canada&apos;, &apos;United States&apos;, &apos;Netherlands&apos;, &apos;Germany&apos;, &apos;Switzerland&apos;, &apos;Belarus&apos;, &apos;Austria&apos;, &apos;France&apos;, &apos;Poland&apos;, &apos;China&apos;, &apos;Korea&apos;, &apos;Sweden&apos;, &apos;Czech Republic&apos;, &apos;Slovenia&apos;, &apos;Japan&apos;, &apos;Finland&apos;, &apos;Great Britain&apos;, &apos;Ukraine&apos;, &apos;Slovakia&apos;, &apos;Italy&apos;, &apos;Latvia&apos;, &apos;Australia&apos;, &apos;Croatia&apos;, &apos;Kazakhstan&apos;] gold = [13, 11, 10, 9, 8, 8, 6, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] silver = [11, 5, 10, 7, 7, 6, 3, 0, 8, 4, 1, 4, 3, 7, 4, 2, 4, 3, 1, 0, 0, 2, 2, 2, 1, 0] bronze = [9, 10, 5, 12, 9, 5, 2, 1, 5, 7, 1, 2, 2, 6, 2, 4, 3, 1, 2, 1, 0, 6, 2, 1, 0, 1] # YOUR CODE HERE olympic_medal_counts = &#123;&apos;country_name&apos;:countries, &apos;gold&apos;: Series(gold), &apos;silver&apos;: Series(silver), &apos;bronze&apos;: Series(bronze)&#125; df = DataFrame(olympic_medal_counts) df[&apos;points&apos;] = df[[&apos;gold&apos;,&apos;silver&apos;,&apos;bronze&apos;]].dot([4, 2, 1]) olympic_points_df = df[[&apos;country_name&apos;,&apos;points&apos;]] return olympic_points_df https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EQMUh4Id4Po.mp4 numpy which will allow us to process large amount of numerical data. pandas which allow us to store large datasets and extract information from them. Numpy Playground Pandas Playground – Series Pandas Playground – Dataframe Pandas Playground - Indexing Dataframes map()andapplymap()create a new Series or DataFrame by applying the lambda function to each element. Note thatmap()can only be used on a Series to return a new Series andapplymap()can only be used on a DataFrame to return a new DataFrame. Here is a link to the pandas documentation. Here’s also an excellent series of tutorials as IPython notebooks intro-to-pandas-data-structures DATA MODELINGscikit-learn Tutorial windows + r typepip show scikit-learn --version,I have installed scikit-learn typepip install --upgrade &#39;scikit-learn&gt;=0.17,&lt;0.18&#39;It shows系统找不到指定的文件。 typeconda -c install scikit-learn=0.17It showsconda-script.py: error: unrecognized arguments: -c opengit bash typepip install --upgrade &#39;scikit-learn&gt;=0.17,&lt;0.18&#39;,success typeconda -c install scikit-learn=0.17failedconda-script.py: error: unrecognized arguments: -c Introduction to scikit-learnScikit-LearnScikit-learn is an open source Machine Learning library that is built on NumPy, SciPy and matplotlib. It uses a Python interface and supports various regression, classification and clustering algorithms. You will be using this library throughout this program to implement in your projects. Example using Scikit-learnTo show you how we can leverage sklearn(short for scikit-learn), here is an example of a simple Linear Regression Classifier being used to make predictions on the Boston housing prices dataset which comes as a preloaded dataset with sklearn. We will not dive deep into the dataset per se, nor will we split our dataset into training and testing splits just yet(you will learn about the importance of this in the next lesson), the goal of this node is to give you a high level view of how with just a few lines of code, you can make predictions on a dataset using the sklearn tool. This data sets consists of 506 samples with a dimensionality of 13. We will run a Linear Regression classifier on the feature set to make predictions on the prices. We start by getting the necessary imports. from sklearn import datasets # sklearn comes with a variety of preloaded datasets from sklearn import metrics # calculate how well our model is doing from sklearn.linear_model import LinearRegression There are several ways in which we can load datasets in sklearn. For now, we will start the most basic way using a dataset which is pre loaded. # Load the dataset housing_data = datasets.load_boston() We now define the model we want to use and herein lies one of the main advantages of using this library. linear_regression_model = LinearRegression() Next, we can fit our Linear Regression model on our feature set to make predictions for our labels(the price of the houses). Here, housing_data.data is our feature set and housing_data.target are the labels we are trying to predict. linear_regression_model.fit(housing_data.data, housing_data.target)Once our model is fit, we make predictions as follows: predictions = linear_regression_model.predict(housing_data.data) Lastly, we want to check how our model does by comparing our predictions with the actual label values. Since this is a regression problem, we will use the r2 score metric. You will learn about the various classification and regression metrics in future lessons. score = metrics.r2_score(housing_data.target, predictions) And there we have it. We have trained a regression model on a dataset and calculated how well our model does all with just a few lines of code and with all the math abstracted from us. In the next nodes, we will walk you through installing sklearn on your system, and you will work with Katie on a sample problem. scikit-learn Installationscikit-learn InstallationFirst, check that you have a working python installation. Udacity uses python 2.7 for our code templates and in-browser exercises. We recommend using pip to install packages. First get and install pip from here. If you are using Anaconda, you can also use the conda command to install packages. To install scikit-learn via pip or anaconda: open your terminal (terminal on a mac or cmd on a PC) install sklearn with the command: pip install scikit-learn or conda install scikit-learn If you do not use pip or conda, further installation instructions can be found here. Important note about scikit-learn versioningscikit-learn has recently come out with a stable release of its library with version v0.18. With this version comes a few changes to some of the functions we will talk about extensively in this course, such as train_test_split, gridSearchCV, ShuffleSplit, and learning_curves. The documentation available on scikit-learn’s website will reference v0.18, however Katie, Udacity’s quizzes, and our projects, are still written in v0.17. Please make sure that when using the documentation and scikit-learn, you reference version v0.17 and not version v0.18. In the near future, we will be updating our content to match the most current version. Please see this forum post that provides more detail on this topic. If you have any additional questions or concerns, feel free to discuss them in the forums or email machine-support@udacity.com. If you’ve accidentally installed version v0.18 through pip, not to worry! Use the command below to downgrade your scikit-learn version to v0.17: pip install --upgrade &apos;scikit-learn&gt;=0.17,&lt;0.18&apos; If you are using the Anaconda distribution of Python and have scikit-learn installed as version v0.18, you can also use the command below to downgrade your scikit-learn version to v0.17: conda -c install scikit-learn=0.17 scikit-learn CodeIn this next section Katie will walk through using the scikit-learn (or sklearn) documentation with a Gaussian Naive Bayes model. For this exercise it is not important to know all of the details of Naive Bayes or the code Katie is demonstrating. Focus on taking in the basic layout of sklearn, which we can then use to evaluate and validate any data model. We will cover Naive Bayes along with other useful supervised models in much more detail in the upcoming Supervised Machine Learning course and use what we learn from this course to evaluate each model’s strengths and weaknesses. If you want a sneak peak into Naive Bayes, you can check out the documentation here. Getting Started With sklearnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/olGPVtH7KGU.mp4 Gaussian NB Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wpnDwiqTCJA.mp4 GaussianNB Deployment on Terrain Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VBs6D4ggnYY.mp4Quiz: GaussianNB Deployment On Terrain DataTo find the ClassifyNB.py script that you need to update for the quiz, you can click on the dropdown in the classroom code editor to get a list of files that will be used. In the quiz that follows, the line that readspred = clf.predict(features_test)is not necessary for drawing the decision boundary, at least as we’ve written the code. However, the whole point of making a classifier is that you can make predictions with it, so be sure to keep it in mind since you’ll be using it in the quiz after this one. 1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/python&quot;&quot;&quot; Complete the code in ClassifyNB.py with the sklearn Naive Bayes classifier to classify the terrain data. The objective of this exercise is to recreate the decision boundary found in the lesson video, and make a plot that visually shows the decision boundary &quot;&quot;&quot;from prep_terrain_data import makeTerrainDatafrom class_vis import prettyPicture, output_imagefrom ClassifyNB import classifyimport numpy as npimport pylab as plfeatures_train, labels_train, features_test, labels_test = makeTerrainData()### the training data (features_train, labels_train) have both &quot;fast&quot; and &quot;slow&quot; points mixed### in together--separate them so we can give them different colors in the scatterplot,### and visually identify themgrade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]# You will need to complete this function imported from the ClassifyNB script.# Be sure to change to that code tab to complete this quiz.clf = classify(features_train, labels_train)### draw the decision boundary with the text points overlaidprettyPicture(clf, features_test, labels_test)output_image(&quot;test.png&quot;, &quot;png&quot;, open(&quot;test.png&quot;, &quot;rb&quot;).read()) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TcSnd3_hAy8.mp4 Nature of Data one-hot encoding This article describes seven different possible encodings of categorical data. find the answer of One-Hot Encoding in forum that I have written Enron Email Dataset Data Types 1 - Numeric Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xmuWRRTPS4k.mp4 Data Types 2 - Categorical Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/uc_FRItxRMs.mp4 Data Types 3 - Time Series Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6OmiG5zzZoA.mp4 Treatment of categorical dataMany algorithms assume that input data is numerical. For categorical data, this often means converting categorical data into numerical data that represents the same patterns. One standard way of doing this is with one-hot encoding. There are built-in methods for this in scikit-learn. Essentially, a categorical feature with 3 possible values is converted into three binary features corresponding to the values. The new feature corresponding to the category a datum belongs to has value 1, while the other new features have value 0. For example, in a dataset on baseball players, one feature might be “Handedness” which can take values “left” or “right”. Then the data: Joe Handedness: right Jim Handedness: leftWould become: Joe Handedness/right: 1 Handedness/left: 0 Jim Handedness/right: 0 Handedness/left: 1For ordinal data, it often makes sense to simply assign the values to integers. So the following data: JoeSkill: lowJimSkill: mediumJaneSkill: highWould become: JoeSkill: 0JimSkill: 1JaneSkill: 2These approaches are not all that is possible. However in general these simple approaches suffice, and if there is a reason to use another encoding that will be subject to the nature of the data. Encoding using sklearnEncoding in sklearn is done using the preprocessing module which comes with a variety of options of manipulating data before going into the analysis of data. We will focus on two forms of encoding for now, the LabelEncoder and the OneHotEncoder. Label EncoderFirst, we have to import the preprocessing library. from sklearn import preprocessing Let’s create a dummy dataframe named data with a column whose values we want to transform from categories to integers. # creating sample data sample_data = {&apos;name&apos;: [&apos;Ray&apos;, &apos;Adam&apos;, &apos;Jason&apos;, &apos;Varun&apos;, &apos;Xiao&apos;], &apos;health&apos;:[&apos;fit&apos;, &apos;slim&apos;, &apos;obese&apos;, &apos;fit&apos;, &apos;slim&apos;]} # storing sample data in the form of a dataframe data = pandas.DataFrame(sample_data, columns = [&apos;name&apos;, &apos;health&apos;]) We have 3 different labels that we are looking to categorize: slim, fit, obese. To do this, we will call LabelEncoder() and fit it to the column we are looking to categorize. label_encoder = preprocessing.LabelEncoder() label_encoder.fit(data[&apos;health&apos;]) Once you have fit the label encoder to the column you want to encode, you can then transform that column to integer data based on the categories found in that column. That can be done as follows: label_encoder.transform(data[&apos;health&apos;]) This will give you the output: array([0, 2, 1, 0, 2]) You can combine the fit and transform statements above by using label_encoder.fit_transform(data[&#39;health&#39;]). The string categorical health data has been mapped as follows: fit : 0 obese: 1 slim: 2 One thing to keep in mind when encoding data is the fact that you do not want to skew your analysis because of the numbers that are assigned to your categories. For example, in the above example, slim is assigned a value 2 and obese a value 1. This is not to say that the intention here is to have slim be a value that is empirically twice is likely to affect your analysis as compared to obese. In such situations it is better to one-hot encode your data as all categories are assigned a 0 or a 1 value thereby removing any unwanted biases that may creep in if you simply label encode your data. One-hot EncoderIf we were to apply the one-hot transformation to the same example we had above, we’d do it in Pandas using get_dummies as follows: pandas.get_dummies(data[&apos;health&apos;]) We could do this in sklearn on the label encoded data using OneHotEncoder as follows: ohe = preprocessing.OneHotEncoder() # creating OneHotEncoder object label_encoded_data = label_encoder.fit_transform(data[&apos;health&apos;]) ohe.fit_transform(label_encoded_data.reshape(-1,1)) One-Hot Encoding12345678910111213141516171819202122232425262728293031323334353637383940# In this exercise we&apos;ll load the titanic data (from Project 0)# And then perform one-hot encoding on the feature namesimport numpy as npimport pandas as pd# Load the datasetX = pd.read_csv(&apos;titanic_data.csv&apos;)# Limit to categorical dataX = X.select_dtypes(include=[object])from sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import OneHotEncoder# TODO: Create a LabelEncoder object, which will turn all labels present in# in each feature to numbers. For example, the labels [&apos;cat&apos;, &apos;dog&apos;, &apos;fish&apos;]# might be transformed into [0, 1, 2]le = LabelEncoder()# TODO: For each feature in X, apply the LabelEncoder&apos;s fit_transform# function, which will first learn the labels for the feature (fit)# and then change the labels to numbers (transform). for feature in X: le.fit(X[feature]) X[feature] = le.transform(X[feature])# TODO: Create a OneHotEncoder object, which will create a feature for each# label present in the data. For example, for a feature &apos;animal&apos; that had# the labels [&apos;cat&apos;,&apos;dog&apos;,&apos;fish&apos;], the new features (instead of &apos;animal&apos;) # could be [&apos;animal_cat&apos;, &apos;animal_dog&apos;, &apos;animal_fish&apos;]ohe = OneHotEncoder()# TODO: Apply the OneHotEncoder&apos;s fit_transform function to all of X, which will# first learn of all the (now numerical) labels in the data (fit), and then# change the data to one-hot encoded entries (transform).ohe.fit(X)onehotlabels = ohe.transform(X).toarray() Quiz: One-Hot EncodingHaving trouble? Here is a useful forum discussion about this quiz.Here are some other links you may find helpful - LabelEncoder, OneHotEncoder Time series data leakageWhen dealing with time-series data, it can be tempting to simply disregard the timing structure and simply treat it as the appropriate form of categorical or numerical data. One important concern, however, is that if you are building a predictive project looking at forecasting future data points. In this case, it is important NOT to use the future as a source of information! Since “hindsight is 20/20” and retrodictions are much easier than predictions, in predictive tasks it’s generally a good idea to use a training set made up of data from before a certain point, a validation set of data from some dates beyond that, and testing data leading up to the present. This way your algorithm won’t overfit by learning future trends. A Hands-on ExampleIn the next section we’ll explore the famous Enron Email Dataset which was the focus of much of the Introduction to Machine Learning course. While this specific dataset will play a less central role in this Nanodegree program, we will return to it a few times as an example to get practice with various techniques as they are introduced. You can download our copy of the dataset here, along with the starting code for a variety of mini-projects. None of these mini-projects are required for completing the Nanodegree program, but they are great practice! #### Datasets and QuestionsIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TdopVWltgqM.mp4 What Is A POIhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wDQhif-MWuY.mp4 EVALUATION AND VALIDATIONTraining &amp; Testing Train/Test Split in sklearn Benefits of Testinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7LGaeYfvRug.mp4 Features and LabelsAs you continue your journey into designing ML algorithms using sklearn, you will come across two new terms namely, Features and Labels. Features are individual measurable properties that you will be using to make predictions about your labels. To understand this better, let’s use an example. Let’s say you are trying to design a model that will be able to predict whether you will like a particular kind of cuisine or not. For this case, the label is a Yes for when the model thinks you will like said cuisine and No for when it thinks otherwise. The features here could be things like Sweetness, Spicyness, Bitterness, Tangyness and the like. One thing to note here is that when using our features we have to make sure that they are represented in a way that doesn’t skew one feature over another, in other words it’s usually a good idea to normalize or standardize your features; you will learn about these concepts in future lessons. For now, as long as you understand the premise of what features and labels are and how they are used, you can proceed to the next node where Sebastian will explain this concept using a visual example. Features and Labels Musical Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rnv0-lG9yKU.mp4 Evaluation Metrics Classification metrics Accuracy Confusion Matrix F1 ScoreF1 = 2 * (precision * recall) / (precision + recall) Regression metrics mean absolute error mean squared error R2 score explained variance score Welcome to Evaluation Metrics Lessonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IHuFWRM9f9Q.mp4 Overview of Lessonn this lesson we’ll look at a small selection of common performance metrics and evaluate some algorithms with them on the Titanic dataset you used earlier. There are a few important things to keep in mind here: There is a big difference in performance based on whether a train/test split is used. In general, performance on all metrics is correlated. But some algorithms may end up doing better or worse in different situations. The practical coding of any metric looks almost exactly the same! The difficulty comes in how to make the choice, not in how to implement it. The topics covered in this lesson are: Accuracy Precision Recall Confusion Matrix F1 score Mean Absolute Error Mean Squared Error If you are familiar with these concepts you can skip ahead, but we do recommend completing this lesson as a refresher nonetheless. MANAGING ERROR AND COMPLEXITYCauses of Error matplotlib.pyplot.plot 12plt.plot(training_sizes,training_scores,&apos;go&apos;,label=&apos;training_scores&apos;)plt.plot(training_sizes,testing_scores,&apos;rs&apos;,label=&apos;testing_scores&apos;) Bias-Variance Tradeoff Causes of ErrorNow that we have covered some basic metrics for measuring model performance, let us turn our attention to reasons why models exhibit errors in the first place. In model prediction there are two main sources of errors that a model can suffer from. BiasBias due to a model being unable to represent the complexity of the underlying data. A high Bias model is said to underfit the data. VarianceVariance due to a model being overly sensitive to the limited data it has been trained on. A high Variance model is said to overfit the data. In the coming videos, we will go over each in detail. Error due to BiasError due to Bias - Accuracy and UnderfittingBias occurs when a model has enough data but is not complex enough to capture the underlying relationships. As a result, the model consistently and systematically misrepresents the data, leading to low accuracy in prediction. This is known as underfitting. Simply put, bias occurs when we have an inadequate model. Example 1An example might be when we have objects that are classified by color and shape, for example easter eggs, but our model can only partition and classify objects by color. It would therefore consistently mislabel future objects–for example labeling rainbows as easter eggs because they are colorful. Example 2Another example would be continuous data that is polynomial in nature, with a model that can only represent linear relationships. In this case it does not matter how much data we feed the model because it cannot represent the underlying relationship. To overcome error from bias, we need a more complex model. Error due to VarianceError due to Variance - Precision and OverfittingWhen training a model, we typically use a limited number of samples from a larger population. If we repeatedly train a model with randomly selected subsets of data, we would expect its predictons to be different based on the specific examples given to it. Here variance is a measure of how much the predictions vary for any given test sample. Some variance is normal, but too much variance indicates that the model is unable to generalize its predictions to the larger population. High sensitivity to the training set is also known as overfitting, and generally occurs when either the model is too complex or when we do not have enough data to support it. We can typically reduce the variability of a model’s predictions and increase precision by training on more data. If more data is unavailable, we can also control variance by limiting our model’s complexity. Learning CurveLearning CurveNow that you have understood the Bias and Variance concepts let us learn about ways we can identify when our model performs well. The Learning Curve functionality from sklearn can help us in this respect. It allows us to study the behavior of our model with respect to the number of data points being considered to understand if our model is performing well or not. To start with , we have to import the module: from sklearn.learning_curve import learning_curve # sklearn 0.17 from sklearn.model_selection import learning_curve # sklearn 0.18 From the documentation, a reasonable implementation of the function would be as follows: learning_curve( estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) Here, estimator is the model which we are using to make our predictions with, for example it could be defined as GaussianNB(), X and y are the features and label respectively, cv is the cross validation generator, for example KFold(), n_jobs is the parameter that decides if we want to run multiple operations in parallel and train_sizes is the number of training examples that will be considered to generate the curve. In the following quiz, you will define your learning curve for a model that we have designed for you and you will observe the results. Noisy Data, Complex ModelHere’s the code123456789101112131415161718192021222324252627282930313233343536373839404142# In this exercise we&apos;ll examine a learner which has high variance, and tries to learn# nonexistant patterns in the data.# Use the learning curve function from sklearn.learning_curve to plot learning curves# of both training and testing error.from sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as pltfrom sklearn.learning_curve import learning_curvefrom sklearn.cross_validation import KFoldfrom sklearn.metrics import explained_variance_score, make_scorerimport numpy as np# Set the learning curve parameters; you&apos;ll need this for learning_curvessize = 1000cv = KFold(size,shuffle=True)score = make_scorer(explained_variance_score)# Create a series of data that forces a learner to have high varianceX = np.round(np.reshape(np.random.normal(scale=5,size=2*size),(-1,2)),2)y = np.array([[np.sin(x[0]+np.sin(x[1]))] for x in X])def plot_curve(): reg = DecisionTreeRegressor() reg.fit(X,y) print &quot;Regressor score: &#123;:.4f&#125;&quot;.format(reg.score(X,y)) # TODO: Use learning_curve imported above to create learning curves for both the # training data and testing data. You&apos;ll need &apos;size&apos;, &apos;cv&apos; and &apos;score&apos; from above. training_sizes, training_scores, testing_scores = learning_curve(DecisionTreeRegressor(),X,y, cv=cv, scoring=score) # TODO: Plot the training curves and the testing curves # Use plt.plot twice -- one for each score. Be sure to give them labels! plt.plot(training_sizes,training_scores,&apos;go&apos;,label=&apos;training_scores&apos;) plt.plot(training_sizes,testing_scores,&apos;rs&apos;,label=&apos;testing_scores&apos;) # Plot aesthetics plt.ylim(-0.1, 1.1) plt.ylabel(&quot;Curve Score&quot;) plt.xlabel(&quot;Training Points&quot;) plt.legend(bbox_to_anchor=(1.1, 1.1)) plt.show() Improving the Validity of a ModelThere is a trade-off in the value of simplicity or complexity of a model given a fixed set of data. If it is too simple, our model cannot learn about the data and misrepresents the data. However if our model is too complex, we need more data to learn the underlying relationship. Otherwise it is very common for a model to infer relationships that might not actually exist in the data. The key is to find the sweet spot that minimizes bias and variance by finding the right level of model complexity. Of course with more data any model can improve, and different models may be optimal. To learn more about bias and variance, we recommend this essay by Scott Fortmann-Roe. In addition to the subset of data chosen for training, what features you use from a given dataset can also greatly affect the bias and variance of your model. Bias, Variance, and Number of Featureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OurfO1ZR2GU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mpYpT6nZVEo.mp4 Bias, Variance &amp; Number of Features Pt 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1lNAvDubBfI.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/X_AS8NBngsk.mp4 Overfitting by Eyehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sJgPnuiHrs8.mp4 Representative Power of a ModelIntroductionCurse of DimensionalityIn this short lesson, we have Charles Isbell, Senior Associate Dean at Georgia Tech School of Computing and Michael Littman, former CS department chair at Rutgers University and current Professor at Brown University teach you about the curse of dimensionality. These videos are from the OMSCS program at Georgia Tech. Curse of Dimensionalityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QZ0DtNFdDko.mp4 Curse of Dimensionality Twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OyPcbeiwps8.mp4 MODEL EVALUATION AND VALIDATION PROJECTPredicting Boston Housing prices.OverviewProject OverviewIn this project, you will apply basic machine learning concepts on data collected for housing prices in the Boston, Massachusetts area to predict the selling price of a new home. You will first explore the data to obtain important features and descriptive statistics about the dataset. Next, you will properly split the data into testing and training subsets, and determine a suitable performance metric for this problem. You will then analyze performance graphs for a learning algorithm with varying parameters and training set sizes. This will enable you to pick the optimal model that best generalizes for unseen data. Finally, you will test this optimal model on a new sample and compare the predicted selling price to your statistics. Project HighlightsThis project is designed to get you acquainted to working with datasets in Python and applying basic machine learning techniques using NumPy and Scikit-Learn. Before being expected to use many of the available algorithms in the sklearn library, it will be helpful to first practice analyzing and interpreting the performance of your model. Things you will learn by completing this project: How to use NumPy to investigate the latent features of a dataset. How to analyze various learning performance plots for variance and bias. How to determine the best-guess model for predictions from unseen data. How to evaluate a model’s performance on unseen data using previous data. Software RequirementsDescriptionThe Boston housing market is highly competitive, and you want to be the best real estate agent in the area. To compete with your peers, you decide to leverage a few basic machine learning concepts to assist you and a client with finding the best selling price for their home. Luckily, you’ve come across the Boston Housing dataset which contains aggregated data on various features for houses in Greater Boston communities, including the median value of homes for each of those areas. Your task is to build an optimal model based on a statistical analysis with the tools available. This model will then be used to estimate the best selling price for your clients’ homes. Software and LibrariesThis project uses the following software and Python libraries: Python 2.7 NumPy pandas scikit-learn (v0.17) matplotlib Jupyter Notebook If you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. Starting the ProjectFor this assignment, you can find the boston_housing folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three files: boston_housing.ipynb: This is the main file where you will be performing your work on the project. housing.csv: The project dataset. You’ll load this data in the notebook. visuals.py: This Python script contains helper functions that create the necessary visualizations. In the Terminal or Command Prompt, navigate to the folder containing the project files, and then use the command jupyter notebook boston_housing.ipynb to open up a browser window or tab to work with your notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the project. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Predicting Boston Housing Prices project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named boston_housing for ease of access: The boston_housing.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionPredicting Boston Housing PricesThe Boston housing market is highly competitive, and you want to be the best real estate agent in the area. To compete with your peers, you decide to leverage a few basic machine learning concepts to assist you and a client with finding the best selling price for their home. Luckily, you’ve come across the Boston Housing dataset which contains aggregated data on various features for houses in Greater Boston communities, including the median value of homes for each of those areas. Your task is to build an optimal model based on a statistical analysis with the tools available. This model will then be used to estimate the best selling price for your clients’ homes. Project FilesFor this assignment, you can find the boston_housing folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Predicting Boston Housing Prices project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named boston_housing for ease of access: The boston_housing.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! PROJECT Predicting Boston Housing Prices project rubric open jupyterwithWindows + rtypecd &lt;path&gt;``&lt;path&gt; Question 9 Do not forget to modify the parameter cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)to fit 0.17 version Project modification statistics with numpy second submit@viadanna third submit Kaggle(stay tuned)Career: Job Search StrategiesOpportunity can come when you least expect it, so when your dream job comes along, you want to be ready! After completing these lessons, be sure to complete the Cover Letter Review project and 1 of the 3 Resume Review projects. If you are a Nanodegree Plus student, Career Content and Career Development Projects are required for graduation. If you are enrolled in a standard Nanodegree program, Career Content and Career Development Projects are optional and do not affect your graduation. JOB SEARCH STRATEGIESBuild Your Resume Cover LetterResume Review (Entry-level)Resume Review (Career Change)Resume Review (Prior Industry Experience)Cover Letter ReviewConduct a Job SearchIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/axcFtHK6If4.mp4 NVIDIAhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/C6Rt9lxMqHs.mp4 Job Search Mindsethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cBk7bno3KS0.mp4 Target Your Application to An Employerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/X9JBzbrkcvs.mp4 Open Yourself Up to Opportunityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1OamTNkk1xM.mp4 Refine Your ResumeConvey Your Skills Conciselyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xnQr3ohml9s.mp4 Effective Resume Componentshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AiFcaHRGdEA.mp4 Resume Structurehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/POM0MqLTj98.mp4 Describe Your Work Experienceshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B1LED4txinI.mp4Description bullet points should convey: Action Numbers Success Resume Reflectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8Cj_tCp8mls.mp4 Resume Reviewhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L3F2BFGYMtI.mp4Types of Resume FormatsResume formats can be split into three categories, depending on the job candidate: Entry-level (about 0-3 years of experience) Career Change (3+ years of work experience, looking to change career paths) Prior Industry Experience (3+ years of work experience; looking to level up in their career path by upskilling) Build Your ResumeResumes are required in job applications and recruiting. The most effective resumes are ones aimed at a specific job. In this project, you will find a job posting and target your resume to those job duties and requirements. Once you complete this project, you’ve successfully applied targeted job search strategies and are ready to look for work! Receive a review of your resume that is tailored to your level of experience. The format and organization of your resume will vary dependent on your experience in the field. To ensure you’re highlighting your most relevant skills, you will submit your resume to 1 of 3 review projects that match your experience level best. Entry-level: For entry-level job applicants with 0-3 years experience in the field. Best suited for applicants who have recently graduated from their formal education and have limited work experience. Career Change: For those seeking a career change with 3+ years experience in an unrelated field. For example, if you’re a teacher looking for work as a data analyst, or even from project management to front-end development. Prior Industry Experience: For applicants with 3+ years of prior experience in a related field. This would include those with experience in software development looking for work in mobile development, or even from data science to machine learning. Project Resources Project Rubrics: Your project will be reviewed by a Udacity Career Reviewer against these rubrics. Entry-level Rubric Career Change Rubric Prior Industry Experience Rubric Project Checklists: Based on the project rubric, this is a handy checklist to use during your resume building. Entry-level Checklist Career Change Checklist Prior Industry Experience Checklist Career Resource Center: Find additional tips and guides on developing your resume. Resume Template Options* Build your own! This will ensure your resume is unique. * [Resume Genius: Resume Templates](https://resumegenius.com/resume-templates) * [Resume Builder](https://www.livecareer.com/resume-builder) Tips for Bullet Points Describe the following in your projects and experiences bullet points: Action Numbers Results UC Berkeley Action Verb List for Resumes &amp; Cover Letters Submit Your Resume for ReviewSubmission Instructions Find a job posting that you would apply to now or after your Nanodegree graduation. Judge if you would be a good fit for the role. (Note: If you’re more than 75% qualified for the job on paper, you’re probably a good candidate and should give applying a shot!) Refine your resume to target it to that job posting. Copy and paste, or link, the job posting in “Notes to reviewer” during submission. Optional: Remove any sensitive information, such as your phone number, from the submission. Submit your targeted resume as a .pdf to one of the following project submission pages dependent on your experience: Entry-level Project Submission Career Change Project Submission Prior Industry Experience Project Submission Share your Resume with Udacity Hiring PartnersUdacity partners with employers, who are able to contact Udacity students and alumni via your Professional Profile. Once you’ve completed the resume review project, make sure to upload your reviewed resume to your Profile! #### Supervised LearningLearn how Supervised Learning models such as Decision Trees, SVMs, Neural Networks, etc. are trained to model and predict labeled data. Project: Finding Donors for CharityML For most students, this project takes approximately 8 - 21 hours to complete (about 1 - 3 weeks).17 LESSONS, 1 PROJECT P2 Finding Donors for CharityML SUPERVISED LEARNING TASKS polyfit sklearn.preprocessing.PolynomialFeatures DECISION TREESID3(stay tuned) ARTIFICIAL NEURAL NETWORKSSUPPORT VECTOR MACHINESNONPARAMETRIC MODELSBAYESIAN METHODS sklearn.naive_bayes.GaussianNB sklearn.metrics.accuracy_score this Kaggle project Joint Distribution Analysis(stay tuned) ENSEMBLE OF LEARNERS weak leanrner boosting-survey INTRODUCTION TO SUPERVISED LEARNINGSupervised Learning IntroSupervised Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Vc6KuGcfVPM.mp4 What You’ll Watch and Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zoN8QJYFka4.mp4 ML in The Google Self-Driving Carhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lL16AQItG1g.mp4 Supervised Learning What You’ll Dohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jgYqzo7UFsU.mp4 Acerous Vs. Non-Aceroushttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TeFF9wXiFfs.mp4 Supervised Classification Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/buxApBhZCO0.mp4 Features and Labels Musical Examplehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rnv0-lG9yKU.mp4 Features Visualization Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t0iflCpBUDA.mp4 Classification By Eyehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xeMDpSRTLWc.mp4 Introduction to RegressionMore RegressionsIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_CznJ6phPsg.mp4 Parametric regressionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6EC1w_fs5u8.mp4 K nearest neighborhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CWCLQ6eu2Do.mp4 How to predicthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/go7ITLl79h8.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/r8PsDjf9scc.mp4 Kernel regressionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ZhJTGBbR18o.mp4 Parametric vs non parametrichttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wKT8Ztzt6r0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PVOWHYJV8P4.mp4 Which problems are regression? Are Polynomials Linear? Regressions in sklearnContinuous Output Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/udJvijJvs1M.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FOwEL4S-SVo.mp4 Continuous Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Bp6oBbLw8qE.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IC-fo_A0PxQ.mp4 DECISION TREESDecision TreesDifference between Classification and Regressionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i04Pfrb71vk.mp4 More Decision TreeLinearly Separable Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lCWGV6ZuXt0.mp4 Multiple Linear Questionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/t1Y-nzgI1L4.mp4 ARTIFICIAL NEURAL NETWORKSNeural NetworksNeural Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/L_6idb3ZXB0.mp4 Neural Nets Mini-ProjectIntroductionThis section has a mix of coding assignments, multiple choice questions and fill in the blank type questions. Please do check the instructor notes as we have included relevant forum posts that will help you work through these problems. You can find the instructor notes below the text/video nodes in the classroom. Build a Perceptron12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# ----------# # In this exercise, you will put the finishing touches on a perceptron class.## Finish writing the activate() method by using np.dot to compute signal# strength and then add in a threshold for perceptron activation.## ----------import numpy as npclass Perceptron: &quot;&quot;&quot; This class models an artificial neuron with step activation function. &quot;&quot;&quot; def __init__(self, weights = np.array([1]), threshold = 0): &quot;&quot;&quot; Initialize weights and threshold based on input arguments. Note that no type-checking is being performed here for simplicity. &quot;&quot;&quot; self.weights = weights self.threshold = threshold def activate(self,inputs): &quot;&quot;&quot; Takes in @param inputs, a list of numbers equal to length of weights. @return the output of a threshold perceptron with given inputs based on perceptron weights and threshold. &quot;&quot;&quot; # INSERT YOUR CODE HERE # TODO: calculate the strength with which the perceptron fires strength = np.dot(inputs,self.weights) # TODO: return 0 or 1 based on the threshold if strength&gt;self.threshold: result = 1 else: result = 0 return resultdef test(): &quot;&quot;&quot; A few tests to make sure that the perceptron class performs as expected. Nothing should show up in the output if all the assertions pass. &quot;&quot;&quot; p1 = Perceptron(np.array([1, 2]), 0.) assert p1.activate(np.array([ 1,-1])) == 0 # &lt; threshold --&gt; 0 assert p1.activate(np.array([-1, 1])) == 1 # &gt; threshold --&gt; 1 assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --&gt; 0if __name__ == &quot;__main__&quot;: test() Here is relevant forum post for this quiz. Note that here, and the rest of the mini-project, that signal strength equal to the threshold results in a 0 being output (rather than 1). It is required that the dot product be strictly greater than the threshold, rather than greater than or equal to the threshold, to pass the assertion tests. Threshold Meditation Where to train Perceptrons Perceptron Inputs Neural Net Outputs Perceptron Update Rule123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# ----------## In this exercise, you will update the perceptron class so that it can update# its weights.## Finish writing the update() method so that it updates the weights according# to the perceptron update rule. Updates should be performed online, revising# the weights after each data point.# # ----------import numpy as npclass Perceptron: &quot;&quot;&quot; This class models an artificial neuron with step activation function. &quot;&quot;&quot; def __init__(self, weights = np.array([1]), threshold = 0): &quot;&quot;&quot; Initialize weights and threshold based on input arguments. Note that no type-checking is being performed here for simplicity. &quot;&quot;&quot; self.weights = weights.astype(float) self.threshold = threshold def activate(self, values): &quot;&quot;&quot; Takes in @param values, a list of numbers equal to length of weights. @return the output of a threshold perceptron with given inputs based on perceptron weights and threshold. &quot;&quot;&quot; # First calculate the strength with which the perceptron fires strength = np.dot(values,self.weights) # Then return 0 or 1 depending on strength compared to threshold return int(strength &gt; self.threshold) def update(self, values, train, eta=.1): &quot;&quot;&quot; Takes in a 2D array @param values consisting of a LIST of inputs and a 1D array @param train, consisting of a corresponding list of expected outputs. Updates internal weights according to the perceptron training rule using these values and an optional learning rate, @param eta. &quot;&quot;&quot; # YOUR CODE HERE #self.weights = np.transpose(np.zeros(values.shape)) # TODO: for each data point... for i in range(len(values)): # TODO: obtain the neuron&apos;s prediction for that point prediction = self.activate(values[i]) # TODO: update self.weights based on prediction accuracy, learning # rate and input value self.weights += eta * (train[i] - prediction) * values[i] print values print self.weightsdef test(): &quot;&quot;&quot; A few tests to make sure that the perceptron class performs as expected. Nothing should show up in the output if all the assertions pass. &quot;&quot;&quot; def sum_almost_equal(array1, array2, tol = 1e-6): return sum(abs(array1 - array2)) &lt; tol p1 = Perceptron(np.array([1,1,1]),0) p1.update(np.array([[2,0,-3]]), np.array([1])) assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7])) p2 = Perceptron(np.array([1,2,3]),0) p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0])) assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9])) p3 = Perceptron(np.array([3,0,2]),0) p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0])) assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))if __name__ == &quot;__main__&quot;: test() This is relevant forum post for this quiz. Layered Network Example Linear Representational Power Activation Function Quiz Perceptron Vs Sigmoid Sigmoid Learning Gradient Descent Issues SUPPORT VECTOR MACHINESMath behind SVMsIntroductionIn this lesson, Charles and Mike will walk you through the Math behind Support Vector Machines. If you would like to jump straight to the higher level concepts and start coding it up using scikit-learn, you can head to the next lesson where Sebastian and Katie will walk you through everything you will need to get up and running with working SVM model. The Best Linehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5yzSv4jYMyI.mp4 SVMs in PracticeWelcome to SVMhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gnAmmyQ_ZcQ.mp4 Separating Linehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mzKPXz-Yhwk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NTm_mA4akP4.mp4 NONPARAMETRIC MODELSInstance Based LearningInstance Based Learning Beforehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ZTjot416e-g.mp4 BAYESIAN METHODSNaive BayesSpeed Scatterplot: Grade and Bumpinesshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IMWsjjIeOrY.mp4 Bayesian LearningIntrohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PfJoHBLjkR8.mp4 Bayes Rulehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kv38EnIXSkY.mp4 Bayesian InferenceIntrohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AL9LH06uztM.mp4 Joint Distributionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RN7drTE2_oI.mp4 Bayes NLP Mini-ProjectBad Handwriting ExpositionImagine your boss has left you a message from a location with terrible reception. Several words are impossible to hear. Based on some transcriptions of previous messages he’s left, you want to fill in the remaining words. To do this, we will use Bayes’ Rule to find the probability that a given word is in the blank, given some other information about the message. Recall Bayes Rule: P(A|B) = P(B|A)*P(A)/P(B) Or in our case P(a certain word|surrounding words) = P(surrounding words|a certain word)*P(a certain word) / P(surrounding words) Calculations Maximum LikelihoodHere’s the code12345678910111213141516171819202122232425262728293031323334sample_memo = &apos;&apos;&apos;Milt, we&apos;re gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.Oh, oh, and I almost forgot. Ahh, I&apos;m also gonna need you to go ahead and come in on Sunday, too...Hello Peter, whats happening? Ummm, I&apos;m gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I&apos;m also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.&apos;&apos;&apos;## Maximum Likelihood Hypothesis### In this quiz we will find the maximum likelihood word based on the preceding word## Fill in the NextWordProbability procedure so that it takes in sample text and a word,# and returns a dictionary with keys the set of words that come after, whose values are# the number of times the key comes after that word.# # Just use .split() to split the sample_memo text into words separated by spaces.def NextWordProbability(sampletext,word): wordlist = sampletext.split() if word in wordlist: indecies = [i for i,x in enumerate(wordlist) if x == word] else: pass indecies_after = [i+1 for i in indecies] newwordlist = [wordlist[i] for i in indecies_after] wordcount = &#123;&#125; for word in newwordlist: if word in wordcount: wordcount[word] += 1 else: wordcount[word] = 1 return wordcount NLP DisclaimerIn the previous exercise, you may have thought of some ways we might want to clean up the text available to us. For example, we would certainly want to remove punctuation, and generally want to make all strings lowercase for consistency. In most language processing tasks we will have a much larger corpus of data, and will want to remove certain features. Overall, just keep in mind that this mini-project is about Bayesian probability. If you’re interested in the details of language processing, you might start with this Kaggle project, which introduces a more detailed and standard approach to text processing very different from what we cover here. Optimal Classifier Example Optimal Classifier ExerciseHere’s the code123456789101112131415161718192021222324252627282930313233343536373839404142434445#------------------------------------------------------------------## Bayes Optimal Classifier## In this quiz we will compute the optimal label for a second missing word in a row# based on the possible words that could be in the first blank## Finish the procedurce, LaterWords(), below## You may want to import your code from the previous programming exercise!#sample_memo = &apos;&apos;&apos;Milt, we&apos;re gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.Oh, oh, and I almost forgot. Ahh, I&apos;m also gonna need you to go ahead and come in on Sunday, too...Hello Peter, whats happening? Ummm, I&apos;m gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I&apos;m also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.&apos;&apos;&apos;corrupted_memo = &apos;&apos;&apos;Yeah, I&apos;m gonna --- you to go ahead --- --- complain about this. Oh, and if you could --- --- and sit at the kids&apos; table, that&apos;d be --- &apos;&apos;&apos;data_list = sample_memo.strip().split()words_to_guess = [&quot;ahead&quot;,&quot;could&quot;]def LaterWords(sample,word,distance): &apos;&apos;&apos;@param sample: a sample of text to draw from @param word: a word occuring before a corrupted sequence @param distance: how many words later to estimate (i.e. 1 for the next word, 2 for the word after that) @returns: a single word which is the most likely possibility &apos;&apos;&apos; # TODO: Given a word, collect the relative probabilities of possible following words # from @sample. You may want to import your code from the maximum likelihood exercise. # TODO: Repeat the above process--for each distance beyond 1, evaluate the words that # might come after each word, and combine them weighting by relative probability # into an estimate of what might appear next. return &#123;&#125; print LaterWords(sample_memo,&quot;ahead&quot;,2) Which Words Meditation Joint Distribution Analysis Domain Knowledge Quiz Domain Knowledge Fill In ENSEMBLE OF LEARNERSEnsemble B&amp;BEnsemble Learning Boostinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/w75WyRjRpAg.mp4 Back to Boostinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PHBd2glewzM.mp4 Boosting Tends to Overfithttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UHxYXwvjH5c.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Hp4gJjSFSYc.mp4 SUPERVISED LEARNING PROJECTFinding donors for CharityMLOverviewProject OverviewIn this project, you will apply supervised learning techniques and an analytical mind on data collected for the U.S. census to help CharityML (a fictitious charity organization) identify people most likely to donate to their cause. You will first explore the data to learn how the census data is recorded. Next, you will apply a series of transformations and preprocessing techniques to manipulate the data into a workable format. You will then evaluate several supervised learners of your choice on the data, and consider which is best suited for the solution. Afterwards, you will optimize the model you’ve selected and present it as your solution to CharityML. Finally, you will explore the chosen model and its predictions under the hood, to see just how well it’s performing when considering the data it’s given. Project HighlightsThis project is designed to get you acquainted with the many supervised learning algorithms available in sklearn, and to also provide for a method of evaluating just how each model works and performs on a certain type of data. It is important in machine learning to understand exactly when and where a certain algorithm should be used, and when one should be avoided. Things you will learn by completing this project: How to identify when preprocessing is needed, and how to apply it. How to establish a benchmark for a solution to the problem. What each of several supervised learning algorithms accomplishes given a specific dataset. How to investigate whether a candidate solution model is adequate for the problem. Software RequirementsDescriptionCharityML is a fictitious charity organization located in the heart of Silicon Valley that was established to provide financial support for people eager to learn machine learning. After nearly 32,000 letters sent to people in the community, CharityML determined that every donation they received came from someone that was making more than $50,000 annually. To expand their potential donor base, CharityML has decided to send letters to residents of California, but to only those most likely to donate to the charity. With nearly 15 million working Californians, CharityML has brought you on board to help build an algorithm to best identify potential donors and reduce overhead cost of sending mail. Your goal will be evaluate and optimize several different supervised learners to determine which algorithm will provide the highest donation yield while also reducing the total number of letters being sent. Software and LibrariesThis project uses the following software and Python libraries: Python 2.7NumPypandasscikit-learn (v0.17)matplotlibYou will also need to have software installed to run and execute a Jupyter Notebook. If you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. Starting the ProjectFor this assignment, you can find the finding_donors folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three files: finding_donors.ipynb: This is the main file where you will be performing your work on the project. census.csv: The project dataset. You?ll load this data in the notebook. visuals.py: This Python script provides supplementary visualizations for the project. Do not modify. In the Terminal or Command Prompt, navigate to the folder containing the project files, and then use the command jupyter notebook finding_donors.ipynb to open up a browser window or tab to work with your notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the project. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Finding Donors for CharityML project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named finding_donors for ease of access: The finding_donors.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionFinding Donors for CharityMLCharityML is a fictitious charity organization located in the heart of Silicon Valley that was established to provide financial support for people eager to learn machine learning. After nearly 32,000 letters sent to people in the community, CharityML determined that every donation they received came from someone that was making more than $50,000 annually. To expand their potential donor base, CharityML has decided to send letters to residents of California, but to only those most likely to donate to the charity. With nearly 15 million working Californians, CharityML has brought you on board to help build an algorithm to best identify potential donors and reduce overhead cost of sending mail. Your goal will be evaluate and optimize several different supervised learners to determine which algorithm will provide the highest donation yield while also reducing the total number of letters being sent. Project FilesFor this assignment, you can find the finding_donors folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Finding Donors for CharityML project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named finding_donors for ease of access: The finding_donors.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated.I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! PROJECT windows + r cd + &lt;path&gt; my path is G:\Udacity\MLND\machine-learning-master\projects\finding_donors jupyter notebook finding_donors.ipynb Finding Donors for CharityML project rubric pandas.get_dummies() reviews submit second submit in review second review third review Unsupervised LearningLearn how to find patterns and structures in unlabeled data, perform feature transformations and improve the predictive performance of your models.Project: Creating Customer SegmentsFor most students, this project takes approximately 10 - 15 hours to complete (about 1 - 2 weeks).P3 Creating Customer Segments CLUSTERING play with k-means clustering sklearn.cluster.KMeans Expectation Maximization The Enron dataset sklearn.preprocessing.MinMaxScaler Introduction to Unsupervised LearningUnsupervised Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8oZpT6Hekhk.mp4 What You’ll Watch and Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1a68kAJAgIU.mp4 ClusteringUnsupervised Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Mx9f99bRB3Q.mp4 Clustering Movieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/g8PKffm8IRY.mp4 More ClusteringSingle Linkage Clusteringhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HfikjFVM3dg.mp4Quiz: Single Linkage ClusteringPlease use a comma to separate the two objects that will be linked in your answer. For instance, to describe a link from a to b, write “a,b” as your answer in the box. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vytc9CsjjAs.mp4 Single Linkage Clustering Twohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aojgUed9M0w.mp4 Clustering Mini-ProjectClustering Mini-Project Videohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/68EGMItJiNM.mp4 K-Means Clustering Mini-ProjectIn this project, we’ll apply k-means clustering to our Enron financial data. Our final goal, of course, is to identify persons of interest; since we have labeled data, this is not a question that particularly calls for an unsupervised approach like k-means clustering. Nonetheless, you’ll get some hands-on practice with k-means in this project, and play around with feature scaling, which will give you a sneak preview of the next lesson’s material.The Enron dataset can be found here. Clustering FeaturesThe starter code can be found in k_means/k_means_cluster.py, which reads in the email + financial (E+F) dataset and gets us ready for clustering. You’ll start with performing k-means based on just two financial features–take a look at the code, and determine which features the code uses for clustering. Run the code, which will create a scatterplot of the data. Think a little bit about what clusters you would expect to arise if 2 clusters are created. Deploying ClusteringDeploy k-means clustering on the financial_features data, with 2 clusters specified as a parameter. Store your cluster predictions to a list called pred, so that the Draw() command at the bottom of the script works properly. In the scatterplot that pops up, are the clusters what you expected? FEATURE ENGINEERINGFeature ScalingChris’s T-Shirt Size (Intuition)https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oaqjLyiKOIA.mp4 A Metric for Chrishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/O0bvLU4l0is.mp4 Feature SelectionIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UAMwTr3cnok.mp4 Feature Selectionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8CpRLplmdqE.mp4 DIMENSIONALITY REDUCTION sklearn.decomposition.PCA This paper gives a fairly in-depth look at how the ICA algorithm works. It’s long, but comprehensive Cocktail Party Demo PCAData Dimensionalityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gg7SAMMl4kM.mp4 Trickier Data Dimensionalityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-dcNhrSPmoY.mp4 PCA Mini-ProjectPCA Mini-Project Introhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rR68JXwKBxE.mp4 PCA Mini-ProjectOur discussion of PCA spent a lot of time on theoretical issues, so in this mini-project we’ll ask you to play around with some sklearn code. The eigenfaces code is interesting and rich enough to serve as the testbed for this entire mini-project. The starter code can be found in pca/eigenfaces.py. This was mostly taken from the example found here, on the sklearn documentation.Take note when running the code, that there are changes in one of the parameters for the SVC function called on line 94 of pca/eigenfaces.py. For the ‘class_weight’ parameter, the argument string “auto” is a valid value for sklearn version 0.16 and prior, but will be depreciated by 0.19. If you are running sklearn version 0.17 or later, the expected argument string should be “balanced”. If you get an error or warning when running pca/eigenfaces.py, make sure that you have the correct argument on line 98 that matches your installed version of sklearn. Feature TransformationIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/J9JsMNownYM.mp4 Feature Transformationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B6mPphwAXZk.mp4 SummaryWhat we have learnedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/74oyGTdFp0Y.mp4 UNSUPERVISED LEARNING PROJECTThings you will learn by completing this project: How to apply preprocessing techniques such as feature scaling and outlier detection. How to interpret data points that have been scaled, transformed, or reduced from PCA. How to analyze PCA dimensions and construct a new feature space. How to optimally cluster a set of data to find hidden patterns in a dataset. How to assess information given by cluster data and use it in a meaningful way. jupyter notebook customer_segments.ipynbCreating Customer Segments project rubricsubmit projectreviewsecond reviewseaborn.heatmap() git project: opengit bashand tpyecd &lt;path&gt;with\before space key and/between directory git init git status git add &lt;&gt; git commmit -m &quot;description&quot; create a repo in github git remote add origin URL git push origin master git push origin master git add &lt;&gt;``git commit -m &quot;second submit&quot;``git push origin master Identifying customers by clustering them.OverviewProject OverviewIn this project you will apply unsupervised learning techniques on product spending data collected for customers of a wholesale distributor in Lisbon, Portugal to identify customer segments hidden in the data. You will first explore the data by selecting a small subset to sample and determine if any product categories highly correlate with one another. Afterwards, you will preprocess the data by scaling each product category and then identifying (and removing) unwanted outliers. With the good, clean customer spending data, you will apply PCA transformations to the data and implement clustering algorithms to segment the transformed customer data. Finally, you will compare the segmentation found with an additional labeling and consider ways this information could assist the wholesale distributor with future service changes. Project HighlightsThis project is designed to give you a hands-on experience with unsupervised learning and work towards developing conclusions for a potential client on a real-world dataset. Many companies today collect vast amounts of data on customers and clientele, and have a strong desire to understand the meaningful relationships hidden in their customer base. Being equipped with this information can assist a company engineer future products and services that best satisfy the demands or needs of their customers. Things you will learn by completing this project: How to apply preprocessing techniques such as feature scaling and outlier detection. How to interpret data points that have been scaled, transformed, or reduced from PCA. How to analyze PCA dimensions and construct a new feature space. How to optimally cluster a set of data to find hidden patterns in a dataset. How to assess information given by cluster data and use it in a meaningful way. Software RequirementsDescriptionA wholesale distributor recently tested a change to their delivery method for some customers, by moving from a morning delivery service five days a week to a cheaper evening delivery service three days a week. Initial testing did not discover any significant unsatisfactory results, so they implemented the cheaper option for all customers. Almost immediately, the distributor began getting complaints about the delivery service change and customers were canceling deliveries — losing the distributor more money than what was being saved. You’ve been hired by the wholesale distributor to find what types of customers they have to help them make better, more informed business decisions in the future. Your task is to use unsupervised learning techniques to see if any similarities exist between customers, and how to best segment customers into distinct categories. Software and LibrariesThis project uses the following software and Python libraries: Python 2.7 NumPy pandas scikit-learn (v0.17) matplotlib You will also need to have software installed to run and execute a Jupyter Notebook. If you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. Starting the ProjectFor this assignment, you can find the customer_segments folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three files: customer_segments.ipynb: This is the main file where you will be performing your work on the project. customers.csv: The project dataset. You’ll load this data in the notebook. visuals.py: This Python script provides supplementary visualizations for the project. Do not modify. In the Terminal or Command Prompt, navigate to the folder containing the project files, and then use the command jupyter notebook customer_segments.ipynb to open up a browser window or tab to work with your notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the project. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Creating Customer Segments project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named customer_segments for ease of access: The customer_segments.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionCreating Customer SegmentsA wholesale distributor recently tested a change to their delivery method for some customers, by moving from a morning delivery service five days a week to a cheaper evening delivery service three days a week.Initial testing did not discover any significant unsatisfactory results, so they implemented the cheaper option for all customers. Almost immediately, the distributor began getting complaints about the delivery service change and customers were canceling deliveries — losing the distributor more money than what was being saved. You’ve been hired by the wholesale distributor to find what types of customers they have to help them make better, more informed business decisions in the future. Your task is to use unsupervised learning techniques to see if any similarities exist between customers, and how to best segment customers into distinct categories. Project FilesFor this assignment, you can find the customer_segments folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Creating Customer Segments project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named customer_segments for ease of access: The customer_segments.ipynb notebook file with all questions answered and all code cells executed and displaying output. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! Supporting MaterialsVideos Zip FileCareer: NetworkingIn the following lesson, you will learn how tell your unique story to recruiters in a succinct and professional but relatable way. After completing these lessons, be sure to complete the online profile review projects, such as LinkedIn Profile Review. If you are a Nanodegree Plus student, Career Content and Career Development Projects are required for graduation. If you are enrolled in a standard Nanodegree program, Career Content and Career Development Projects are optional and do not affect your graduation. NETWORKINGDevelop Your Personal BrandWhy Network?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/exjEm9Paszk.mp4 Elevator Pitchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/S-nAHPrkQrQ.mp4 Personal BrandingHow to Stand OutImagine you’re a hiring manager for a company, and you need to pick 5 people to interview for a role. But you get 50 applications, and everyone seems pretty qualified. How do you compare job candidates? You’ll probably pick the candidates that stand out the most to you. Personal StoriesThe thing that always makes a job candidate unique is their personal story - their passion and how they got there. Employers aren’t just looking for someone with the skills, but they’re looking for someone who can drive the company’s mission and will be a part of innovation. That’s why they need to know your work ethic and what drives you. As someone wanting to impress an employer, you need to tell your personal story. You want employers to know how you solve problems, overcome challenges, achieve results. You want employers to know what excites you, what motivates you, what drives you forward. All of this can be achieved through effective storytelling, and effective branding. I’ll let you know I’ve branded and rebranded myself many times. That’s okay - people are complex and have multiple interests that change over time. In this next video, we’ll meet my coworker Chris who will show us how he used personal branding to help him in his recent career change.ResourcesBlog post: Storytelling, Personal Branding, and Getting Hired Meet Chrishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ccflD9x5WU.mp4 ResourcesBlog post: Overcome Imposter Syndrome Elevator Pitchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0QtgTG49E9I.mp4 Pitching to a Recruiterhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LxAdWaA-qTQ.mp4 Use Your Elevator Pitchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e-v60ieggSs.mp4 Optimize Your LinkedIn ProfileWhy LinkedInLinkedIn is the most popular professional networking platform out there, so most recruiters use it to find job seekers. It’s so common for hiring teams to use LinkedIn to find and look at candidates, that it’s almost a red flag if they’re unable to find a LinkedIn profile for you. It’s also a great platform for you to connect with other people in your field. Udacity for example has an Alumni LinkedIn group where graduates can collaborate on projects, practice job interviews, or discuss new trends in the industry together. Connecting with a fellow alum and asking for a referral would increase your chances of getting an interview. Find ConnectionsThe best way to use your LinkedIn effectively, however, is to have over 500 connections. This may seem like a lot, but once you get rolling, you’ll get to that number fast. After you actively start using it it, by joining groups and going to networking events, your number of connections will climb. You are more likely to show up in search results on LinkedIn if you have more connections, which means you’ll be more visible to recruiters. Join GroupsIncreasing the group of people you’re connected with also exposes you to what they’re working on or have done. For example, if you move to a new city, you can search your network to see who lives in the area, and ask for recommendations on apartment hunting, job leads, or other advice on adjusting to life in another city. Also, if you’re active in a LinkedIn group or if you frequently write LinkedIn blog posts, you’ll increase your visibility on the platform and likelihood that a recruiter will find your profile. How to Build Your LinkedIn ProfileLinkedIn guides you well when filling out your profile. It tells you if your profile is strong and offers recommendations on how to improve it. We recommend you follow LinkedIn’s advice because it’ll increase your visibility on the network, thus increasing the number of opportunities you may come across. Tips for an Awesome LinkedIn ProfileIn the lessons on conducting a successful job search and resume writing, we talk about how you can describe your work experiences in a way that targets a specific job. Use what you learn to describe your experiences in LinkedIn’s projects and work sections. You can even copy and paste over the bullet points in your resume to the work or project sections of LinkedIn. Making sure your resume and LinkedIn are consistent helps build your personal brand. Find Other Networking PlatformsRemember that LinkedIn isn’t the only professional networking platform out there. If you do have a great LinkedIn profile, that means you can also build an amazing profile on other platforms. Find some recommendations for online profiles on the Career Resource Center. Up NextBy now, you know how to target your job profile to your dream job. You know how to market yourself effectively through building off your elevator pitch. Being confident in this will help you network naturally, whether on LinkedIn or at an event in-person. Move on to the LinkedIn Profile Review and get personalized feedback on your online presence. Networking Your Way to a New JobCareer and Job Fairs Do’s and Don’tsWhat are career mixers? GitHub Profile Review Project Rubric. Your project will be reviewed by a Udacity Career Reviewer against this rubric. Project checklist. Based on the project rubric, this is a handy checklist on GitHub best practices. Career Resource Center. Find additional tips and guides on developing your GitHub Profile. Rubrics submit reviews Third reviews LinkedIn Profile Review rubrics submit Udacity Professional Profile ReviewReinforcement LearningDUE OCT 19Use Reinforcement Learning algorithms like Q-Learning to train artificial agents to take optimal actions in an environment. Project: Train a Smartcab to Drive For most students, this project takes approximately 15 - 21 hours to complete (about 2 - 3 weeks).P4 Train a Smartcab to Drive Markov Decision Processes Further details on this quiz can be found in Chapter 17 of Artificial Intelligence: A Modern Approach REINFORCEMENT LEARNING Andrew Moore’s slides on Zero-Sum Games Andrew Moore’s slides on Non-Zero-Sum Games This paper offers a summary and an investigation of the field of reinforcement learning. It’s long, but chock-full of information! PROJECTSoftware Requirementspygame123Mac: conda install -c https://conda.anaconda.org/quasiben pygameLinux: conda install -c https://conda.anaconda.org/tlatorre pygameWindows: conda install -c https://conda.anaconda.org/prkrekel pygame Common Problems with PyGame Getting Started PyGame Information Google Group PyGame subreddit use the discussion forums MLND Student Slack Community Train a Smartcab to Drive project rubricsubmitwindows + rtypepip install pygame reviewREINFORCEMENT LEARNINGIntroduction to Reinforcement LearningReinforcement Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PeAHckcWFS0.mp4 What You’ll Watch and Learnhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Z6ATPu4b9nc.mp4 Reinforcement Learning What You’ll Dohttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1vQQphPLnkM.mp4 Markov Decision processesIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_ocNerSvh5Y.mp4 Reinforcement LearningReinforcement Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HeYSFWPX_4k.mp4 Rat Dinosaurshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h7ExhVneBDU.mp4 GAME THEORYGame TheoryGame Theoryhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vYHk1SPpnmQ.mp4 What Is Game Theory?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jwlteKFyiHU.mp4 PROJECTTrain a cab to drive itself.Overview Software RequirementsDescriptionIn the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents, known as smartcabs, to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to depend on smartcabs to get to where they need to go as safely and reliably as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or reliable as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, your task as an employee for a national taxicab company is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and reliability can be achieved. Software RequirementsThis project uses the following software and Python libraries: Python 2.7 NumPy pandas matplotlib PyGameIf you do not have Python installed yet, it is highly recommended that you install the Anaconda distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer. pygame can then be installed using one of the following commands: Mac: conda install -c https://conda.anaconda.org/quasiben pygameLinux: conda install -c https://conda.anaconda.org/tlatorre pygameWindows: conda install -c https://conda.anaconda.org/prkrekel pygame Please note that installing pygame can be done using pip as well. You can run an example to make sure pygame is working before actually performing the project by running: python -m pygame.examples.aliens Common Problems with PyGameFixing Common PyGame ProblemsThe PyGame library can in some cases require a bit of troubleshooting to work correctly for this project. While the PyGame aspect of the project is not required for a successful submission (you can complete the project without a visual simulation, although it is more difficult), it is very helpful to have it working! If you encounter an issue with PyGame, first see these helpful links below that are developed by communities of users working with the library: Getting Started PyGame Information Google Group PyGame subreddit Problems most often reported by students“PyGame won’t install on my machine; there was an issue with the installation.”Solution: As has been recommended for previous projects, Udacity suggests that you are using the Anaconda distribution of Python, which can then allow you to install PyGame through the conda-specific command. “I’m seeing a black screen when running the code; output says that it can’t load car images.”Solution: The code will not operate correctly unless it is run from the top-level directory for smartcab. The top-level directory is the one that contains the README and the project notebook. If you continue to have problems with the project code in regards to PyGame, you can also use the discussion forums to find posts from students that encountered issues that you may be experiencing. Additionally, you can seek help from a swath of students in the MLND Student Slack Community. Starting the ProjectFor this assignment, you can find the smartcab folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains three directories: /logs/: This folder will contain all log files that are given from the simulation when specific prerequisites are met. /images/: This folder contains various images of cars to be used in the graphical user interface. You will not need to modify or create any files in this directory. /smartcab/: This folder contains the Python scripts that create the environment, graphical user interface, the simulation, and the agents. You will not need to modify or create any files in this directory except for agent.py. It also contains two files: smartcab.ipynb: This is the main file where you will answer questions and provide an analysis for your work. -visuals.py: This Python script provides supplementary visualizations for the analysis. Do not modify.Finally, in /smartcab/ are the following four files: Modify: agent.py: This is the main Python file where you will be performing your work on the project. Do not modify: environment.py: This Python file will create the smartcab environment. planner.py: This Python file creates a high-level planner for the agent to follow towards a set goal. simulator.py: This Python file creates the simulation and graphical user interface. Running the CodeIn a terminal or command window, navigate to the top-level project directory smartcab/ (that contains the three project directories) and run one of the following commands: python smartcab/agent.py orpython -m smartcab.agent This will run the agent.py file and execute your implemented agent code into the environment. Additionally, use the command jupyter notebook smartcab.ipynbfrom this same directory to open up a browser window or tab to work with your analysis notebook. Alternatively, you can use the command jupyter notebook or ipython notebook and navigate to the notebook file in the browser window that opens. Follow the instructions in the notebook and answer each question presented to successfully complete the implementation necessary for your agent.py agent file. A README file has also been provided with the project files which may contain additional necessary information or instruction for the project. DefinitionsEnvironmentThe smartcab operates in an ideal, grid-like city (similar to New York City), with roads going in the North-South and East-West directions. Other vehicles will certainly be present on the road, but there will be no pedestrians to be concerned with. At each intersection there is a traffic light that either allows traffic in the North-South direction or the East-West direction. U.S. Right-of-Way rules apply: On a green light, a left turn is permitted if there is no oncoming traffic making a right turn or coming straight through the intersection. On a red light, a right turn is permitted if no oncoming traffic is approaching from your left through the intersection. To understand how to correctly yield to oncoming traffic when turning left, you may refer to this official drivers’ education video, or this passionate exposition. Inputs and OutputsAssume that the smartcab is assigned a route plan based on the passengers’ starting location and destination. The route is split at each intersection into waypoints, and you may assume that the smartcab, at any instant, is at some intersection in the world. Therefore, the next waypoint to the destination, assuming the destination has not already been reached, is one intersection away in one direction (North, South, East, or West). The smartcab has only an egocentric view of the intersection it is at: It can determine the state of the traffic light for its direction of movement, and whether there is a vehicle at the intersection for each of the oncoming directions. For each action, the smartcab may either idle at the intersection, or drive to the next intersection to the left, right, or ahead of it. Finally, each trip has a time to reach the destination which decreases for each action taken (the passengers want to get there quickly). If the allotted time becomes zero before reaching the destination, the trip has failed. Rewards and GoalThe smartcab will receive positive or negative rewards based on the action it as taken. Expectedly, the smartcab will receive a small positive reward when making a good action, and a varying amount of negative reward dependent on the severity of the traffic violation it would have committed. Based on the rewards and penalties the smartcab receives, the self-driving agent implementation should learn an optimal policy for driving on the city roads while obeying traffic rules, avoiding accidents, and reaching passengers’ destinations in the allotted time. Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Train a Smartcab to Drive project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named smartcab for ease of access: The agent.py Python file with all code implemented as required in the instructed tasks. The /logs/ folder which should contain five log files that were produced from your simulation and used in the analysis. The smartcab.ipynb notebook file with all questions answered and all visualization cells executed and displaying results. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionTrain a Smartcab to DriveIn the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents — known as smartcabs — to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to rely on smartcabs to get to where they need to go as safely and efficiently as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or efficient as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, your task as an employee for a national taxicab company is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and efficiency can be achieved. Project FilesFor this assignment, you can find the smartcab folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! EvaluationYour project will be reviewed by a Udacity reviewer against the Train a Smartcab to Drive project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named smartcab for ease of access: Theagent.py Python file with all code implemented as required in the instructed tasks. The /logs/ folder which should contain five log files that were produced from your simulation and used in the analysis. The smartcab.ipynb notebook file with all questions answered and all visualization cells executed and displaying results. An HTML export of the project notebook with the name report.html. This file must be present for your project to be evaluated. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of this page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! Supporting MaterialsVideos Zip File View Submissionsubmission Deep LearningP5 Build a Digit Recognition Program FROM MACHINE LEARNING TO DEEP LEARNINGSOFTWARE AND TOOLSTensorFlow Download and SetupMethod 1: Pre-built Docker container with TensorFlow and all assignmentsTo get started with TensorFlow quickly and work on your assignments, follow the instructions in this README.Note: If you are on a Windows machine, Method 1 is your only option due to lack of native TensorFlow support. (not needed) Check your GPUright click computer-&gt;property-&gt;设备管理器-&gt;显示适配器I use the CPU only method (failed) First try from discussion at Udacity Install Docker Toolbox (you can get it here). I recommend installing every optional package. -&gt;failed Create a virtual machine for your udacity tensorflow work:docker-machine create -d virtualbox --virtualbox-memory 2048 tensorflow In a cmd.exe prompt, runFOR /f &quot;tokens=*&quot; %i IN (&#39;docker-machine env --shell cmd tensorflow&#39;) DO %i Next, rundocker run -p 8888:8888 --name tensorflow-udacity -it b.gcr.io/tensorflow-udacity/assignments:0.5.0 In a browser, go tohttp://192.168.99.100:8888/tree (failed) Second tryI have 2 versions in python, so I will not use this one. Click here and follow the instructions. Download Python 3.5.3 and choose Windows x86-64 executable installer -&gt; install Python3.5.x and add path. windows + r-&gt;pip3 install --upgrade tensorflow (failed) Third try from discussion at Udacitywindows + rohe = preprocessing.OneHotEncoder() # creating OneHotEncoder object label_encoded_data = label_encoder.fit_transform(data[&apos;health&apos;]) ohe.fit_transform(label_encoded_data.reshape(-1,1)) After executing the above steps, I can use tensorflow by selecting the following option in Jupyter notebook: Kernel =&gt; Change kernel =&gt; python [conda env:py35] Note: I used python 2.7 and jupyter notebook for the earlier assignments. (Useful) Forth methodFollow this video and install Ubuntu in Virtualbox. 虚拟硬盘文件保存位置C:\Users\SSQ\VirtualBox VMs\Deep Learning Ubuntu\Deep Learning Ubuntu.vdi location of shared file C:\Users\SSQ\virtualbox share Follow this blog to copy files between host OS and guest OS.for me I usesudo mount -t vboxsf virtualbox_share /mnt/ Follow this TensorFlow for mac ox, follow this videoregister mega https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows (success) Fifth try with pip installFollow this website When I type pip install tensorflow in Virtualbox (OS:Linux),it always shows ReadTimeoutError: HTTPSConnectionPool(host=&#39;pypi.python.org&#39;, port=443): Read timed out.,so I choosesudo pip install --upgrade https://pypi.tuna.tsinghua.edu.cn/packages/7b/c5/a97ed48fcc878e36bb05a3ea700c077360853c0994473a8f6b0ab4c2ddd2/tensorflow-1.0.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=a7483a4da4d70cc628e9e207238f77c0to install tensorflow Collecting numpy&gt;=1.11.0 (from tensorflow==1.0.0) Downloading numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl (16.5MB) sudo pip install --upgrade https://pypi.python.org/packages/cb/47/19e96945ee6012459e85f87728633f05b1e8791677ae64370d16ac4c849e/numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=9f9bc53d2e281831e1a75be0c09a9548 From this mirror sudo pip install --upgrade https://mirrors.ustc.edu.cn/pypi/web/packages/cb/47/19e96945ee6012459e85f87728633f05b1e8791677ae64370d16ac4c849e/numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=9f9bc53d2e281831e1a75be0c09a9548 Try again successpip install --index https://pypi.mirrors.ustc.edu.cn/simple/ tensorflow Validate your installation$ python import tensorflow as tfhello = tf.constant(‘Hello, TensorFlow!’)sess = tf.Session()print(sess.run(hello)) Hello, TensorFlow! (success) Sixth try with anaconda inatallFollow this website Download anaconda in the VirtualBoxfor me it shows readtimeouterror So I decide to download it in my host OS and copy it to my share file C:\Users\SSQ\virtualbox share and I can find it in the /mnt from my Linux system.type bash /mnt/Anaconda2-4.3.0-Linux-x86_64.shtype yesAnaconda2 will now be installed into this location:/home/ssq/anaconda2 Press ENTER to confirm the locationPress CTRL-C to abort the installationOr specify a different location below click Enter Do you wish the installer to prepend the Anaconda2 install locationto PATH in your /home/ssq/.bashrc ? [yes|no] yes Open new terminal and type conda create -n tensorflow Fetching package metadata … CondaHTTPError: HTTP None None for url Elapsed: None An HTTP error occurred when trying to retrieve this URL.ConnectionError(ReadTimeoutError(“HTTPSConnectionPool(host=’repo.continuum.io’, port=443): Read timed out.”,),) Try again conda create -n tensorflow source activate tensorflow From ssq@ssq-VirtualBox:~$ to (tensorflow) ssq@ssq-VirtualBox:~$ Successy pip install --index https://pypi.mirrors.ustc.edu.cn/simple/ tensorflow Validate your installation$ python import tensorflow as tfhello = tf.constant(‘Hello, TensorFlow!’)sess = tf.Session()print(sess.run(hello)) Hello, TensorFlow! source deactivate tensorflow From (tensorflow) ssq@ssq-VirtualBox:~$ to ssq@ssq-VirtualBox:~$ (failed)docker installInstall docker with sudo apt install docker.io AssignmentsAssignmentsNote: If you installed TensorFlow using the pre-built Docker container, you do not have to fetch assignment code separately. Just run the container and access the notebooks as mentioned here. Get Starter CodeStarter code packages (Jupyter notebooks) are available from the main TensorFlow repository. Clone it and navigate to the tensorflow/examples/udacity/ directory. This contains all the Jupyter notebooks (.ipynb files) as well as a Docker spec (Dockerfile). RunDepending on how you installed TensorFlow, do one of the following to run assignment code: **Pip/virtualenv**: Run `ipython notebook` and open http://localhost:8888 in a browser. **Docker**: As mentioned in README.md: First build a local Docker container: docker build -t $USER/assignments . Run the container: docker run -p 8888:8888 -it --rm $USER/assignments Now find your VM&apos;s IP using docker-machine ip default (say, 192.168.99.100) and open http://192.168.99.100:8888 You should be able to see a list of notebooks, one for each assignment. Click on the appropriate one to open it, and follow the inline instructions. And you’re ready to start exploring! To get further help on each assignment, navigate to the appropriate node. If you want to learn more about iPython (or Jupyter) notebooks, visit jupyter.org. Assignment 1: notMNISTAssignment 1: notMNISTPreprocess notMNIST data and train a simple logistic regression model on it notMNIST dataset samples Starter CodeOpen the iPython notebook for this assignment (1_notmnist.ipynb), and follow the instructions to implement and run each indicated step. Some of the early steps that preprocess the data have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer the posed questions (save your responses as markdown in the notebook). In the end, you should have a model trained on the notMNIST dataset, which is able to recognize a subset of English letters in different fonts. How accurately does your model predict the correct labels on the test dataset? Problem 2: Verify normalized imagesNote how imshow() displays an image using a color map. You can change this using the cmap parameter. Check out more options in the API reference. DEEP NEURAL NETWORKSDeep Neural NetworksAssignment 2: SGDAssignment 2: Stochastic Gradient DescentTrain a fully-connected network using Gradient Descent and Stochastic Gradient Descent Note: The assignments in this course build on each other, so please finish Assignment 1 before attempting this. Starter CodeOpen the iPython notebook for this assignment (2_fullyconnected.ipynb), and follow the instructions to implement and/or run each indicated step. Some steps have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). Your new model should perform better than the one you developed for Assignment 1. Also, the time required to train using Stochastic Gradient Descent (SGD) should be considerably less than simple Gradient Descent (GD). ErrorsError: valueError: Only call softmax_cross_entropy_with_logits with named arguments (labels=…, logits=…, …) Fix: loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) Assignment 3: RegularizationAssignment 3: RegularizationUse regularization techniques to improve a deep learning model Note: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (3_regularization.ipynb), and follow the instructions to implement and run each indicated step. Some steps have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). Try to apply the different regularization techniques you have learnt, and compare their results. Which seems to work better? Is one clearly better than the others? Error in VirtualBoxError:1234567Unable to allocate and lock memory. The virtual machine will be paused. Please close applications to free up memory or close the VM.错误 ID: HostMemoryLow严重: 非致命性错误 How to fix:Close all the process in the host OS and free up memory.Restart VM CONVOLUTIONAL NEURAL NETWORKSReadingsReadingsFor a closer look at the arithmetic behind convolution, and how it is affected by your choice of padding scheme, stride and other parameters, please refer to this illustrated guide: V. Dumoulin and F. Visin, A guide to convolution arithmetic for deep learning. Assignment 4: Convolutional ModelsDesign and train a Convolutional Neural NetworkNote: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (4_convolutions.ipynb), and follow the instructions to implement and run each indicated step. Some steps have been implemented for you. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). Improve the model by experimenting with its structure - how many layers, how they are connected, stride, pooling, etc. For more efficient training, try applying techniques such as dropout and learning rate decay. What does your final architecture look like? DEEP MODELS FOR TEXT AND SEQUENCEStSNELaurens van der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE. Journal of Machine Learning Research, 2008. Vol. 9, pp. 2579-2605. Assignment 5: Word2Vec and CBOWAssignment 5: Word2Vec and CBOWTrain a skip-gram model on Text8 data and visualize the output Note: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (5_word2vec.ipynb), and follow the instructions to implement and run each indicated step. The first model (Word2Vec) has been implemented for you. Using that as a reference, train a CBOW (Continuous Bag of Words) model. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). How does your CBOW model perform compared to the given Word2Vec model? Opensudo mount -t vboxsf virtualbox_share /mnt/jupyter notebook runTypeError:Input &#39;y&#39; of &#39;Mul&#39; Op has type float32 that does not match type int32 of argument &#39;x&#39;. 12python -c &apos;import tensorflow as tf; print(tf.__version__)&apos;1.0.0 Method:tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, train_labels, embed,num_sampled, vocabulary_size)) Reference:https://github.com/nlintz/TensorFlow-Tutorials/issues/80 Assignment 6: LSTMsAssignment 6: LSTMsTrain a Long Short-Term Memory network to predict character sequences Note: The assignments in this course build on each other, so please finish them in order. Starter CodeOpen the iPython notebook for this assignment (6_lstm.ipynb), and follow the instructions to implement and run each indicated step. A basic LSTM model has been provided; improve it by solving the given problems. EvaluationThis is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook). What changes did you make to use bigrams as input instead of individual characters? Were you able to implement the sequence-to-sequence LSTM? If so, what additional challenges did you have to solve? RunAttributeError:&#39;module&#39; object has no attribute &#39;concat_v2&#39; 12# Classifier.logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits, tf.concat_v2(train_labels, 0))) ValueError:Only call softmax_cross_entropy_with_logits with named arguments (labels=…, logits=…, …) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits, tf.concat(train_labels, 0))) Method12# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits, tf.concat(train_labels, 0))) 12345# Classifier.logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( logits=logits, labels=tf.concat(train_labels, 0))) Have a trypip uninstall tensorflow pip install --ignore-installed --upgrade https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb sudo pip install --index https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb sudo pip install --upgrade https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb sudo pip install --index https://mirrors.ustc.edu.cn/pypi/web/packages/01/c5/adefd2d5c83e6d8b4a8efa5dd00e44dc05de317b744fb58aef6d8366ce2b/tensorflow-0.12.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=ebcd1b32ccf2279bfa688542cbdad5fb PROJECT submit (new) Deep LearningMACHINE LEARNING TO DEEP LEARNINGDeep LearningDeep LearningUp to this point you’ve been introduced to a number of different learning schemes that take place in machine learning. You’ve seen supervised learning, where we try to extrapolate labels for new data given labelled data we already have. You’ve seen unsupervised learning, where we try to classify data into groups and extract new information hidden in the data. Lastly, you’ve seen reinforcement learning, where we try to create a model that learns the rules of an environment to best maximize its return or reward. In this lesson, you’ll learn about a relatively new branch of machine learning called deep learning, which attempts to model high-level abstractions about data using networks of graphs. Deep learning, much like the other branches of machine learning you’ve seen, is similarly focused on learning representations in data. Additionally, modeling high-level abstractions about data is very similar to artificial intelligence — the idea that knowledge can be represented and acted upon intelligently. What You’ll Watch and LearnFor this lesson, you’ll want to learn about algorithms that help you to construct the deep network graphs necessary to model high-level abstractions about data. In addition, you’ll also want to learn how to construct deep models that can interpret and identify words and letters in text — just like how a human reads! To do that, you’ll work on Udacity’s Deep Learning course, co-authored by Google. Vincent Vanhoucke, Principle Scientist at Google Brain, will be your instructor for this lesson. With Vincent as your guide, you’ll learn the ins and outs of Deep Learning and TensorFlow, which is Google’s Deep Learning framework. Deep Learning What You’ll DoIn this lesson, you’ll learn how you can develop algorithms that are suitable to model high-level abstractions of data and create a type of “intelligence” that is able to use this abstraction for processing new information. First, you’ll learn about deep neural networks — artificial neural networks that have multiple hidden layers of information between its input and output. Next, you’ll learn about convolutional neural networks — a different flavor of neural networks that are modeled after biological processes like visual and aural feedback. Finally, you’ll learn about deep models for sequence learning — models that can “understand” written and spoken language and text. The underlying lesson from these concepts is that, with enough data and time to learn, we can develop intelligent agents that think and act in many of the same ways we as humans do. Being able to model complex human behaviors and tasks like driving a car, processing spoken language, or even building a winning strategy for the game of Go, is a task that could not be done without use of deep learning. Software and ToolsTensorFlowTensorFlowWe will be using TensorFlow™, an open-source library developed by Google, to build deep learning models throughout the course. Coding will be in Python 2.7 using iPython notebooks, which you should be familiar with. Download and SetupMethod 1: Pre-built Docker container with TensorFlow and all assignmentsTo get started with TensorFlow quickly and work on your assignments, follow the instructions in this README. Note: If you are on a Windows machine, Method 1 is your only option due to lack of native TensorFlow support. – OR – Method 2: Install TensorFlow on your computer (Linux or Mac OS X only), then fetch assignment code separatelyFollow the instructions to download and setup TensorFlow. Choose one of the three ways to install: Pip: Install TensorFlow directly on your computer. You need to have Python 2.7 and pip installed; and this may impact other Python packages that you may have.Virtualenv: Install TensorFlow in an isolated (virtual) Python environment. You need to have Python 2.7 and virtualenv installed; this will not affect Python packages in any other environment.Docker: Run TensorFlow in an isolated Docker container (virtual machine) on your computer. You need to have Vagrant, Docker and virtualization software like VirtualBox installed; this will keep TensorFlow completely isolated from the rest of your computer, but may require more memory to run.Links: Tutorials, How-Tos, Resources, Source code, Stack Overflow INTRO TO TENSORFLOWIntro to TensorFlowWhat is Deep Learninghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/INt1nULYPak.mp4 Solving Problems - Big and Smallhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WHcRQMGSbqg.mp4 Let’s Get Startedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ySIDqaXLhHw.mp4 Installing TensorFlowThroughout this lesson, you’ll apply your knowledge of neural networks on real datasets using TensorFlow (link for China), an open source Deep Learning library created by Google. You’ll use TensorFlow to classify images from the notMNIST dataset - a dataset of images of English letters from A to J. You can see a few example images below.Your goal is to automatically detect the letter based on the image in the dataset. You’ll be working on your own computer for this lab, so, first things first, install TensorFlow! InstallAs usual, we’ll be using Conda to install TensorFlow. You might already have a TensorFlow environment, but check to make sure you have all the necessary packages. OS X or LinuxRun the following commands to setup your environment: conda create -n tensorflow python=3.5 source activate tensorflow conda install pandas matplotlib jupyter notebook scipy scikit-learn pip install tensorflow WindowsAnd installing on Windows. In your console or Anaconda shell, conda create -n tensorflow python=3.5 activate tensorflow conda install pandas matplotlib jupyter notebook scipy scikit-learn pip install tensorflow Hello, world!Try running the following code in your Python console to make sure you have TensorFlow properly installed. The console will print “Hello, world!” if TensorFlow is installed. Don’t worry about understanding what it does. You’ll learn about it in the next section. import tensorflow as tf # Create TensorFlow object called tensor hello_constant = tf.constant(&apos;Hello World!&apos;) with tf.Session() as sess: # Run the tf.constant operation in the session output = sess.run(hello_constant) print(output) Tryopen cmd with admin conda create -n tensorflow python=3.5 C:\windows\system32&gt;conda create -n tensorflow python=3.5 Fetching package metadata ........... Solving package specifications: . Package plan for installation in environment C:\Program Files\Anaconda2\envs\ten sorflow: The following NEW packages will be INSTALLED: pip: 9.0.1-py35_1 python: 3.5.3-0 setuptools: 27.2.0-py35_1 vs2015_runtime: 14.0.25123-0 wheel: 0.29.0-py35_0 Proceed ([y]/n)? y vs2015_runtime 100% |###############################| Time: 0:00:02 776.58 kB/s python-3.5.3-0 100% |###############################| Time: 0:01:29 361.95 kB/s setuptools-27. 100% |###############################| Time: 0:00:00 1.09 MB/s wheel-0.29.0-p 100% |###############################| Time: 0:00:00 1.55 MB/s pip-9.0.1-py35 100% |###############################| Time: 0:00:01 997.36 kB/s # # To activate this environment, use: # &gt; activate tensorflow # # To deactivate this environment, use: # &gt; deactivate tensorflow # # * for power-users using bash, you must source # activate tensorflow (tensorflow) C:\windows\system32&gt; conda install pandas matplotlib jupyter notebook scipy scikit-learn Fetching package metadata ………..Solving package specifications: . Package plan for installation in environment C:\Program Files\Anaconda2\envs\tensorflow: The following NEW packages will be INSTALLED: bleach: 1.5.0-py35_0 colorama: 0.3.7-py35_0 cycler: 0.10.0-py35_0 decorator: 4.0.11-py35_0 entrypoints: 0.2.2-py35_1 html5lib: 0.999-py35_0 icu: 57.1-vc14_0 [vc14] ipykernel: 4.5.2-py35_0 ipython: 5.3.0-py35_0 ipython_genutils: 0.1.0-py35_0 ipywidgets: 6.0.0-py35_0 jinja2: 2.9.5-py35_0 jpeg: 9b-vc14_0 [vc14] jsonschema: 2.5.1-py35_0 jupyter: 1.0.0-py35_3 jupyter_client: 5.0.0-py35_0 jupyter_console: 5.1.0-py35_0 jupyter_core: 4.3.0-py35_0 libpng: 1.6.27-vc14_0 [vc14] markupsafe: 0.23-py35_2 matplotlib: 2.0.0-np112py35_0 mistune: 0.7.4-py35_0 mkl: 2017.0.1-0 nbconvert: 5.1.1-py35_0 nbformat: 4.3.0-py35_0 notebook: 4.4.1-py35_0 numpy: 1.12.0-py35_0 openssl: 1.0.2k-vc14_0 [vc14] pandas: 0.19.2-np112py35_1 pandocfilters: 1.4.1-py35_0 path.py: 10.1-py35_0 pickleshare: 0.7.4-py35_0 prompt_toolkit: 1.0.13-py35_0 pygments: 2.2.0-py35_0 pyparsing: 2.1.4-py35_0 pyqt: 5.6.0-py35_2 python-dateutil: 2.6.0-py35_0 pytz: 2016.10-py35_0 pyzmq: 16.0.2-py35_0 qt: 5.6.2-vc14_3 [vc14] qtconsole: 4.2.1-py35_2 scikit-learn: 0.18.1-np112py35_1 scipy: 0.19.0-np112py35_0 simplegeneric: 0.8.1-py35_1 sip: 4.18-py35_0 six: 1.10.0-py35_0 testpath: 0.3-py35_0 tk: 8.5.18-vc14_0 [vc14] tornado: 4.4.2-py35_0 traitlets: 4.3.2-py35_0 wcwidth: 0.1.7-py35_0 widgetsnbextension: 2.0.0-py35_0 win_unicode_console: 0.5-py35_0 zlib: 1.2.8-vc14_3 [vc14] Proceed ([y]/n)? y mkl-2017.0.1-0 100% |###############################| Time: 0:04:46 470.85 kB/s icu-57.1-vc14_ 100% |###############################| Time: 0:01:28 403.91 kB/s jpeg-9b-vc14_0 100% |###############################| Time: 0:00:00 379.04 kB/s openssl-1.0.2k 100% |###############################| Time: 0:00:13 393.72 kB/s tk-8.5.18-vc14 100% |###############################| Time: 0:00:04 473.45 kB/s zlib-1.2.8-vc1 100% |###############################| Time: 0:00:00 503.24 kB/s colorama-0.3.7 100% |###############################| Time: 0:00:00 622.07 kB/s decorator-4.0. 100% |###############################| Time: 0:00:00 690.00 kB/s entrypoints-0. 100% |###############################| Time: 0:00:00 625.06 kB/s ipython_genuti 100% |###############################| Time: 0:00:00 597.35 kB/s jsonschema-2.5 100% |###############################| Time: 0:00:00 503.91 kB/s libpng-1.6.27- 100% |###############################| Time: 0:00:01 432.48 kB/s markupsafe-0.2 100% |###############################| Time: 0:00:00 520.82 kB/s mistune-0.7.4- 100% |###############################| Time: 0:00:00 441.53 kB/s numpy-1.12.0-p 100% |###############################| Time: 0:00:10 354.48 kB/s pandocfilters- 100% |###############################| Time: 0:00:00 363.00 kB/s path.py-10.1-p 100% |###############################| Time: 0:00:00 293.57 kB/s pygments-2.2.0 100% |###############################| Time: 0:00:04 302.43 kB/s pyparsing-2.1. 100% |###############################| Time: 0:00:00 270.85 kB/s pytz-2016.10-p 100% |###############################| Time: 0:00:00 233.38 kB/s pyzmq-16.0.2-p 100% |###############################| Time: 0:00:02 266.24 kB/s simplegeneric- 100% |###############################| Time: 0:00:00 373.89 kB/s sip-4.18-py35_ 100% |###############################| Time: 0:00:00 268.95 kB/s six-1.10.0-py3 100% |###############################| Time: 0:00:00 409.00 kB/s testpath-0.3-p 100% |###############################| Time: 0:00:00 329.72 kB/s tornado-4.4.2- 100% |###############################| Time: 0:00:02 253.88 kB/s wcwidth-0.1.7- 100% |###############################| Time: 0:00:00 329.53 kB/s win_unicode_co 100% |###############################| Time: 0:00:00 302.28 kB/s cycler-0.10.0- 100% |###############################| Time: 0:00:00 393.21 kB/s html5lib-0.999 100% |###############################| Time: 0:00:00 260.77 kB/s jinja2-2.9.5-p 100% |###############################| Time: 0:00:01 250.23 kB/s pickleshare-0. 100% |###############################| Time: 0:00:00 326.15 kB/s prompt_toolkit 100% |###############################| Time: 0:00:01 281.79 kB/s python-dateuti 100% |###############################| Time: 0:00:00 280.81 kB/s qt-5.6.2-vc14_ 100% |###############################| Time: 0:02:03 469.10 kB/s scipy-0.19.0-n 100% |###############################| Time: 0:00:20 656.15 kB/s traitlets-4.3. 100% |###############################| Time: 0:00:00 418.63 kB/s bleach-1.5.0-p 100% |###############################| Time: 0:00:00 508.29 kB/s ipython-5.3.0- 100% |###############################| Time: 0:00:02 406.32 kB/s jupyter_core-4 100% |###############################| Time: 0:00:00 365.87 kB/s pandas-0.19.2- 100% |###############################| Time: 0:00:13 548.51 kB/s pyqt-5.6.0-py3 100% |###############################| Time: 0:00:08 586.14 kB/s scikit-learn-0 100% |###############################| Time: 0:00:16 282.73 kB/s jupyter_client 100% |###############################| Time: 0:00:00 250.90 kB/s matplotlib-2.0 100% |###############################| Time: 0:00:17 508.36 kB/s nbformat-4.3.0 100% |###############################| Time: 0:00:00 1.41 MB/s ipykernel-4.5. 100% |###############################| Time: 0:00:00 1.39 MB/s nbconvert-5.1. 100% |###############################| Time: 0:00:00 1.42 MB/s jupyter_consol 100% |###############################| Time: 0:00:00 397.64 kB/s notebook-4.4.1 100% |###############################| Time: 0:00:06 890.12 kB/s qtconsole-4.2. 100% |###############################| Time: 0:00:00 705.98 kB/s widgetsnbexten 100% |###############################| Time: 0:00:01 727.40 kB/s ipywidgets-6.0 100% |###############################| Time: 0:00:00 632.13 kB/s jupyter-1.0.0- 100% |###############################| Time: 0:00:00 665.76 kB/s ERROR conda.core.link:_execute_actions(330): An error occurred while installing package &apos;defaults::qt-5.6.2-vc14_3&apos;. UnicodeDecodeError(&apos;utf8&apos;, &apos;\xd2\xd1\xb8\xb4\xd6\xc6 1 \xb8\xf6\xce\xc4\ xbc\xfe\xa1\xa3\r\n&apos;, 0, 1, &apos;invalid continuation byte&apos;) Attempting to roll back. UnicodeDecodeError(&apos;utf8&apos;, &apos;\xd2\xd1\xb8\xb4\xd6\xc6 1 \xb8\xf6\xce\xc4\ xbc\xfe\xa1\xa3\r\n&apos;, 0, 1, &apos;invalid continuation byte&apos;) (tensorflow) C:\windows\system32&gt;pip install tensorflow Hello, Tensor World!Hello, Tensor World!Let’s analyze the Hello World script you ran. For reference, I’ve added the code below. import tensorflow as tf # Create TensorFlow object called hello_constant hello_constant = tf.constant(&apos;Hello World!&apos;) with tf.Session() as sess: # Run the tf.constant operation in the session output = sess.run(hello_constant) print(output) TensorIn TensorFlow, data isn’t stored as integers, floats, or strings. These values are encapsulated(封装) in an object called a tensor. In the case of hello_constant = tf.constant(&#39;Hello World!&#39;), hello_constant is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below: # A is a 0-dimensional int32 tensor A = tf.constant(1234) # B is a 1-dimensional int32 tensor B = tf.constant([123,456,789]) # C is a 2-dimensional int32 tensor C = tf.constant([ [123,456,789], [222,333,444] ]) tf.constant() is one of many TensorFlow operations you will use in this lesson. The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes. SessionTensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process which you learned about in the MiniFlow lesson. Let’s take the TensorFlow code you ran and turn that into a graph:A “TensorFlow Session”, as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let’s see how you use it. with tf.Session() as sess: output = sess.run(hello_constant) The code has already created the tensor, hello_constant, from the previous lines. The next step is to evaluate the tensor in a session. The code creates a session instance, sess, using tf.Session. The sess.run() function then evaluates the tensor and returns the results. Quiz: TensorFlow InputInputIn the last section, you passed a tensor into a session and it returned the result. What if you want to use a non-constant? This is where tf.placeholder() and feed_dict come into place. In this section, you’ll go over the basics of feeding data into TensorFlow. tf.placeholder()Sadly you can’t just set x to your dataset and put it in TensorFlow, because over time you’ll want your TensorFlow model to take in different datasets with different parameters. You need tf.placeholder()! tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs. Session’s feed_dictx = tf.placeholder(tf.string) with tf.Session() as sess: output = sess.run(x, feed_dict={x: &apos;Hello World&apos;}) Use the feed_dict parameter in tf.session.run() to set the placeholder tensor. The above example shows the tensor x being set to the string &quot;Hello, world&quot;. It’s also possible to set more than one tensor using feed_dict as shown below. x = tf.placeholder(tf.string) y = tf.placeholder(tf.int32) z = tf.placeholder(tf.float32) with tf.Session() as sess: output = sess.run(x, feed_dict={x: &apos;Test String&apos;, y: 123, z: 45.67}) Note: If the data passed to the feed_dict doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error “ValueError: invalid literal for...”. QuizLet’s see how well you understand tf.placeholder() and feed_dict. The code below throws an error, but I want you to make it return the number 123. Change line 11, so that the code returns the number 123. Note: The quizzes are running TensorFlow version 0.12.1. However, all the code used in this course is compatible with version 1.0. We’ll be upgrading our in class quizzes to the newest version in the near future. # Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf def run(): output = None x = tf.placeholder(tf.int32) with tf.Session() as sess: # TODO: Feed the x tensor 123 output = sess.run(x,feed_dict={x:123}) return output Quiz: TensorFlow MathTensorFlow MathGetting the input is great, but now you need to use it. You’re going to use basic math functions that everyone knows and loves - add, subtract, multiply, and divide - with tensors. (There’s many more math functions you can check out in the documentation.) Additionx = tf.add(5, 2) # 7 You’ll start with the add function. The tf.add() function does exactly what you expect it to do. It takes in two numbers, two tensors, or one of each, and returns their sum as a tensor. Subtraction and MultiplicationHere’s an example with subtraction and multiplication. x = tf.subtract(10, 4) # 6 y = tf.multiply(2, 5) # 10 The x tensor will evaluate to 6, because 10 - 4 = 6. The y tensor will evaluate to 10, because 2 * 5 = 10. That was easy! Converting typesIt may be necessary to convert between types to make certain operators work together. For example, if you tried the following, it would fail with an exception: tf.subtract(tf.constant(2.0),tf.constant(1)) # Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: That’s because the constant 1 is an integer but the constant 2.0 is a floating point value and subtract expects them to match. In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the 2.0 to an integer before subtracting, like so, will give the correct result: tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1)) # 1 QuizLet’s apply what you learned to convert an algorithm to TensorFlow. The code below is a simple algorithm using division and subtraction. Convert the following algorithm in regular Python to TensorFlow and print the results of the session. You can use tf.constant() for the values 10, 2, and 1. # Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf # TODO: Convert the following to TensorFlow: x = 10 y = 2 z = x/y - 1 x=tf.constant(x) y=tf.constant(y) z=tf.constant(z) #z=tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1),tf.float64)) # TODO: Print z from a session with tf.Session() as sess: output = sess.run(z) print(output) Transition to ClassificationGood job! You’ve accomplished a lot. In particular, you did the following: Ran operations in tf.Session. Created a constant tensor with tf.constant(). Used tf.placeholder() and feed_dict to get input. Applied the tf.add(), tf.subtract(), tf.multiply(), and tf.divide() functions using numeric data. Learned about casting between types with tf.cast()You know the basics of TensorFlow, so let’s take a break and get back to the theory of neural networks. In the next few videos, you’re going to learn about one of the most popular applications of neural networks - classification. Supervised Classificationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XTGsutypAPE.mp4 Training Your Logistic Classifierhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WQsdr1EJgz8.mp4 Quiz: TensorFlow Linear FunctionLinear functions in TensorFlowThe most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation asHere, W is a matrix of the weights connecting two layers. The output y, the input x, and the biases b are all vectors. Weights and Bias in TensorFlowThe goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you’ll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can’t be modified. This is where tf.Variable class comes in. tf.Variable()x = tf.Variable(5) The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You’ll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors. Initialization init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init)The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen. Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You’ll learn more about this in the next lesson, when you study gradient descent. Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You’ll use the tf.truncated_normal() function to generate random numbers from a normal distribution. tf.truncated_normal()n_features = 120 n_labels = 5 weights = tf.Variable(tf.truncated_normal((n_features, n_labels))) The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean. Since the weights are already helping prevent the model from getting stuck, you don’t need to randomize the bias. Let’s use the simplest solution, setting the bias to 0. tf.zeros()n_labels = 5 bias = tf.Variable(tf.zeros(n_labels)) The tf.zeros() function returns a tensor with all zeros. Linear Classifier QuizYou’ll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. The above is a small sample of the data you’ll be training on. Notice how some of the 1s are written with a serif at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model. The images above are trained weights for each label (0, 1, and 2). The weights display the unique properties of each digit they have found. Complete this quiz to train your own weights using the MNIST dataset. Instructions Open quiz.py. Implement get_weights to return a tf.Variable of weights Implement get_biases to return a tf.Variable of biases Implement xW + b in the linear function Open sandbox.py Initialize all weightsSince xW in xW + b is matrix multiplication, you have to use the tf.matmul() function instead of tf.multiply(). Don’t forget that order matters in matrix multiplication, so tf.matmul(a,b) is not the same as tf.matmul(b,a). quiz.py# Solution is available in the other &quot;quiz_solution.py&quot; tab import tensorflow as tf def get_weights(n_features, n_labels): &quot;&quot;&quot; Return TensorFlow weights :param n_features: Number of features :param n_labels: Number of labels :return: TensorFlow weights &quot;&quot;&quot; # TODO: Return weights return tf.Variable(tf.truncated_normal((n_features, n_labels))) def get_biases(n_labels): &quot;&quot;&quot; Return TensorFlow bias :param n_labels: Number of labels :return: TensorFlow bias &quot;&quot;&quot; # TODO: Return biases return tf.Variable(tf.zeros(n_labels)) def linear(input, w, b): &quot;&quot;&quot; Return linear function in TensorFlow :param input: TensorFlow input :param w: TensorFlow weights :param b: TensorFlow biases :return: TensorFlow linear function &quot;&quot;&quot; # TODO: Linear Function (xW + b) return tf.add(tf.matmul(input,w),b) sandbox.py# Solution is available in the other &quot;sandbox_solution.py&quot; tab import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data from quiz import get_weights, get_biases, linear def mnist_features_labels(n_labels): &quot;&quot;&quot; Gets the first &lt;n&gt; labels from the MNIST dataset :param n_labels: Number of labels to use :return: Tuple of feature list and label list &quot;&quot;&quot; mnist_features = [] mnist_labels = [] mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # In order to make quizzes run faster, we&apos;re only looking at 10000 images for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)): # Add features and labels if it&apos;s for the first &lt;n&gt;th labels if mnist_label[:n_labels].any(): mnist_features.append(mnist_feature) mnist_labels.append(mnist_label[:n_labels]) return mnist_features, mnist_labels # Number of features (28*28 image is 784 features) n_features = 784 # Number of labels n_labels = 3 # Features and Labels features = tf.placeholder(tf.float32) labels = tf.placeholder(tf.float32) # Weights and Biases w = get_weights(n_features, n_labels) b = get_biases(n_labels) # Linear Function xW + b logits = linear(features, w, b) # Training data train_features, train_labels = mnist_features_labels(n_labels) with tf.Session() as session: # TODO: Initialize session variables session.run(tf.global_variables_initializer()) # Softmax prediction = tf.nn.softmax(logits) # Cross entropy # This quantifies how far off the predictions were. # You&apos;ll learn more about this in future lessons. cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1) # Training loss # You&apos;ll learn more about this in future lessons. loss = tf.reduce_mean(cross_entropy) # Rate at which the weights are changed # You&apos;ll learn more about this in future lessons. learning_rate = 0.08 # Gradient Descent # This is the method used to train the model # You&apos;ll learn more about this in future lessons. optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) # Run optimizer and get loss _, l = session.run( [optimizer, loss], feed_dict={features: train_features, labels: train_labels}) # Print loss print(&apos;Loss: {}&apos;.format(l)) Quiz: TensorFlow SoftmaxTensorFlow SoftmaxYou might remember in the Intro to TFLearn lesson we used the softmax function to calculate class probabilities as output from the network. The softmax function squashes it’s inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It’s the perfect function to use as the output activation for a network predicting multiple classes. TensorFlow SoftmaxWe’re using TensorFlow to build neural networks and, appropriately, there’s a function for calculating softmax. x = tf.nn.softmax([2.0, 1.0, 0.2]) Easy as that! tf.nn.softmax() implements the softmax function for you. It takes in logits and returns softmax activations. QuizUse the softmax function in the quiz below to return the softmax of the logits. quiz.py# Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf def run(): output = None logit_data = [2.0, 1.0, 0.1] logits = tf.placeholder(tf.float32) # TODO: Calculate the softmax of the logits # softmax = softmax = tf.nn.softmax([2.0, 1.0, 0.1]) with tf.Session() as sess: # TODO: Feed in the logit data # output = sess.run(softmax, ) output = sess.run(softmax,feed_dict={logits:logit_data} ) return output One-Hot Encoding https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/phYsxqlilUk.mp4 13 L One Hot EncodingOne-Hot Encoding With Scikit-LearnTransforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using LabelBinarizer. Check it out below! import numpy as np from sklearn import preprocessing # Example labels labels = np.array([1,5,3,2,1,4,2,1,3]) # Create the encoder lb = preprocessing.LabelBinarizer() # Here the encoder finds the classes and assigns one-hot vectors lb.fit(labels) # And finally, transform the labels into one-hot encoded vectors lb.transform(labels) &gt;&gt;&gt; array([[1, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0]]) Quiz: TensorFlow Cross EntropyCross Entropy in TensorFlowIn the Intro to TFLearn lesson we discussed using cross entropy as the cost function for classification with one-hot encoded labels. Again, TensorFlow has a function to do the cross entropy calculations for us. Let’s take what you learned from the video and create a cross entropy function in TensorFlow. To create a cross entropy function in TensorFlow, you’ll need to use two new functions: tf.reduce_sum() tf.log() Reduce Sumx = tf.reduce_sum([1, 2, 3, 4, 5]) # 15 The tf.reduce_sum() function takes an array of numbers and sums them together. Natural Logx = tf.log(100) # 4.60517 This function does exactly what you would expect it to do. tf.log() takes the natural log of a number. QuizPrint the cross entropy using softmax_data and one_hot_encod_label.(Alternative link for users in China.) quiz.py# Solution is available in the other &quot;solution.py&quot; tab import tensorflow as tf softmax_data = [0.7, 0.2, 0.1] one_hot_data = [1.0, 0.0, 0.0] softmax = tf.placeholder(tf.float32) one_hot = tf.placeholder(tf.float32) # TODO: Print cross entropy from session cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax))) with tf.Session() as sess: print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})) 0.356675 Minimizing Cross Entropy https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YrDMXFhvh9E.mp4 Transition into Practical Aspects of Learning https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bKqkRFOOKoA.mp4 Quiz: Numerical Stability https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_SbGcOS-jcQ.mp4 a = 1000000000 for i in range(1000000): a = a + 1e-6 print(a - 1000000000) 0.953674316406 Normalized Inputs and Initial Weights Measuring Performance Optimizing a Logistic Classifier Stochastic Gradient Descent Momentum and Learning Rate Decay Parameter Hyperspace Quiz: Mini-batchMini-batchingIn this section, you’ll go over what mini-batching is and how to apply it in TensorFlow. Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset. Mini-batching is computationally inefficient, since you can’t calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all. It’s also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you’re performing SGD with each batch. Let’s look at the MNIST dataset with weights and a bias to see if your machine can handle it. from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # The features are already scaled and the data is shuffled train_features = mnist.train.images test_features = mnist.test.images train_labels = mnist.train.labels.astype(np.float32) test_labels = mnist.test.labels.astype(np.float32) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) Question 1Calculate the memory size of train_features, train_labels, weights, and bias in bytes. Ignore memory for overhead, just calculate the memory required for the stored data. You may have to look up how much memory a float32 requires, using this link. train_features Shape: (55000, 784) Type: float32 train_labels Shape: (55000, 10) Type: float32 weights Shape: (784, 10) Type: float32 bias Shape: (10,) Type: float32 How many bytes of memory does train_features need?550007844=172480000 How many bytes of memory does train_labels need?2200000 How many bytes of memory does weights need?31360 How many bytes of memory does bias need?40 The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn’t that much memory. You could train this whole dataset on most CPUs and GPUs. But larger datasets that you’ll use in the future measured in gigabytes or more. It’s possible to purchase more memory, but it’s expensive. A Titan X GPU with 12 GB of memory costs over $1,000. Instead, in order to run large models on your machine, you’ll learn how to use mini-batching. Let’s look at how you implement mini-batching in TensorFlow. TensorFlow Mini-batchingIn order to use mini-batching, you must first divide your data into batches. Unfortunately, it’s sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you’d like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you’d wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7128 + 1104 = 1000) In that case, the size of the batches would vary, so you need to take advantage of TensorFlow’s tf.placeholder() function to receive the varying batch sizes. Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes]. # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) What does None do here? The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0. Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples. Question 2Use the parameters below, how many batches are there, and what is the last batch size? features is (50000, 400) labels is (50000, 10) batch_size is 128 How many batches are there?50000/128+1=391 What is the last batch size?50000%128=80 Now that you know the basics, let’s learn how to implement mini-batching. Question 3Implement the batches function to batch features and labels. The function should return each batch with a maximum size of batch_size. To help you with the quiz, look at the following example output of a working batches function. # 4 Samples of features example_features = [ [&apos;F11&apos;,&apos;F12&apos;,&apos;F13&apos;,&apos;F14&apos;], [&apos;F21&apos;,&apos;F22&apos;,&apos;F23&apos;,&apos;F24&apos;], [&apos;F31&apos;,&apos;F32&apos;,&apos;F33&apos;,&apos;F34&apos;], [&apos;F41&apos;,&apos;F42&apos;,&apos;F43&apos;,&apos;F44&apos;]] # 4 Samples of labels example_labels = [ [&apos;L11&apos;,&apos;L12&apos;], [&apos;L21&apos;,&apos;L22&apos;], [&apos;L31&apos;,&apos;L32&apos;], [&apos;L41&apos;,&apos;L42&apos;]] example_batches = batches(3, example_features, example_labels) The example_batches variable would be the following: [ # 2 batches: # First is a batch of size 3. # Second is a batch of size 1 [ # First Batch is size 3 [ # 3 samples of features. # There are 4 features per sample. [&apos;F11&apos;, &apos;F12&apos;, &apos;F13&apos;, &apos;F14&apos;], [&apos;F21&apos;, &apos;F22&apos;, &apos;F23&apos;, &apos;F24&apos;], [&apos;F31&apos;, &apos;F32&apos;, &apos;F33&apos;, &apos;F34&apos;] ], [ # 3 samples of labels. # There are 2 labels per sample. [&apos;L11&apos;, &apos;L12&apos;], [&apos;L21&apos;, &apos;L22&apos;], [&apos;L31&apos;, &apos;L32&apos;] ] ], [ # Second Batch is size 1. # Since batch size is 3, there is only one sample left from the 4 samples. [ # 1 sample of features. [&apos;F41&apos;, &apos;F42&apos;, &apos;F43&apos;, &apos;F44&apos;] ], [ # 1 sample of labels. [&apos;L41&apos;, &apos;L42&apos;] ] ] ] Implement the batches function in the “quiz.py” file below. “quiz.py”import math def batches(batch_size, features, labels): &quot;&quot;&quot; Create batches of features and labels :param batch_size: The batch size :param features: List of features :param labels: List of labels :return: Batches of (Features, Labels) &quot;&quot;&quot; assert len(features) == len(labels) # TODO: Implement batching output_batches = [] sample_size = len(features) for start_i in range(0, sample_size, batch_size): end_i = start_i + batch_size batch = [features[start_i:end_i], labels[start_i:end_i]] output_batches.append(batch) return output_batches “sandbox.py”from quiz import batches from pprint import pprint # 4 Samples of features example_features = [ [&apos;F11&apos;,&apos;F12&apos;,&apos;F13&apos;,&apos;F14&apos;], [&apos;F21&apos;,&apos;F22&apos;,&apos;F23&apos;,&apos;F24&apos;], [&apos;F31&apos;,&apos;F32&apos;,&apos;F33&apos;,&apos;F34&apos;], [&apos;F41&apos;,&apos;F42&apos;,&apos;F43&apos;,&apos;F44&apos;]] # 4 Samples of labels example_labels = [ [&apos;L11&apos;,&apos;L12&apos;], [&apos;L21&apos;,&apos;L22&apos;], [&apos;L31&apos;,&apos;L32&apos;], [&apos;L41&apos;,&apos;L42&apos;]] # PPrint prints data structures like 2d arrays, so they are easier to read pprint(batches(3, example_features, example_labels)) Let’s use mini-batching to feed batches of MNIST features and labels into a linear model. Set the batch size and run the optimizer over all the batches with the batches function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller. “quiz.py”from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf import numpy as np from helper import batches learning_rate = 0.001 n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # The features are already scaled and the data is shuffled train_features = mnist.train.images test_features = mnist.test.images train_labels = mnist.train.labels.astype(np.float32) test_labels = mnist.test.labels.astype(np.float32) # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) # Logits - xW + b logits = tf.add(tf.matmul(features, weights), bias) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost) # Calculate accuracy correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # TODO: Set batch size batch_size = 128 assert batch_size is not None, &apos;You must set the batch size&apos; init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) # TODO: Train optimizer on all batches # for batch_features, batch_labels in ______ for batch_features, batch_labels in batches(batch_size, train_features, train_labels): sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels}) # Calculate accuracy for test dataset test_accuracy = sess.run( accuracy, feed_dict={features: test_features, labels: test_labels}) print(&apos;Test Accuracy: {}&apos;.format(test_accuracy)) The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times. You’ll go over this subject in the next section where we talk about “epochs”. EpochsEpochsAn epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs. The following TensorFlow code trains a model using 10 epochs. from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf import numpy as np from helper import batches # Helper function created in Mini-batching section def print_epoch_stats(epoch_i, sess, last_features, last_labels): &quot;&quot;&quot; Print cost and validation accuracy of an epoch &quot;&quot;&quot; current_cost = sess.run( cost, feed_dict={features: last_features, labels: last_labels}) valid_accuracy = sess.run( accuracy, feed_dict={features: valid_features, labels: valid_labels}) print(&apos;Epoch: {:&lt;4} - Cost: {:&lt;8.3} Valid Accuracy: {:&lt;5.3}&apos;.format( epoch_i, current_cost, valid_accuracy)) n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;/datasets/ud730/mnist&apos;, one_hot=True) # The features are already scaled and the data is shuffled train_features = mnist.train.images valid_features = mnist.validation.images test_features = mnist.test.images train_labels = mnist.train.labels.astype(np.float32) valid_labels = mnist.validation.labels.astype(np.float32) test_labels = mnist.test.labels.astype(np.float32) # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) # Logits - xW + b logits = tf.add(tf.matmul(features, weights), bias) # Define loss and optimizer learning_rate = tf.placeholder(tf.float32) cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost) # Calculate accuracy correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) init = tf.global_variables_initializer() batch_size = 128 epochs = 10 learn_rate = 0.001 train_batches = batches(batch_size, train_features, train_labels) with tf.Session() as sess: sess.run(init) # Training cycle for epoch_i in range(epochs): # Loop over all batches for batch_features, batch_labels in train_batches: train_feed_dict = { features: batch_features, labels: batch_labels, learning_rate: learn_rate} sess.run(optimizer, feed_dict=train_feed_dict) # Print cost and validation accuracy of an epoch print_epoch_stats(epoch_i, sess, batch_features, batch_labels) # Calculate accuracy for test dataset test_accuracy = sess.run( accuracy, feed_dict={features: test_features, labels: test_labels}) print(&apos;Test Accuracy: {}&apos;.format(test_accuracy)) Running the code will output the following: Epoch: 0 - Cost: 11.0 Valid Accuracy: 0.204 Epoch: 1 - Cost: 9.95 Valid Accuracy: 0.229 Epoch: 2 - Cost: 9.18 Valid Accuracy: 0.246 Epoch: 3 - Cost: 8.59 Valid Accuracy: 0.264 Epoch: 4 - Cost: 8.13 Valid Accuracy: 0.283 Epoch: 5 - Cost: 7.77 Valid Accuracy: 0.301 Epoch: 6 - Cost: 7.47 Valid Accuracy: 0.316 Epoch: 7 - Cost: 7.2 Valid Accuracy: 0.328 Epoch: 8 - Cost: 6.96 Valid Accuracy: 0.342 Epoch: 9 - Cost: 6.73 Valid Accuracy: 0.36 Test Accuracy: 0.3801000118255615 Each epoch attempts to move to a lower cost, leading to better accuracy. This model continues to improve accuracy up to Epoch 9. Let’s increase the number of epochs to 100. ... Epoch: 79 - Cost: 0.111 Valid Accuracy: 0.86 Epoch: 80 - Cost: 0.11 Valid Accuracy: 0.869 Epoch: 81 - Cost: 0.109 Valid Accuracy: 0.869 .... Epoch: 85 - Cost: 0.107 Valid Accuracy: 0.869 Epoch: 86 - Cost: 0.107 Valid Accuracy: 0.869 Epoch: 87 - Cost: 0.106 Valid Accuracy: 0.869 Epoch: 88 - Cost: 0.106 Valid Accuracy: 0.869 Epoch: 89 - Cost: 0.105 Valid Accuracy: 0.869 Epoch: 90 - Cost: 0.105 Valid Accuracy: 0.869 Epoch: 91 - Cost: 0.104 Valid Accuracy: 0.869 Epoch: 92 - Cost: 0.103 Valid Accuracy: 0.869 Epoch: 93 - Cost: 0.103 Valid Accuracy: 0.869 Epoch: 94 - Cost: 0.102 Valid Accuracy: 0.869 Epoch: 95 - Cost: 0.102 Valid Accuracy: 0.869 Epoch: 96 - Cost: 0.101 Valid Accuracy: 0.869 Epoch: 97 - Cost: 0.101 Valid Accuracy: 0.869 Epoch: 98 - Cost: 0.1 Valid Accuracy: 0.869 Epoch: 99 - Cost: 0.1 Valid Accuracy: 0.869 Test Accuracy: 0.8696000006198883 From looking at the output above, you can see the model doesn’t increase the validation accuracy after epoch 80. Let’s see what happens when we increase the learning rate. learn_rate = 0.1 Epoch: 76 - Cost: 0.214 Valid Accuracy: 0.752 Epoch: 77 - Cost: 0.21 Valid Accuracy: 0.756 Epoch: 78 - Cost: 0.21 Valid Accuracy: 0.756 ... Epoch: 85 - Cost: 0.207 Valid Accuracy: 0.756 Epoch: 86 - Cost: 0.209 Valid Accuracy: 0.756 Epoch: 87 - Cost: 0.205 Valid Accuracy: 0.756 Epoch: 88 - Cost: 0.208 Valid Accuracy: 0.756 Epoch: 89 - Cost: 0.205 Valid Accuracy: 0.756 Epoch: 90 - Cost: 0.202 Valid Accuracy: 0.756 Epoch: 91 - Cost: 0.207 Valid Accuracy: 0.756 Epoch: 92 - Cost: 0.204 Valid Accuracy: 0.756 Epoch: 93 - Cost: 0.206 Valid Accuracy: 0.756 Epoch: 94 - Cost: 0.202 Valid Accuracy: 0.756 Epoch: 95 - Cost: 0.2974 Valid Accuracy: 0.756 Epoch: 96 - Cost: 0.202 Valid Accuracy: 0.756 Epoch: 97 - Cost: 0.2996 Valid Accuracy: 0.756 Epoch: 98 - Cost: 0.203 Valid Accuracy: 0.756 Epoch: 99 - Cost: 0.2987 Valid Accuracy: 0.756 Test Accuracy: 0.7556000053882599 Looks like the learning rate was increased too much. The final accuracy was lower, and it stopped improving earlier. Let’s stick with the previous learning rate, but change the number of epochs to 80. Epoch: 65 - Cost: 0.122 Valid Accuracy: 0.868 Epoch: 66 - Cost: 0.121 Valid Accuracy: 0.868 Epoch: 67 - Cost: 0.12 Valid Accuracy: 0.868 Epoch: 68 - Cost: 0.119 Valid Accuracy: 0.868 Epoch: 69 - Cost: 0.118 Valid Accuracy: 0.868 Epoch: 70 - Cost: 0.118 Valid Accuracy: 0.868 Epoch: 71 - Cost: 0.117 Valid Accuracy: 0.868 Epoch: 72 - Cost: 0.116 Valid Accuracy: 0.868 Epoch: 73 - Cost: 0.115 Valid Accuracy: 0.868 Epoch: 74 - Cost: 0.115 Valid Accuracy: 0.868 Epoch: 75 - Cost: 0.114 Valid Accuracy: 0.868 Epoch: 76 - Cost: 0.113 Valid Accuracy: 0.868 Epoch: 77 - Cost: 0.113 Valid Accuracy: 0.868 Epoch: 78 - Cost: 0.112 Valid Accuracy: 0.868 Epoch: 79 - Cost: 0.111 Valid Accuracy: 0.868 Epoch: 80 - Cost: 0.111 Valid Accuracy: 0.869 Test Accuracy: 0.86909999418258667 The accuracy only reached 0.86, but that could be because the learning rate was too high. Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy. In the upcoming TensorFLow Lab, you’ll get the opportunity to choose your own learning rate, epoch count, and batch size to improve the model’s accuracy. More about epoch in Quora. INTRO TO NEURAL NETWORKSIntro to Neural NetworksIntroducing Luishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nto-stLuN6M.mp4 Logistic Regression Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kSs6O3R7JUI.mp4 Logistic Regression Answerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1iNylA3fJDs.mp4 Neural Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Mqogpnp1lrU.mp4 Perceptron PerceptronNow you’ve seen how a simple neural network makes decisions: by taking in input data, processing that information, and finally, producing an output in the form of a decision! Let’s take a deeper dive into the university admission example and learn more about how this input data is processed. Data, like test scores and grades, is fed into a network of interconnected nodes. These individual nodes are called perceptrons or neurons, and they are the basic unit of a neural network. Each one looks at input data and decides how to categorize that data. In the example above, the input either passes a threshold for grades and test scores or doesn’t, and so the two categories are: yes (passed the threshold) and no (didn’t pass the threshold). These categories then combine to form a decision – for example, if both nodes produce a “yes” output, then this student gains admission into the university.Let’s zoom in even further and look at how a single perceptron processes input data. The perceptron above is one of the two perceptrons from the video that help determine whether or not a student is accepted to a university. It decides whether a student’s grades are high enough to be accepted to the university. You might be wondering: “How does it know whether grades or test scores are more important in making this acceptance decision?” Well, when we initialize a neural network, we don’t know what information will be most important in making a decision. It’s up to the neural network to learn for itself which data is most important and adjust how it considers that data. It does this with something called weights. WeightsWhen input data comes into a perceptron, it gets multiplied by a weight value that is assigned to this particular input. For example, the perceptron above have two inputs, tests for test scores and grades, so it has two associated weights that can be adjusted individually. These weights start out as random values, and as the neural network learns more about what kind of input data leads to a student being accepted into a university, the network adjusts the weights based on any errors in categorization that the previous weights resulted in. This is called training the neural network. A higher weight means the neural network considers that input more important than other inputs, and lower weight means that the data is considered less important. An extreme example would be if test scores had no affect at all on university acceptance; then the weight of the test score input data would be zero and it would have no affect on the output of the perceptron. Summing the Input DataSo, each input to a perceptron has an associated weight that represents its importance and these weights are determined during the learning process of a neural network, called training. In the next step, the weighted input data is summed up to produce a single value, that will help determine the final output - whether a student is accepted to a university or not. Let’s see a concrete example of this.When writing equations related to neural networks, the weights will always be represented by some type of the letter w. It will usually look like a W when it represents a matrix of weights or a w when it represents an individual weight, and it may include some additional information in the form of a subscript to specify which weights (you’ll see more on that next). But remember, when you see the letter w, think weights. In this example, we’ll use $w{​grades}$ for the weight of grades and $w{​test}$ for the weight of test. For the image above, let’s say that the weights are: $w{​grades}=-1$,$w{​test}=−0.2$. You don’t have to be concerned with the actual values, but their relative values are important. $w{​grades}$ is 5 times larger than ​​$w{​test}$, which means the neural network considers grades input 5 times more important than test in determining whether a student will be accepted into a university. The perceptron applies these weights to the inputs and sums them in a process known as linear combination. In our case, this looks like $$w{​grades}x{​grades}+w{​test}x{​test}=-1x{​grades}-0.2x{​test}$$​. Now, to make our equation less wordy, let’s replace the explicit names with numbers. Let’s use 1 for grades and 2 for tests. So now our equation becomes $$w{1}*x{1}+w{1}*x{1}$$​. In this example, we just have 2 simple inputs: grades and tests. Let’s imagine we instead had m different inputs and we labeled them $x{1},x{2}…x{m}$. Let’s also say that the weight corresponding to $x{1}$​ is $w_{1}$ and so on. In that case, we would express the linear combination succintly as: $$\Sigma1^mw{i}*x{i}$$Here, the Greek letter Sigma $\Sigma$ is used to represent summation. It simply means to evaluate the equation to the right multiple times and add up the results. In this case, the equation it will sum is $w{i}*x_{i}$ But where do we get $w{i}$ and $x{i}$ ? $\Sigma_1^m$ means to iterate over all i values, from 1 to m. So to put it all together, $\Sigma1^mw{i}*x_{i}$ means the following: Start at i=1 Evaluate $w{1}*x{1}$ and remember the results Move to i=2 Evaluate $w{2}*x{2}$ and add these results to $w{1}*x{1}$ Continue repeating that process until i=m, where m is the number of inputs. One last thing: you’ll see equations written many different ways, both here and when reading on your own. For example, you will often just see $\Sigmai$ instead of $\Sigma{i=1}^m$. The first is simply a shorter way of writing the second. That is, if you see a summation without a starting number or a defined end value, it just means perform the sum for all of the them. And sometimes, if the value to iterate over can be inferred, you’ll see it as just $\Sigma$. Just remember they’re all the same thing: $\Sigma{i=1}^m w{i}*x_{i} = \Sigmai w{i}*x{i} = \Sigma w{i}*x_{i}$. Calculating the Output with an Activation FunctionFinally, the result of the perceptron’s summation is turned into an output signal! This is done by feeding the linear combination into an activation function. Activation functions are functions that decide, given the inputs into the node, what should be the node’s output? Because it’s the activation function that decides the actual output, we often refer to the outputs of a layer as its “activations”. One of the simplest activation functions is the Heaviside step function. This function returns a 0 if the linear combination is less than 0. It returns a 1 if the linear combination is positive or equal to zero. The Heaviside step function is shown below, where h is the calculated linear combination:In the university acceptance example above, we used the weights $w{grades} = -1$, $w{​test} = −0.2$. Since​​ $w{grades}$ and $w{​test}$ are negative values, the activation function will only return a 1 if grades and test are 0! This is because the range of values from the linear combination using these weights and inputs are (−∞,0] (i.e. negative infinity to 0, including 0 itself). It’s easiest to see this with an example in two dimensions. In the following graph, imagine any points along the line or in the shaded area represent all the possible inputs to our node. Also imagine that the value along the y-axis is the result of performing the linear combination on these inputs and the appropriate weights. It’s this result that gets passed to the activation function. Now remember that the step activation function returns 1 for any inputs greater than or equal to zero. As you can see in the image, only one point has a y-value greater than or equal to zero – the point right at the origin, (0,0):Now, we certainly want more than one possible grade/test combination to result in acceptance, so we need to adjust the results passed to our activation function so it activates – that is, returns 1 – for more inputs. Specifically, we need to find a way so all the scores we’d like to consider acceptable for admissions produce values greater than or equal to zero when linearly combined with the weights into our node. One way to get our function to return 1 for more inputs is to add a value to the results of our linear combination, called a bias. A bias, represented in equations as b, lets us move values in one direction or another. For example, the following diagram shows the previous hypothetical function with an added bias of +3. The blue shaded area shows all the values that now activate the function. But notice that these are produced with the same inputs as the values shown shaded in grey – just adjusted higher by adding the bias term:Of course, with neural networks we won’t know in advance what values to pick for biases. That’s ok, because just like the weights, the bias can also be updated and changed by the neural network during training. So after adding a bias, we now have a complete perceptron formula:This formula returns 1 if the input $x{1},x{2}…x_{m}$ belongs to the accepted-to-university category or returns 0 if it doesn’t. The input is made up of one or more real numbers, each one represented by $x_{i}$, where m is the number of inputs. Then the neural network starts to learn! Initially, the weights $w_{i}$ and bias (b) are assigned a random value, and then they are updated using a learning algorithm like gradient descent. The weights and biases change so that the next training example is more accurately categorized, and patterns in data are “learned” by the neural network. Now that you have a good understanding of perceptions, let’s put that knowledge to use. In the next section, you’ll create the AND perceptron from the Neural Networks video by setting the values for weights and bias. AND Perceptron Quiz What are the weights and bias for the AND perceptron?Set the weights (weight1, weight2) and bias bias to the correct values that calculate AND operation as shown above.In this case, there are two inputs as seen in the table above (let’s call the first column input1 and the second column input2), and based on the perceptron formula, we can calculate the output. First, the linear combination will be the sum of the weighted inputs: linear_combination = weight1*input1 + weight2*input2 then we can put this value into the biased Heaviside step function, which will give us our output (0 or 1): If you still need a hint, think of a concrete example like so: Consider input1 and input2 both = 1, for an AND perceptron, we want the output to also equal 1! The output is determined by the weights and Heaviside step function such that output = 1, if weight1*input1 + weight2*input2 + bias &gt;= 0 or output = 0, if weight1*input1 + weight2*input2 + bias &lt; 0 So, how can you choose the values for weights and bias so that if both inputs = 1, the output = 1? Gradient Descenthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/29PmNG7fuuM.mp4Gradient is another term for rate of change or slope. If you need to brush up on this concept, check out Khan Academy’s great lectures on the topic. Gradient Descent: The Mathhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7sxA5Ap8AWM.mp4 Gradient Descent: The CodeImplementing Gradient DescentMultilayer Perceptronshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Rs9petvTBLk.mp4Khan Academy’s introduction to vectors.Khan Academy’s introduction to matrices. Backpropagationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/MZL97-2joxQ.mp4 Implementing BackpropagationFurther ReadingFrom Andrej Karpathy: Yes, you should understand backpropAlso from Andrej Karpathy, a lecture from Stanford’s CS231n course DEEP NEURAL NETWORKSTwo-Layer Neural Network Multilayer Neural NetworksIn this lesson, you’ll learn how to build multilayer neural networks with TensorFlow. Adding a hidden layer to a network allows it to model more complex functions. Also, using a non-linear activation function on the hidden layer lets it model non-linear functions. We shall learn about ReLU, a non-linear function, or rectified linear unit. The ReLU function is 0 for negative inputs and x for all inputs x&gt;0. Next, you’ll see how a ReLU hidden layer is implemented in TensorFlow. Quiz: TensorFlow ReLUs1234567891011121314151617181920212223242526272829303132333435# Quiz Solution# Note: You can&apos;t run code in this tabimport tensorflow as tfoutput = Nonehidden_layer_weights = [ [0.1, 0.2, 0.4], [0.4, 0.6, 0.6], [0.5, 0.9, 0.1], [0.8, 0.2, 0.8]]out_weights = [ [0.1, 0.6], [0.2, 0.1], [0.7, 0.9]]# Weights and biasesweights = [ tf.Variable(hidden_layer_weights), tf.Variable(out_weights)]biases = [ tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))]# Inputfeatures = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])# TODO: Create Modelhidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])hidden_layer = tf.nn.relu(hidden_layer)logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])# TODO: Print session resultswith tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(logits)) Deep Neural Network in TensorFlowDeep Neural Network in TensorFlowYou’ve seen how to build a logistic classifier using TensorFlow. Now you’re going to see how to use the logistic classifier to build a deep neural network. Step by StepIn the following walkthrough, we’ll step through TensorFlow code written to classify the letters in the MNIST database. If you would like to run the network on your computer, the file is provided here. You can find this and many more examples of TensorFlow at Aymeric Damien’s GitHub repository. CodeTensorFlow MNIST from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;.&quot;, one_hot=True, reshape=False) You’ll use the MNIST dataset provided by TensorFlow, which batches and One-Hot encodes the data for you. Learning Parametersimport tensorflow as tf # Parameters learning_rate = 0.001 training_epochs = 20 batch_size = 128 # Decrease batch size if you don&apos;t have enough memory display_step = 1 n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) The focus here is on the architecture of multilayer neural networks, not parameter tuning, so here we’ll just give you the learning parameters. Hidden Layer Parametersn_hidden_layer = 256 # layer number of features The variable n_hidden_layer determines the size of the hidden layer in the neural network. This is also known as the width of a layer. Weights and Biases# Store layers weight &amp; bias weights = { &apos;hidden_layer&apos;: tf.Variable(tf.random_normal([n_input, n_hidden_layer])), &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_layer, n_classes])) } biases = { &apos;hidden_layer&apos;: tf.Variable(tf.random_normal([n_hidden_layer])), &apos;out&apos;: tf.Variable(tf.random_normal([n_classes])) } Deep neural networks use multiple layers with each layer requiring it’s own weight and bias. The &#39;hidden_layer&#39; weight and bias is for the hidden layer. The &#39;out&#39; weight and bias is for the output layer. If the neural network were deeper, there would be weights and biases for each additional layer. Input# tf Graph input x = tf.placeholder(&quot;float&quot;, [None, 28, 28, 1]) y = tf.placeholder(&quot;float&quot;, [None, n_classes]) x_flat = tf.reshape(x, [-1, n_input]) The MNIST data is made up of 28px by 28px images with a single channel. The tf.reshape() function above reshapes the 28px by 28px matrices in x into row vectors of 784px. Multilayer Perceptron # Hidden layer with RELU activation layer_1 = tf.add(tf.matmul(x_flat, weights[&apos;hidden_layer&apos;]),\ biases[&apos;hidden_layer&apos;]) layer_1 = tf.nn.relu(layer_1) # Output layer with linear activation logits = tf.add(tf.matmul(layer_1, weights[&apos;out&apos;]), biases[&apos;out&apos;]) You’ve seen the linear function tf.add(tf.matmul(x_flat, weights[&#39;hidden_layer&#39;]), biases[&#39;hidden_layer&#39;])before, also known as xw + b. Combining linear functions together using a ReLU will give you a two layer network. Optimizer# Define loss and optimizer cost = tf.reduce_mean(\ tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ .minimize(cost) This is the same optimization technique used in the Intro to TensorFLow lab. Session# Initializing the variables init = tf.global_variables_initializer() # Launch the graph with tf.Session() as sess: sess.run(init) # Training cycle for epoch in range(training_epochs): total_batch = int(mnist.train.num_examples/batch_size) # Loop over all batches for i in range(total_batch): batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) and cost op (to get loss value) sess.run(optimizer, feed_dict={x: batch_x, y: batch_y}) The MNIST library in TensorFlow provides the ability to receive the dataset in batches. Calling the mnist.train.next_batch() function returns a subset of the training data. Deeper Neural NetworkThat’s it! Going from one layer to two is easy. Adding more layers to the network allows you to solve more complicated problems. In the next video, you’ll see how changing the number of layers can affect your network. Training a Deep Learning Networkhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/CsB7yUtMJyk.mp4 Save and Restore TensorFlow ModelsSave and Restore TensorFlow ModelsTraining a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again! Fortunately, TensorFlow gives you the ability to save your progress using a class called tf.train.Saver. This class provides the functionality to save any tf.Variable to your file system. Saving VariablesLet’s start with a simple example of saving weights and bias Tensors. For the first example you’ll just save two variables. Later examples will save all the weights in a practical model. import tensorflow as tf # The file path to save the data save_file = &apos;./model.ckpt&apos; # Two Tensor Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3])) bias = tf.Variable(tf.truncated_normal([3])) # Class used to save and/or restore Tensor Variables saver = tf.train.Saver() with tf.Session() as sess: # Initialize all the Variables sess.run(tf.global_variables_initializer()) # Show the values of weights and bias print(&apos;Weights:&apos;) print(sess.run(weights)) print(&apos;Bias:&apos;) print(sess.run(bias)) # Save the model saver.save(sess, save_file) Weights: [[-0.97990924 1.03016174 0.74119264] [-0.82581609 -0.07361362 -0.86653847]] Bias: [ 1.62978125 -0.37812829 0.64723819] The Tensors weights and bias are set to random values using the tf.truncated_normal() function. The values are then saved to the save_file location, “model.ckpt”, using the tf.train.Saver.save() function. (The “.ckpt” extension stands for “checkpoint”.) If you’re using TensorFlow 0.11.0RC1 or newer, a file called “model.ckpt.meta” will also be created. This file contains the TensorFlow graph. Loading VariablesNow that the Tensor Variables are saved, let’s load them back into a new model. # Remove the previous weights and bias tf.reset_default_graph() # Two Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3])) bias = tf.Variable(tf.truncated_normal([3])) # Class used to save and/or restore Tensor Variables saver = tf.train.Saver() with tf.Session() as sess: # Load the weights and bias saver.restore(sess, save_file) # Show the values of weights and bias print(&apos;Weight:&apos;) print(sess.run(weights)) print(&apos;Bias:&apos;) print(sess.run(bias)) Weights: [[-0.97990924 1.03016174 0.74119264] [-0.82581609 -0.07361362 -0.86653847]] Bias: [ 1.62978125 -0.37812829 0.64723819] You’ll notice you still need to create the weights and bias Tensors in Python. The tf.train.Saver.restore() function loads the saved data into weights and bias. Since tf.train.Saver.restore() sets all the TensorFlow Variables, you don’t need to call tf.global_variables_initializer(). Save a Trained ModelLet’s see how to train a model and save its weights. First start with a model: # Remove previous Tensors and Operations tf.reset_default_graph() from tensorflow.examples.tutorials.mnist import input_data import numpy as np learning_rate = 0.001 n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # Import MNIST data mnist = input_data.read_data_sets(&apos;.&apos;, one_hot=True) # Features and Labels features = tf.placeholder(tf.float32, [None, n_input]) labels = tf.placeholder(tf.float32, [None, n_classes]) # Weights &amp; bias weights = tf.Variable(tf.random_normal([n_input, n_classes])) bias = tf.Variable(tf.random_normal([n_classes])) # Logits - xW + b logits = tf.add(tf.matmul(features, weights), bias) # Define loss and optimizer cost = tf.reduce_mean(\ tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ .minimize(cost) # Calculate accuracy correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) Let’s train that model, then save the weights: import math save_file = &apos;./train_model.ckpt&apos; batch_size = 128 n_epochs = 100 saver = tf.train.Saver() # Launch the graph with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # Training cycle for epoch in range(n_epochs): total_batch = math.ceil(mnist.train.num_examples / batch_size) # Loop over all batches for i in range(total_batch): batch_features, batch_labels = mnist.train.next_batch(batch_size) sess.run( optimizer, feed_dict={features: batch_features, labels: batch_labels}) # Print status for every 10 epochs if epoch % 10 == 0: valid_accuracy = sess.run( accuracy, feed_dict={ features: mnist.validation.images, labels: mnist.validation.labels}) print(&apos;Epoch {:&lt;3} - Validation Accuracy: {}&apos;.format( epoch, valid_accuracy)) # Save the model saver.save(sess, save_file) print(&apos;Trained Model Saved.&apos;) Epoch 0 - Validation Accuracy: 0.06859999895095825 Epoch 10 - Validation Accuracy: 0.20239999890327454 Epoch 20 - Validation Accuracy: 0.36980000138282776 Epoch 30 - Validation Accuracy: 0.48820000886917114 Epoch 40 - Validation Accuracy: 0.5601999759674072 Epoch 50 - Validation Accuracy: 0.6097999811172485 Epoch 60 - Validation Accuracy: 0.6425999999046326 Epoch 70 - Validation Accuracy: 0.6733999848365784 Epoch 80 - Validation Accuracy: 0.6916000247001648 Epoch 90 - Validation Accuracy: 0.7113999724388123 Trained Model Saved. Load a Trained ModelLet’s load the weights and bias from memory, then check the test accuracy. saver = tf.train.Saver() # Launch the graph with tf.Session() as sess: saver.restore(sess, save_file) test_accuracy = sess.run( accuracy, feed_dict={features: mnist.test.images, labels: mnist.test.labels}) print(&apos;Test Accuracy: {}&apos;.format(test_accuracy)) Test Accuracy: 0.7229999899864197 That’s it! You now know how to save and load a trained model in TensorFlow. Let’s look at loading weights and biases into modified models in the next section. FinetuningLoading the Weights and Biases into a New ModelSometimes you might want to adjust, or “finetune” a model that you have already trained and saved. However, loading saved Variables directly into a modified model can generate errors. Let’s go over how to avoid these problems. Naming ErrorTensorFlow uses a string identifier for Tensors and Operations called name. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name , and then give the name &lt;Type&gt;_&lt;number&gt; for the subsequent nodes. Let’s see how this can affect loading a model with a different order of weights and bias: import tensorflow as tf # Remove the previous weights and bias tf.reset_default_graph() save_file = &apos;model.ckpt&apos; # Two Tensor Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3])) bias = tf.Variable(tf.truncated_normal([3])) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Save Weights: {}&apos;.format(weights.name)) print(&apos;Save Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) saver.save(sess, save_file) # Remove the previous weights and bias tf.reset_default_graph() # Two Variables: weights and bias bias = tf.Variable(tf.truncated_normal([3])) weights = tf.Variable(tf.truncated_normal([2, 3])) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Load Weights: {}&apos;.format(weights.name)) print(&apos;Load Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: # Load the weights and bias - ERROR saver.restore(sess, save_file) The code above prints out the following: Save Weights: Variable:0 Save Bias: Variable_1:0 Load Weights: Variable_1:0 Load Bias: Variable:0 … InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. … You’ll notice that the name properties for weights and bias are different than when you saved the model. This is why the code produces the “Assign requires shapes of both tensors to match” error. The code saver.restore(sess, save_file) is trying to load weight data into bias and bias data into weights. Instead of letting TensorFlow set the name property, let’s set it manually: import tensorflow as tf tf.reset_default_graph() save_file = &apos;model.ckpt&apos; # Two Tensor Variables: weights and bias weights = tf.Variable(tf.truncated_normal([2, 3]), name=&apos;weights_0&apos;) bias = tf.Variable(tf.truncated_normal([3]), name=&apos;bias_0&apos;) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Save Weights: {}&apos;.format(weights.name)) print(&apos;Save Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) saver.save(sess, save_file) # Remove the previous weights and bias tf.reset_default_graph() # Two Variables: weights and bias bias = tf.Variable(tf.truncated_normal([3]), name=&apos;bias_0&apos;) weights = tf.Variable(tf.truncated_normal([2, 3]) ,name=&apos;weights_0&apos;) saver = tf.train.Saver() # Print the name of Weights and Bias print(&apos;Load Weights: {}&apos;.format(weights.name)) print(&apos;Load Bias: {}&apos;.format(bias.name)) with tf.Session() as sess: # Load the weights and bias - No Error saver.restore(sess, save_file) print(&apos;Loaded Weights and Bias successfully.&apos;) Save Weights: weights_0:0 Save Bias: bias_0:0 Load Weights: weights_0:0 Load Bias: bias_0:0 Loaded Weights and Bias successfully. That worked! The Tensor names match and the data loaded correctly. Regularization Introhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pECnr-5F3_Q.mp4 Regularizationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/QcJBhbuCl5g.mp4 Regularization Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E0eEW6V0_sA.mp4 Dropouthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6DcImJS8uV8.mp4 Dropout Pt. 2https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8nG8zzJMbZw.mp4 Quiz: TensorFlow DropoutTensorFlow Dropouthttps://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdfDropout is a regularization technique for reducing overfitting. The technique temporarily drops units (artificial neurons) from the network, along with all of those units’ incoming and outgoing connections. Figure 1 illustrates how dropout works. TensorFlow provides the tf.nn.dropout() function, which you can use to implement dropout. Let’s look at an example of how to use tf.nn.dropout(). keep_prob = tf.placeholder(tf.float32) # probability to keep units hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0]) hidden_layer = tf.nn.relu(hidden_layer) hidden_layer = tf.nn.dropout(hidden_layer, keep_prob) logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) The code above illustrates how to apply dropout to a neural network. The tf.nn.dropout() function takes in two parameters: hidden_layer: the tensor to which you would like to apply dropout keep_prob: the probability of keeping (i.e. not dropping) any given unit keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob. During training, a good starting value for keep_prob is 0.5. During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model. Quiz 1Take a look at the code snippet below. Do you see what’s wrong? There’s nothing wrong with the syntax, however the test accuracy is extremely low. ... keep_prob = tf.placeholder(tf.float32) # probability to keep units hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0]) hidden_layer = tf.nn.relu(hidden_layer) hidden_layer = tf.nn.dropout(hidden_layer, keep_prob) logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) ... with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch_i in range(epochs): for batch_i in range(batches): .... sess.run(optimizer, feed_dict={ features: batch_features, labels: batch_labels, keep_prob: 0.5}) validation_accuracy = sess.run(accuracy, feed_dict={ features: test_features, labels: test_labels, keep_prob: 0.5}) QUESTION 1 OF 2 What’s wrong with the above code? Dropout doesn’t work with batching. The keep_prob value of 0.5 is too low. (correct)There shouldn’t be a value passed to keep_prob when testing for accuracy.keep_prob should be set to 1.0 when evaluating validation accuracy. Quiz 2This quiz will be starting with the code from the ReLU Quiz and applying a dropout layer. Build a model with a ReLU layer and dropout layer using the keep_prob placeholder to pass in a probability of 0.5. Print the logits from the model. Note: Output will be different every time the code is run. This is caused by dropout randomizing the units it drops. “solution.py”# Quiz Solution # Note: You can&apos;t run code in this tab import tensorflow as tf hidden_layer_weights = [ [0.1, 0.2, 0.4], [0.4, 0.6, 0.6], [0.5, 0.9, 0.1], [0.8, 0.2, 0.8]] out_weights = [ [0.1, 0.6], [0.2, 0.1], [0.7, 0.9]] # Weights and biases weights = [ tf.Variable(hidden_layer_weights), tf.Variable(out_weights)] biases = [ tf.Variable(tf.zeros(3)), tf.Variable(tf.zeros(2))] # Input features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]]) # TODO: Create Model with Dropout keep_prob = tf.placeholder(tf.float32) hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0]) hidden_layer = tf.nn.relu(hidden_layer) hidden_layer = tf.nn.dropout(hidden_layer, keep_prob) logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1]) # TODO: Print logits from a session with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(logits, feed_dict={keep_prob: 0.5})) [[ 1.10000002 6.60000038] [ 0.30800003 0.7700001 ] [ 9.56000042 4.78000021]] CONVOLUTIONAL NEURAL NETWORKSIntro To CNNshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B61jxZ4rkMs.mp4 Colorhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BdQccpMwk80.mp4QUIZ QUESTION What would be easier for your classifier to learn? R, G, B(correct)(R + G + B) / 3 Statistical Invariancehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0Hr5YwUUhr0.mp4 Convolutional Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ISHGyvsT0QY.mp4 IntuitionIntuitionLet’s develop better intuition for how Convolutional Neural Networks (CNN) work. We’ll examine how humans classify images, and then see how CNNs use similar approaches. Let’s say we wanted to classify the following image of a dog as a Golden Retriever. As humans, how do we do this? One thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur. We essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog. In this case, we might break down the image into a combination of the following: A nose Two eyes Golden fur These pieces can be seen below: Going One Step FurtherBut let’s take this one step further. How do we determine what exactly a nose is? A Golden Retriever nose can be seen as an oval with two black holes inside it. Thus, one way of classifying a Retriever’s nose is to to break it up into smaller pieces and look for black holes (nostrils) and curves that define an oval as shown below. Broadly speaking, this is what a CNN learns to do. It learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image. Finally, the CNN classifies the image by combining the larger, more complex objects. In our case, the levels in the hierarchy are: Simple shapes, like ovals and dark circles Complex objects (combinations of simple shapes), like eyes, nose, and fur The dog as a whole (a combination of complex objects) With deep learning, we don’t actually program the CNN to recognize these specific features. Rather, the CNN learns on its own to recognize such objects through forward propagation and backpropagation! It’s amazing how well a CNN can learn to classify images, even though we never program the CNN with information about specific features to look for. A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects. The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors. The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs. Once again, the CNN learns all of this on its own. We don’t ever have to tell the CNN to go looking for lines or curves or noses or fur. The CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for. That’s a good start! Hopefully you’ve developed some intuition about how CNNs work. Next, let’s look at some implementation details. FiltersBreaking up an ImageThe first step for a CNN is to break up the image into smaller pieces. We do this by selecting a width and height that defines a filter. The filter looks at small pieces, or patches, of the image. These patches are the same size as the filter. We then simply slide this filter horizontally or vertically to focus on a different piece of the image. The amount by which the filter slides is referred to as the ‘stride’. The stride is a hyperparameter which you, the engineer, can tune. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy. Let’s look at an example. In this zoomed in image of the dog, we first start with the patch outlined in red. The width and height of our filter define the size of this square. We then move the square over to the right by a given stride (2 in this case) to get another patch. What’s important here is that we are grouping together adjacent pixels and treating them as a collective. In a normal, non-convolutional neural network, we would have ignored this adjacency. In a normal network, we would have connected every pixel in the input image to a neuron in the next layer. In doing so, we would not have taken advantage of the fact that pixels in an image are close together for a reason and have special meaning. By taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image. Filter DepthIt’s common to have more than one filter. Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. The amount of filters in a convolutional layer is called the filter depth. How many neurons does each patch connect to? That’s dependent on our filter depth. If we have a depth of k, we connect each patch of pixels to k neurons in the next layer. This gives us the height of k in the next layer, as shown below. In practice, k is a hyperparameter we tune, and most CNNs tend to pick the same starting values. But why connect a single patch to multiple neurons in the next layer? Isn’t one neuron good enough? Multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture. For example, one patch might include some white teeth, some blonde whiskers, and part of a red tongue. In that case, we might want a filter depth of at least three - one for each of teeth, whiskers, and tongue. Having multiple neurons for a given patch ensures that our CNN can learn to capture whatever characteristics the CNN learns are important. Remember that the CNN isn’t “programmed” to look for certain characteristics. Rather, it learns on its own which characteristics to notice. Feature Map Sizeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lp1NrLZnCUM.mp4What are the width, height and depth for padding = ‘same’, stride = 1? Enter your answers in the format “width, height, depth”28,28,8 What are the width, height and depth for padding = ‘valid’, stride = 1? Enter your answers in the format “width, height, depth”26,26,8 What are the width, height and depth for padding = ‘valid’, stride = 2? Enter your answers in the format “width, height, depth”13,13,8 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/W4xtf8LTz1c.mp4 dropped Convolutions continuedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/utOv-BKI_vo.mp4 Convolutions Cont.Note, a “Fully Connected” layer is a standard, non convolutional layer, where all inputs are connected to all output neurons. This is also referred to as a “dense” layer, and is what we used in the previous two lessons. ParametersParameter SharingThe weights, w, are shared across patches for a given layer in a CNN to detect the cat above regardless of where in the image it is located. When we are trying to classify a picture of a cat, we don’t care where in the image a cat is. If it’s in the top left or the bottom right, it’s still a cat in our eyes. We would like our CNNs to also possess this ability known as translation invariance. How can we achieve this? As we saw earlier, the classification of a given patch in an image is determined by the weights and biases corresponding to that patch. If we want a cat that’s in the top left patch to be classified in the same way as a cat in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way. This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren’t shared across the output channels. There’s an additional benefit to sharing our parameters. If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images. Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model. PaddingA 5x5 grid with a 3x3 filter. Source: Andrej Karpathy. Let’s say we have a 5x5 grid (as shown above) and a filter of size 3x3 with a stride of 1. What’s the width and height of the next layer? We see that we can fit at most three patches in each direction, giving us a dimension of 3x3 in our next layer. As we can see, the width and height of each subsequent layer decreases in such a scheme. In an ideal world, we’d be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency. How might we achieve this? One way is to simply add a border of 0s to our original 5x5 image. You can see what this looks like in the below image.The same grid with 0 padding. Source: Andrej Karpathy. This would expand our original image to a 7x7. With this, we now see how our next layer’s size is again a 5x5, keeping our dimensionality consistent. DimensionalityFrom what we’ve learned so far, how can we calculate the number of neurons of each layer in our CNN? Given: our input layer has a width of W and a height of H our convolutional layer has a filter size F we have a stride of S a padding of P and a filter depth of K, the following formula gives us the width of the next layer: W_out = (W−F+2P)/S+1. The output height would be H_out = (H-F+2P)/S + 1. And the output depth would be equal to the filter depth D_out = K. The output volume would be W_out * H_out * D_out. Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network. Quiz: Convolution Output ShapIntroductionFor the next few quizzes we’ll test your understanding of the dimensions in CNNs. Understanding dimensions will help you make accurate tradeoffs between model size and performance. As you’ll see, some parameters have a much bigger impact on model size than others. SetupH = height, W = width, D = depth We have an input of shape 32x32x3 (HxWxD) 20 filters of shape 8x8x3 (HxWxD) A stride of 2 for both the height and width (S) Valid padding of size 1 ( P ) Recall the formula for calculating the new height or width: new_height = (input_height - filter_height + 2 * P)/S + 1 new_width = (input_width - filter_width + 2 * P)/S + 1 Convolutional Layer Output ShapeWhat’s the shape of the output? The answer format is HxWxD, so if you think the new height is 9, new width is 9, and new depth is 5, then type 9x9x5. 14x14x20 Solution: Convolution OutputSolutionThe answer is 14x14x20. We can get the new height and width with the formula resulting in: (32 - 8 + 2 * 1)/2 + 1 = 14 (32 - 8 + 2 * 1)/2 + 1 = 14 The new depth is equal to the number of filters, which is 20.This would correspond to the following code: input = tf.placeholder(tf.float32, (None, 32, 32, 3)) filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth) filter_bias = tf.Variable(tf.zeros(20)) strides = [1, 2, 2, 1] # (batch, height, width, depth) padding = &apos;VALID&apos; conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias Note the output shape of conv will be [1, 13, 13, 20]. It’s 4D to account for batch size, but more importantly, it’s not [1, 14, 14, 20]. This is because the padding algorithm TensorFlow uses is not exactly the same as the one above. An alternative algorithm is to switch padding from &#39;VALID&#39; to SAME which would result in an output shape of [1, 16, 16, 20]. If you’re curious how padding works in TensorFlow, read this document. Quiz: Number of ParametersWe’re now going to calculate the number of parameters of the convolutional layer. The answer from the last quiz will come into play here! Being able to calculate the number of parameters in a neural network is useful since we want to have control over how much memory a neural network uses. SetupH = height, W = width, D = depth We have an input of shape 32x32x3 (HxWxD) 20 filters of shape 8x8x3 (HxWxD) A stride of 2 for both the height and width (S) Valid padding of size 1 ( P ) Output Layer 14x14x20 (HxWxD) HintWithout parameter sharing, each neuron in the output layer must connect to each neuron in the filter. In addition, each neuron in the output layer must also connect to a single bias neuron. Solution: Number of ParametersSolutionThere are 756560 total parameters. That’s a HUGE amount! Here’s how we calculate it: (8 * 8 * 3 + 1) * (14 * 14 * 20) = 756560 8 * 8 * 3 is the number of weights, we add 1 for the bias. Remember, each weight is assigned to every single part of the output (14 * 14 * 20). So we multiply these two numbers together and we get the final answer. Quiz: Parameter SharingNow we’d like you to calculate the number of parameters in the convolutional layer, if every neuron in the output layer shares its parameters with every other neuron in its same channel. This is the number of parameters actually used in a convolution layer (tf.nn.conv2d()). SetupH = height, W = width, D = depth We have an input of shape 32x32x3 (HxWxD) 20 filters of shape 8x8x3 (HxWxD) A stride of 2 for both the height and width (S) Zero padding of size 1 (P) Output Layer 14x14x20 (HxWxD) HintWith parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel. So the number of parameters is equal to the number of neurons in the filter, plus a bias neuron, all multiplied by the number of channels in the output layer. Convolution Layer Parameters 2How many parameters does the convolution layer have (with parameter sharing)?3860 Solution: Parameter SharingSolutionThere are 3860 total parameters. That’s 196 times fewer parameters! Here’s how the answer is calculated: (8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860 That’s 3840 weights and 20 biases. This should look similar to the answer from the previous quiz. The difference being it’s just 20 instead of (14 * 14 * 20). Remember, with weight sharing we use the same filter for an entire depth slice. Because of this we can get rid of 14 * 14 and be left with only 20. Visualizing CNNsVisualizing CNNsLet’s look at an example CNN to see how it works in action. The CNN we will look at is trained on ImageNet as described in this paper by Zeiler and Fergus. In the images below (from the same paper), we’ll see what each layer in this network detects and see how each layer detects more and more complex ideas. Layer 1Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle). The images above are from Matthew Zeiler and Rob Fergus’ deep visualization toolbox, which lets us visualize what each layer in a CNN focuses on. Each image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. These squares are shown below again for reference.As visualized here, the first layer of the CNN can recognize -45 degree lines. The first layer of the CNN is also able to recognize +45 degree lines, like the one above. Let’s now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.Example patches that activate the -45 degree line detector in the first layer. So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs(斑点). Layer 2A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or “what it sees”) based on the corresponding images from the grid on the right. The second layer of the CNN captures complex ideas. As you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right). The CNN learns to do this on its own. There is no special instruction for the CNN to focus on more complex objects in deeper layers. That’s just how it normally works out when you feed training data into a CNN. Layer 3A visualization of the third layer in the CNN. The gray grid on the left represents how this layer of the CNN activates (or “what it sees”) based on the corresponding images from the grid on the right. The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column). Layer 5A visualization of the fifth and final layer of the CNN. The gray grid on the left represents how this layer of the CNN activates (or “what it sees”) based on the corresponding images from the grid on the right. We’ll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN. The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles. On to TensorFlowThis concludes our high-level discussion of Convolutional Neural Networks. Next you’ll practice actually building these networks in TensorFlow. TensorFlow Convolution LayerTensorFlow Convolution LayerLet’s examine how to implement a CNN in TensorFlow. TensorFlow provides the tf.nn.conv2d() and tf.nn.bias_add() functions to create your own convolutional layers. # Output depth k_output = 64 # Image Properties image_width = 10 image_height = 10 color_channels = 3 # Convolution filter filter_size_width = 5 filter_size_height = 5 # Input/Image input = tf.placeholder( tf.float32, shape=[None, image_height, image_width, color_channels]) # Weight and bias weight = tf.Variable(tf.truncated_normal( [filter_size_height, filter_size_width, color_channels, k_output])) bias = tf.Variable(tf.zeros(k_output)) # Apply Convolution conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) # Add bias conv_layer = tf.nn.bias_add(conv_layer, bias) # Apply activation function conv_layer = tf.nn.relu(conv_layer) The code above uses the tf.nn.conv2d() function to compute the convolution with weight as the filter and [1, 2, 2, 1] for the strides. TensorFlow uses a stride for each input dimension,[batch, input_height, input_width, input_channels]. We are generally always going to set the stride for batch and input_channels (i.e. the first and fourth element in the strides array) to be 1. You’ll focus on changing input_height and input_width while setting batch and input_channels to 1. The input_height and input_width strides are for striding the filter over input. This example code uses a stride of 2 with 5x5 filter over input. The tf.nn.bias_add() function adds a 1-d bias to the last dimension in a matrix. Explore The Design Spacehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FG7M9tWH2nQ.mp4 TensorFlow Max PoolingTensorFlow Max PoolingBy Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons The image above is an example of max pooling with a 2x2 filter and stride of 2. The four 2x2 colors represent each time the filter was applied to find the maximum value. For example, [[1, 0], [4, 6]] becomes 6, because 6 is the maximum value in this set. Similarly, [[2, 3], [6, 8]] becomes 8. Conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values. TensorFlow provides the tf.nn.max_pool() function to apply max pooling to your convolutional layers. ... conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) conv_layer = tf.nn.bias_add(conv_layer, bias) conv_layer = tf.nn.relu(conv_layer) # Apply Max Pooling conv_layer = tf.nn.max_pool( conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) The tf.nn.max_pool() function performs max pooling with the ksize parameter as the size of the filter and the strides parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice. The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). For both ksize and strides, the batch and channel dimensions are typically set to 1. Quiz: Pooling IntuitionThe next few quizzes will test your understanding of pooling layers.QUIZ QUESTION A pooling layer is generally used to … Increase the size of the output(correct)Decrease the size of the output(correct)Prevent overfitting Gain information Solution: Pooling IntuitionSolutionThe correct answer is decrease the size of the output and prevent overfitting. Preventing overfitting is a consequence of reducing the output size, which in turn, reduces the number of parameters in future layers.Recently, pooling layers have fallen out of favor. Some reasons are: Recent datasets are so big and complex we’re more concerned about underfitting. Dropout is a much better regularizer. Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely. Quiz: Pooling MechanicsSetupH = height, W = width, D = depth We have an input of shape 4x4x5 (HxWxD) Filter of shape 2x2 (HxW) A stride of 2 for both the height and width (S) Recall the formula for calculating the new height or width: new_height = (input_height - filter_height)/S + 1 new_width = (input_width - filter_width)/S + 1 NOTE: For a pooling layer the output depth is the same as the input depth. Additionally, the pooling operation is applied individually for each depth slice. The image below gives an example of how a max pooling layer works. In this case, the max pooling filter has a shape of 2x2. As the max pooling filter slides across the input layer, the filter will output the maximum value of the 2x2 square. Pooling Layer Output Shape What’s the shape of the output? Format is HxWxD.2x2x5 Solution: Pooling MechanicsSolutionThe answer is 2x2x5. Here’s how it’s calculated using the formula: (4 - 2)/2 + 1 = 2 (4 - 2)/2 + 1 = 2 The depth stays the same.Here’s the corresponding code: input = tf.placeholder(tf.float32, (None, 4, 4, 5)) filter_shape = [1, 2, 2, 1] strides = [1, 2, 2, 1] padding = &apos;VALID&apos; pool = tf.nn.max_pool(input, filter_shape, strides, padding) The output shape of pool will be [1, 2, 2, 5], even if padding is changed to &#39;SAME&#39;. Quiz: Pooling PracticeGreat, now let’s practice doing some pooling operations manually.Max PoolingWhat’s the result of a max pooling operation on the input: [[[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]]]Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1. The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4. Work from the top left to the bottom right Enter your response hereSUBMIT NEXT Solution: Pooling PracticeSolutionThe correct answer is 2.5,10,15,6. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time. max(0, 1, 2, 2.5) = 2.5 max(0.5, 10, 1, -8) = 10 max(4, 0, 15, 1) = 15 max(5, 6, 2, 3) = 6 Quiz: Average PoolingMean PoolingWhat’s the result of a average (or mean) pooling? [[[0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]]]Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1. The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4. Answer to 3 decimal places. Work from the top left to the bottom right Solution: Average PoolingSolutionThe correct answer is 1.375,0.875,5,4. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time. mean(0, 1, 2, 2.5) = 1.375 mean(0.5, 10, 1, -8) = 0.875 mean(4, 0, 15, 1) = 5 mean(5, 6, 2, 3) = 4 1x1 Convolutionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Zmzgerm6SjA.mp4 Inception Modulehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SlTm03bEOxA.mp4 Convolutional Network in TensorFlowConvolutional Network in TensorFlowIt’s time to walk through an example Convolutional Neural Network (CNN) in TensorFlow. The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers. The code you’ll be looking at is similar to what you saw in the segment on Deep Neural Network in TensorFlow, except we restructured the architecture of this network as a CNN. Just like in that segment, here you’ll study the line-by-line breakdown of the code. If you want, you can even download the code and run it yourself. Thanks to Aymeric Damien for providing the original TensorFlow model on which this segment is based. Time to dive in! DatasetYou’ve seen this section of code from previous lessons. Here we’re importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data. from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;.&quot;, one_hot=True, reshape=False) import tensorflow as tf # Parameters learning_rate = 0.00001 epochs = 10 batch_size = 128 # Number of samples to calculate validation and accuracy # Decrease this if you&apos;re running out of memory to calculate accuracy test_valid_size = 256 # Network Parameters n_classes = 10 # MNIST total classes (0-9 digits) dropout = 0.75 # Dropout, probability to keep units Weights and Biases# Store layers weight &amp; bias weights = { &apos;wc1&apos;: tf.Variable(tf.random_normal([5, 5, 1, 32])), &apos;wc2&apos;: tf.Variable(tf.random_normal([5, 5, 32, 64])), &apos;wd1&apos;: tf.Variable(tf.random_normal([7*7*64, 1024])), &apos;out&apos;: tf.Variable(tf.random_normal([1024, n_classes]))} biases = { &apos;bc1&apos;: tf.Variable(tf.random_normal([32])), &apos;bc2&apos;: tf.Variable(tf.random_normal([64])), &apos;bd1&apos;: tf.Variable(tf.random_normal([1024])), &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))} ConvolutionsConvolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution The above is an example of a convolution with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, [[1, 0, 1], [0, 1, 0], [1, 0, 1]], then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using tf.nn.conv2d() and tf.nn.bias_add(). def conv2d(x, W, b, strides=1): x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=’SAME’) x = tf.nn.bias_add(x, b) return tf.nn.relu(x)The tf.nn.conv2d() function computes the convolution against weight W as shown above. In TensorFlow, strides is an array of 4 elements; the first element in this array indicates the stride for batch and last element indicates stride for features. It’s good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them. You can always set the first and last element to 1 in strides in order to use all batches and features. The middle two elements are the strides for height and width respectively. I’ve mentioned stride as one number because you usually have a square stride where height = width. When someone says they are using a stride of 3, they usually mean tf.nn.conv2d(x, W, strides=[1, 3, 3, 1]). To make life easier, the code is using tf.nn.bias_add() to add the bias. Using tf.add() doesn’t work when the tensors aren’t the same shape. Max PoolingMax Pooling with 2x2 filter and stride of 2. Source: http://cs231n.github.io/convolutional-networks/ The above is an example of max pooling with a 2x2 filter and stride of 2. The left square is the input and the right square is the output. The four 2x2 colors in input represents each time the filter was applied to create the max on the right side. For example, [[1, 1], [5, 6]] becomes 6 and [[3, 2], [1, 2]] becomes 3. def maxpool2d(x, k=2): return tf.nn.max_pool( x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=’SAME’)The tf.nn.max_pool() function does exactly what you would expect, it performs max pooling with the ksize parameter as the size of the filter. ModelImage from Explore The Design Space video In the code below, we’re creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step. Then next step applies max pooling, turning each sample into 14x14x32. All the layers are applied from conv1 to output, producing 10 class predictions. def conv_net(x, weights, biases, dropout): # Layer 1 - 28*28*1 to 14*14*32 conv1 = conv2d(x, weights[&apos;wc1&apos;], biases[&apos;bc1&apos;]) conv1 = maxpool2d(conv1, k=2) # Layer 2 - 14*14*32 to 7*7*64 conv2 = conv2d(conv1, weights[&apos;wc2&apos;], biases[&apos;bc2&apos;]) conv2 = maxpool2d(conv2, k=2) # Fully connected layer - 7*7*64 to 1024 fc1 = tf.reshape(conv2, [-1, weights[&apos;wd1&apos;].get_shape().as_list()[0]]) fc1 = tf.add(tf.matmul(fc1, weights[&apos;wd1&apos;]), biases[&apos;bd1&apos;]) fc1 = tf.nn.relu(fc1) fc1 = tf.nn.dropout(fc1, dropout) # Output Layer - class prediction - 1024 to 10 out = tf.add(tf.matmul(fc1, weights[&apos;out&apos;]), biases[&apos;out&apos;]) return out SessionNow let’s run it! # tf Graph input x = tf.placeholder(tf.float32, [None, 28, 28, 1]) y = tf.placeholder(tf.float32, [None, n_classes]) keep_prob = tf.placeholder(tf.float32) # Model logits = conv_net(x, weights, biases, keep_prob) # Define loss and optimizer cost = tf.reduce_mean(\ tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)) optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ .minimize(cost) # Accuracy correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # Initializing the variables init = tf. global_variables_initializer() # Launch the graph with tf.Session() as sess: sess.run(init) for epoch in range(epochs): for batch in range(mnist.train.num_examples//batch_size): batch_x, batch_y = mnist.train.next_batch(batch_size) sess.run(optimizer, feed_dict={ x: batch_x, y: batch_y, keep_prob: dropout}) # Calculate batch loss and accuracy loss = sess.run(cost, feed_dict={ x: batch_x, y: batch_y, keep_prob: 1.}) valid_acc = sess.run(accuracy, feed_dict={ x: mnist.validation.images[:test_valid_size], y: mnist.validation.labels[:test_valid_size], keep_prob: 1.}) print(&apos;Epoch {:&gt;2}, Batch {:&gt;3} -&apos; &apos;Loss: {:&gt;10.4f} Validation Accuracy: {:.6f}&apos;.format( epoch + 1, batch + 1, loss, valid_acc)) # Calculate Test Accuracy test_acc = sess.run(accuracy, feed_dict={ x: mnist.test.images[:test_valid_size], y: mnist.test.labels[:test_valid_size], keep_prob: 1.}) print(&apos;Testing Accuracy: {}&apos;.format(test_acc)) That’s it! That is a CNN in TensorFlow. Now that you’ve seen a CNN in TensorFlow, let’s see if you can apply it on your own! TensorFlow Convolution LayerUsing Convolution Layers in TensorFlowLet’s now apply what we’ve learned to build real CNNs in TensorFlow. In the below exercise, you’ll be asked to set up the dimensions of the Convolution filters, the weights, the biases. This is in many ways the trickiest part to using CNNs in TensorFlow. Once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straight forward. ReviewYou should go over the TensorFlow documentation for 2D convolutions. Most of the documentation is straightforward, except perhaps the padding argument. The padding might differ depending on whether you pass &#39;VALID&#39; or &#39;SAME&#39;. Here are a few more things worth reviewing: Introduction to TensorFlow -&gt; TensorFlow Variables.How to determine the dimensions of the output based on the input size and the filter size (shown below). You’ll use this to determine what the size of your filter should be. new_height = (input_height - filter_height + 2 P)/S + 1 new_width = (input_width - filter_width + 2 P)/S + 1 Instructions Finish off each TODO in the conv2d function. Setup the strides, padding and filter weight/bias (F_w and F_b) such that the output shape is (1, 2, 2, 3). Note that all of these except strides should be TensorFlow variables. 12345678910111213141516171819202122232425262728293031323334&quot;&quot;&quot;Setup the strides, padding and filter weight/bias such thatthe output shape is (1, 2, 2, 3).&quot;&quot;&quot;import tensorflow as tfimport numpy as np# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)# (1, 4, 4, 1)x = np.array([ [0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))X = tf.constant(x)def conv2d(input): # Filter (weights and bias) # The shape of the filter weight is (height, width, input_depth, output_depth) # The shape of the filter bias is (output_depth,) # TODO: Define the filter weights `F_W` and filter bias `F_b`. # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all. F_W = ? F_b = ? # TODO: Set the stride for each dimension (batch_size, height, width, depth) strides = [?, ?, ?, ?] # TODO: set the padding, either &apos;VALID&apos; or &apos;SAME&apos;. padding = ? # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after. return tf.nn.conv2d(input, F_W, strides, padding) + F_bout = conv2d(X) Solution: TensorFlow Convolution LayerSolutionHere’s how I did it. NOTE: there’s more than 1 way to get the correct output shape. Your answer might differ from mine. def conv2d(input): # Filter (weights and bias) F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) F_b = tf.Variable(tf.zeros(3)) strides = [1, 2, 2, 1] padding = &apos;VALID&apos; return tf.nn.conv2d(input, F_W, strides, padding) + F_b I want to transform the input shape (1, 4, 4, 1) to (1, 2, 2, 3). I choose ‘VALID’ for the padding algorithm. I find it simpler to understand and it achieves the result I’m looking for. out_height = ceil(float(in_height - filter_height + 1) / float(strides[1])) out_width = ceil(float(in_width - filter_width + 1) / float(strides[2])) Plugging in the values: out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 out_width = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 In order to change the depth from 1 to 3, I have to set the output depth of my filter appropriately: F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) # (height, width, input_depth, output_depth) F_b = tf.Variable(tf.zeros(3)) # (output_depth) The input has a depth of 1, so I set that as the input_depth of the filter. TensorFlow Pooling LayerUsing Pooling Layers in TensorFlowIn the below exercise, you’ll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding. You should go over the TensorFlow documentation for tf.nn.max_pool(). Padding works the same as it does for a convolution. InstructionsFinish off each TODO in the maxpool function.Setup the strides, padding and ksize such that the output shape after pooling is (1, 2, 2, 1). 123456789101112131415161718192021222324252627&quot;&quot;&quot;Set the values to `strides` and `ksize` such thatthe output shape after pooling is (1, 2, 2, 1).&quot;&quot;&quot;import tensorflow as tfimport numpy as np# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)# (1, 4, 4, 1)x = np.array([ [0, 1, 0.5, 10], [2, 2.5, 1, -8], [4, 0, 5, 6], [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))X = tf.constant(x)def maxpool(input): # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth) ksize = [?, ?, ?, ?] # TODO: Set the stride for each dimension (batch_size, height, width, depth) strides = [?, ?, ?, ?] # TODO: set the padding, either &apos;VALID&apos; or &apos;SAME&apos;. padding = ? # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool return tf.nn.max_pool(input, ksize, strides, padding) out = maxpool(X) Solution: TensorFlow Pooling LayerSolutionHere’s how I did it. NOTE: there’s more than 1 way to get the correct output shape. Your answer might differ from mine. def maxpool(input): ksize = [1, 2, 2, 1] strides = [1, 2, 2, 1] padding = ‘VALID’ return tf.nn.max_pool(input, ksize, strides, padding)I want to transform the input shape (1, 4, 4, 1) to (1, 2, 2, 1). I choose &#39;VALID&#39; for the padding algorithm. I find it simpler to understand and it achieves the result I’m looking for. out_height = ceil(float(in_height - filter_height + 1) / float(strides[1])) out_width = ceil(float(in_width - filter_width + 1) / float(strides[2])) Plugging in the values: out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 out_width = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2 The depth doesn’t change during a pooling operation so I don’t have to worry about that. CNNs - Additional ResourcesAdditional ResourcesThere are many wonderful free resources that allow you to go into more depth around Convolutional Neural Networks. In this course, our goal is to give you just enough intuition to start applying this concept on real world problems so you have enough of an exposure to explore more on your own. We strongly encourage you to explore some of these resources more to reinforce your intuition and explore different ideas. These are the resources we recommend in particular: Andrej Karpathy’s CS231n Stanford course on Convolutional Neural Networks. Michael Nielsen’s free book on Deep Learning. Goodfellow, Bengio, and Courville’s more advanced free book on Deep Learning. DEEP LEARNING PROJECTProject DetailsIntroduction to the Projecthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/awEYy2Df3hg.mp4 Starting the ProjectStarting the ProjectFor this assignment, you can find the image_classification folder containing the necessary project files on the Machine Learning projects GitHub, under the projects folder. You may download all of the files for projects we’ll use in this Nanodegree program directly from this repo. Please make sure that you use the most recent version of project files when completing a project! This project contains 3 files: image_classification.ipynb: This is the main file where you will be performing your work on the project. Two helper files, problem_unittests.py and helper.py Submitting the ProjectSubmitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Object Classification Program project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesWhen you are ready to submit your project, collect the following files and compress them into a single archive for upload. Alternatively, you may supply the following files on your GitHub Repo in a folder named image_recognition for ease of access: The image_classification.ipynb notebook file with all questions answered and all code cells executed and displaying output along with the .html version of the notebook. All helper files. Once you have collected these files and reviewed the project rubric, proceed to the project submission page. PROJECTImplement this project download this file from github open virtualbox copy downloaded file to my shared file C:\Users\SSQ\virtualbox share type sudo mount -t vboxsf virtualbox_share /mnt/ in ubuntu terminal type jupyter notebook image_classification.ipynb in the right directoryerror ImportError: No module named request (failed)try with anaconda3 in ubuntu download anaconda3 from this web type ./Anaconda3-4.3.1-Linux-x86_64.sh in terminal to run sh file Anaconda3 will now be installed into this location:/home/ssq/anaconda3installation finished.Do you wish the installer to prepend the Anaconda3 install locationto PATH in your /home/ssq/.bashrc ? [yes|no][no] &gt;&gt;&gt;You may wish to edit your .bashrc or prepend the Anaconda3 install location: $ export PATH=/home/ssq/anaconda3/bin:$PATH Thank you for installing Anaconda3! Share your notebooks and packages on Anaconda Cloud!Sign up for free: https://anaconda.org export PATH=/home/ssq/anaconda3/bin:$PATH in your .ipynb location conda create -n tensorflow error:ModuleNotFoundError: No module named &#39;tqdm&#39;method:conda install -c conda-forge tqdm Package plan for installation in environment /home/ssq/anaconda3: The following NEW packages will be INSTALLED: tqdm: 4.11.2-py36_0 conda-forge The following packages will be SUPERCEDED by a higher-priority channel: conda: 4.3.14-py36_0 --&gt; 4.2.13-py36_0 conda-forge conda-env: 2.6.0-0 --&gt; 2.6.0-0 conda-forge Proceed ([y]/n)? y Anaconda installation export PATH=/home/ssq/anaconda3/bin:$PATH source activate tensorflow 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yesconda install tensorflow conda install -c conda-forge tensorflow 12conda install pandas matplotlib jupyter notebook scipy scikit-learnpip install tensorflow pip3 install --upgrade pip (failed)pip3 install tensorflow in ubuntu from this web to create a new vb 安装增强 剪切板双向 set the shared file from this blog and type sudo mount -t vboxsf virtualbox_share /mnt/ type sudo apt install python3-pip in terminal to install python3 pip3 install tensorflow python3 and test (success)anaconda3 install in Win7 tensorflow download anaconda3 from this web 1234conda create -n tensorflow python=3.5activate tensorflowconda install pandas matplotlib jupyter notebook scipy scikit-learnpip install tensorflow (tensorflow) C:\Users\SSQ&gt;cd C:\Users\SSQ\virtualbox share\image-classification (tensorflow) C:\Users\SSQ\virtualbox share\image-classification&gt;jupyter notebook image_classification.ipynb ModuleNotFoundError: No module named &#39;tqdm&#39;method:(tensorflow) C:\Users\SSQ\virtualbox share\image-classification&gt;conda install tqdm anaconda3 install in win7 tensorflow-gpu view this page and this blog cuda_8.0.61_windows in win7 cudnn-8.0-windows7-x64-v6.0 1234conda create -n tensorflow-gpu python=3.5activate tensorflow-gpuconda install pandas matplotlib jupyter notebook scipy scikit-learnpip install tensorflow-gpu SubmissionImage ClassificationProject Submission Image Classification IntroductionIn this project, you’ll classify images from the CIFAR-10 dataset. The dataset consists of airplanes, dogs, cats, and other objects. The dataset will need to be preprocessed, then train a convolutional neural network on all the samples. You’ll normalize the images, one-hot encode the labels, build a convolutional layer, max pool layer, and fully connected layer. At then end, you’ll see their predictions on the sample images. Getting the project filesThe project files can be found in our public GitHub repo, in the image-classification folder. You can download the files from there, but it’s better to clone the repository to your computer This way you can stay up to date with any changes we make by pulling the changes to your local repository with git pull. Submission Ensure you’ve passed all the unit tests in the notebook. Ensure you pass all points on the rubric. When you’re done with the project, please save the notebook as an HTML file. You can do this by going to the File menu in the notebook and choosing “Download as” &gt; HTML. Ensure you submit both the Jupyter Notebook and it’s HTML version together. Package the “dlnd_image_classification.ipynb”, “helper.py”, “problem_unittests.py”, and the HTML file into a zip archive, or push the files from your GitHub repo. Hit Submit Project below! Submit Your Projectsubmitview submissionreference Career: Interview Practice INTERVIEW PRACTICE ML Interview Practice Technical Interview Practice Machine Learning SpecializationsCapstone ProposalPROJECTWriting up a Capstone proposalOverviewCapstone Proposal OverviewPlease note that once your Capstone Proposal has been submitted and you have passed the evaluation, you have to submit your Capstone project using the same proposal that you submitted. We do not allow the Capstone Proposal and the Capstone project to differ in terms of dataset and approach. In this capstone project proposal, prior to completing the following Capstone Project, you you will leverage what you’ve learned throughout the Nanodegree program to author a proposal for solving a problem of your choice by applying machine learning algorithms and techniques. A project proposal encompasses seven key points: The project’s domain background — the field of research where the project is derived; A problem statement — a problem being investigated for which a solution will be defined; The datasets and inputs — data or inputs being used for the problem; A solution statement — a the solution proposed for the problem given; A benchmark model — some simple or historical model or result to compare the defined solution to; A set of evaluation metrics — functional representations for how the solution can be measured; An outline of the project design — how the solution will be developed and results obtained. Capstone Proposal HighlightsThe capstone project proposal is designed to introduce you to writing proposals for major projects. Typically, before you begin working on a solution to a problem, a proposal is written to your peers, advisor, manager, etc., to outline the details of the problem, your research, and your approach to a solution. Things you will learn by completing this project proposal: How to research a real-world problem of interest. How to author a technical proposal document. How to organize a proposed workflow for designing a solution. DescriptionCapstone Proposal DescriptionThink about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as datasets, inputs, and research) to complete this project, and make the appropriate citations wherever necessary in your proposal. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. To determine whether your project and the problem you want to solve fits Udacity’s vision of a Machine Learning Capstone Project , please refer to the capstone proposal rubric and the capstone project rubric and make a note of each rubric criteria you will be evaluated on. A satisfactory project will have a proposal that clearly satisfies these requirements. Software and Data RequirementsSoftware RequirementsYour proposed project must be written in Python 2.7. Given the free-form nature of the machine learning capstone, the software and libraries you will need to successfully complete your work will vary depending on the chosen application area and problem definition. Because of this, it is imperative that all necessary software and libraries you consider using in your capstone project are accessible clearly documented. Please note that proprietary software, software that requires private licenses, or software behind a paywall or login account should be avoided. Data RequirementsEvery machine learning capstone project will most certainly require some form of dataset or input data structure (input text files, images, etc.). Similar to the software requirements above, the data you are considering must either be publicly accessible or provided by you during the submission process, and private or proprietary data should not be used without expressed permission. Please take into consideration the file size of your data — while there is no strict upper limit, input files that are excessively large may require reviewers longer than an acceptable amount of time to acquire all of your project files. This can take away from the reviewer’s time that could be put towards evaluating your proposal. If the data you are considering fits the criteria of being too large, consider whether you could work with a subset of the data instead, or provide a representative sample of the data. EthicsUdacity’s A/B Testing course, as part of the Data Analyst Nanodegree, has a segment that discusses the sensitivity of data and the expectation of privacy from those whose information has been collected. While most data you find available to the public will not have any ethical complications, it is extremely important that you are considering where the data you are using came from, and whether that data contains any sensitive information. For example, if you worked for a bank and wanted to use customers’ bank statements as part of your project, this would most likely be an unethical choice of data and should be avoided. If you have any questions regarding the nature of a dataset or software you intend to use for the capstone project, please send an email to machine-support@udacity.com with the subject “Capstone Project Dataset/Software Inquiry”. Proposal GuidelinesReport GuidelinesYour project submission will be evaluated on the written proposal that is submitted. Additionally, depending on the project you are proposing, other materials such as the data being used will be evaluated. It is expected that the proposal contains enough detail, documentation, analysis, and discussion to adequately reflect the work you intend to complete for the project. Because of this, it is extremely important that the proposal is written in a professional, standardized way, so those who review your project’s proposal are able to clearly identify each component of your project in the report. Without a properly written proposal, your project cannot be sufficiently evaluated. A project proposal template is provided for you to understand how a project proposal should be structured. We strongly encourage students to have a proposal that is approximately two to three pages in length. The Machine Learning Capstone Project proposal should be treated no different than a written research paper for academics. Your goal is to ultimately present the research you’ve discovered into the respective problem domain you’ve chosen, and then clearly articulate your intended project to your peers. The narrative found in the project proposal template provides for a “proposal checklist” that will aid you in fully completing a documented proposal. Please make use of this resource! Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Capstone Project Proposal rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include A project proposal, in PDF format only, with the name proposal.pdf, addressing each of the seven key points of a proposal. The recommended page length for a proposal is approximately two to three pages. Any additional supporting material such as datasets, images, or input files that are necessary for your project and proposal. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in an included README.md file.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionCapstone ProposalProject SubmissionIn this capstone project proposal, prior to completing the following Capstone Project, you you will leverage what you’ve learned throughout the Nanodegree program to author a proposal for solving a problem of your choice by applying machine learning algorithms and techniques. A project proposal encompasses seven key points: The project’s domain background — the field of research where the project is derived; A problem statement — a problem being investigated for which a solution will be defined; The datasets and inputs — data or inputs being used for the problem; A solution statement — a the solution proposed for the problem given; A benchmark model — some simple or historical model or result to compare the defined solution to; A set of evaluation metrics — functional representations for how the solution can be measured; An outline of the project design — how the solution will be developed and results obtained. Think about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as datasets, inputs, and research) to complete this project, and make the appropriate citations wherever necessary in your proposal. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. EvaluationYour project will be reviewed by a Udacity reviewer against the Capstone Project Proposal rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include A project proposal, in PDF format only, with the name proposal.pdf, addressing each of the seven key points of a proposal. The recommended page length for a proposal is approximately two to three pages. Any additional supporting material such as datasets, images, or input files that are necessary for your project and proposal. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in an included README.md file.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! submit Capstone ProjectPROJECTMachine Learning Capstone ProjectOverviewCapstone Project OverviewIn this capstone project, you will leverage what you’ve learned throughout the Nanodegree program to solve a problem of your choice by applying machine learning algorithms and techniques. You will first define the problem you want to solve and investigate potential solutions and performance metrics. Next, you will analyze the problem through visualizations and data exploration to have a better understanding of what algorithms and features are appropriate for solving it. You will then implement your algorithms and metrics of choice, documenting the preprocessing, refinement, and postprocessing steps along the way. Afterwards, you will collect results about the performance of the models used, visualize significant quantities, and validate/justify these values. Finally, you will construct conclusions about your results, and discuss whether your implementation adequately solves the problem. Capstone Project HighlightsThis project is designed to prepare you for delivering a polished, end-to-end solution report of a real-world problem in a field of interest. When developing new technology, or deriving adaptations of previous technology, properly documenting your process is critical for both validating and replicating your results. Things you will learn by completing this project: How to research and investigate a real-world problem of interest. How to accurately apply specific machine learning algorithms and techniques. How to properly analyze and visualize your data and results for validity. How to document and write a report of your work. DescriptionCapstone DescriptionThink about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as data sets) to complete this project, and make the appropriate citations wherever necessary in your report. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. Note: For students who have enrolled before October 17th, we strongly encourage that you look at the Capstone Proposal project that is available as an elective before this project. If you have an idea for your capstone project but aren’t ready to begin working on the implementation, or even if you want to get feedback on how you will approach a solution to your problem, you can use the Capstone Proposal project to have a peer-review from one of our Capstone Project reviewers! For whichever application area or problem you ultimately investigate, there are five major stages to this capstone project which you will move through and subsequently document. Each stage plays a significant role in the development life cycle of beginning with a problem definition and finishing with a polished, working solution. As you make your way through developing your project, be sure that you are also working on a rough draft of your project report, as it is the most important aspect to your submission! To determine whether your project and the problem you want to solve fits Udacity’s vision of a Machine Learning Capstone Project , please refer to the capstone project rubric and make a note of each rubric criteria you will be evaluated on. A satisfactory project will have a report that encompasses each stage and component of the rubric. Software and Data RequirementsSoftware RequirementsYour project must be written in Python 2.7. Given the free-form nature of the machine learning capstone, the software and libraries you will need to successfully complete your work will vary depending on the chosen application area and problem definition. Because of this, it is imperative that all necessary software and libraries used in your capstone project are accessible to the reviewer and clearly documented. Information regarding the software and libraries your project makes use of should be included in the README along with your submission. Please note that proprietary software, software that requires private licenses, or software behind a paywall or login account should be avoided. Data RequirementsEvery machine learning capstone project will most certainly require some form of dataset or input data structure (input text files, images, etc.). Similar to the software requirements above, the data you use must either be publicly accessible or provided by you during the submission process, and private or proprietary data should not be used without expressed permission. Please take into consideration the file size of your data — while there is no strict upper limit, input files that are excessively large may require reviewers longer than an acceptable amount of time to acquire all of your project files and/or execute the provided development code. This can take away from the reviewer’s time that could be put towards evaluating your submission. If the data you are working with fits the criteria of being too large, consider whether you can work with a subset of the data instead, or provide a representative sample of the data which the reviewer may use to verify the solution explored in the project. EthicsUdacity’s A/B Testing course, as part of the Data Analyst Nanodegree, has a segment that discusses the sensitivity of data and the expectation of privacy from those whose information has been collected. While most data you find available to the public will not have any ethical complications, it is extremely important that you are considering where the data you are using came from, and whether that data contains any sensitive information. For example, if you worked for a bank and wanted to use customers’ bank statements as part of your project, this would most likely be an unethical choice of data and should be avoided. Report GuidelinesReport GuidelinesYour project submission will be evaluated primarily on the report that is submitted. It is expected that the project report contains enough detail, documentation, analysis, and discussion to adequately reflect the work you completed for your project. Because of this, it is extremely important that the report is written in a professional, standardized way, so those who review your project submission are able to clearly identify each component of your project in the report. Without a properly written report, your project cannot be sufficiently evaluated. A project report template is provided for you to understand how a project report should be structured. We strongly encourage students to have a report that is approximately nine to fifteen pages in length. The Machine Learning Capstone Project report should be treated no different than a written research paper for academics. Your goal is to ultimately present the research you’ve discovered into the respective problem domain you’ve chosen, and then discuss each stage of the project as they are completed. The narrative found in the A project report template provides for a “report checklist” that will aid you in staying on track for both your project and the documentation in your report. Each stage can be found as a section that will guide you through each component of the project development life cycle. Please make use of this resource! Example ReportsExample Machine Learning Capstone ReportsIncluded in the project files for the Capstone are three example reports that were written by students just like yourselves. Because the written report for your project will be how you are evaluated, it is absolutely critical that you are producing a clear, detailed, well-written report that adequately reflects the work that you’ve completed for your Capstone. Following along with the Capstone Guidelines will be very helpful as you begin writing your report. Our first example report comes from graduate Martin Bede, whose project design in the field of computer vision, named “Second Sight”, was to create an Android application that would extract text from the device’s camera and read it aloud. Martin’s project cites the growing concern of vision loss as motivation for developing software that can aid those unable to see or read certain print. Our second example report comes from an anonymous graduate whose project design in the field of image recognition was to implement a Convolutional Neural Network (CNN) to train on the Cifar-10 dataset and successfully identify different objects in new images. This student describes with thorough detail how a CNN can be used quite effectively as a descriptor-learning image recognition algorithm. Our third example report comes from graduate Naoki Shibuya, who took advantage of the pre-curated robot motion planning “Plot and Navigate a Virtual Maze” project. Pay special attention to the emphasis Naoki places on discussing the methodology and results: Projects relying on technical implementations require valuable observations and visualizations of how the solution performs under various circumstances and constraints. Each example report given has many desirable qualities we expect from students when completing the Machine Learning Capstone project. Once you begin writing your project report for which ever problem domain you choose, be sure to reference these examples whenever necessary! Submitting the ProjectEvaluationYour project will be reviewed by a Udacity reviewer against the Machine Learning Capstone project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include Your capstone proposal document as proposal.pdf if you have completed the pre-requisite Capstone Proposal project. Please also include your review link in the student submission notes. A project report (in PDF format only) addressing the five major project development stages. The recommended page length for a project report is approximately nine to fifteen pages. Please do not export an iPython Notebook as PDF for your project report. All development Python code used for your project that is required to reproduce your implemented solution and result. Your code should be in a neat and well-documented format. Using iPython Notebooks is strongly encouraged for development. A README documentation file which briefly describes the software and libraries used in your project, including any necessary references to supporting material. If your project requires setup/startup, ensure that your README includes the necessary instructions. Any additional supporting material such as datasets, images, or input files that are necessary for your project’s development and implementation. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in your included README.Once you have collected these files and reviewed the project rubric, proceed to the project submission page. SubmissionCapstone ProjectProject SubmissionIn this capstone project, you will leverage what you’ve learned throughout the Nanodegree program to solve a problem of your choice by applying machine learning algorithms and techniques. You will first define the problem you want to solve and investigate potential solutions and performance metrics. Next, you will analyze the problem through visualizations and data exploration to have a better understanding of what algorithms and features are appropriate for solving it. You will then implement your algorithms and metrics of choice, documenting the preprocessing, refinement, and postprocessing steps along the way. Afterwards, you will collect results about the performance of the models used, visualize significant quantities, and validate/justify these values. Finally, you will construct conclusions about your results, and discuss whether your implementation adequately solves the problem. Think about a technical field or domain that you are passionate about, such as robotics, virtual reality, finance, natural language processing, or even artificial intelligence (the possibilities are endless!). Then, choose an existing problem within that domain that you are interested in which you could solve by applying machine learning algorithms and techniques. Be sure that you have collected all of the resources needed (such as datasets, inputs, and research) to complete this project, and make the appropriate citations wherever necessary in your proposal. Below are a few suggested problem areas you could explore if you are unsure what your passion is: Robot Motion Planning Healthcare Computer Vision Education Investment and Trading In addition, you may find a technical domain (along with the problem and dataset) as competitions on platforms such as Kaggle, or Devpost. This can be helpful for discovering a particular problem you may be interested in solving as an alternative to the suggested problem areas above. In many cases, some of the requirements for the capstone proposal are already defined for you when choosing from these platforms. Note: For students who have enrolled before October 17th, we strongly encourage that you look at the Capstone Proposal project that is available as an elective before this project. If you have an idea for your capstone project but aren’t ready to begin working on the implementation, or even if you want to get feedback on how you will approach a solution to your problem, you can use the Capstone Proposal project to have a peer-review from one of our Capstone Project reviewers! For whichever application area or problem you ultimately investigate, there are five major stages to this capstone project which you will move through and subsequently document. Each stage plays a significant role in the development life cycle of beginning with a problem definition and finishing with a polished, working solution. As you make your way through developing your project, be sure that you are also working on a rough draft of your project report, as it is the most important aspect to your submission! EvaluationYour project will be reviewed by a Udacity reviewer against the Machine Learning Capstone project rubric. Be sure to review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must be meeting specifications for you to pass. Submission FilesAt minimum, your submission will be required to have the following files listed below. If your submission method of choice is uploading an archive (*.zip), please take into consideration the total file size. You will need to include Your capstone proposal document as proposal.pdf if you have completed the pre-requisite Capstone Proposal project. Please also include your review link in the student submission notes. A project report (in PDF format only) addressing the five major project development stages. The recommended page length for a project report is approximately nine to fifteen pages. Please do not export an iPython Notebook as PDF for your project report. All development Python code used for your project that is required to reproduce your implemented solution and result. Your code should be in a neat and well-documented format. Using iPython Notebooks is strongly encouraged for development. A README documentation file which briefly describes the software and libraries used in your project, including any necessary references to supporting material. If your project requires setup/startup, ensure that your README includes the necessary instructions. Any additional supporting material such as datasets, images, or input files that are necessary for your project’s development and implementation. If these files are too large and you are uploading your submission, instead provide appropriate means of acquiring the necessary files in your included README. I’m Ready!When you’re ready to submit your project, click on the Submit Project button at the bottom of the page. If you are having any problems submitting your project or wish to check on the status of your submission, please email us at machine-support@udacity.com or visit us in the discussion forums. What’s Next?You will get an email as soon as your reviewer has feedback for you. In the meantime, review your next project and feel free to get started on it or the courses supporting it! Supporting MaterialsVideos Zip FileTHE MNIST DATABASE of handwritten digitsMachine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networksselfdrivingcars (2,2),25,500,128,200,success Testing Accuracy: 0.8081980186934564 First result1 convnet1 fully_con12345678conv2d_maxpool(x, 25, (2,2), (1,1), (2,2), (2,2))flatten(x)fully_conn(x, 500)tf.nn.dropout(x, keep_prob)output(x,10)epochs = 200batch_size = 128keep_probability = 0.5 save as capstone_model.meta Testing Accuracy: 0.808136261212445 Second result3 convnets1 fully_con123456789101112x = conv2d_maxpool(x, 10, (2,2), (1,1), (2,2), (2,2))x = conv2d_maxpool(x, 10, (2,2), (1,1), (2,2), (2,2))x = conv2d_maxpool(x, 10, (2,2), (1,1), (2,2), (2,2))flatten(x)fully_conn(x, 500)tf.nn.dropout(x, keep_prob)output(x,10)epochs = 200batch_size = 128keep_probability = 0.5validation_accuracy: 0.7073670029640198 Testing Accuracy: 0.8427798201640447with fully datasetsTesting Accuracy: 0.8851721937559089 Submit Your ProjectML Stanford]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Udacity</tag>
        <tag>NanoDegree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity cs101 Intro to CS Notebook]]></title>
    <url>%2F2017%2F02%2F05%2FUdacity%20cs101%20Intro%20to%20CS%20Notebook%2F</url>
    <content type="text"><![CDATA[Course can be found hereclassroomDone in 2017-05-01 LESSONSHow to manage dataMeasure Udacity123456789101112131415161718192021222324# Define a procedure, measure_udacity,# that takes as its input a list of strings,# and returns a number that is a count# of the number of elements in the input# list that start with the uppercase # letter &apos;U&apos;.def measure_udacity(word): count = 0 for i in word: if i.find(&apos;U&apos;)&gt;=0: print &apos;i: &apos;,i count+=1 print &apos;count: &apos;,count return count print measure_udacity([&apos;Dave&apos;,&apos;Sebastian&apos;,&apos;Katy&apos;])#&gt;&gt;&gt; 0print measure_udacity([&apos;Umika&apos;,&apos;Umberto&apos;])#&gt;&gt;&gt; 2 Find Element123456789101112131415161718192021# Define a procedure, find_element,# that takes as its inputs a list# and a value of any type, and# returns the index of the first# element in the input list that# matches the value.# If there is no matching element,# return -1.def find_element(U,u): if u in U: return U.index(u) else: return -1print find_element([1,2,3],3)#&gt;&gt;&gt; 2print find_element([&apos;alpha&apos;,&apos;beta&apos;],&apos;gamma&apos;)#&gt;&gt;&gt; -1 Union1234567891011121314151617181920212223# Define a procedure, union,# that takes as inputs two lists.# It should modify the first input# list to be the set union of the two# lists. You may assume the first list# is a set, that is, it contains no # repeated elements.def union(a,b): for e in b: if e not in a: a.append(e)# To test, uncomment all lines # below except those beginning with &gt;&gt;&gt;.a = [1,2,3]b = [2,4,6]union(a,b)print a #&gt;&gt;&gt; [1,2,3,4,6]print b#&gt;&gt;&gt; [2,4,6] Pop Quiz31.1 Get All LinksPrint All Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Inr_DYUqxk8.mp4 Quiz: Print All Linksprint_all_links must keep going until there are no more links to print. Think about looping forever (set while loop condition) until there are no more links (i.e. else:). What do you do when there are no more links (body of else: condition)? At 1:26, Dave uses a procedure get_page(). The code for this procedure is given later in the course, in Lesson 4. This is the code: def get_page(url): try: import urllib return urllib.urlopen(url).read() except: return &apos;&apos; Include this code above your get_next_target() procedure in your answer. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BFYeJzcejxM.mp4 Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xssLR71EuUw.mp4 Starting Get All Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/m3oEwba-yxU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/f4L30M_25AI.mp4 Updating Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8krkKyimMUA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4-5169UHZTM.mp4 Finishing Get All Linkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E3_IlnR_j44.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LUnnw_TBxPI.mp4 Finishing the Web Crawlerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/BWKxjRDadkI.mp4 Crawling Processhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/l_ulQLpFQJQ.mp4 Quiz: Crawling ProcessPseudo code from the video: start with tocrawl = [seed] crawled = [] while there are more pages tocrawl: pick a page from tocrawl add that page to crawled add all the link targets on this page to tocrawl return crawled The seed page where crawling begins. https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_DESjvmuSsA.mp4 Crawl Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bI3rP7tAGdA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XRkqyIvx39w.mp4 Crawl Web Loophttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h4pJFmz7l1g.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jRm4rYw1w6c.mp4 Crawl Ifhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lZhKW6QTmX0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sopj7b5XEfk.mp4 Finishing Crawl Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nQl4F9uMvGU.mp4 Quiz: Finishing Crawl WebHint: at some point, you will have to call get_page on page. It seems counterintuitive, but we use the word page to refer to both the url and the html of a webpage. The get_page procedure takes a url and returns html. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#Finish crawl webdef get_page(url): # This is a simulated get_page procedure so that you can test your # code on two pages &quot;http://xkcd.com/353&quot; and &quot;http://xkcd.com/554&quot;. # A procedure which actually grabs a page from the web will be # introduced in unit 4. try: if url == &quot;http://xkcd.com/353&quot;: return &apos;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt;&lt;?xml-stylesheet href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; type=&quot;text/css&quot; media=&quot;screen&quot; ?&gt;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.1//EN&quot; &quot;http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd&quot;&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt; &lt;head&gt; &lt;title&gt;xkcd: Python&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt; &lt;!--[if IE]&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/ecbbecc.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt;&lt;![endif]--&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/atom+xml&quot; title=&quot;Atom 1.0&quot; href=&quot;/atom.xml&quot; /&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/rss+xml&quot; title=&quot;RSS 2.0&quot; href=&quot;/rss.xml&quot; /&gt; &lt;link rel=&quot;icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;link rel=&quot;shortcut icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;div id=&quot;topContainer&quot;&gt; &lt;div id=&quot;topLeft&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt;\t&lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://xkcd.com/554&quot;&quot;&gt;Archive&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;\t &lt;li&gt;&lt;a href=&quot;http://blag.xkcd.com/&quot;&gt;News/Blag&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://store.xkcd.com/&quot;&gt;Store&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/about/&quot;&gt;About&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://forums.xkcd.com/&quot;&gt;Forums&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;topRight&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;div id=&quot;topRightContainer&quot;&gt; &lt;div id=&quot;logo&quot;&gt; &lt;a href=&quot;/&quot;&gt;&lt;img src=&quot;http://imgs.xkcd.com/s/9be30a7.png&quot; alt=&quot;xkcd.com logo&quot; height=&quot;83&quot; width=&quot;185&quot;/&gt;&lt;/a&gt; &lt;h2&gt;&lt;br /&gt;A webcomic of romance,&lt;br/&gt; sarcasm, math, and language.&lt;/h2&gt; &lt;div class=&quot;clearleft&quot;&gt;&lt;/div&gt; &lt;br /&gt;XKCD updates every Monday, Wednesday, and Friday. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;contentContainer&quot;&gt; &lt;div id=&quot;middleContent&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt;&lt;br/&gt;&lt;br /&gt;&lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/352/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_t&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/354/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;br/&gt;&lt;br/&gt;&lt;img src=&quot;http://imgs.xkcd.com/comics/python.png&quot; title=&quot;I wrote 20 short programs in Python yesterday. It was wonderful. Perl, Im leaving you.&quot; alt=&quot;Python&quot; /&gt;&lt;br/&gt;&lt;br/&gt;&lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/352/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_b&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/354/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;h3&gt;Permanent link to this comic: http://xkcd.com/353/&lt;/h3&gt;&lt;h3&gt;Image URL (for hotlinking/embedding): http://imgs.xkcd.com/comics/python.png&lt;/h3&gt;&lt;div id=&quot;transcript&quot; style=&quot;display: none&quot;&gt;[[ Guy 1 is talking to Guy 2, who is floating in the sky ]]Guy 1: You39;re flying! How?Guy 2: Python!Guy 2: I learned it last night! Everything is so simple!Guy 2: Hello world is just 39;print &amp;quot;Hello, World!&amp;quot; 39;Guy 1: I dunno... Dynamic typing? Whitespace?Guy 2: Come join us! Programming is fun again! It39;s a whole new world up here!Guy 1: But how are you flying?Guy 2: I just typed 39;import antigravity39;Guy 1: That39;s it?Guy 2: ...I also sampled everything in the medicine cabinet for comparison.Guy 2: But i think this is the python.&#123;&#123; I wrote 20 short programs in Python yesterday. It was wonderful. Perl, I39;m leaving you. &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;middleFooter&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;img src=&quot;http://imgs.xkcd.com/s/a899e84.jpg&quot; width=&quot;520&quot; height=&quot;100&quot; alt=&quot;Selected Comics&quot; usemap=&quot; comicmap&quot; /&gt; &lt;map name=&quot;comicmap&quot;&gt; &lt;area shape=&quot;rect&quot; coords=&quot;0,0,100,100&quot; href=&quot;/150/&quot; alt=&quot;Grownups&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;104,0,204,100&quot; href=&quot;/730/&quot; alt=&quot;Circuit Diagram&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;208,0,308,100&quot; href=&quot;/162/&quot; alt=&quot;Angular Momentum&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;312,0,412,100&quot; href=&quot;/688/&quot; alt=&quot;Self-Description&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;416,0,520,100&quot; href=&quot;/556/&quot; alt=&quot;Alternative Energy Revolution&quot; /&gt; &lt;/map&gt;&lt;br/&gt;&lt;br /&gt;Search comic titles and transcripts:&lt;br /&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/jsapi&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; google.load(\&quot;search\&quot;, \&quot;1\&quot;); google.setOnLoadCallback(function() &#123; google.search.CustomSearchControl.attachAutoCompletion( \&quot;012652707207066138651:zudjtuwe28q\&quot;, document.getElementById(\&quot;q\&quot;), \&quot;cse-search-box\&quot;); &#125;);&lt;/script&gt;&lt;form action=&quot;//www.google.com/cse&quot; id=&quot;cse-search-box&quot;&gt; &lt;div&gt; &lt;input type=&quot;hidden&quot; name=&quot;cx&quot; value=&quot;012652707207066138651:zudjtuwe28q&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;ie&quot; value=&quot;UTF-8&quot; /&gt; &lt;input type=&quot;text&quot; name=&quot;q&quot; id=&quot;q&quot; autocomplete=&quot;off&quot; size=&quot;31&quot; /&gt; &lt;input type=&quot;submit&quot; name=&quot;sa&quot; value=&quot;Search&quot; /&gt; &lt;/div&gt;&lt;/form&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/cse/brand?form=cse-search-box&amp;lang=en&quot;&gt;&lt;/script&gt;&lt;a href=&quot;/rss.xml&quot;&gt;RSS Feed&lt;/a&gt; - &lt;a href=&quot;/atom.xml&quot;&gt;Atom Feed&lt;/a&gt;&lt;br /&gt; &lt;br/&gt; &lt;div id=&quot;comicLinks&quot;&gt; Comics I enjoy:&lt;br/&gt; &lt;a href=&quot;http://www.qwantz.com&quot;&gt;Dinosaur Comics&lt;/a&gt;, &lt;a href=&quot;http://www.asofterworld.com&quot;&gt;A Softer World&lt;/a&gt;, &lt;a href=&quot;http://pbfcomics.com/&quot;&gt;Perry Bible Fellowship&lt;/a&gt;, &lt;a href=&quot;http://www.boltcity.com/copper/&quot;&gt;Copper&lt;/a&gt;, &lt;a href=&quot;http://questionablecontent.net/&quot;&gt;Questionable Content&lt;/a&gt;, &lt;a href=&quot;http://achewood.com/&quot;&gt;Achewood&lt;/a&gt;, &lt;a href=&quot;http://wondermark.com/&quot;&gt;Wondermark&lt;/a&gt;, &lt;a href=&quot;http://thisisindexed.com/&quot;&gt;Indexed&lt;/a&gt;, &lt;a href=&quot;http://www.buttercupfestival.com/buttercupfestival.htm&quot;&gt;Buttercup Festival&lt;/a&gt; &lt;/div&gt; &lt;br/&gt; Warning: this comic occasionally contains strong language (which may be unsuitable for children), unusual humor (which may be unsuitable for adults), and advanced mathematics (which may be unsuitable for liberal-arts majors).&lt;br/&gt; &lt;br/&gt; &lt;h4&gt;We did not invent the algorithm. The algorithm consistently finds Jesus. The algorithm killed Jeeves. &lt;br /&gt;The algorithm is banned in China. The algorithm is from Jersey. The algorithm constantly finds Jesus.&lt;br /&gt;This is not the algorithm. This is close.&lt;/h4&gt;&lt;br/&gt; &lt;div class=&quot;line&quot;&gt;&lt;/div&gt; &lt;br/&gt; &lt;div id=&quot;licenseText&quot;&gt; &lt;!-- &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border:none&quot; src=&quot;http://imgs.xkcd.com/static/somerights20.png&quot; /&gt;&lt;/a&gt;&lt;br/&gt; --&gt; This work is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;Creative Commons Attribution-NonCommercial 2.5 License&lt;/a&gt;.&lt;!-- &lt;rdf:RDF xmlns=&quot;http://web.resource.org/cc/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns &quot;&gt;&lt;Work rdf:about=&quot;&quot;&gt;&lt;dc:creator&gt;Randall Munroe&lt;/dc:creator&gt;&lt;dcterms:rightsHolder&gt;Randall Munroe&lt;/dcterms:rightsHolder&gt;&lt;dc:type rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;dc:source rdf:resource=&quot;http://www.xkcd.com/&quot;/&gt;&lt;license rdf:resource=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot; /&gt;&lt;/Work&gt;&lt;License rdf:about=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Reproduction&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Distribution&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Notice&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Attribution&quot; /&gt;&lt;prohibits rdf:resource=&quot;http://web.resource.org/cc/CommercialUse&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/DerivativeWorks&quot; /&gt;&lt;/License&gt;&lt;/rdf:RDF&gt; --&gt; &lt;br/&gt; This means you\&quot;re free to copy and share these comics (but not to sell them). &lt;a href=&quot;/license.html&quot;&gt;More details&lt;/a&gt;.&lt;br/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; &apos; elif url == &quot;http://xkcd.com/554&quot;: return &apos;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot; ?&gt; &lt;?xml-stylesheet href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; type=&quot;text/css&quot; media=&quot;screen&quot; ?&gt; &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.1//EN&quot; &quot;http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd&quot;&gt; &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt; &lt;head&gt; &lt;title&gt;xkcd: Not Enough Work&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/c40a9f8.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt; &lt;!--[if IE]&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://imgs.xkcd.com/s/ecbbecc.css&quot; media=&quot;screen&quot; title=&quot;Default&quot; /&gt;&lt;![endif]--&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/atom+xml&quot; title=&quot;Atom 1.0&quot; href=&quot;/atom.xml&quot; /&gt; &lt;link rel=&quot;alternate&quot; type=&quot;application/rss+xml&quot; title=&quot;RSS 2.0&quot; href=&quot;/rss.xml&quot; /&gt; &lt;link rel=&quot;icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;link rel=&quot;shortcut icon&quot; href=&quot;http://imgs.xkcd.com/s/919f273.ico&quot; type=&quot;image/x-icon&quot; /&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;div id=&quot;topContainer&quot;&gt; &lt;div id=&quot;topLeft&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/archive/&quot;&gt;Archive&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://blag.xkcd.com/&quot;&gt;News/Blag&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://store.xkcd.com/&quot;&gt;Store&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/about/&quot;&gt;About&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://forums.xkcd.com/&quot;&gt;Forums&lt;/a&gt;&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;topRight&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;div id=&quot;topRightContainer&quot;&gt; &lt;div id=&quot;logo&quot;&gt; &lt;a href=&quot;/&quot;&gt;&lt;img src=&quot;http://imgs.xkcd.com/s/9be30a7.png&quot; alt=&quot;xkcd.com logo&quot; height=&quot;83&quot; width=&quot;185&quot;/&gt;&lt;/a&gt; &lt;h2&gt;&lt;br /&gt;A webcomic of romance,&lt;br/&gt; sarcasm, math, and language.&lt;/h2&gt; &lt;div class=&quot;clearleft&quot;&gt;&lt;/div&gt; XKCD updates every Monday, Wednesday, and Friday. &lt;br /&gt; Blag: Remember geohashing? &lt;a href=&quot;http://blog.xkcd.com/2012/02/27/geohashing-2/&quot;&gt;Something pretty cool&lt;/a&gt; happened Sunday. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;contentContainer&quot;&gt; &lt;div id=&quot;middleContent&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;h1&gt;Not Enough Work&lt;/h1&gt;&lt;br/&gt; &lt;br /&gt; &lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/553/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_t&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/555/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;br/&gt; &lt;br/&gt; &lt;img src=&quot;http://imgs.xkcd.com/comics/not_enough_work.png&quot; title=&quot;It39;s even harder if you39;re an asshole who pronounces &amp;lt;&amp;gt; brackets.&quot; alt=&quot;Not Enough Work&quot; /&gt;&lt;br/&gt; &lt;br/&gt; &lt;div class=&quot;menuCont&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;/1/&quot;&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/553/&quot; accesskey=&quot;p&quot;&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://dynamic.xkcd.com/random/comic/&quot; id=&quot;rnd_btn_b&quot;&gt;Random&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/555/&quot; accesskey=&quot;n&quot;&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/&quot;&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;h3&gt;Permanent link to this comic: http://xkcd.com/554/&lt;/h3&gt; &lt;h3&gt;Image URL (for hotlinking/embedding): http://imgs.xkcd.com/comics/not_enough_work.png&lt;/h3&gt; &lt;div id=&quot;transcript&quot; style=&quot;display: none&quot;&gt;Narration: Signs your coders don39;t have enough work to do: [[A man sitting at his workstation; a female co-worker behind him]] Man: I39;m almost up to my old typing speed in dvorak [[Two men standing by a server rack]] Man 1: Our servers now support gopher. Man 1: Just in case. [[A woman standing near her workstation speaking to a male co-worker]] Woman: Our pages are now HTML, XHTML-STRICT, and haiku-compliant Man: Haiku? Woman: &amp;lt;div class=&amp;quot;main&amp;quot;&amp;gt; Woman: &amp;lt;span id=&amp;quot;marquee&amp;quot;&amp;gt; Woman: Blog!&amp;lt; span&amp;gt;&amp;lt; div&amp;gt; [[A woman sitting at her workstation]] Woman: Hey! Have you guys seen this webcomic? &#123;&#123;title text: It39;s even harder if you39;re an asshole who pronounces &amp;lt;&amp;gt; brackets.&#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;div id=&quot;middleFooter&quot; class=&quot;dialog&quot;&gt; &lt;div class=&quot;hd&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;div class=&quot;bd&quot;&gt; &lt;div class=&quot;c&quot;&gt; &lt;div class=&quot;s&quot;&gt; &lt;img src=&quot;http://imgs.xkcd.com/s/a899e84.jpg&quot; width=&quot;520&quot; height=&quot;100&quot; alt=&quot;Selected Comics&quot; usemap=&quot; comicmap&quot; /&gt; &lt;map name=&quot;comicmap&quot;&gt; &lt;area shape=&quot;rect&quot; coords=&quot;0,0,100,100&quot; href=&quot;/150/&quot; alt=&quot;Grownups&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;104,0,204,100&quot; href=&quot;/730/&quot; alt=&quot;Circuit Diagram&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;208,0,308,100&quot; href=&quot;/162/&quot; alt=&quot;Angular Momentum&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;312,0,412,100&quot; href=&quot;/688/&quot; alt=&quot;Self-Description&quot; /&gt; &lt;area shape=&quot;rect&quot; coords=&quot;416,0,520,100&quot; href=&quot;/556/&quot; alt=&quot;Alternative Energy Revolution&quot; /&gt; &lt;/map&gt;&lt;br/&gt;&lt;br /&gt; Search comic titles and transcripts:&lt;br /&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/jsapi&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; google.load(&quot;search&quot;, &quot;1&quot;); google.search.CustomSearchControl.attachAutoCompletion( &quot;012652707207066138651:zudjtuwe28q&quot;, document.getElementById(&quot;q&quot;), &quot;cse-search-box&quot;); &#125;); &lt;/script&gt; &lt;form action=&quot;//www.google.com/cse&quot; id=&quot;cse-search-box&quot;&gt; &lt;div&gt; &lt;input type=&quot;hidden&quot; name=&quot;cx&quot; value=&quot;012652707207066138651:zudjtuwe28q&quot; /&gt; &lt;input type=&quot;hidden&quot; name=&quot;ie&quot; value=&quot;UTF-8&quot; /&gt; &lt;input type=&quot;text&quot; name=&quot;q&quot; id=&quot;q&quot; autocomplete=&quot;off&quot; size=&quot;31&quot; /&gt; &lt;input type=&quot;submit&quot; name=&quot;sa&quot; value=&quot;Search&quot; /&gt; &lt;/div&gt; &lt;/form&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;//www.google.com/cse/brand?form=cse-search-box&amp;lang=en&quot;&gt;&lt;/script&gt; &lt;a href=&quot;/rss.xml&quot;&gt;RSS Feed&lt;/a&gt; - &lt;a href=&quot;/atom.xml&quot;&gt;Atom Feed&lt;/a&gt; &lt;br /&gt; &lt;br/&gt; &lt;div id=&quot;comicLinks&quot;&gt; Comics I enjoy:&lt;br/&gt; &lt;a href=&quot;http://threewordphrase.com/&quot;&gt;Three Word Phrase&lt;/a&gt;, &lt;a href=&quot;http://oglaf.com/&quot;&gt;Oglaf&lt;/a&gt; (nsfw), &lt;a href=&quot;http://www.smbc-comics.com/&quot;&gt;SMBC&lt;/a&gt;, &lt;a href=&quot;http://www.qwantz.com&quot;&gt;Dinosaur Comics&lt;/a&gt;, &lt;a href=&quot;http://www.asofterworld.com&quot;&gt;A Softer World&lt;/a&gt;, &lt;a href=&quot;http://buttersafe.com/&quot;&gt;Buttersafe&lt;/a&gt;, &lt;a href=&quot;http://pbfcomics.com/&quot;&gt;Perry Bible Fellowship&lt;/a&gt;, &lt;a href=&quot;http://questionablecontent.net/&quot;&gt;Questionable Content&lt;/a&gt;, &lt;a href=&quot;http://www.buttercupfestival.com/buttercupfestival.htm&quot;&gt;Buttercup Festival&lt;/a&gt; &lt;/div&gt; &lt;br/&gt; Warning: this comic occasionally contains strong language (which may be unsuitable for children), unusual humor (which may be unsuitable for adults), and advanced mathematics (which may be unsuitable for liberal-arts majors).&lt;br/&gt; &lt;br/&gt; &lt;h4&gt;We did not invent the algorithm. The algorithm consistently finds Jesus. The algorithm killed Jeeves. &lt;br /&gt;The algorithm is banned in China. The algorithm is from Jersey. The algorithm constantly finds Jesus.&lt;br /&gt;This is not the algorithm. This is close.&lt;/h4&gt;&lt;br/&gt; &lt;div class=&quot;line&quot;&gt;&lt;/div&gt; &lt;br/&gt; &lt;div id=&quot;licenseText&quot;&gt; &lt;!-- &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; style=&quot;border:none&quot; src=&quot;http://imgs.xkcd.com/static/somerights20.png&quot; /&gt;&lt;/a&gt;&lt;br/&gt; --&gt; This work is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;Creative Commons Attribution-NonCommercial 2.5 License&lt;/a&gt;. &lt;!-- &lt;rdf:RDF xmlns=&quot;http://web.resource.org/cc/&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns &quot;&gt;&lt;Work rdf:about=&quot;&quot;&gt;&lt;dc:creator&gt;Randall Munroe&lt;/dc:creator&gt;&lt;dcterms:rightsHolder&gt;Randall Munroe&lt;/dcterms:rightsHolder&gt;&lt;dc:type rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;dc:source rdf:resource=&quot;http://www.xkcd.com/&quot;/&gt;&lt;license rdf:resource=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot; /&gt;&lt;/Work&gt;&lt;License rdf:about=&quot;http://creativecommons.org/licenses/by-nc/2.5/&quot;&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Reproduction&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/Distribution&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Notice&quot; /&gt;&lt;requires rdf:resource=&quot;http://web.resource.org/cc/Attribution&quot; /&gt;&lt;prohibits rdf:resource=&quot;http://web.resource.org/cc/CommercialUse&quot; /&gt;&lt;permits rdf:resource=&quot;http://web.resource.org/cc/DerivativeWorks&quot; /&gt;&lt;/License&gt;&lt;/rdf:RDF&gt; --&gt; &lt;br/&gt; This means you&quot;re free to copy and share these comics (but not to sell them). &lt;a href=&quot;/license.html&quot;&gt;More details&lt;/a&gt;.&lt;br/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;ft&quot;&gt;&lt;div class=&quot;c&quot;&gt;&lt;/div&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; &apos; except: return &quot;&quot; return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef union(p,q): for e in q: if e not in p: p.append(e)def get_all_links(page): links = [] while True: url,endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] while tocrawl: page = tocrawl.pop() if page not in crawled: union(tocrawl,get_all_links(get_page(page))) crawled.append(page) return crawled https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sII5zYOFywM.mp4 Conclusionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Qm4wJi2Me6Y.mp4 Problem SetListshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fzaaNzGDcCg.mp4 Mutating Listshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kFEMVfPAP-A.mp4 Product List12345678910111213141516171819202122232425# Define a procedure, product_list,# that takes as input a list of numbers,# and returns a number that is# the result of multiplying all# those numbers together.def product_list(list_of_numbers): tem = 1 for i in list_of_numbers: tem = tem * i return temprint product_list([9])#&gt;&gt;&gt; 9print product_list([1,2,3,4])#&gt;&gt;&gt; 24print product_list([])#&gt;&gt;&gt; 1 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RTPL87SBv6o.mp4 Greatest123456789101112131415161718192021# Define a procedure, greatest,# that takes as input a list# of positive numbers, and# returns the greatest number# in that list. If the input# list is empty, the output# should be 0.def greatest(list_of_numbers): tem = 0 for i in list_of_numbers: if i&gt;tem: tem = i return temprint greatest([4,23,1])#&gt;&gt;&gt; 23print greatest([])#&gt;&gt;&gt; 0 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/okAtgJROqgs.mp4 Lists of Lists1234567891011121314151617181920212223242526272829303132333435363738394041424344# Define a procedure, total_enrollment,# that takes as an input a list of elements,# where each element is a list containing# three elements: a university name,# the total number of students enrolled,# and the annual tuition fees.# The procedure should return two numbers,# not a string, # giving the total number of students# enrolled at all of the universities# in the list, and the total tuition fees# (which is the sum of the number# of students enrolled times the# tuition fees for each university).udacious_univs = [[&apos;Udacity&apos;,90000,0]]usa_univs = [ [&apos;California Institute of Technology&apos;,2175,37704], [&apos;Harvard&apos;,19627,39849], [&apos;Massachusetts Institute of Technology&apos;,10566,40732], [&apos;Princeton&apos;,7802,37000], [&apos;Rice&apos;,5879,35551], [&apos;Stanford&apos;,19535,40569], [&apos;Yale&apos;,11701,40500] ]def total_enrollment(p): total_stu = 0 total_fee = 0 for a,b,c in p: total_stu = total_stu + b total_fee = total_fee + b* c return total_stu, total_fee#print total_enrollment(udacious_univs)#&gt;&gt;&gt; (90000,0)# The L is automatically added by Python to indicate a long# number. If you are trying the question in an outside # interpreter you might not see it.#print total_enrollment(usa_univs)#&gt;&gt;&gt; (77285,3058581079) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xk4fB0yfq58.mp4 Max Pages12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# The web crawler we built at the end of Unit 3 has some serious# flaws if we were going to use it in a real crawler. One# problem is if we start with a good seed page, it might# run for an extremely long time (even forever, since the# number of URLS on the web is not actually finite). This# question and the following one explore two different ways# to limit the pages that it can crawl.# Modify the crawl_web procedure to take a second parameter,# max_pages, that limits the number of pages to crawl.# Your procedure should terminate the crawl after# max_pages different pages have been crawled, or when# there are no more pages to crawl.# The following definition of get_page provides an interface# to the website found at http://www.udacity.com/cs101x/index.html# The function output order does not affect grading.def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl! &apos; &apos;&lt;p&gt; It is a good idea to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to &apos; &apos;crawl&lt;/a&gt; before you try to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; &apos; &apos;or &lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;. &apos; &apos;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &apos;) elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I &apos; &apos;am quite good at &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I cant get enough &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; The magic words are Squeamish Ossifrage! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) except: return &quot;&quot; return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef union(p,q): for e in q: if e not in p: p.append(e)def get_all_links(page): links = [] while True: url,endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed, max_pages): tocrawl = [seed] crawled = [] while tocrawl: page = tocrawl.pop() if page not in crawled and len(crawled) &lt; max_pages : union(tocrawl, get_all_links(get_page(page))) crawled.append(page) return crawledprint crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,1) #&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,3) #&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,500) #&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/crawling.html&apos;, #&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/kicking.html&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Mh0Rw9fV9UU.mp4 Max Depth123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148# # This question explores a different way (from the previous question)# to limit the pages that it can crawl.######### THREE GOLD STARS ## Yes, we really mean it! This is really tough (but doable) unless # you have some previous experience before this course.# Modify the crawl_web procedure to take a second parameter,# max_depth, that limits the depth of the search. We can # define the depth of a page as the number of links that must# be followed to reach that page starting from the seed page,# that is, the length of the shortest path from the seed to# the page. No pages whose depth exceeds max_depth should be# included in the crawl. # # For example, if max_depth is 0, the only page that should# be crawled is the seed page. If max_depth is 1, the pages# that should be crawled are the seed page and every page that # it links to directly. If max_depth is 2, the crawl should # also include all pages that are linked to by these pages.## Note that the pages in the crawl may be in any order.## The following definition of get_page provides an interface# to the website found at http://www.udacity.com/cs101x/index.html# The function output order does not affect grading.def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl! &apos; &apos;&lt;p&gt; It is a good idea to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to &apos; &apos;crawl&lt;/a&gt; before you try to &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; &apos; &apos;or &lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;. &apos; &apos;&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &apos;) elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I &apos; &apos;am quite good at &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; I cant get enough &apos; &apos;&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return (&apos;&lt;html&gt; &lt;body&gt; The magic words are Squeamish Ossifrage! &apos; &apos;&lt;/body&gt; &lt;/html&gt;&apos;) elif url == &quot;http://top.contributors/velak.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/jesyspa.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/forbiddenvoid.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/jesyspa.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/elssar.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/kilaws.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/forbiddenvoid.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/charlzz.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/johang.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/graemeblake.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/kilaws.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/tomvandenbosch.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/mathprof.html&quot;&gt;&apos;) elif url == &quot;http://top.contributors/graemeblake.html&quot;: return (&apos;&lt;a href=&quot;http://top.contributors/dreyescat.html&quot;&gt;&apos; &apos;&lt;a href=&quot;http://top.contributors/angel.html&quot;&gt;&apos;) elif url == &quot;A1&quot;: return &apos;&lt;a href=&quot;B1&quot;&gt; &lt;a href=&quot;C1&quot;&gt; &apos; elif url == &quot;B1&quot;: return &apos;&lt;a href=&quot;E1&quot;&gt;&apos; elif url == &quot;C1&quot;: return &apos;&lt;a href=&quot;D1&quot;&gt;&apos; elif url == &quot;D1&quot;: return &apos;&lt;a href=&quot;E1&quot;&gt; &apos; elif url == &quot;E1&quot;: return &apos;&lt;a href=&quot;F1&quot;&gt; &apos; except: return &quot;&quot; return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef union(p,q): for e in q: if e not in p: p.append(e)def get_all_links(page): links = [] while True: url,endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed,max_depth): tocrawl = [seed] crawled = [] count = 0 while tocrawl: page = tocrawl.pop() if page not in crawled and count &lt; max_depth: union(tocrawl, get_all_links(get_page(page))) crawled.append(page) count +=1 return crawledprint crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,0)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,1)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/crawling.html&apos;]print crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;,50)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/flying.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/walking.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/crawling.html&apos;,#&gt;&gt;&gt; &apos;http://www.udacity.com/cs101x/kicking.html&apos;]print crawl_web(&quot;http://top.contributors/forbiddenvoid.html&quot;,2)#&gt;&gt;&gt; [&apos;http://top.contributors/forbiddenvoid.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/graemeblake.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/angel.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/dreyescat.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/johang.html&apos;,#&gt;&gt;&gt; &apos;http://top.contributors/charlzz.html&apos;]print crawl_web(&quot;A1&quot;,3)#&gt;&gt;&gt; [&apos;A1&apos;, &apos;C1&apos;, &apos;B1&apos;, &apos;E1&apos;, &apos;D1&apos;, &apos;F1&apos;]# (May be in any order) 1234567891011121314def crawl_web(seed,max_depth): tocrawl = [seed] crawled = [] next_depth = [] depth = 0 while tocrawl and depth &lt;= max_depth: page = tocrawl.pop() if page not in crawled: union(next_depth, get_all_links(get_page(page))) crawled.append(page) if not tocrawl: tocrawl, next_depth = next_depth, [] depth = depth + 1 return crawled https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TRNyIIrB73Q.mp4 Sudoku12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# THREE GOLD STARS# Sudoku [http://en.wikipedia.org/wiki/Sudoku]# is a logic puzzle where a game# is defined by a partially filled# 9 x 9 square of digits where each square# contains one of the digits 1,2,3,4,5,6,7,8,9.# For this question we will generalize# and simplify the game.# Define a procedure, check_sudoku,# that takes as input a square list# of lists representing an n x n# sudoku puzzle solution and returns the boolean# True if the input is a valid# sudoku square and returns the boolean False# otherwise.# A valid sudoku square satisfies these# two properties:# 1. Each column of the square contains# each of the whole numbers from 1 to n exactly once.# 2. Each row of the square contains each# of the whole numbers from 1 to n exactly once.# You may assume the the input is square and contains at# least one row and column.correct = [[1,2,3], [2,3,1], [3,1,2]]incorrect = [[1,2,3,4], [2,3,1,3], [3,1,2,3], [4,4,4,4]]incorrect2 = [[1,2,3,4], [2,3,1,4], [4,1,2,3], [3,4,1,2]]incorrect3 = [[1,2,3,4,5], [2,3,1,5,6], [4,5,2,1,3], [3,4,5,2,1], [5,6,4,3,2]]incorrect4 = [[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;], [&apos;b&apos;,&apos;c&apos;,&apos;a&apos;], [&apos;c&apos;,&apos;a&apos;,&apos;b&apos;]]incorrect5 = [ [1, 1.5], [1.5, 1]] def check_sudoku(): #print check_sudoku(incorrect)#&gt;&gt;&gt; False#print check_sudoku(correct)#&gt;&gt;&gt; True#print check_sudoku(incorrect2)#&gt;&gt;&gt; False#print check_sudoku(incorrect3)#&gt;&gt;&gt; False#print check_sudoku(incorrect4)#&gt;&gt;&gt; False#print check_sudoku(incorrect5)#&gt;&gt;&gt; False https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/60wESSZRSp0.mp41234567891011121314151617181920def check_sudoku(p): n = len(p) digit = 1 while digit &lt;=n: i = 0 while i &lt; n: row_count = 0 col_count = 0 j = 0 while j &lt; n: if p[i][j] == digit: row_count = row_count + 1 if p[j][i] == digit: col_count = col_count + 1 j = j + 1 if row_count != 1 or col_count != 1: return False i = i+1 digit = digit + 1 return True Problem Set(Optional)Exploring List Properties1234567891011121314# Investigating adding and appending to lists# If you run the following four lines of codes, what are list1 and list2?list1 = [1,2,3,4]list2 = [1,2,3,4]list1 = list1 + [5, 6]list2.append([5, 6])# to check, you can print them out using the print statements below.print &quot;showing list1 and list2:&quot;print list1print list2 showing list1 and list2: [1, 2, 3, 4, 5, 6] [1, 2, 3, 4, [5, 6]] Symmetric Square1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# A list is symmetric if the first row is the same as the first column,# the second row is the same as the second column and so on. Write a# procedure, symmetric, which takes a list as input, and returns the# boolean True if the list is symmetric and False if it is not.def symmetric(p): # Your code here n = len(p) if n == 0: return True if len(p[0]) != n: return False i = 0 while i &lt; n: j = 0 while j &lt; n: if p[i][j] != p[j][i]: return False j = j +1 i = i +1 return True print symmetric([[1, 2, 3], [2, 3, 4], [3, 4, 1]])#&gt;&gt;&gt; Trueprint symmetric([[&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;], [&quot;dog&quot;, &quot;dog&quot;, &quot;fish&quot;], [&quot;fish&quot;, &quot;fish&quot;, &quot;cat&quot;]])#&gt;&gt;&gt; Trueprint symmetric([[&quot;cat&quot;, &quot;dog&quot;, &quot;fish&quot;], [&quot;dog&quot;, &quot;dog&quot;, &quot;dog&quot;], [&quot;fish&quot;,&quot;fish&quot;,&quot;cat&quot;]])#&gt;&gt;&gt; Falseprint symmetric([[1, 2], [2, 1]])#&gt;&gt;&gt; Trueprint symmetric([[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]])#&gt;&gt;&gt; Falseprint symmetric([[1,2,3], [2,3,1]])#&gt;&gt;&gt; False Mean of a List123456789101112131415161718192021222324252627# The mean of a set of numbers is the sum of the numbers divided by the# number of numbers. Write a procedure, list_mean, which takes a list of numbers# as its input and return the mean of the numbers in the list.# Hint: You will need to work out how to make your division into decimal# division instead of integer division. You get decimal division if any of# the numbers involved are decimals.def list_mean(p): n = len(p) if n ==0: return -1 sum = 0 for i in p: sum = sum+i return sum*1.0 / n print list_mean([1,2,3,4])#&gt;&gt;&gt; 2.5print list_mean([1,3,4,5,2])#&gt;&gt;&gt; 3.0print list_mean([])#&gt;&gt;&gt; ??? You decide. If you decide it should give an error, comment# out the print line above to prevent your code from being graded as# incorrect.print list_mean([2])#&gt;&gt;&gt; 2.0 Notes on lists Problem Set(Optional 2)Antisymmetric Square12345678910111213141516171819202122232425262728293031# By Dimitris_GR from forums# Modify Problem Set 31&apos;s (Optional) Symmetric Square to return True # if the given square is antisymmetric and False otherwise. # An nxn square is called antisymmetric if A[i][j]=-A[j][i] # for each i=0,1,...,n-1 and for each j=0,1,...,n-1.def antisymmetric(A): #Write your code here# Test Cases:print antisymmetric([[0, 1, 2], [-1, 0, 3], [-2, -3, 0]]) #&gt;&gt;&gt; Trueprint antisymmetric([[0, 0, 0], [0, 0, 0], [0, 0, 0]])#&gt;&gt;&gt; Trueprint antisymmetric([[0, 1, 2], [-1, 0, -2], [2, 2, 3]])#&gt;&gt;&gt; Falseprint antisymmetric([[1, 2, 5], [0, 1, -9], [0, 0, 1]])#&gt;&gt;&gt; False Recognize Identity Matrix12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# By Ashwath from forums# Given a list of lists representing a n * n matrix as input, # define a procedure that returns True if the input is an identity matrix # and False otherwise.# An IDENTITY matrix is a square matrix in which all the elements # on the principal/main diagonal are 1 and all the elements outside # the principal diagonal are 0. # (A square matrix is a matrix in which the number of rows # is equal to the number of columns)def is_identity_matrix(matrix): #Write your code here# Test Cases:matrix1 = [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]]print is_identity_matrix(matrix1)#&gt;&gt;&gt;Truematrix2 = [[1,0,0], [0,1,0], [0,0,0]]print is_identity_matrix(matrix2)#&gt;&gt;&gt;Falsematrix3 = [[2,0,0], [0,2,0], [0,0,2]]print is_identity_matrix(matrix3)#&gt;&gt;&gt;Falsematrix4 = [[1,0,0,0], [0,1,1,0], [0,0,0,1]]print is_identity_matrix(matrix4)#&gt;&gt;&gt;Falsematrix5 = [[1,0,0,0,0,0,0,0,0]]print is_identity_matrix(matrix5)#&gt;&gt;&gt;Falsematrix6 = [[1,0,0,0], [0,1,0,1], [0,0,1,0], [0,0,0,1]]print is_identity_matrix(matrix6)#&gt;&gt;&gt;Falsematrix7 = [[1, -1, 1], [0, 1, 0], [0, 0, 1]]print is_identity_matrix(matrix7)#&gt;&gt;&gt;False Numbers in Lists1234567891011121314151617181920212223242526272829# Numbers in lists by SeanMc from forums# define a procedure that takes in a string of numbers from 1-9 and# outputs a list with the following parameters:# Every number in the string should be inserted into the list.# If a number x in the string is less than or equal # to the preceding number y, the number x should be inserted # into a sublist. Continue adding the following numbers to the # sublist until reaching a number z that# is greater than the number y. # Then add this number z to the normal list and continue.#Hint - &quot;int()&quot; turns a string&apos;s element into a numberdef numbers_in_lists(string): # YOUR CODE#testcasesstring = &apos;543987&apos;result = [5,[4,3],9,[8,7]]print repr(string), numbers_in_lists(string) == resultstring= &apos;987654321&apos;result = [9,[8,7,6,5,4,3,2,1]]print repr(string), numbers_in_lists(string) == resultstring = &apos;455532123266&apos;result = [4, 5, [5, 5, 3, 2, 1, 2, 3, 2], 6, [6]]print repr(string), numbers_in_lists(string) == resultstring = &apos;123456789&apos;result = [1, 2, 3, 4, 5, 6, 7, 8, 9]print repr(string), numbers_in_lists(string) == result Frequency Analysis12345678910111213141516171819202122232425262728293031323334# Crypto Analysis: Frequency Analysis## To analyze encrypted messages, to find out information about the possible # algorithm or even language of the clear text message, one could perform # frequency analysis. This process could be described as simply counting # the number of times a certain symbol occurs in the given text. # For example:# For the text &quot;test&quot; the frequency of &apos;e&apos; is 1, &apos;s&apos; is 1 and &apos;t&apos; is 2.## The input to the function will be an encrypted body of text that only contains # the lowercase letters a-z. # As output you should return a list of the normalized frequency # for each of the letters a-z. # The normalized frequency is simply the number of occurrences, i, # divided by the total number of characters in the message, n.def freq_analysis(message): ## # Your code here ## return freq_list#Testsprint freq_analysis(&quot;abcd&quot;)#&gt;&gt;&gt; [0.25, 0.25, 0.25, 0.25, 0.0, ..., 0.0]print freq_analysis(&quot;adca&quot;)#&gt;&gt;&gt; [0.5, 0.0, 0.25, 0.25, 0.0, ..., 0.0]print freq_analysis(&apos;bewarethebunnies&apos;)#&gt;&gt;&gt; [0.0625, 0.125, 0.0, 0.0, ..., 0.0] Responding to QueriesIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gXkELecZYlk.mp4 Welcome to Unit 4! The notes for Unit 4 are here: PDF and web. By the end of this unit, we’ll have a working search engine that can crawl and build an index of set of web pages, and respond to keyword queries! You’ll also learn about designing and using complex data structures that build on the list structure we introduced in the previous unit. Data Structureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pv5-RgG1pdk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nNEXCEH0dEw.mp4 Add to Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B2J-bDQ4M1o.mp4123456789101112131415161718192021222324252627282930# Define a procedure, add_to_index,# that takes 3 inputs:# - an index: [[&lt;keyword&gt;,[&lt;url&gt;,...]],...]# - a keyword: String# - a url: String# If the keyword is already# in the index, add the url# to the list of urls associated# with that keyword.# If the keyword is not in the index,# add an entry to the index: [keyword,[url]]index = []def add_to_index(index,keyword,url):#add_to_index(index,&apos;udacity&apos;,&apos;http://udacity.com&apos;)#add_to_index(index,&apos;computing&apos;,&apos;http://acm.org&apos;)#add_to_index(index,&apos;udacity&apos;,&apos;http://npr.org&apos;)#print index#&gt;&gt;&gt; [[&apos;udacity&apos;, [&apos;http://udacity.com&apos;, &apos;http://npr.org&apos;]], #&gt;&gt;&gt; [&apos;computing&apos;, [&apos;http://acm.org&apos;]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SGkb6vqS7zA.mp4123456def add_to_index(index,keyword,url): for entry in index: if entry[0]== keyword: entry[1].append(url) return index.append([keyword,[url]]) Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hzDJhLS4yCo.mp412345678910111213141516171819202122232425262728# Define a procedure, lookup,# that takes two inputs:# - an index# - keyword# The procedure should return a list# of the urls associated# with the keyword. If the keyword# is not in the index, the procedure# should return an empty list.index = [[&apos;udacity&apos;, [&apos;http://udacity.com&apos;, &apos;http://npr.org&apos;]], [&apos;computing&apos;, [&apos;http://acm.org&apos;]]]def lookup(index,keyword): for entry in index: if entry[0]==keyword: return entry[1] return []print lookup(index,&apos;udacity&apos;)#&gt;&gt;&gt; [&apos;http://udacity.com&apos;,&apos;http://npr.org&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bVjECgrnKj4.mp4 Building the Web Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aRteT5uKqfg.mp4 Add Page to Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_5rpzWzFnJM.mp41234567891011121314151617181920212223242526272829303132333435# Define a procedure, add_page_to_index,# that takes three inputs:# - index# - url (String)# - content (String)# It should update the index to include# all of the word occurences found in the# page content by adding the url to the# word&apos;s associated url list.index = []def add_to_index(index,keyword,url): for entry in index: if entry[0] == keyword: entry[1].append(url) return index.append([keyword,[url]])def add_page_to_index(index,url,content): contents = content.split() for word in contents: add_to_index(index,word,url) add_page_to_index(index,&apos;fake.text&apos;,&quot;This is a test&quot;)print index#&gt;&gt;&gt; [[&apos;This&apos;, [&apos;fake.text&apos;]], [&apos;is&apos;, [&apos;fake.text&apos;]], [&apos;a&apos;, [&apos;fake.text&apos;]],#&gt;&gt;&gt; [&apos;test&apos;,[&apos;fake.text&apos;]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i3V-Aw4y-hg.mp4 Finishing the Web Crawlerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dQjsf-4cWo0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cPKnNmFTS80.mp4 Startuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1XElSoLZfKQ.mp4 The Internethttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ePw5eGJXuw8.mp4 Networkshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/dy4KsLNw1lU.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_8Xgtd4j7j8.mp4 Smoke Signalshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8B6WSjA7DG8.mp4 Latencyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6_1akTCAnt4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5NoF37dKsAI.mp4 Bandwidthhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/P83jTqcQ10A.mp4 Bitshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6HCFOyZI9tA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4OxrAgA30T8.mp4 Buckets of Bitshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IS7TO_lLXFE.mp4 What Is Your Bandwidth?https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jG252FaodkA.mp4 Traceroutehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kU30juVBCBg.mp4123456789101112131415161718192021222324252627282930313233343536373839C:\Users\SSQ&gt;tracert www.udacity.com通过最多 30 个跃点跟踪到 apollo-mesos-elb-berlioz2-prod-885022263.us-west-2.elb.amazonaws.com [52.32.68.151] 的路由: 1 2 ms 60 ms 1 ms 192.168.1.1 2 14 ms 2 ms 4 ms 222.199.225.1 3 1 ms 1 ms 1 ms 202.4.128.193 4 342 ms 415 ms 227 ms 202.4.128.213 5 631 ms 413 ms 727 ms 172.30.33.5 6 683 ms 390 ms 638 ms 10.255.100.161 7 635 ms 532 ms 680 ms 124.205.98.145 8 * * * 请求超时。 9 * * * 请求超时。 10 * 707 ms * 14.197.246.209 11 483 ms 709 ms * 221.4.0.134 12 784 ms 701 ms 700 ms 221.4.0.133 13 736 ms 698 ms 721 ms 120.80.3.37 14 * 719 ms 768 ms 120.81.0.101 15 * * 793 ms 219.158.111.253 16 886 ms 646 ms 825 ms 219.158.13.98 17 886 ms 605 ms 601 ms 219.158.103.94 18 1135 ms 855 ms 1167 ms 219.158.116.234 19 956 ms 910 ms 1132 ms sjp-brdr-04.inet.qwest.net [63.146.27.85] 20 1157 ms 1001 ms * tuk-edge-13.inet.qwest.net [67.14.4.206] 21 1181 ms 962 ms 1157 ms 65-122-235-170.dia.static.qwest.net [65.122.235.170] 22 * * * 请求超时。 23 * * * 请求超时。 24 * * * 请求超时。 25 * * * 请求超时。 26 * * * 请求超时。 27 1034 ms 995 ms 1037 ms 52.93.14.71 28 1007 ms * 1067 ms 52.93.14.70 29 * * * 请求超时。 30 955 ms 915 ms 926 ms 205.251.232.222跟踪完成。 Traveling Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bQqvRI8NSFo.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/8csDnLICd4w.mp4 Making a Networkhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jFElXIkFEhc.mp4 Protocolshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0U31-O4oEPc.mp4 Conclusionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/1wKlRFhn4zg.mp4 Lesson 16 Problem SetData Structureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6rE8vvdYn2c.mp4 Ben BitdiddleBen Bitdiddle suggests changing the index code by replacing the add_to_index and lookup procedures with the ones shown below the question. def add_to_index(index, keyword, url): index.append([keyword, url]) def lookup(index, keyword): result = [] for entry in index: if entry[0] == keyword: result.append(entry[1]) return result This changes the structure of index, but suppose the only way we use index is by calling add_to_index and lookup. How would this affect the search engine? **It would produce the wrong results for some lookup queries. It would produce the same results for all queries, but lookup would sometimes be faster than the original code. It would produce the same results for all queries, but add_to_index would be faster and lookup would usually be slower than the original code. It would produce the same results and take the same amount of time for all queries** Old Code def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return [] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/h5KA5t8yo3I.mp4 Networkinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rl7zOmndGLY.mp4 Better Splitting123456789101112131415161718192021222324252627282930313233# 1 Gold Star# The built-in &lt;string&gt;.split() procedure works# okay, but fails to find all the words on a page# because it only uses whitespace to split the# string. To do better, we should also use punctuation# marks to split the page into words.# Define a procedure, split_string, that takes two# inputs: the string to split and a string containing# all of the characters considered separators. The# procedure should return a list of strings that break# the source string up by the characters in the# splitlist.def split_string(source,splitlist):#out = split_string(&quot;This is a test-of the,string separation-code!&quot;,&quot; ,!-&quot;)#print out#&gt;&gt;&gt; [&apos;This&apos;, &apos;is&apos;, &apos;a&apos;, &apos;test&apos;, &apos;of&apos;, &apos;the&apos;, &apos;string&apos;, &apos;separation&apos;, &apos;code&apos;]#out = split_string(&quot;After the flood ... all the colors came out.&quot;, &quot; .&quot;)#print out#&gt;&gt;&gt; [&apos;After&apos;, &apos;the&apos;, &apos;flood&apos;, &apos;all&apos;, &apos;the&apos;, &apos;colors&apos;, &apos;came&apos;, &apos;out&apos;]#out = split_string(&quot;First Name,Last Name,Street Address,City,State,Zip Code&quot;,&quot;,&quot;)#print out#&gt;&gt;&gt;[&apos;First Name&apos;, &apos;Last Name&apos;, &apos;Street Address&apos;, &apos;City&apos;, &apos;State&apos;, &apos;Zip Code&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/alpdXaaSfGI.mp412345678910111213def split_string(source,splitlist): output=[] atsplit=True for char in source: if char in splitlist: atsplit =True else: if atsplit: output.append(char) atsplit=False else: output[-1] =output[-1]+char return output Improving the Index123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# The current index includes a url in the list of urls# for a keyword multiple times if the keyword appears# on that page more than once.# It might be better to only include the same url# once in the url list for a keyword, even if it appears# many times.# Modify add_to_index so that a given url is only# included once in the url list for a keyword,# no matter how many times that keyword appears.def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]])def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl!&lt;p&gt; It is a good idea to&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to crawl&lt;/a&gt; before you try to&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; or&lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I amquite good at &lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&lt;/body&gt; &lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I cant get enough&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return &apos;&apos;&apos;&lt;html&gt;&lt;body&gt;The magic words are Squeamish Ossifrage!&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; except: return &quot;&quot; return &quot;&quot;def union(a, b): for e in b: if e not in a: a.append(e)def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return indexdef add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url)def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None#index = crawl_web(&quot;http://www.udacity.com/cs101x/index.html&quot;)#print lookup(index,&quot;is&quot;)#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/index.html&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GD98Z_3cANU.mp412345678def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: if not url in entry[1]: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) Counting Clicks123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# 2 Gold Stars# One way search engines rank pages# is to count the number of times a# searcher clicks on a returned link.# This indicates that the person doing# the query thought this was a useful# link for the query, so it should be# higher in the rankings next time.# (In Unit 6, we will look at a different# way of ranking pages that does not depend# on user clicks.)# Modify the index such that for each url in a# list for a keyword, there is also a number# that counts the number of times a user# clicks on that link for this keyword.# The result of lookup(index,keyword) should# now be a list of url entries, where each url# entry is a list of a url and a number# indicating the number of times that url# was clicked for this query keyword.# You should define a new procedure to simulate# user clicks for a given link:# record_user_click(index,word,url)# that modifies the entry in the index for# the input word by increasing the count associated# with the url by 1.# You also will have to modify add_to_index# in order to correctly create the new data# structure, and to prevent the repetition of# entries as in homework 4-5.def record_user_click(index,keyword,url):def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]])def get_page(url): try: if url == &quot;http://www.udacity.com/cs101x/index.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; This is a test page for learning to crawl!&lt;p&gt; It is a good idea to&lt;a href=&quot;http://www.udacity.com/cs101x/crawling.html&quot;&gt;learn to crawl&lt;/a&gt; before you try to&lt;a href=&quot;http://www.udacity.com/cs101x/walking.html&quot;&gt;walk&lt;/a&gt; or&lt;a href=&quot;http://www.udacity.com/cs101x/flying.html&quot;&gt;fly&lt;/a&gt;.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/crawling.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I have not learned to crawl yet, but I amquite good at &lt;a href=&quot;http://www.udacity.com/cs101x/kicking.html&quot;&gt;kicking&lt;/a&gt;.&lt;/body&gt; &lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/walking.html&quot;: return &apos;&apos;&apos;&lt;html&gt; &lt;body&gt; I cant get enough&lt;a href=&quot;http://www.udacity.com/cs101x/index.html&quot;&gt;crawling&lt;/a&gt;!&lt;/body&gt;&lt;/html&gt;&apos;&apos;&apos; elif url == &quot;http://www.udacity.com/cs101x/flying.html&quot;: return &apos;&lt;html&gt;&lt;body&gt;The magic words are Squeamish Ossifrage!&lt;/body&gt;&lt;/html&gt;&apos; except: return &quot;&quot; return &quot;&quot;def union(a, b): for e in b: if e not in a: a.append(e)def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return indexdef add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url)def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None#Here is an example showing a sequence of interactions:index = crawl_web(&apos;http://www.udacity.com/cs101x/index.html&apos;)print lookup(index, &apos;good&apos;)#&gt;&gt;&gt; [[&apos;http://www.udacity.com/cs101x/index.html&apos;, 0],#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/crawling.html&apos;, 0]]record_user_click(index, &apos;good&apos;, &apos;http://www.udacity.com/cs101x/crawling.html&apos;)print lookup(index, &apos;good&apos;)#&gt;&gt;&gt; [[&apos;http://www.udacity.com/cs101x/index.html&apos;, 0],#&gt;&gt;&gt; [&apos;http://www.udacity.com/cs101x/crawling.html&apos;, 1]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XAb3iFZfOl0.mp4123456789101112131415161718def record_user_click(index, keyword, url): urls = lookup(index, keyword) if urls: for entry in urls: if entry[0] == url: entry[1] = entry[1]+1def add_to_index(index, keyword, url): # format of index: [[keyword, [[url, count], [url, count],..]],...] for entry in index: if entry[0] == keyword: for urls in entry[1]: if urls[0] == url: return entry[1].append([url,0]) return # not found, add new keyword to index index.append([keyword, [[url,0]]]) Time Spent at Routershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/jqNiHcMsS_Y.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TWEE2f1w55U.mp4 Problem Set(Optional)Word CountLatencyConverting SecondsDownload CalculatorHow Programs RunIntroductionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XJfrUOoQSOI.mp4Welcome to Unit 5! The notes for Unit 5 are here: PDF and web. The main goal of Unit 5 is to learn about how computer scientists measure cost, which is mostly about understanding how the resources needed to run a program scale with the size of its input. We’ll also learn about implementing and using a hash table, a data structure that will massively improve the performance of our search engine. It was a privilege to meet with Gabriel Weinberg, the founder of DuckDuckGo, to film this introduction. DuckDuckGo protects the privacy of its users and gets around 3 million searches per day. Gabriel’s blog is full of interesting articles about computing and startups. Making Things Fasthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/JIeuI6mknUk.mp4 Measuring Speedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5vnXm71KECU.mp4 Stopwatchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ms0iENK29jA.mp4 Spin Loophttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hBw8qWGHrEs.mp4 Predicting Run Timehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HBwT29hWXrs.mp4 Make Big Indexhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zxfXpB6U_0w.mp4 Index Size Vs. Timehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yYm5t1wLarM.mp4Sample timings: &gt;&gt;&gt; time_execution(&apos;lookup(index10000, &quot;udacity&quot;)&apos;) (None, 0.000968000000000302) &gt;&gt;&gt; time_execution(&apos;lookup(index10000, &quot;udacity&quot;)&apos;) (None, 0.000905999999863066) &gt;&gt;&gt; time_execution(&apos;lookup(index100000, &quot;udacity&quot;)&apos;) (None, 0.008590000000002652) &gt;&gt;&gt; time_execution(&apos;lookup(index100000, &quot;udacity&quot;)&apos;) (None, 0.008517999999998093) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5DHrCwtBuGU.mp4 Lookup Timehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/PsCqA6fJ1hk.mp4This quiz depends on the code for make_big_index(size) from a few segments before: def make_big_index(size): index = [] letters = [&apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;, &apos;a&apos;] while len(index) &lt; size: word = make_string(letters) add_to_index(index, word, &apos;fake&apos;) for i in range(len(letters) - 1, 0, -1): if letters[i] &lt; &apos;z&apos;: letters[i] = chr(ord(letters[i]) + 1) break else: letters[i] = &apos;a&apos; return index This quiz depends on the code for make_big_index(size) from a few segments before (as well as the code for lookup and add_to_index):https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/WtfufPxl8Mw.mp4 Worst Casehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TBylO5VopA4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/26jeGBtszyk.mp4 Fast Enoughhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lSakl4WtFiE.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rbYT97miQMY.mp4 Making Lookup Fasterhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/AArXvYMTCOM.mp4 Hash Tablehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/KxGQbWGPeak.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/fdddZ5zcHyI.mp4 Hash Functionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/xzQy09kBswM.mp4ord()ord(&#39;a&#39;)-&gt;97chr() Modulus Operatorhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/b2J5RyLdNy8.mp4 Modulus Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/l8cjHI9UbW4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nmV96-OcGi4.mp4 Equivalent Expressionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/yoUU_QDJv4o.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TRuWp6uRBKI.mp4 Bad Hashhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gGSY4yAusdk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qn99D3acUnA.mp4 Better Hash Functionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SKbp6T6C-0Q.mp41234567891011121314151617181920212223242526# Define a function, hash_string,# that takes as inputs a keyword# (string) and a number of buckets,# and returns a number representing# the bucket for that keyword.def hash_string(keyword,buckets):#print hash_string(&apos;a&apos;,12)#&gt;&gt;&gt; 1#print hash_string(&apos;b&apos;,12)#&gt;&gt;&gt; 2#print hash_string(&apos;a&apos;,13)#&gt;&gt;&gt; 6#print hash_string(&apos;au&apos;,12)#&gt;&gt;&gt; 10#print hash_string(&apos;udacity&apos;,12)#&gt;&gt;&gt; 11 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9vGXclxo8Kc.mp4My claim about the performance being better with the % buckets inside the loop is (often, and possibly always?) incorrect. Some enterprising students have done experiments showing this, and there is more discussion in the forum. Testing Hash Functionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TjWwI-MvEhI.mp4 Keywords and Bucketshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2cL69wIOpVk.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0k-hAMfA5uY.mp4 Implementing Hash Tableshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sRfcPW1Rj_4.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/3AN9tyu_w-I.mp4 Empty Hash Tablehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/a_NjE-wJQGc.mp41234567# Creating an Empty Hash Table# Define a procedure, make_hashtable,# that takes as input a number, nbuckets,# and returns an empty hash table with# nbuckets empty buckets.def make_hashtable(nbuckets): https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e3gDr_MWqDA.mp4 The Hard Wayhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cpAkNOOdzLw.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GY8OtZj6LZA.mp4 Finding Bucketshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ZOo7GAm2qU.mp4123456789101112131415161718192021222324252627282930313233# Define a procedure, hashtable_get_bucket,# that takes two inputs - a hashtable, and# a keyword, and returns the bucket where the# keyword could occur.def hashtable_get_bucket(htable,keyword):def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return table#table = [[[&apos;Francis&apos;, 13], [&apos;Ellis&apos;, 11]], [], [[&apos;Bill&apos;, 17],#[&apos;Zoe&apos;, 14]], [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Rochelle&apos;, 4], [&apos;Nick&apos;, 2]]]#print hashtable_get_bucket(table, &quot;Zoe&quot;)#&gt;&gt;&gt; [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]]#print hashtable_get_bucket(table, &quot;Brick&quot;)#&gt;&gt;&gt; []#print hashtable_get_bucket(table, &quot;Lilith&quot;)#&gt;&gt;&gt; [[&apos;Louis&apos;, 29], [&apos;Rochelle&apos;, 4], [&apos;Nick&apos;, 2]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e_ZLxgElqks.mp4 Adding Keywordshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VFulPFO-OS0.mp4123456789101112131415161718192021222324252627282930313233343536373839404142434445# Define a procedure,## hashtable_add(htable,key,value)## that adds the key to the hashtable (in # the correct bucket), with the correct # value and returns the new hashtable.## (Note that the video question and answer# do not return the hashtable, but your code# should do this to pass the test cases.)def hashtable_add(htable,key,value): # your code here return htable def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return table#table = make_hashtable(5)#hashtable_add(table,&apos;Bill&apos;, 17)#hashtable_add(table,&apos;Coach&apos;, 4)#hashtable_add(table,&apos;Ellis&apos;, 11)#hashtable_add(table,&apos;Francis&apos;, 13)#hashtable_add(table,&apos;Louis&apos;, 29)#hashtable_add(table,&apos;Nick&apos;, 2)#hashtable_add(table,&apos;Rochelle&apos;, 4)#hashtable_add(table,&apos;Zoe&apos;, 14)#print table#&gt;&gt;&gt; [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]], #&gt;&gt;&gt; [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ge6HmR7EuDI.mp412345678910111213141516171819202122232425262728293031323334353637383940414243444546# Define a procedure,## hashtable_add(htable,key,value)## that adds the key to the hashtable (in # the correct bucket), with the correct # value and returns the new hashtable.## (Note that the video question and answer# do not return the hashtable, but your code# should do this to pass the test cases.)def hashtable_add(htable,key,value): # your code here bucket = hashtable_get_bucket(htable,key) bucket.append([key,value]) return htable def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return tabletable = make_hashtable(5)hashtable_add(table,&apos;Bill&apos;, 17)hashtable_add(table,&apos;Coach&apos;, 4)hashtable_add(table,&apos;Ellis&apos;, 11)hashtable_add(table,&apos;Francis&apos;, 13)hashtable_add(table,&apos;Louis&apos;, 29)hashtable_add(table,&apos;Nick&apos;, 2)hashtable_add(table,&apos;Rochelle&apos;, 4)hashtable_add(table,&apos;Zoe&apos;, 14)print table#&gt;&gt;&gt; [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]], #&gt;&gt;&gt; [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]] Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vJJ0wakAu7s.mp41234567891011121314151617181920212223242526272829303132333435363738394041424344# Define a procedure,# hashtable_lookup(htable,key)# that takes two inputs, a hashtable# and a key (string),# and returns the value associated# with that key.def hashtable_lookup(htable,key):def hashtable_add(htable,key,value): bucket = hashtable_get_bucket(htable,key) bucket.append([key,value])def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return table#table = [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]],#[[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]]#print hashtable_lookup(table, &apos;Francis&apos;)#&gt;&gt;&gt; 13#print hashtable_lookup(table, &apos;Louis&apos;)#&gt;&gt;&gt; 29#print hashtable_lookup(table, &apos;Zoe&apos;)#&gt;&gt;&gt; 14 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/cKkaGzt9pwk.mp4 Update1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Define a procedure,# hashtable_update(htable,key,value)# that updates the value associated with key. If key is already in the# table, change the value to the new value. Otherwise, add a new entry# for the key and value.# Hint: Use hashtable_lookup as a starting point.# Make sure that you return the new htabledef hashtable_update(htable,key,value): # Your code here return htabledef hashtable_lookup(htable,key): bucket = hashtable_get_bucket(htable,key) for entry in bucket: if entry[0] == key: return entry[1] return Nonedef hashtable_add(htable,key,value): bucket = hashtable_get_bucket(htable,key) bucket.append([key,value])def hashtable_get_bucket(htable,keyword): return htable[hash_string(keyword,len(htable))]def hash_string(keyword,buckets): out = 0 for s in keyword: out = (out + ord(s)) % buckets return outdef make_hashtable(nbuckets): table = [] for unused in range(0,nbuckets): table.append([]) return tabletable = [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [], [[&apos;Bill&apos;, 17], [&apos;Zoe&apos;, 14]],[[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], [&apos;Rochelle&apos;, 4]]]#hashtable_update(table, &apos;Bill&apos;, 42)#hashtable_update(table, &apos;Rochelle&apos;, 94)#hashtable_update(table, &apos;Zed&apos;, 68)#print table#&gt;&gt;&gt; [[[&apos;Ellis&apos;, 11], [&apos;Francis&apos;, 13]], [[&apos;Zed&apos;, 68]], [[&apos;Bill&apos;, 42], #&gt;&gt;&gt; [&apos;Zoe&apos;, 14]], [[&apos;Coach&apos;, 4]], [[&apos;Louis&apos;, 29], [&apos;Nick&apos;, 2], #&gt;&gt;&gt; [&apos;Rochelle&apos;, 94]]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UPiqKaXshfw.mp4 Dictionarieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Tne9hgBqCUY.mp4 Using Dictionarieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5wTxBLzR5aM.mp4For further information on Hash Tables in Python, please refer to this article here Populationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/em3CWlSaEKY.mp412345678910111213# Define a Dictionary, population,# that provides information# on the world&apos;s largest cities.# The key is the name of a city# (a string), and the associated# value is its population in# millions.# Key | Value# Shanghai | 17.8# Istanbul | 13.3# Karachi | 13.0# Mumbai | 12.5 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/H-eGAkgjg_s.mp4 A Noble Gashttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qJNsfRfFw-c.mp4 Modifying the Search Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ncC1XboU_lo.mp4Here’s the code in a more readable format:12345678910111213141516171819202122232425262728293031323334353637383940def get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return index def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None Here’s the code in a more readable way: (thanks to Christina-49) def get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return links12345678910111213141516171819202122232425262728293031def crawl_web(seed): tocrawl = [seed] crawled = [] index = [] while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) union(tocrawl, get_all_links(content)) crawled.append(page) return index def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): for entry in index: if entry[0] == keyword: entry[1].append(url) return # not found, add new keyword to index index.append([keyword, [url]]) def lookup(index, keyword): for entry in index: if entry[0] == keyword: return entry[1] return None Here’s the code in a more readable format: (thanks to Christina-49)https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/NBJx7q8XNpE.mp4 Changing Lookuphttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/OdToP6LQRoc.mp412345678# Change the lookup procedure# to now work with dictionaries.def lookup(index, keyword): if keyword in index: return index[keyword] else: return None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/avNhSME0qxQ.mp4 Coming Up Nexthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-qYyswP4FqI.mp4 Problem SetGrowth Measuring CostFor which of these procedures does the worst-case running time scale linearly in the number of elements in the input list p? (You may assume that the elements in the list are all small numbers) Sum_listdef sum_list(p): sum = 0 for e in p: sum = sum + e return sum Has_duplicate_element def has_duplicate_element(p): res = [] for i in range(0, len(p)): for j in range(0, len(p)): if i != j and p[i] == p[j]: return True return False Mystery def mystery(p): i = 0 while True: if i &gt;= len(p): break if p[i] % 2: i = i + 2 else: i = i + 1 return i Peter muddles up odd and even in the last question. The statement p[i] % 2 is True whenp[i] is odd and False when it is even, so the worst case is when all the elements in the list are even.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/tlFdhxXJzaw.mp4 Hash String Hash StringSuppose we have a hash table implemented as described in Unit 5 using the hash_string function. def hash_string(keyword, buckets): h = 0 for c in keyword: h = (h + ord(c)) % buckets return h Which of the following are true statements? Statement 1The number of string comparisons done to lookup a keyword that is not a key in the hash table may be less than the number needed to lookup a keyword that is a key in the hash table. Statement 2We should expect the time to lookup most keywords in the hash table will decrease as we increase the number of buckets. Statement 3It is always better to have more buckets in a hash table. Statement 4The time to lookup a keyword in the hash table is always less than the time it would take in a linear time list (as used in Unit 4). Is Offered12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# Dictionaries of Dictionaries (of Dictionaries)# The next several questions concern the data structure below for keeping# track of Udacity&apos;s courses (where all of the values are strings):# &#123; &lt;hexamester&gt;, &#123; &lt;class&gt;: &#123; &lt;property&gt;: &lt;value&gt;, ... &#125;,# ... &#125;,# ... &#125;#For example,courses = &#123; &apos;feb2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;, &apos;assistant&apos;: &apos;Andy&apos;&#125;&#125;, &apos;apr2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Sarah&apos;&#125;, &apos;cs212&apos;: &#123;&apos;name&apos;: &apos;The Design of Computer Programs&apos;, &apos;teacher&apos;: &apos;Peter N.&apos;, &apos;assistant&apos;: &apos;Andy&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs253&apos;: &#123;&apos;name&apos;: &apos;Web Application Engineering - Building a Blog&apos;, &apos;teacher&apos;: &apos;Steve&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs262&apos;: &#123;&apos;name&apos;: &apos;Programming Languages - Building a Web Browser&apos;, &apos;teacher&apos;: &apos;Wes&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;&#125;, &apos;cs387&apos;: &#123;&apos;name&apos;: &apos;Applied Cryptography&apos;, &apos;teacher&apos;: &apos;Dave&apos;&#125;&#125;, &apos;jan2044&apos;: &#123; &apos;cs001&apos;: &#123;&apos;name&apos;: &apos;Building a Quantum Holodeck&apos;, &apos;teacher&apos;: &apos;Dorina&apos;&#125;, &apos;cs003&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Robotics Teacher&apos;, &apos;teacher&apos;: &apos;Jasper&apos;&#125;, &#125; &#125;# If you want to loop through the keys in the dictionary,# you can use the construct below.# for &lt;key&gt; in &lt;dictionary&gt;:# &lt;block&gt; # For example, this procedure returns a list of all the courses offered # in the given hexamester:def courses_offered(courses, hexamester): res = [] for c in courses[hexamester]: res.append(c) return res# You do not need to use this code if you do not want to and may find another, # simpler method to answer this question, although later ones may require this.# Define a procedure, is_offered(courses, course, hexamester), that returns # True if the input course is offered in the input hexamester, and returns # False otherwise. For example,#print is_offered(courses, &apos;cs101&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; True#print is_offered(courses, &apos;cs003&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; False# (Note: it is okay if your procedure produces an error if the input # hexamester is not included in courses.# For example, is_offered(courses, &apos;cs101&apos;, &apos;dec2011&apos;) can produce an error.)# However, do not leave any uncommented statements in your code which lead # to an error as your code will be graded as incorrect.def is_offered(courses, course, hexamester):#print is_offered(courses, &apos;cs101&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; True#print is_offered(courses, &apos;cs003&apos;, &apos;apr2012&apos;)#&gt;&gt;&gt; False#print is_offered(courses, &apos;cs001&apos;, &apos;jan2044&apos;)#&gt;&gt;&gt; True#print is_offered(courses, &apos;cs253&apos;, &apos;feb2012&apos;)#&gt;&gt;&gt; False https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Qq8Hd290n5c.mp4 When Offered123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# Dictionaries of Dictionaries (of Dictionaries)# The next several questions concern the data structure below for keeping# track of Udacity&apos;s courses (where all of the values are strings):# &#123; &lt;hexamester&gt;, &#123; &lt;class&gt;: &#123; &lt;property&gt;: &lt;value&gt;, ... &#125;,# ... &#125;,# ... &#125;# For example,courses = &#123; &apos;feb2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;, &apos;assistant&apos;: &apos;Andy&apos;&#125;&#125;, &apos;apr2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Sarah&apos;&#125;, &apos;cs212&apos;: &#123;&apos;name&apos;: &apos;The Design of Computer Programs&apos;, &apos;teacher&apos;: &apos;Peter N.&apos;, &apos;assistant&apos;: &apos;Andy&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs253&apos;: &#123;&apos;name&apos;: &apos;Web Application Engineering - Building a Blog&apos;, &apos;teacher&apos;: &apos;Steve&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs262&apos;: &#123;&apos;name&apos;: &apos;Programming Languages - Building a Web Browser&apos;, &apos;teacher&apos;: &apos;Wes&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;&#125;, &apos;cs387&apos;: &#123;&apos;name&apos;: &apos;Applied Cryptography&apos;, &apos;teacher&apos;: &apos;Dave&apos;&#125;&#125;, &apos;jan2044&apos;: &#123; &apos;cs001&apos;: &#123;&apos;name&apos;: &apos;Building a Quantum Holodeck&apos;, &apos;teacher&apos;: &apos;Dorina&apos;&#125;, &apos;cs003&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Robotics Teacher&apos;, &apos;teacher&apos;: &apos;Jasper&apos;&#125;, &#125; &#125;# For the following questions, you will find the# for &lt;key&gt; in &lt;dictionary&gt;:# &lt;block&gt;# construct useful. This loops through the key values in the Dictionary. For# example, this procedure returns a list of all the courses offered in the given# hexamester:def courses_offered(courses, hexamester): res = [] for c in courses[hexamester]: res.append(c) return res# Define a procedure, when_offered(courses, course), that takes a courses data# structure and a string representing a class, and returns a list of strings# representing the hexamesters when the input course is offered.def when_offered(courses,course):#print when_offered (courses, &apos;cs101&apos;)#&gt;&gt;&gt; [&apos;apr2012&apos;, &apos;feb2012&apos;]#print when_offered(courses, &apos;bio893&apos;)#&gt;&gt;&gt; [] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hftOGwEW4qY.mp4 Involved12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# Dictionaries of Dictionaries (of Dictionaries)# The next several questions concern the data structure below for keeping# track of Udacity&apos;s courses (where all of the values are strings):# &#123; &lt;hexamester&gt;, &#123; &lt;class&gt;: &#123; &lt;property&gt;: &lt;value&gt;, ... &#125;,# ... &#125;,# ... &#125;# For example,courses = &#123; &apos;feb2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;, &apos;assistant&apos;: &apos;Andy&apos;&#125;&#125;, &apos;apr2012&apos;: &#123; &apos;cs101&apos;: &#123;&apos;name&apos;: &apos;Building a Search Engine&apos;, &apos;teacher&apos;: &apos;Dave&apos;, &apos;assistant&apos;: &apos;Sarah&apos;&#125;, &apos;cs212&apos;: &#123;&apos;name&apos;: &apos;The Design of Computer Programs&apos;, &apos;teacher&apos;: &apos;Peter N.&apos;, &apos;assistant&apos;: &apos;Andy&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs253&apos;: &#123;&apos;name&apos;: &apos;Web Application Engineering - Building a Blog&apos;, &apos;teacher&apos;: &apos;Steve&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs262&apos;: &#123;&apos;name&apos;: &apos;Programming Languages - Building a Web Browser&apos;, &apos;teacher&apos;: &apos;Wes&apos;, &apos;assistant&apos;: &apos;Peter C.&apos;, &apos;prereq&apos;: &apos;cs101&apos;&#125;, &apos;cs373&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Car&apos;, &apos;teacher&apos;: &apos;Sebastian&apos;&#125;, &apos;cs387&apos;: &#123;&apos;name&apos;: &apos;Applied Cryptography&apos;, &apos;teacher&apos;: &apos;Dave&apos;&#125;&#125;, &apos;jan2044&apos;: &#123; &apos;cs001&apos;: &#123;&apos;name&apos;: &apos;Building a Quantum Holodeck&apos;, &apos;teacher&apos;: &apos;Dorina&apos;&#125;, &apos;cs003&apos;: &#123;&apos;name&apos;: &apos;Programming a Robotic Robotics Teacher&apos;, &apos;teacher&apos;: &apos;Jasper&apos;&#125;, &#125; &#125;# For the following questions, you will find the# for &lt;key&gt; in &lt;dictionary&gt;:# &lt;block&gt;# construct useful. This loops through the key values in the Dictionary. For# example, this procedure returns a list of all the courses offered in the given# hexamester:def courses_offered(courses, hexamester): res = [] for c in courses[hexamester]: res.append(c) return res# [Double Gold Star] Define a procedure, involved(courses, person), that takes # as input a courses structure and a person and returns a Dictionary that # describes all the courses the person is involved in. A person is involved # in a course if they are a value for any property for the course. The output # Dictionary should have hexamesters as its keys, and each value should be a # list of courses that are offered that hexamester (the courses in the list # can be in any order).def involved(courses, person):# For example:#print involved(courses, &apos;Dave&apos;)#&gt;&gt;&gt; &#123;&apos;apr2012&apos;: [&apos;cs101&apos;, &apos;cs387&apos;], &apos;feb2012&apos;: [&apos;cs101&apos;]&#125;#print involved(courses, &apos;Peter C.&apos;)#&gt;&gt;&gt; &#123;&apos;apr2012&apos;: [&apos;cs262&apos;], &apos;feb2012&apos;: [&apos;cs101&apos;]&#125;#print involved(courses, &apos;Dorina&apos;)#&gt;&gt;&gt; &#123;&apos;jan2044&apos;: [&apos;cs001&apos;]&#125;#print involved(courses,&apos;Peter&apos;)#&gt;&gt;&gt; &#123;&#125;#print involved(courses,&apos;Robotic&apos;)#&gt;&gt;&gt; &#123;&#125;#print involved(courses, &apos;&apos;)#&gt;&gt;&gt; &#123;&#125; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ej9rXa13kr4.mp4 Refactoring1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 6. In video 28. Update, it was suggested that some of the duplicate code in# lookup and update could be avoided by a better design. We can do this by# defining a procedure that finds the entry corresponding to a given key, and# using that in both lookup and update.# Here are the original procedures:def hashtable_update(htable, key, value): bucket = hashtable_get_bucket(htable, key) for entry in bucket: if entry[0] == key: entry[1] = value return bucket.append([key, value])def hashtable_lookup(htable, key): bucket = hashtable_get_bucket(htable, key) for entry in bucket: if entry[0] == key: return entry[1] return Nonedef make_hashtable(size): table = [] for unused in range(0, size): table.append([]) return tabledef hash_string(s, size): h = 0 for c in s: h = h + ord(c) return h % sizedef hashtable_get_bucket(htable, key): return htable[hash_string(key, len(htable))]# Whenever we have duplicate code like the loop that finds the entry in# hashtable_update and hashtable_lookup, we should think if there is a better way# to write this that would avoid the duplication. We should be able to rewrite# these procedures to be shorter by defining a new procedure and rewriting both# hashtable_update and hashtable_lookup to use that procedure.# Modify the code for both hashtable_update and hashtable_lookup to have the same# behavior they have now, but using fewer lines of code in each procedure. You# should define a new procedure to help with this. Your new version should have# approximately the same running time as the original version, but neither# hashtable_update or hashtable_lookup should include any for or while loop, and# the block of each procedure should be no more than 6 lines long.# Your procedures should have the same behavior as the originals. For example,table = make_hashtable(10)hashtable_update(table, &apos;Python&apos;, &apos;Monty&apos;)hashtable_update(table, &apos;CLU&apos;, &apos;Barbara Liskov&apos;)hashtable_update(table, &apos;JavaScript&apos;, &apos;Brendan Eich&apos;)hashtable_update(table, &apos;Python&apos;, &apos;Guido van Rossum&apos;)print hashtable_lookup(table, &apos;Python&apos;)#&gt;&gt;&gt; Guido van Rossum https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EU9NvdGoAt4.mp4 Memoization1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# [Double Gold Star] Memoization is a way to make code run faster by saving# previously computed results. Instead of needing to recompute the value of an# expression, a memoized computation first looks for the value in a cache of# pre-computed values.# Define a procedure, cached_execution(cache, proc, proc_input), that takes in# three inputs: a cache, which is a Dictionary that maps inputs to proc to# their previously computed values, a procedure, proc, which can be called by# just writing proc(proc_input), and proc_input which is the input to proc.# Your procedure should return the value of the proc with input proc_input,# but should only evaluate it if it has not been previously called.def cached_execution(cache, proc, proc_input): # Your code here# Here is an example showing the desired behavior of cached_execution:def factorial(n): print &quot;Running factorial&quot; result = 1 for i in range(2, n + 1): result = result * i return resultcache = &#123;&#125; # start cache as an empty dictionary### first execution (should print out Running factorial and the result)print cached_execution(cache, factorial, 50)print &quot;Second time:&quot;### second execution (should only print out the result)print cached_execution(cache, factorial, 50)# Here is a more interesting example using cached_execution# (do not worry if you do not understand this, though,# it will be clearer after Unit 6):def cached_fibo(n): if n == 1 or n == 0: return n else: return (cached_execution(cache, cached_fibo, n - 1 ) + cached_execution(cache, cached_fibo, n - 2 )) cache = &#123;&#125; # new cache for this procedure# do not try this at home...at least without a cache!print cached_execution(cache, cached_fibo,100) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_aPiXzmiems.mp4 Problem Set(Optional)Shift a LetterShift n LetterRotateQ&amp;AHash Tableshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eiktSrhdrxs.mp4 Rehashinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/UMsVMW2S53w.mp4 Importing Librarieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/G3ovp33txfc.mp4 Programming Literacyhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0oXF2nOTX6I.mp4 How to Have Infinite PowerInfinite Powerhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/emhiUKHuBXY.mp4The notes for Unit 6 are here: PDF and web. This unit introduces what I think is the most fascinating and powerful idea in all of computing - recursive definitions. Understanding them requires some mind-bending gymnastics, but once you do, you will find elegant and powerful new ways to think about nearly all problems you encounter. The course moves through this pretty quickly, but fortunately many students have contributed great additional resources that explain things very well and with more detail than I do in the course, and give you more practice with recursive programs. Here are a few that I think are especially good: Yet Another Attempt to Explain Recursion by GoldsongUnderstanding Recursion: The Stack Model by Charles LinStG’s Recursion Collection by Sam the Great Long Wordshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-PhZlJuDf_o.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XzJO5xc3QIk.mp4 Counterhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ap3okJ5jIUE.mp4 Counter Quizhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6s-aT1rO3JQ.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YnPLnU9D3mQ.mp4 Expanding Our Grammarhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/qYsl757ShjA.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nU2DBYNw1jM.mp4 Recursive Definitionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LinhpqM4cCg.mp4 Ancestorshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_AQRlt9UA4o.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ip3vojOsIkI.mp4 Recursive Procedureshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Wa6y0I_uojk.mp4 Recursive FactorialAround ~0:06, Dave says that factorial takes a positive whole number as its input, but factorial can also take 0 as an input as well. Instead, then, the input to factorial can be any positive integer or 0. (Side note: whole numbers are defined differently in different contexts, but they are often defined as all of the non-negative integers. This means the whole numbers are 0, 1, 2, 3, 4…, and if we use this terminology, factorial could take any whole number as its input.) Note: If you get a The server encountered an error. Please try running again. error, that may mean that your program is not terminating when tested. Make sure your recursion will eventually reach a base case.1234567891011121314151617# Define a procedure, factorial, that takes a natural number as its input, and# returns the number of ways to arrange the input number of items.def factorial(n):#print factorial(0)#&gt;&gt;&gt; 1#print factorial(5)#&gt;&gt;&gt; 120#print factorial(10)#&gt;&gt;&gt; 3628800 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/gWoWZHonPdE.mp4123456789101112131415161718# Define a procedure, factorial, that takes a natural number as its input, and# returns the number of ways to arrange the input number of items.def factorial(n): if n==0: return 1 else: return n*factorial(n-1)print factorial(0)#&gt;&gt;&gt; 1print factorial(5)#&gt;&gt;&gt; 120print factorial(10)#&gt;&gt;&gt; 3628800 Palindromeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/zWnI5eACaLM.mp41234567891011121314151617181920212223242526# Define a procedure is_palindrome, that takes as input a string, and returns a# Boolean indicating if the input string is a palindrome.# Base Case: &apos;&apos; =&gt; True# Recursive Case: if first and last characters don&apos;t match =&gt; False# if they do match, is the middle a palindrome?def is_palindrome(s): if len(s)==0: return True else: if s[0]==s[-1]: s=s[1:-1] return is_palindrome(s) else: return False print is_palindrome(&apos;&apos;)#&gt;&gt;&gt; Trueprint is_palindrome(&apos;abab&apos;)#&gt;&gt;&gt; Falseprint is_palindrome(&apos;abba&apos;)#&gt;&gt;&gt; True https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LTZXRoLZaJQ.mp4 Recursive Vs Iterativehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kirruxKHaqk.mp4 Bunnieshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/E6oGm_Z2aQk.mp4123456789101112131415161718192021222324# Define a procedure, fibonacci, that takes a natural number as its input, and# returns the value of that fibonacci number.# Two Base Cases:# fibonacci(0) =&gt; 0# fibonacci(1) =&gt; 1# Recursive Case:# n &gt; 1 : fibonacci(n) =&gt; fibonacci(n-1) + fibonacci(n-2)def fibonacci(n): if n&gt;1: return fibonacci(n-1) + fibonacci(n-2) else: if n==1: return 1 else: return 0print fibonacci(0)#&gt;&gt;&gt; 0print fibonacci(1)#&gt;&gt;&gt; 1print fibonacci(15)#&gt;&gt;&gt; 610 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/S6wCTLG8BJg.mp4 Divide and Be Conqueredhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/eBu1Z_FBwb4.mp4Note that this is the standard mathematical definition of the Fibonacci sequence, which is a bit different from the counting rabbits motivation in the original problem. The mathematical sequence starts with 0, which is more elegant mathematically, but wouldn’t make as much sense for rabbits multiplying. Counting Callshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Cai4WuKg4SM.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/bqF8QuYX8YA.mp4At 1:58 minutes onwards, the formula should be fibo(36 - (n - 1)) = fibo(36 - n + 1) and not fibo(36 - n - 1). Faster Fibonaccihttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oJAHMzgSTyA.mp41234567891011#Define a faster fibonacci procedure that will enable us to computer#fibonacci(36).def fibonacci(n): current = 0 # fibonacci(0) at the bginning after = 1 # fibonacci(1) at the beginning for i in range(0, n): current, after = after, current + after return currentprint fibonacci(36)#&gt;&gt;&gt; 14930352 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7k7tMKxH6Dg.mp4 Ranking Web Pageshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/k32gyEM5H3Y.mp4 Popularityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EP55W6keH7E.mp4 Good Definitionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FX8RlDKEEiU.mp4A note on the notation: friends(p) is a list of all friends of p.https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/HbLwTw6N-0s.mp4 Circular Definitionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Rxp6JuoNqL0.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YUwZCZVtLaU.mp4 Relaxationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/e7-gweWZ0io.mp4https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/kpXVV8aiZFU.mp4 Page Rankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/IKXvSKaI2Ko.mp4 Altavistahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/T6AeBtfaLco.mp4AltaVista was finally shut down in July 2013. Here’s an interesting article from the Washington Post: AltaVista is dead. Here’s why it’s so hard to compete with Google. (mostly an interview with Gabriel Weinberg).https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/0ZlFPQ2qQo0.mp4 Urankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/H8vrZMAllIY.mp4 Implementing Urankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/B5lAVjLd76Q.mp4123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204# Modify the crawl_web procedure so that instead of just returning the # index, it returns an index and a graph. The graph should be a # Dictionary where the key:value entries are:# url: [list of pages url links to] def crawl_web(seed): # returns index, graph of outlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;:[list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) #Insert Code Here graph[page]=outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphcache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipes:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt; and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbanzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablespoons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &#125;def get_page(url): if url in cache: return cache[url] else: return None def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url]def lookup(index, keyword): if keyword in index: return index[keyword] else: return Noneindex , graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;) if &apos;http://udacity.com/cs101x/urank/index.html&apos; in graph: print graph[&apos;http://udacity.com/cs101x/urank/index.html&apos;]#&gt;&gt;&gt; [&apos;http://udacity.com/cs101x/urank/hummus.html&apos;,#&apos;http://udacity.com/cs101x/urank/arsenic.html&apos;,#&apos;http://udacity.com/cs101x/urank/kathleen.html&apos;,#&apos;http://udacity.com/cs101x/urank/nickel.html&apos;,#&apos;http://udacity.com/cs101x/urank/zinc.html&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sR8EJLpWwb4.mp4 Computing Page Rankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_ctzQdS3EfA.mp4 Formal Calculationshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YZ3kRWKL0DI.mp4 Computer Rankshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/sPaVbELrmh0.mp4 Finishing Urankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/GovJzUltdL8.mp4rank(page, time) is defined as: ∑​p∈inlinks​​​outlinks​​d⋅rank(t−1,p)​​ or: rank(page, 0) = 1/npages rank(page, t) = (1-d)/npages + sum (d * rank(p, t - 1) / number of outlinks from p) over all pages p that link to this page Thanks to Henry for suggesting to add this.The URLs have changed around a bit! Here’s a new index page you can start with to test out your search engine: https://www.udacity.com/cs101x/urank/index.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225#Finishing the page ranking algorithm.def compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages #Insert Code Here for p in graph: if page in graph[p]: newrank = newrank + d * ranks[p] / len(graph[p]) newranks[page] = newrank ranks = newranks return rankscache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipies:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt; and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbonzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablesppons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &#125;def crawl_web(seed): # returns index, graph of inlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;, [list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) graph[page] = outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphdef get_page(url): if url in cache: return cache[url] else: return None def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url]def lookup(index, keyword): if keyword in index: return index[keyword] else: return Noneindex, graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;)#print graphranks = compute_ranks(graph)print ranks#&gt;&gt;&gt; &#123;&apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: 0.11661866666666663,#&apos;http://udacity.com/cs101x/urank/zinc.html&apos;: 0.038666666666666655,#&apos;http://udacity.com/cs101x/urank/hummus.html&apos;: 0.038666666666666655,#&apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: 0.054133333333333325,#&apos;http://udacity.com/cs101x/urank/index.html&apos;: 0.033333333333333326,#&apos;http://udacity.com/cs101x/urank/nickel.html&apos;: 0.09743999999999997&#125; https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/k861qM5OqvU.mp4 Search Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/7IlDnp39b0U.mp4 Problem SetRecursive Grammarshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Ej9obZ0QECY.mp4 Rabbits Multiplying123456789101112131415161718192021222324252627282930313233343536373839404142# Rabbits Multiplying# A (slightly) more realistic model of rabbit multiplication than the Fibonacci# model, would assume that rabbits eventually die. For this question, some# rabbits die from month 6 onwards.## Thus, we can model the number of rabbits as:## rabbits(1) = 1 # There is one pair of immature rabbits in Month 1# rabbits(2) = 1 # There is one pair of mature rabbits in Month 2## For months 3-5:# Same as Fibonacci model, no rabbits dying yet# rabbits(n) = rabbits(n - 1) + rabbits(n - 2)### For months &gt; 5:# All the rabbits that are over 5 months old die along with a few others# so that the number that die is equal to the number alive 5 months ago.# Before dying, the bunnies reproduce.# rabbits(n) = rabbits(n - 1) + rabbits(n - 2) - rabbits(n - 5)## This produces the rabbit sequence: 1, 1, 2, 3, 5, 7, 11, 16, 24, 35, 52, ...## Define a procedure rabbits that takes as input a number n, and returns a# number that is the value of the nth number in the rabbit sequence.# For example, rabbits(10) -&gt; 35. (It is okay if your procedure takes too# long to run on inputs above 30.)def rabbits(n):#print rabbits(10)#&gt;&gt;&gt; 35#s = &quot;&quot;#for i in range(1,12):# s = s + str(rabbits(i)) + &quot; &quot;#print s#&gt;&gt;&gt; 1 1 2 3 5 7 11 16 24 35 52 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/pcGGCOPPtmE.mp4 Spreading Udaciousness12345678910111213141516171819202122232425262728293031323334353637383940414243# Spreading Udaciousness # One of our modest goals is to teach everyone in the world to program and# understand computer science. To estimate how long this will take we have# developed a (very flawed!) model:# Everyone answering this question will convince a number, spread, (input to # the model) of their friends to take the course next offering. This will # continue, so that all of the newly recruited students, as well as the original# students, will convince spread of their# friends to take the following offering of the course.# recruited friends are unique, so there is no duplication among the newly# recruited students. Define a procedure, hexes_to_udaciousness(n, spread,# target), that takes three inputs: the starting number of Udacians, the spread# rate (how many new friends each Udacian convinces to join each hexamester),# and the target number, and outputs the number of hexamesters needed to reach # (or exceed) the target.# For credit, your procedure must not use: while, for, or import math. def hexes_to_udaciousness(n, spread, target):# 0 more needed, since n already exceeds target#print hexes_to_udaciousness(100000, 2, 36230) #&gt;&gt;&gt; 0# after 1 hexamester, there will be 50000 + (50000 * 2) Udacians#print hexes_to_udaciousness(50000, 2, 150000) #&gt;&gt;&gt; 1 # need to match or exceed the target#print hexes_to_udaciousness(50000, 2, 150001)#&gt;&gt;&gt; 2 # only 12 hexamesters (2 years) to world domination!#print hexes_to_udaciousness(20000, 2, 7 * 10 ** 9) #&gt;&gt;&gt; 12 # more friends means faster world domination!#print hexes_to_udaciousness(15000, 3, 7 * 10 ** 9)#&gt;&gt;&gt; 10 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/q_atgGWy57Y.mp4 Deep Count1234567891011121314151617181920212223242526272829303132333435363738# Deep Count # The built-in len operator outputs the number of top-level elements in a List,# but not the total number of elements. For this question, your goal is to count# the total number of elements in a list, including all of the inner lists.# Define a procedure, deep_count, that takes as input a list, and outputs the# total number of elements in the list, including all elements in lists that it# contains.# For this procedure, you will need a way to test if a value is a list. We have# provided a procedure, is_list(p) that does this:def is_list(p): return isinstance(p, list)# It is not necessary to understand how is_list works. It returns True if the# input is a List, and returns False otherwise.def deep_count(p):#print deep_count([1, 2, 3])#&gt;&gt;&gt; 3# The empty list still counts as an element of the outer list#print deep_count([1, [], 3]) #&gt;&gt;&gt; 3 #print deep_count([1, [1, 2, [3, 4]]])#&gt;&gt;&gt; 7#print deep_count([[[[[[[[1, 2, 3]]]]]]]])#&gt;&gt;&gt; 10 Feeling Lucky123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231#Feeling Lucky #In Unit 6, we implemented a page ranking algorithm, but didn&apos;t finish the final#step of using it to improve our search results. For this question, you will use#the page rankings to produce the best output for a given query.#Define a procedure, lucky_search, that takes as input an index, a ranks#dictionary (the result of compute_ranks), and a keyword, and returns the one#URL most likely to be the best site for that keyword. If the keyword does not#appear in the index, lucky_search should return None.def lucky_search(index, ranks, keyword): cache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipies:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt; and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbonzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablesppons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;,&#125;def get_page(url): if url in cache: return cache[url] return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url) def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url] def lookup(index, keyword): if keyword in index: return index[keyword] else: return Nonedef crawl_web(seed): # returns index, graph of inlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;, [list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) graph[page] = outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphdef compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: newrank = newrank + d * (ranks[node] / len(graph[node])) newranks[page] = newrank ranks = newranks return ranks#Here&apos;s an example of how your procedure should work on the test site: #index, graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;)#ranks = compute_ranks(graph)#print lucky_search(index, ranks, &apos;Hummus&apos;)#&gt;&gt;&gt; http://udacity.com/cs101x/urank/kathleen.html#print lucky_search(index, ranks, &apos;the&apos;)#&gt;&gt;&gt; http://udacity.com/cs101x/urank/nickel.html#print lucky_search(index, ranks, &apos;babaganoush&apos;)#&gt;&gt;&gt; None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/6qVB4lZmzMc.mp4123456789def lucky_search(index, ranks, keyword): pages=lookup(index,keyword) if not pages: return None best_page=pages[0] for candidate in pages: if ranks[candidate]&gt;ranks[best_page]: best_page=candidate return best_page Problem Set 6 StarredFamily Trees123456789101112131415161718192021222324252627282930313233343536373839404142434445# Single Gold Star# Family Trees# In the lecture, we showed a recursive definition for your ancestors. For this# question, your goal is to define a procedure that finds someone&apos;s ancestors,# given a Dictionary that provides the parent relationships.# Here&apos;s an example of an input Dictionary:ada_family = &#123; &apos;Judith Blunt-Lytton&apos;: [&apos;Anne Isabella Blunt&apos;, &apos;Wilfrid Scawen Blunt&apos;], &apos;Ada King-Milbanke&apos;: [&apos;Ralph King-Milbanke&apos;, &apos;Fanny Heriot&apos;], &apos;Ralph King-Milbanke&apos;: [&apos;Augusta Ada King&apos;, &apos;William King-Noel&apos;], &apos;Anne Isabella Blunt&apos;: [&apos;Augusta Ada King&apos;, &apos;William King-Noel&apos;], &apos;Byron King-Noel&apos;: [&apos;Augusta Ada King&apos;, &apos;William King-Noel&apos;], &apos;Augusta Ada King&apos;: [&apos;Anne Isabella Milbanke&apos;, &apos;George Gordon Byron&apos;], &apos;George Gordon Byron&apos;: [&apos;Catherine Gordon&apos;, &apos;Captain John Byron&apos;], &apos;John Byron&apos;: [&apos;Vice-Admiral John Byron&apos;, &apos;Sophia Trevannion&apos;] &#125;# Define a procedure, ancestors(genealogy, person), that takes as its first input# a Dictionary in the form given above, and as its second input the name of a# person. It should return a list giving all the known ancestors of the input# person (this should be the empty list if there are none). The order of the list# does not matter and duplicates will be ignored.def ancestors(genealogy, person):# Here are some examples:#print ancestors(ada_family, &apos;Augusta Ada King&apos;)#&gt;&gt;&gt; [&apos;Anne Isabella Milbanke&apos;, &apos;George Gordon Byron&apos;,# &apos;Catherine Gordon&apos;,&apos;Captain John Byron&apos;]#print ancestors(ada_family, &apos;Judith Blunt-Lytton&apos;)#&gt;&gt;&gt; [&apos;Anne Isabella Blunt&apos;, &apos;Wilfrid Scawen Blunt&apos;, &apos;Augusta Ada King&apos;,# &apos;William King-Noel&apos;, &apos;Anne Isabella Milbanke&apos;, &apos;George Gordon Byron&apos;,# &apos;Catherine Gordon&apos;, &apos;Captain John Byron&apos;]#print ancestors(ada_family, &apos;Dave&apos;)#&gt;&gt;&gt; [] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/SQ6508of_ZA.mp4 Khayyam Triangle12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Double Gold Star# Khayyam Triangle# The French mathematician, Blaise Pascal, who built a mechanical computer in# the 17th century, studied a pattern of numbers now commonly known in parts of# the world as Pascal&apos;s Triangle (it was also previously studied by many Indian,# Chinese, and Persian mathematicians, and is known by different names in other# parts of the world).# The pattern is shown below:# 1# 1 1# 1 2 1# 1 3 3 1# 1 4 6 4 1# ...# Each number is the sum of the number above it to the left and the number above# it to the right (any missing numbers are counted as 0).# Define a procedure, triangle(n), that takes a number n as its input, and# returns a list of the first n rows in the triangle. Each element of the# returned list should be a list of the numbers at the corresponding row in the# triangle.def triangle(n):#For example:#print triangle(0)#&gt;&gt;&gt; []#print triangle(1)#&gt;&gt;&gt; [[1]]#print triangle(2)#&gt;&gt; [[1], [1, 1]]#print triangle(3)#&gt;&gt;&gt; [[1], [1, 1], [1, 2, 1]]#print triangle(6)#&gt;&gt;&gt; [[1], [1, 1], [1, 2, 1], [1, 3, 3, 1], [1, 4, 6, 4, 1], [1, 5, 10, 10, 5, 1]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/i8X3KHanfXE.mp412345678910111213141516def make_next_row(row): prev=0 result=[] for i in row: result.append(i+prev) prev=i result.append(prev) return resultdef triangle(n): current=[1] result=[] for unuse in range(0,n): result.append(current) current=make_next_row(current) return result Only a Little Lucky123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269# Triple Gold Star# Only A Little Lucky# The Feeling Lucky question (from the regular homework) assumed it was enough# to find the best-ranked page for a given query. For most queries, though, we# don&apos;t just want the best page (according to the page ranking algorithm), we# want a list of many pages that match the query, ordered from the most likely# to be useful to the least likely.# Your goal for this question is to define a procedure, ordered_search(index,# ranks, keyword), that takes the same inputs as lucky_search from Question 5,# but returns an ordered list of all the URLs that match the query.# To order the pages, use the quicksort algorithm, invented by Sir Tony Hoare in# 1959. Quicksort provides a way to sort any list of data, using an expected# number of comparisons that scales as n log n where n is the number of elements# in the list.# The idea of quicksort is quite simple:# If the list has zero or one elements, it is already sorted.# Otherwise, pick a pivot element, and split the list into two partitions: one# contains all the elements equal to or lower than the value of the pivot# element, and the other contains all the elements that are greater than the# pivot element. Recursively sort each of the sub-lists, and then return the# result of concatenating the sorted left sub-list, the pivot element, and the# sorted right sub-list.# For simplicity, use the first element in the list as your pivot element (this# is not usually a good choice, since it means if the input list is already# nearly sorted, the actual work will be much worse than expected).def ordered_search(index, ranks, keyword):cache = &#123; &apos;http://udacity.com/cs101x/urank/index.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Dave&apos;s Cooking Algorithms&lt;/h1&gt;&lt;p&gt;Here are my favorite recipies:&lt;ul&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/hummus.html&quot;&gt;Hummus Recipe&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;World&apos;s Best Hummus&lt;/a&gt;&lt;li&gt; &lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;Kathleen&apos;s Hummus Recipe&lt;/a&gt;&lt;/ul&gt;For more expert opinions, check out the&lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;and &lt;a href=&quot;http://udacity.com/cs101x/urank/zinc.html&quot;&gt;Zinc Chef&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/zinc.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Zinc Chef&lt;/h1&gt;&lt;p&gt;I learned everything I know from&lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;the Nickel Chef&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For great hummus, try&lt;a href=&quot;http://udacity.com/cs101x/urank/arsenic.html&quot;&gt;this recipe&lt;/a&gt;.&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/nickel.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Nickel Chef&lt;/h1&gt;&lt;p&gt;This is the&lt;a href=&quot;http://udacity.com/cs101x/urank/kathleen.html&quot;&gt;best Hummus recipe!&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/kathleen.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Kathleen&apos;s Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Open a can of garbonzo beans.&lt;li&gt; Crush them in a blender.&lt;li&gt; Add 3 tablesppons of tahini sauce.&lt;li&gt; Squeeze in one lemon.&lt;li&gt; Add salt, pepper, and buttercream frosting to taste.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;The Arsenic Chef&apos;s World Famous Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Kidnap the &lt;a href=&quot;http://udacity.com/cs101x/urank/nickel.html&quot;&gt;Nickel Chef&lt;/a&gt;.&lt;li&gt; Force her to make hummus for you.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;, &apos;http://udacity.com/cs101x/urank/hummus.html&apos;: &quot;&quot;&quot;&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hummus Recipe&lt;/h1&gt;&lt;p&gt;&lt;ol&gt;&lt;li&gt; Go to the store and buy a container of hummus.&lt;li&gt; Open it.&lt;/ol&gt;&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;,&#125;def get_page(url): if url in cache: return cache[url] return &quot;&quot;def get_next_target(page): start_link = page.find(&apos;&lt;a href=&apos;) if start_link == -1: return None, 0 start_quote = page.find(&apos;&quot;&apos;, start_link) end_quote = page.find(&apos;&quot;&apos;, start_quote + 1) url = page[start_quote + 1:end_quote] return url, end_quotedef get_all_links(page): links = [] while True: url, endpos = get_next_target(page) if url: links.append(url) page = page[endpos:] else: break return linksdef union(a, b): for e in b: if e not in a: a.append(e)def add_page_to_index(index, url, content): words = content.split() for word in words: add_to_index(index, word, url)def add_to_index(index, keyword, url): if keyword in index: index[keyword].append(url) else: index[keyword] = [url]def lookup(index, keyword): if keyword in index: return index[keyword] else: return Nonedef crawl_web(seed): # returns index, graph of inlinks tocrawl = [seed] crawled = [] graph = &#123;&#125; # &lt;url&gt;, [list of pages it links to] index = &#123;&#125; while tocrawl: page = tocrawl.pop() if page not in crawled: content = get_page(page) add_page_to_index(index, page, content) outlinks = get_all_links(content) graph[page] = outlinks union(tocrawl, outlinks) crawled.append(page) return index, graphdef compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: newrank = newrank + d * (ranks[node] / len(graph[node])) newranks[page] = newrank ranks = newranks return ranks# Here are some example showing what ordered_search should do:# Observe that the result list is sorted so the highest-ranking site is at the# beginning of the list.# Note: the intent of this question is for students to write their own sorting# code, not to use the built-in sort procedure.index, graph = crawl_web(&apos;http://udacity.com/cs101x/urank/index.html&apos;)ranks = compute_ranks(graph)#print ordered_search(index, ranks, &apos;Hummus&apos;)#&gt;&gt;&gt; [&apos;http://udacity.com/cs101x/urank/kathleen.html&apos;,# &apos;http://udacity.com/cs101x/urank/nickel.html&apos;,# &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;,# &apos;http://udacity.com/cs101x/urank/hummus.html&apos;,# &apos;http://udacity.com/cs101x/urank/index.html&apos;]#print ordered_search(index, ranks, &apos;the&apos;)#&gt;&gt;&gt; [&apos;http://udacity.com/cs101x/urank/nickel.html&apos;,# &apos;http://udacity.com/cs101x/urank/arsenic.html&apos;,# &apos;http://udacity.com/cs101x/urank/hummus.html&apos;,# &apos;http://udacity.com/cs101x/urank/index.html&apos;]#print ordered_search(index, ranks, &apos;babaganoush&apos;)#&gt;&gt;&gt; None The provided solution isn’t complete - it doesn’t actually include the ordered_search code, but only the code for sorting the pages. Atlas7-115196 provided a more complete solution to this problem! See the forum post: http://forums.udacity.com/questions/100371211/corrected-udacity-solution#cs101 1234567891011121314151617def quicksort_urls(ranks,urls): if not urls or len(urls)&lt;=1: return urls else: pivot=ranks[urls[0]] before=[] after=[] for url in urls[1:]: if ranks[url]&gt;=pivot: before.append(url) else: after.append(url) return quicksort_urls(ranks,before)+[urls[0]]+quicksort_urls(ranks,after)def ordered_search(index, ranks, keyword): urls=lookup(index, keyword) return quicksort_urls(ranks,urls) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lI9O8wUEDFc.mp4 Q&amp;APythonichttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/2g6qtjwKkA0.mp4Correction - Peter Norvig teaches class CS212. Python Versionshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/owH7bqKiR-g.mp4 Using Recursionhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/VWyHjEh0tfA.mp4 Recursion in Other Languageshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/siNYLJ1YaAc.mp4 Pagerankhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/rN-5K_q4JDc.mp4 Challenges in Searchhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ulkWpQl6izE.mp4 International Charactershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9QbeX7LOl0g.mp4 Past, Present, and Future of ComputerPast, Present, and Futurehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4TChbk9pnxQ.mp4 Themeshttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-6OLwm10pqs.mp4 Overviewhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-atl1N1mvu0.mp4 Computer Sciencehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YBJk5Z5bAEA.mp4 Computer Sciencehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/DIbtX0GqIA8.mp4 Past of Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/vvIj_PWFoyY.mp4 Computer History Museumhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/ClTnWszPp3Q.mp4 Babbage Enginehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5nYcND7WjCY.mp4 First Hard Drivehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oSCCFDZLRgY.mp4 Search Before Computershttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/F2DTZoa-zPo.mp4 Search on the Webhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/mgR9sInLwfc.mp4 Present of Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/9fxDFZGwUiA.mp4 Slac and Big Datahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/4_0sCB_csRI.mp4 Mozillahttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/_IfqKBbEqck.mp4 Open Sourcehttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TUBAD93kCfA.mp4 Getting Involvedhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/56KQGpGOwLM.mp4 Having an Impacthttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/lL36LxpsXBI.mp4 Benetechhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/FqQFwXOJTeU.mp4 Future of Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/EsTiQxNDQfo.mp4 Text Analysishttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/679-n8LWaVo.mp4 Energy Aware Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/wLyAANVyJQM.mp4 Computer Securityhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/H87Yxc4p-C8.mp4 Theory of Computationhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/W7nD3AMJDAI.mp4 Quantum Computinghttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/XafsCK3yk4U.mp4 Stay Udacioushttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/oDxqlHY6V1w.mp4 Cumulative Practice ProblemsPick One12345678910111213141516171819202122232425# Question 1: Pick One# Define a procedure, pick_one, that takes three inputs: a Boolean # and two other values. If the first input is True, it should return # the second input. If the first input is False, it should return the # third input.# For example, pick_one(True, 37, &apos;hello&apos;) should return 37, and# pick_one(False, 37, &apos;hello&apos;) should return &apos;hello&apos;.def pick_one():print pick_one(True, 37, &apos;hello&apos;)#&gt;&gt;&gt; 37print pick_one(False, 37, &apos;hello&apos;)#&gt;&gt;&gt; helloprint pick_one(True, &apos;red pill&apos;, &apos;blue pill&apos;)#&gt;&gt;&gt; red pillprint pick_one(False, &apos;sunny&apos;, &apos;rainy&apos;)#&gt;&gt;&gt; rainy https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/K2s3b4XTaN0.mp412 Triangular Numbers1234567891011121314151617181920212223242526# Question 2. Triangular Numbers# The triangular numbers are the numbers 1, 3, 6, 10, 15, 21, ...# They are calculated as follows.# 1# 1 + 2 = 3# 1 + 2 + 3 = 6# 1 + 2 + 3 + 4 = 10# 1 + 2 + 3 + 4 + 5 = 15# Write a procedure, triangular, that takes as its input a positive # integer n and returns the nth triangular number.def triangular():print triangular(1)#&gt;&gt;&gt;1print triangular(3)#&gt;&gt;&gt; 6print triangular(10)#&gt;&gt;&gt; 55 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/tQplGM4DtTA.mp4 Linear TimeFor the procedures below, check the procedures whose running time scales linearly with the length of the input in the worst case. You may assume the elements in input_list are fairly small numbers. def proc1(input_list): maximum = None for element in input_list : if not maximum or maximum &lt; element: maximum = element return maximum def proc2(input_list): sum = 0 while len(input_list) &gt; 0: sum = sum + input_list[0] # Assume input_list[0] is constant time input_list = input_list[1:] # Assume input_list[1:] is constant time return sum def proc3(input_list): for i in range(0, len(input_list)): for j in range(0, len(input_list)): if input_list[i] == input_list[j] and i != j: return False return True https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/nnNQTjmc3DY.mp4 Remove Tags12345678910111213141516171819202122232425262728# Question 4: Remove Tags# When we add our words to the index, we don&apos;t really want to include# html tags such as &lt;body&gt;, &lt;head&gt;, &lt;table&gt;, &lt;a href=&quot;...&quot;&gt; and so on.# Write a procedure, remove_tags, that takes as input a string and returns# a list of words, in order, with the tags removed. Tags are defined to be# strings surrounded by &lt; &gt;. Words are separated by whitespace or tags. # You may assume the input does not include any unclosed tags, that is, # there will be no &apos;&lt;&apos; without a following &apos;&gt;&apos;.def remove_tags():print remove_tags(&apos;&apos;&apos;&lt;h1&gt;Title&lt;/h1&gt;&lt;p&gt;This is a &lt;a href=&quot;http://www.udacity.com&quot;&gt;link&lt;/a&gt;.&lt;p&gt;&apos;&apos;&apos;)#&gt;&gt;&gt; [&apos;Title&apos;,&apos;This&apos;,&apos;is&apos;,&apos;a&apos;,&apos;link&apos;,&apos;.&apos;]print remove_tags(&apos;&apos;&apos;&lt;table cellpadding=&apos;3&apos;&gt; &lt;tr&gt;&lt;td&gt;Hello&lt;/td&gt;&lt;td&gt;World!&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt;&apos;&apos;&apos;)#&gt;&gt;&gt; [&apos;Hello&apos;,&apos;World!&apos;]print remove_tags(&quot;&lt;hello&gt;&lt;goodbye&gt;&quot;)#&gt;&gt;&gt; []print remove_tags(&quot;This is plain text.&quot;)#&gt;&gt;&gt; [&apos;This&apos;, &apos;is&apos;, &apos;plain&apos;, &apos;text.&apos;] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/5LrTUeawyfI.mp41234567def remove_tags(content): start_pos = content.find(&apos;&lt;&apos;) while start_pos !=-1: end_pos = content.find(&apos;&gt;&apos;,start_pos) content=content[:start_pos]+&apos; &apos;+content[end_pos+1:] start_pos = content.find(&apos;&lt;&apos;) return content.split() Date Converter1234567891011121314151617181920212223242526272829303132333435363738# Question 5: Date Converter# Write a procedure date_converter which takes two inputs. The first is # a dictionary and the second a string. The string is a valid date in # the format month/day/year. The procedure should return# the date written in the form &lt;day&gt; &lt;name of month&gt; &lt;year&gt;.# For example , if the# dictionary is in English,english = &#123;1:&quot;January&quot;, 2:&quot;February&quot;, 3:&quot;March&quot;, 4:&quot;April&quot;, 5:&quot;May&quot;, 6:&quot;June&quot;, 7:&quot;July&quot;, 8:&quot;August&quot;, 9:&quot;September&quot;,10:&quot;October&quot;, 11:&quot;November&quot;, 12:&quot;December&quot;&#125;# then &quot;5/11/2012&quot; should be converted to &quot;11 May 2012&quot;. # If the dictionary is in Swedishswedish = &#123;1:&quot;januari&quot;, 2:&quot;februari&quot;, 3:&quot;mars&quot;, 4:&quot;april&quot;, 5:&quot;maj&quot;, 6:&quot;juni&quot;, 7:&quot;juli&quot;, 8:&quot;augusti&quot;, 9:&quot;september&quot;,10:&quot;oktober&quot;, 11:&quot;november&quot;, 12:&quot;december&quot;&#125;# then &quot;5/11/2012&quot; should be converted to &quot;11 maj 2012&quot;.# Hint: int(&apos;12&apos;) converts the string &apos;12&apos; to the integer 12.def date_converter():print date_converter(english, &apos;5/11/2012&apos;)#&gt;&gt;&gt; 11 May 2012print date_converter(english, &apos;5/11/12&apos;)#&gt;&gt;&gt; 11 May 12print date_converter(swedish, &apos;5/11/2012&apos;)#&gt;&gt;&gt; 11 maj 2012print date_converter(swedish, &apos;12/5/1791&apos;)#&gt;&gt;&gt; 5 december 1791 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/YgW5c4mH19g.mp41234567def date_converter(dic,string_date): start_pos=string_date.find(&apos;/&apos;) end_pos=string_date.find(&apos;/&apos;,start_pos+1) month=int(string_date[:start_pos]) day = string_date[start_pos+1:end_pos] year=string_date[end_pos+1:] return day+&apos; &apos;+dic[month]+&apos; &apos;+year or123def date_converter(dic,string_date): month,day,year=string_date.split(&apos;/&apos;) return day+&apos; &apos;+dic[int(month)]+&apos; &apos;+year TerminationFor each of the procedures defined below, check the box if the procedure always terminates for all inputs that are natural numbers (1,2,3…). def proc1(n): while True: n = n - 1 if n == 0: break return 3 def proc2(n): if n == 0: return n return 1 + proc2(n - 2) def proc3(n): if n &lt;= 3: return 1 return proc3(n - 1) + proc3(n - 2) + proc3(n - 3) https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/RBc-2ZVuH9E.mp4 Find and Replace123456789101112131415161718192021222324252627282930313233343536# Question 7: Find and Replace# For this question you need to define two procedures:# make_converter(match, replacement)# Takes as input two strings and returns a converter. It doesn&apos;t have# to make a specific type of thing. It can # return anything you would find useful in apply_converter.# apply_converter(converter, string)# Takes as input a converter (produced by make_converter), and # a string, and returns the result of applying the converter to the # input string. This replaces all occurrences of the match used to # build the converter, with the replacement. It keeps doing # replacements until there are no more opportunities for replacements.def make_converter(match, replacement):def apply_converter(converter, string):# For example,c1 = make_converter(&apos;aa&apos;, &apos;a&apos;)print apply_converter(c1, &apos;aaaa&apos;)#&gt;&gt;&gt; ac = make_converter(&apos;aba&apos;, &apos;b&apos;)print apply_converter(c, &apos;aaaaaabaaaaa&apos;)#&gt;&gt;&gt; ab# Note that this process is not guaranteed to terminate for all inputs# (for example, apply_converter(make_converter(&apos;a&apos;, &apos;aa&apos;), &apos;a&apos;) would # run forever). https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/-xUQEeXjC8g.mp41234567891011def make_converter(match, replacement): return [match, replacement]def apply_converter(converter, string): previous=None while previous!=string: previous=string position=string.find(converter[0]) if position!=-1: string=string[:position] + converter[1] + string[position+len(converter[0]):] return string Longest Repetition123456789101112131415161718192021222324252627# Question 8: Longest Repetition# Define a procedure, longest_repetition, that takes as input a # list, and returns the element in the list that has the most # consecutive repetitions. If there are multiple elements that # have the same number of longest repetitions, the result should # be the one that appears first. If the input list is empty, # it should return None.def longest_repetition():#For example,print longest_repetition([1, 2, 2, 3, 3, 3, 2, 2, 1])# 3print longest_repetition([&apos;a&apos;, &apos;b&apos;, &apos;b&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;d&apos;, &apos;d&apos;])# bprint longest_repetition([1,2,3,4,5])# 1print longest_repetition([])# None https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/LWSvPPfH9Cw.mp4123456789101112131415def longest_repetition(input_list): best=None length=0 current=None current_length=0 for element in input_list: if current != element: current_length=1 current=element else: current_length+=1 if current_length&gt;length: best=current length=current_length return best Deep Reverse1234567891011121314151617181920212223242526272829303132# Question 9: Deep Reverse# Define a procedure, deep_reverse, that takes as input a list, # and returns a new list that is the deep reverse of the input list. # This means it reverses all the elements in the list, and if any # of those elements are lists themselves, reverses all the elements # in the inner list, all the way down. # Note: The procedure must not change the input list.# The procedure is_list below is from Homework 6. It returns True if # p is a list and False if it is not.def is_list(p): return isinstance(p, list)def deep_reverse():#For example,p = [1, [2, 3, [4, [5, 6]]]]print deep_reverse(p)#&gt;&gt;&gt; [[[[6, 5], 4], 3, 2], 1]print p#&gt;&gt;&gt; [1, [2, 3, [4, [5, 6]]]]q = [1, [2,3], 4, [5,6]]print deep_reverse(q)#&gt;&gt;&gt; [ [6,5], 4, [3, 2], 1]print q#&gt;&gt;&gt; [1, [2,3], 4, [5,6]] https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/aYLkoPSiiG0.mp412345678def deep_reverse(input_list): if is_list(input_list): new_list=[] for i in range(len(input_list)-1,-1,-1): new_list.append(deep_reverse(input_list[i])) return new_list else: return input_list Challenging Practice ProblemsStirling and Bell123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# One Gold Star# Question 1-star: Stirling and Bell Numbers# The number of ways of splitting n items in k non-empty sets is called# the Stirling number, S(n,k), of the second kind. For example, the group # of people Dave, Sarah, Peter and Andy could be split into two groups in # the following ways.# 1. Dave, Sarah, Peter Andy# 2. Dave, Sarah, Andy Peter# 3. Dave, Andy, Peter Sarah# 4. Sarah, Andy, Peter Dave# 5. Dave, Sarah Andy, Peter# 6. Dave, Andy Sarah, Peter# 7. Dave, Peter Andy, Sarah# so S(4,2) = 7# If instead we split the group into one group, we have just one way to # do it.# 1. Dave, Sarah, Peter, Andy# so S(4,1) = 1# or into four groups, there is just one way to do it as well# 1. Dave Sarah Peter Andy# so S(4,4) = 1# If we try to split into more groups than we have people, there are no# ways to do it.# The formula for calculating the Stirling numbers is# S(n, k) = k*S(n-1, k) + S(n-1, k-1)# Furthermore, the Bell number B(n) is the number of ways of splitting n # into any number of parts, that is,# B(n) is the sum of S(n,k) for k =1,2, ... , n.# Write two procedures, stirling and bell. The first procedure, stirling # takes as its inputs two positive integers of which the first is the # number of items and the second is the number of sets into which those # items will be split. The second procedure, bell, takes as input a # positive integer n and returns the Bell number B(n).def stirling(): def bell(): #print stirling(1,1)#&gt;&gt;&gt; 1#print stirling(2,1)#&gt;&gt;&gt; 1#print stirling(2,2)#&gt;&gt;&gt; 1#print stirling(2,3)#&gt;&gt;&gt;0#print stirling(3,1)#&gt;&gt;&gt; 1#print stirling(3,2)#&gt;&gt;&gt; 3#print stirling(3,3)#&gt;&gt;&gt; 1#print stirling(4,1)#&gt;&gt;&gt; 1#print stirling(4,2)#&gt;&gt;&gt; 7#print stirling(4,3)#&gt;&gt;&gt; 6#print stirling(4,4)#&gt;&gt;&gt; 1#print stirling(5,1)#&gt;&gt;&gt; 1#print stirling(5,2)#&gt;&gt;&gt; 15#print stirling(5,3)#&gt;&gt;&gt; 25#print stirling(5,4)#&gt;&gt;&gt; 10#print stirling(5,5)#&gt;&gt;&gt; 1#print stirling(20,15)#&gt;&gt;&gt; 452329200#print bell(1)#&gt;&gt;&gt; 1#print bell(2)#&gt;&gt;&gt; 2#print bell(3)#&gt;&gt;&gt; 5#print bell(4)#&gt;&gt;&gt; 15#print bell(5)#&gt;&gt;&gt; 52#print bell(15)#&gt;&gt;&gt; 1382958545 https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/TJ4M9ZyAZ3I.mp4123456789101112def stirling(n,k): if n&lt;k: return 0 if n==k or k==1: return 1 return k*stirling(n-1, k) + stirling(n-1, k-1)def bell(n): result=0 for i in range(1,n+1): result+=stirling(n,i) return result Combating Link Spam123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# Two Gold Stars# Question 2: Combatting Link Spam# One of the problems with our page ranking system is pages can # collude with each other to improve their page ranks. We consider # A-&gt;B a reciprocal link if there is a link path from B to A of length # equal to or below the collusion level, k. The length of a link path # is the number of links which are taken to travel from one page to the # other.# If k = 0, then a link from A to A is a reciprocal link for node A, # since no links needs to be taken to get from A to A.# If k=1, B-&gt;A would count as a reciprocal link if there is a link # A-&gt;B, which includes one link and so is of length 1. (it requires # two parties, A and B, to collude to increase each others page rank).# If k=2, B-&gt;A would count as a reciprocal link for node A if there is# a path A-&gt;C-&gt;B, for some page C, (link path of length 2),# or a direct link A-&gt; B (link path of length 1).# Modify the compute_ranks code to # - take an extra input k, which is a non-negative integer, and # - exclude reciprocal links of length up to and including k from # helping the page rank.def compute_ranks(graph): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: newrank = newrank + d * (ranks[node]/len(graph[node])) newranks[page] = newrank ranks = newranks return ranks# For exampleg = &#123;&apos;a&apos;: [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], &apos;b&apos;:[&apos;a&apos;], &apos;c&apos;:[&apos;d&apos;], &apos;d&apos;:[&apos;a&apos;]&#125;#print compute_ranks(g, 0) # the a-&gt;a link is reciprocal#&gt;&gt;&gt; &#123;&apos;a&apos;: 0.26676872354238684, &apos;c&apos;: 0.1216391112164609,# &apos;b&apos;: 0.1216391112164609, &apos;d&apos;: 0.1476647842238683&#125;#print compute_ranks(g, 1) # a-&gt;a, a-&gt;b, b-&gt;a links are reciprocal#&gt;&gt;&gt; &#123;&apos;a&apos;: 0.14761759762962962, &apos;c&apos;: 0.08936469270123457,# &apos;b&apos;: 0.04999999999999999, &apos;d&apos;: 0.12202199703703702&#125;#print compute_ranks(g, 2)# a-&gt;a, a-&gt;b, b-&gt;a, a-&gt;c, c-&gt;d, d-&gt;a links are reciprocal# (so all pages end up with the same rank)#&gt;&gt;&gt; &#123;&apos;a&apos;: 0.04999999999999999, &apos;c&apos;: 0.04999999999999999,# &apos;b&apos;: 0.04999999999999999, &apos;d&apos;: 0.04999999999999999&#125; There was a typo in the last test example. # a-&gt;a, a-&gt;b, b-&gt;a, a-&gt;c, c-&gt;d, c-&gt;a links are reciprocal should read # a-&gt;a, a-&gt;b, b-&gt;a, a-&gt;c, c-&gt;d, d-&gt;a links are reciprocal https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/hP-UDkuUL0o.mp412345678910111213141516171819202122232425262728def is_reciprocal_link(graph,source,destination,k): if k==0: return source==destination if source in graph[destination]: return True for node in graph[destination]: if is_reciprocal_link(graph,source,node,k-1): return True return False def compute_ranks(graph,k): d = 0.8 # damping factor numloops = 10 ranks = &#123;&#125; npages = len(graph) for page in graph: ranks[page] = 1.0 / npages for i in range(0, numloops): newranks = &#123;&#125; for page in graph: newrank = (1 - d) / npages for node in graph: if page in graph[node]: # node links to page if not is_reciprocal_link(graph,node,page,k): newrank = newrank + d * (ranks[node]/len(graph[node])) newranks[page] = newrank ranks = newranks return ranks Elementary Cellular Automatonhttps://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/M_pkidxeGMY.mp412345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# THREE GOLD STARS# Question 3-star: Elementary Cellular Automaton# Please see the video for additional explanation.# A one-dimensional cellular automata takes in a string, which in our # case, consists of the characters &apos;.&apos; and &apos;x&apos;, and changes it according # to some predetermined rules. The rules consider three characters, which # are a character at position k and its two neighbours, and determine # what the character at the corresponding position k will be in the new # string.# For example, if the character at position k in the string is &apos;.&apos; and # its neighbours are &apos;.&apos; and &apos;x&apos;, then the pattern is &apos;..x&apos;. We look up # &apos;..x&apos; in the table below. In the table, &apos;..x&apos; corresponds to &apos;x&apos; which # means that in the new string, &apos;x&apos; will be at position k.# Rules:# pattern in position k in contribution to# Value current string new string pattern number# is 0 if replaced by &apos;.&apos;# and value if replaced# by &apos;x&apos;# 1 &apos;...&apos; &apos;.&apos; 1 * 0# 2 &apos;..x&apos; &apos;x&apos; 2 * 1# 4 &apos;.x.&apos; &apos;x&apos; 4 * 1# 8 &apos;.xx&apos; &apos;x&apos; 8 * 1# 16 &apos;x..&apos; &apos;.&apos; 16 * 0# 32 &apos;x.x&apos; &apos;.&apos; 32 * 0# 64 &apos;xx.&apos; &apos;.&apos; 64 * 0# 128 &apos;xxx&apos; &apos;x&apos; 128 * 1# ----------# 142# To calculate the patterns which will have the central character x, work # out the values required to sum to the pattern number. For example,# 32 = 32 so only pattern 32 which is x.x changes the central position to# an x. All the others have a . in the next line.# 23 = 16 + 4 + 2 + 1 which means that &apos;x..&apos;, &apos;.x.&apos;, &apos;..x&apos; and &apos;...&apos; all # lead to an &apos;x&apos; in the next line and the rest have a &apos;.&apos;# For pattern 142, and starting string# ...........x...........# the new strings created will be# ..........xx........... (generations = 1)# .........xx............ (generations = 2)# ........xx............. (generations = 3)# .......xx.............. (generations = 4)# ......xx............... (generations = 5)# .....xx................ (generations = 6)# ....xx................. (generations = 7)# ...xx.................. (generations = 8)# ..xx................... (generations = 9)# .xx.................... (generations = 10)# Note that the first position of the string is next to the last position # in the string.# Define a procedure, cellular_automaton, that takes three inputs: # a non-empty string, # a pattern number which is an integer between 0 and 255 that# represents a set of rules, and # a positive integer, n, which is the number of generations. # The procedure should return a string which is the result of# applying the rules generated by the pattern to the string n times.def cellular_automaton():print cellular_automaton(&apos;.x.x.x.x.&apos;, 17, 2)#&gt;&gt;&gt; xxxxxxx..print cellular_automaton(&apos;.x.x.x.x.&apos;, 249, 3)#&gt;&gt;&gt; .x..x.x.xprint cellular_automaton(&apos;...x....&apos;, 125, 1)#&gt;&gt;&gt; xx.xxxxxprint cellular_automaton(&apos;...x....&apos;, 125, 2)#&gt;&gt;&gt; .xxx....print cellular_automaton(&apos;...x....&apos;, 125, 3)#&gt;&gt;&gt; .x.xxxxxprint cellular_automaton(&apos;...x....&apos;, 125, 4)#&gt;&gt;&gt; xxxx...xprint cellular_automaton(&apos;...x....&apos;, 125, 5)#&gt;&gt;&gt; ...xxx.xprint cellular_automaton(&apos;...x....&apos;, 125, 6)#&gt;&gt;&gt; xx.x.xxxprint cellular_automaton(&apos;...x....&apos;, 125, 7)#&gt;&gt;&gt; .xxxxx..print cellular_automaton(&apos;...x....&apos;, 125, 8)#&gt;&gt;&gt; .x...xxxprint cellular_automaton(&apos;...x....&apos;, 125, 9)#&gt;&gt;&gt; xxxx.x.xprint cellular_automaton(&apos;...x....&apos;, 125, 10)#&gt;&gt;&gt; ...xxxxx Sorry about this. There is a mistake in the video in generation 3 for pattern 30, which makes all the following lines incorrect as well. The corrected output is: ...x.... (input) ..xxx... ( generations = 1) .xx..x.. ( generations = 2) xx.xxxx. ( generations = 3) x..x.... ( generations = 4) xxxxx..x ( generations = 5) .....xxx ( generations = 6) Additional information: Elementary Cellular Automaton at Wolfram’s Mathworld12345678910111213141516171819202122def cellular_automaton(input_string,pattern_number,generation): patterns=&#123;&#125; pattern_list=[&apos;...&apos;,&apos;..x&apos;,&apos;.x.&apos;,&apos;.xx&apos;,&apos;x..&apos;,&apos;x.x&apos; ,&apos;xx.&apos;,&apos;xxx&apos;] n=len(input_string) # build my patterns dictionary for i in range(7,-1,-1): if pattern_number/(2**i)==1: patterns[pattern_list[i]]=&apos;x&apos; pattern_number=pattern_number-2**i else: patterns[pattern_list[i]]=&apos;.&apos; # apply patterns to input_string # with generation times for unuse in range(generation): new_string=&apos;&apos; for i in range(n): pattern=input_string[i-1]+input_string[i]+input_string[(i+1)%n] new_string = new_string + patterns[pattern] input_string=new_string return new_string https://s3.cn-north-1.amazonaws.com.cn/u-vid-hd/Jc1vOOWfQaA.mp4 Code EditorA Place to Try Things Out1234567# Use this to try out anything you like. Use print to display your answer# when you press the &quot;Test Run&quot; button.# Use the &quot;Reset&quot; button to reset the screen a = 1e+9for i in range(1000000): a+=1print a - 1e+9 Project PrepProject DescriptionFinal Project DescriptionCongratulations on making it to the final project! Your job is to take simple text strings like “Alex likes Carla, Ryan, and Priya” and turn them into a social network. To do this, you must complete a number of required procedures, as described on the next screen. You must also create a “make-your-own” procedure. Most of this project will take place inside the browser and most of it will be auto-graded. Feel free to share your final code with your peers in the Discussion Forum for additional feedback. If you have any questions, ask on the Discussion Forum! Gamer’s Network123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264# --------------------------- ## Intro to CS Final Project ## Gaming Social Network ## --------------------------- ### Background# ==========# You and your friend have decided to start a company that hosts a gaming# social network site. Your friend will handle the website creation (they know # what they are doing, having taken our web development class). However, it is # up to you to create a data structure that manages the game-network information # and to define several procedures that operate on the network. ## In a website, the data is stored in a database. In our case, however, all the # information comes in a big string of text. Each pair of sentences in the text # is formatted as follows: # # &lt;user&gt; is connected to &lt;user1&gt;, ..., &lt;userM&gt;.&lt;user&gt; likes to play &lt;game1&gt;, ..., &lt;gameN&gt;.## For example:# # John is connected to Bryant, Debra, Walter.John likes to play The Movie: The Game, The Legend of Corgi, Dinosaur Diner.# # Note that each sentence will be separated from the next by only a period. There will # not be whitespace or new lines between sentences.# # Your friend records the information in that string based on user activity on # the website and gives it to you to manage. You can think of every pair of# sentences as defining a user&apos;s profile.## Consider the data structures that we have used in class - lists, dictionaries,# and combinations of the two (e.g. lists of dictionaries). Pick one that# will allow you to manage the data above and implement the procedures below. # # You may assume that &lt;user&gt; is a unique identifier for a user. For example, there# can be at most one &apos;John&apos; in the network. Furthermore, connections are not # symmetric - if &apos;Bob&apos; is connected to &apos;Alice&apos;, it does not mean that &apos;Alice&apos; is# connected to &apos;Bob&apos;.## Project Description# ====================# Your task is to complete the procedures according to the specifications below# as well as to implement a Make-Your-Own procedure (MYOP). You are encouraged # to define any additional helper procedures that can assist you in accomplishing # a task. You are encouraged to test your code by using print statements and the # Test Run button. # ----------------------------------------------------------------------------- # Example string input. Use it to test your code.example_input=&quot;John is connected to Bryant, Debra, Walter.\John likes to play The Movie: The Game, The Legend of Corgi, Dinosaur Diner.\Bryant is connected to Olive, Ollie, Freda, Mercedes.\Bryant likes to play City Comptroller: The Fiscal Dilemma, Super Mushroom Man.\Mercedes is connected to Walter, Robin, Bryant.\Mercedes likes to play The Legend of Corgi, Pirates in Java Island, Seahorse Adventures.\Olive is connected to John, Ollie.\Olive likes to play The Legend of Corgi, Starfleet Commander.\Debra is connected to Walter, Levi, Jennie, Robin.\Debra likes to play Seven Schemers, Pirates in Java Island, Dwarves and Swords.\Walter is connected to John, Levi, Bryant.\Walter likes to play Seahorse Adventures, Ninja Hamsters, Super Mushroom Man.\Levi is connected to Ollie, John, Walter.\Levi likes to play The Legend of Corgi, Seven Schemers, City Comptroller: The Fiscal Dilemma.\Ollie is connected to Mercedes, Freda, Bryant.\Ollie likes to play Call of Arms, Dwarves and Swords, The Movie: The Game.\Jennie is connected to Levi, John, Freda, Robin.\Jennie likes to play Super Mushroom Man, Dinosaur Diner, Call of Arms.\Robin is connected to Ollie.\Robin likes to play Call of Arms, Dwarves and Swords.\Freda is connected to Olive, John, Debra.\Freda likes to play Starfleet Commander, Ninja Hamsters, Seahorse Adventures.&quot;# ----------------------------------------------------------------------------- # create_data_structure(string_input): # Parses a block of text (such as the one above) and stores relevant # information into a data structure. You are free to choose and design any # data structure you would like to use to manage the information.# # Arguments: # string_input: block of text containing the network information## You may assume that for all the test cases we will use, you will be given the # connections and games liked for all users listed on the right-hand side of an# &apos;is connected to&apos; statement. For example, we will not use the string # &quot;A is connected to B.A likes to play X, Y, Z.C is connected to A.C likes to play X.&quot;# as a test case for create_data_structure because the string does not # list B&apos;s connections or liked games.# # The procedure should be able to handle an empty string (the string &apos;&apos;) as input, in# which case it should return a network with no users.# # Return:# The newly created network data structuredef create_data_structure(string_input): return network# ----------------------------------------------------------------------------- # # Note that the first argument to all procedures below is &apos;network&apos; This is the ## data structure that you created with your create_data_structure procedure, ## though it may be modified as you add new users or new connections. Each ## procedure below will then modify or extract information from &apos;network&apos; # # ----------------------------------------------------------------------------- ## ----------------------------------------------------------------------------- # get_connections(network, user): # Returns a list of all the connections that user has## Arguments: # network: the gamer network data structure# user: a string containing the name of the user# # Return: # A list of all connections the user has.# - If the user has no connections, return an empty list.# - If the user is not in network, return None.def get_connections(network, user): return []# ----------------------------------------------------------------------------- # get_games_liked(network, user): # Returns a list of all the games a user likes## Arguments: # network: the gamer network data structure# user: a string containing the name of the user# # Return: # A list of all games the user likes.# - If the user likes no games, return an empty list.# - If the user is not in network, return None.def get_games_liked(network,user): return []# ----------------------------------------------------------------------------- # add_connection(network, user_A, user_B): # Adds a connection from user_A to user_B. Make sure to check that both users # exist in network.# # Arguments: # network: the gamer network data structure # user_A: a string with the name of the user the connection is from# user_B: a string with the name of the user the connection is to## Return: # The updated network with the new connection added.# - If a connection already exists from user_A to user_B, return network unchanged.# - If user_A or user_B is not in network, return False.def add_connection(network, user_A, user_B): return network# ----------------------------------------------------------------------------- # add_new_user(network, user, games): # Creates a new user profile and adds that user to the network, along with# any game preferences specified in games. Assume that the user has no # connections to begin with.# # Arguments:# network: the gamer network data structure# user: a string containing the name of the user to be added to the network# games: a list of strings containing the user&apos;s favorite games, e.g.:# [&apos;Ninja Hamsters&apos;, &apos;Super Mushroom Man&apos;, &apos;Dinosaur Diner&apos;]## Return: # The updated network with the new user and game preferences added. The new user # should have no connections.# - If the user already exists in network, return network *UNCHANGED* (do not change# the user&apos;s game preferences)def add_new_user(network, user, games): return network # ----------------------------------------------------------------------------- # get_secondary_connections(network, user): # Finds all the secondary connections (i.e. connections of connections) of a # given user.# # Arguments: # network: the gamer network data structure# user: a string containing the name of the user## Return: # A list containing the secondary connections (connections of connections).# - If the user is not in the network, return None.# - If a user has no primary connections to begin with, return an empty list.# # NOTE: # It is OK if a user&apos;s list of secondary connections includes the user # himself/herself. It is also OK if the list contains a user&apos;s primary # connection that is a secondary connection as well.def get_secondary_connections(network, user): return []# ----------------------------------------------------------------------------- # count_common_connections(network, user_A, user_B): # Finds the number of people that user_A and user_B have in common.# # Arguments: # network: the gamer network data structure# user_A: a string containing the name of user_A# user_B: a string containing the name of user_B## Return: # The number of connections in common (as an integer).# - If user_A or user_B is not in network, return False.def count_common_connections(network, user_A, user_B): return 0# ----------------------------------------------------------------------------- # find_path_to_friend(network, user_A, user_B): # Finds a connections path from user_A to user_B. It has to be an existing # path but it DOES NOT have to be the shortest path.# # Arguments:# network: The network you created with create_data_structure. # user_A: String holding the starting username (&quot;Abe&quot;)# user_B: String holding the ending username (&quot;Zed&quot;)# # Return:# A list showing the path from user_A to user_B.# - If such a path does not exist, return None.# - If user_A or user_B is not in network, return None.## Sample output:# &gt;&gt;&gt; print find_path_to_friend(network, &quot;Abe&quot;, &quot;Zed&quot;)# &gt;&gt;&gt; [&apos;Abe&apos;, &apos;Gel&apos;, &apos;Sam&apos;, &apos;Zed&apos;]# This implies that Abe is connected with Gel, who is connected with Sam, # who is connected with Zed.# # NOTE:# You must solve this problem using recursion!# # Hints: # - Be careful how you handle connection loops, for example, A is connected to B. # B is connected to C. C is connected to B. Make sure your code terminates in # that case.# - If you are comfortable with default parameters, you might consider using one # in this procedure to keep track of nodes already visited in your search. You # may safely add default parameters since all calls used in the grading script # will only include the arguments network, user_A, and user_B.def find_path_to_friend(network, user_A, user_B): # your RECURSIVE solution here! return None# Make-Your-Own-Procedure (MYOP)# ----------------------------------------------------------------------------- # Your MYOP should either perform some manipulation of your network data # structure (like add_new_user) or it should perform some valuable analysis of # your network (like path_to_friend). Don&apos;t forget to comment your MYOP. You # may give this procedure any name you want.# Replace this with your own procedure! You can also uncomment the lines below# to see how your code behaves. Have fun!#net = create_data_structure(example_input)#print net#print get_connections(net, &quot;Debra&quot;)#print get_connections(net, &quot;Mercedes&quot;)#print get_games_liked(net, &quot;John&quot;)#print add_connection(net, &quot;John&quot;, &quot;Freda&quot;)#print add_new_user(net, &quot;Debra&quot;, []) #print add_new_user(net, &quot;Nick&quot;, [&quot;Seven Schemers&quot;, &quot;The Movie: The Game&quot;]) # True#print get_secondary_connections(net, &quot;Mercedes&quot;)#print count_common_connections(net, &quot;Mercedes&quot;, &quot;John&quot;)#print find_path_to_friend(net, &quot;John&quot;, &quot;Ollie&quot;)]]></content>
      <categories>
        <category>Udacity</category>
        <category>Intro to CS</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Udacity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blog = Hexo + Github]]></title>
    <url>%2F2017%2F02%2F03%2Fhexo%20blog%2F</url>
    <content type="text"><![CDATA[stay tuned… update in 2017-07-25 comment donate module 2017-02-03 点击Hexo 官网，按要求配置 主题采用屠夫的Maupassant 在 themes\maupassant/_config.yml 中采用fancybox，disqus，self_search以及google_analytics 注释掉rss ,timeline以及 recent_comments self_search中jQuery-based Local Search Engine for Hexo的操作流程 2017-02-04 ipad竖屏响应问题，更改 hexo\themes\maupassant\source\css\style.scss 文件中的12/* middle*/@media print, screen and (max-width: 48em) 将其中的 48em(48*16=768) 改为 47.9em 图片云床采用 七牛云 ，想注册的话，我有邀请码。Google一下，采用吴小龙同學 的方法 美凡提到图片瘦身功能，很赞 采用Page Analytics Chrome Extension进行访问分析 Emoji 2017-02-05Fix post-nav bug, aim to put the previous post link on the left side and the next post on the right side.Firstly check these files. hexo\themes\maupassant\layout\index.jade controls the format of your posts showing in your home page. hexo\themes\maupassant\layout\post.jade determines the format of your post. hexo\themes\maupassant\source\css\style.scss. Through previous 3 files I find that there is something wrong with page.prev and page.next, for instance page.prev points to the post that I post after the current post.Find .post-nav in file style.scss and change the code in class post-nav as follows: 12345678&amp;.pre &#123; float: right; /* left*/ &amp;:after &#123; /* after*/ font-family: &quot;FontAwesome&quot;; content: &quot;\f0da&quot;; /* f0d9*/ padding-right: 0.3em; &#125;&#125; 2017-02-05 Find bug in categories2017-02-07 Find bug in contents Find bug in web tab, when clicking pageAbout 2017-02-21 Want to add Hits beside the dates to the post listed in the home page. 2017-02-22 Change the theme to Next Add LeanCloud for click counter. Theme: Using Next instead of maupassant. Configure sidebar.swiginG:\hexo\themes\next\layout\_macro for removing sidebar in the home page: Adding ifstatement before sidebar-toggle, do not forget the close tag 123456789&#123;% if is_post %&#125; &lt;div class=&quot;sidebar-toggle&quot;&gt; &lt;div class=&quot;sidebar-toggle-line-wrap&quot;&gt; &lt;span class=&quot;sidebar-toggle-line sidebar-toggle-line-first&quot;&gt;&lt;/span&gt; &lt;span class=&quot;sidebar-toggle-line sidebar-toggle-line-middle&quot;&gt;&lt;/span&gt; &lt;span class=&quot;sidebar-toggle-line sidebar-toggle-line-last&quot;&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt;&#123;% endif %&#125; Comment sidebar-nav-overview for not seeing the overview 123&#123;# &lt;li class=&quot;sidebar-nav-overview&quot; data-target=&quot;site-overview&quot;&gt; #&#125; &#123;# &#123;&#123; __(&apos;sidebar.overview&apos;) &#125;&#125; #&#125;&#123;# &lt;/li&gt; #&#125; 2017-03-03 Add new page with Menu.XXXcd g:hexohexo new page friendsFollow this instruction,add friends: friends in hexo\themes\next\languages\default.yml menu: home: Home archives: Archives categories: Categories tags: Tags about: About friends: friends search: Search 2017-3-09 Add emoji to Hexo cd g:hexo 12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --save 12cd node_modules/hexo-renderer-markdown-it/npm i markdown-it-emoji --save Open _config.yml in g:hexo, add these codes at the bottom1234567891011121314151617181920markdown: render: html: true xhtmlOut: false breaks: false linkify: true typographer: true quotes: &apos;“”‘’&apos; plugins: - markdown-it-footnote - markdown-it-sup - markdown-it-sub - markdown-it-abbr - markdown-it-emoji anchors: level: 1 collisionSuffix: &apos;v&apos; permalink: true permalinkClass: header-anchor permalinkSymbol: &apos; &apos; :smile: U+2764 \xE2\x9D\xA4 :smile: U+2764 \xE2\x9D\xA4 emoji-cheat-sheet Reference:Blog 2017-03-16 Add video to Next Need url 123&lt;video width=&quot;480&quot; height=&quot;320&quot; controls&gt;&lt;source src=&quot;&quot;&gt;&lt;/video&gt; Only YouTube 1&#123;% youtube ID %&#125; Only YouTubeRight click YouTube video, choose “copy embed code…” and paste it to your post. Need url 1&lt;iframe width=&quot;870&quot; height=&quot;489&quot; src=&quot;https://www.youtube.com/embed/WHcRQMGSbqg&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt; 2017-03-20 Add center quotebalabalaba balabalaab 2017-05-06 Markdown TableFrom this post and this website 左对齐标题 右对齐标题 居中对齐标题 短文本 中等文本 稍微长一点的文本 稍微长一点的文本 短文本 中等文本 -:表示内容和标题栏居右对齐，:-表示内容和标题栏居左对齐，:-:表示内容和标题栏居中对齐。 2017-05-07 Markdown checkbox(failed)first try cd g:hexo npm install markdown-it-checkbox --saveit appears 123456789hexo-site@0.0.0 G:\hexo`-- markdown-it-checkbox@1.1.0 `-- underscore@1.8.3npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) cd to directory of markdown-it and type npm install markdown-it-checkbox --save it appears1234markdown-it@5.1.0 G:\hexo\node_modules\markdown-it`-- markdown-it-checkbox@1.1.0 `-- underscore@1.8.3 `` checkbox does not appear [x] hello hello (failed)second try12cd node_modules/hexo-renderer-markdown-it/npm un markdown-it-emoji --save cd g:hexo 12npm un hexo-renderer-markdown-it --savenpm i hexo-renderer-marked --save it appears1234567891011121314151617181920npm WARN prefer global marked@0.3.6 should be installed with -ghexo-site@0.0.0 G:\hexo`-- hexo-renderer-marked@0.3.0 `-- marked@0.3.6npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)npm WARN Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN &#123; Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN errno: -4048,npm WARN code: &apos;EPERM&apos;,npm WARN syscall: &apos;lstat&apos;,npm WARN path: &apos;G:\\hexo\\node_modules\\markdown-it&apos; &#125; tpye npm i hexo-renderer-marked --save -g123456789101112131415161718192021222324252627282930&gt; hexo-util@0.6.0 postinstall C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-renderer-marked\node_modules\hexo-util&gt; npm run build:highlight&gt; hexo-util@0.6.0 build:highlight C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-renderer-marked\node_modules\hexo-util&gt; node scripts/build_highlight_alias.js &gt; highlight_alias.jsonC:\Users\SSQ\AppData\Roaming\npm`-- hexo-renderer-marked@0.3.0 +-- hexo-util@0.6.0 | +-- bluebird@3.5.0 | +-- camel-case@3.0.0 | | +-- no-case@2.3.1 | | | `-- lower-case@1.1.4 | | `-- upper-case@1.1.3 | +-- cross-spawn@4.0.2 | | +-- lru-cache@4.0.2 | | | +-- pseudomap@1.0.2 | | | `-- yallist@2.1.2 | | `-- which@1.2.14 | | `-- isexe@2.0.0 | +-- highlight.js@9.11.0 | +-- html-entities@1.2.1 | `-- striptags@2.2.1 +-- marked@0.3.6 +-- object-assign@4.1.1 `-- strip-indent@1.0.1 `-- get-stdin@4.0.1 add following code at the bottom of _config.yml12345678910marked: gfm: true pedantic: false sanitize: false tables: true breaks: true smartLists: true smartypants: true modifyAnchors: &apos;&apos; autolink: true failed (failed)third tryadd &quot;hexo-renderer-marked&quot;: &quot;^0.3.0&quot;, in the G:\hexo\package.jsoncd g:hexonpm i hexo-renderer-marked --savefailed forth trynpm i hexo-renderer-marked -f 2017-05-09 add passwords for blogsadd &quot;hexo-blog-encrypt&quot;: &quot;^1.1.12&quot;, in the G:\hexo\package.jsoncd g:hexonpm i hexo-blog-encrypt --saveit appears123456789101112131415161718hexo-site@0.0.0 G:\hexo`-- hexo-blog-encrypt@1.1.12npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)npm WARN Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN &#123; Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN errno: -4048,npm WARN code: &apos;EPERM&apos;,npm WARN syscall: &apos;lstat&apos;,npm WARN path: &apos;G:\\hexo\\node_modules\\markdown-it&apos; &#125; try npm i hexo-blog-encrypt -f 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141npm WARN using --force I sure hope you know what you are doing.&gt; fsevents@1.1.1 install G:\hexo\node_modules\fsevents&gt; node installhexo-site@0.0.0 G:\hexo+-- hexo@3.2.2| `-- hexo-fs@0.1.6| `-- chokidar@1.6.1| `-- fsevents@1.1.1| `-- node-pre-gyp@0.6.33| +-- mkdirp@0.5.1| | `-- minimist@0.0.8| +-- nopt@3.0.6| | `-- abbrev@1.1.0| +-- npmlog@4.0.2| | +-- are-we-there-yet@1.1.2| | | +-- delegates@1.0.0| | | `-- readable-stream@2.2.2| | +-- console-control-strings@1.1.0| | +-- gauge@2.7.3| | | +-- aproba@1.1.1| | | +-- has-unicode@2.0.1| | | +-- object-assign@4.1.1| | | +-- signal-exit@3.0.2| | | +-- string-width@1.0.2| | | | +-- code-point-at@1.1.0| | | | `-- is-fullwidth-code-point@1.0.0| | | | `-- number-is-nan@1.0.1| | | +-- strip-ansi@3.0.1| | | | `-- ansi-regex@2.1.1| | | `-- wide-align@1.1.0| | `-- set-blocking@2.0.0| +-- rc@1.1.7| | +-- deep-extend@0.4.1| | +-- ini@1.3.4| | +-- minimist@1.2.0| | `-- strip-json-comments@2.0.1| +-- request@2.79.0| | +-- aws-sign2@0.6.0| | +-- aws4@1.6.0| | +-- caseless@0.11.0| | +-- combined-stream@1.0.5| | | `-- delayed-stream@1.0.0| | +-- extend@3.0.0| | +-- forever-agent@0.6.1| | +-- form-data@2.1.2| | | `-- asynckit@0.4.0| | +-- har-validator@2.0.6| | | +-- chalk@1.1.3| | | | +-- ansi-styles@2.2.1| | | | +-- escape-string-regexp@1.0.5| | | | +-- has-ansi@2.0.0| | | | `-- supports-color@2.0.0| | | +-- commander@2.9.0| | | | `-- graceful-readlink@1.0.1| | | +-- is-my-json-valid@2.15.0| | | | +-- generate-function@2.0.0| | | | +-- generate-object-property@1.2.0| | | | | `-- is-property@1.0.2| | | | +-- jsonpointer@4.0.1| | | | `-- xtend@4.0.1| | | `-- pinkie-promise@2.0.1| | | `-- pinkie@2.0.4| | +-- hawk@3.1.3| | | +-- boom@2.10.1| | | +-- cryptiles@2.0.5| | | +-- hoek@2.16.3| | | `-- sntp@1.0.9| | +-- http-signature@1.1.1| | | +-- assert-plus@0.2.0| | | +-- jsprim@1.3.1| | | | +-- extsprintf@1.0.2| | | | +-- json-schema@0.2.3| | | | `-- verror@1.3.6| | | `-- sshpk@1.10.2| | | +-- asn1@0.2.3| | | +-- assert-plus@1.0.0| | | +-- bcrypt-pbkdf@1.0.1| | | +-- dashdash@1.14.1| | | | `-- assert-plus@1.0.0| | | +-- ecc-jsbn@0.1.1| | | +-- getpass@0.1.6| | | | `-- assert-plus@1.0.0| | | +-- jodid25519@1.0.2| | | +-- jsbn@0.1.1| | | `-- tweetnacl@0.14.5| | +-- is-typedarray@1.0.0| | +-- isstream@0.1.2| | +-- json-stringify-safe@5.0.1| | +-- mime-types@2.1.14| | | `-- mime-db@1.26.0| | +-- oauth-sign@0.8.2| | +-- qs@6.3.1| | +-- stringstream@0.0.5| | +-- tough-cookie@2.3.2| | | `-- punycode@1.4.1| | +-- tunnel-agent@0.4.3| | `-- uuid@3.0.1| +-- rimraf@2.5.4| | `-- glob@7.1.1| | +-- fs.realpath@1.0.0| | +-- inflight@1.0.6| | +-- minimatch@3.0.3| | | `-- brace-expansion@1.1.6| | | +-- balanced-match@0.4.2| | | `-- concat-map@0.0.1| | +-- once@1.4.0| | `-- path-is-absolute@1.0.1| +-- semver@5.3.0| +-- tar@2.2.1| | +-- block-stream@0.0.9| | +-- fstream@1.0.10| | | `-- graceful-fs@4.1.11| | `-- inherits@2.0.3| `-- tar-pack@3.3.0| +-- debug@2.2.0| | `-- ms@0.7.1| +-- fstream-ignore@1.0.5| +-- once@1.3.3| | `-- wrappy@1.0.2| +-- readable-stream@2.1.5| | +-- buffer-shims@1.0.0| | +-- core-util-is@1.0.2| | +-- isarray@1.0.0| | +-- process-nextick-args@1.0.7| | +-- string_decoder@0.10.31| | `-- util-deprecate@1.0.2| `-- uid-number@0.0.6`-- hexo-blog-encrypt@1.1.12npm WARN Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN &#123; Error: EPERM: operation not permitted, lstat &apos;G:\hexo\node_modules\markdown-it&apos;npm WARN at Error (native)npm WARN errno: -4048,npm WARN code: &apos;EPERM&apos;,npm WARN syscall: &apos;lstat&apos;,npm WARN path: &apos;G:\\hexo\\node_modules\\markdown-it&apos; &#125; 2017-05-09 restarthttps://xuanwo.org/2015/03/26/hexo-intor/Given the fact that I have installed Node.js and Git in website.So I open cmd with win+r and just type npm install -g hexo-cliIt appears123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124C:\Users\SSQ\AppData\Roaming\npm\hexo -&gt; C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\bin\hexo&gt; dtrace-provider@0.8.1 install C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\node_modules\dtrace-provider&gt; node scripts/install.js&gt; hexo-util@0.6.0 postinstall C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\node_modules\hexo-util&gt; npm run build:highlight&gt; hexo-util@0.6.0 build:highlight C:\Users\SSQ\AppData\Roaming\npm\node_modules\hexo-cli\node_modules\hexo-util&gt; node scripts/build_highlight_alias.js &gt; highlight_alias.jsonC:\Users\SSQ\AppData\Roaming\npm`-- hexo-cli@1.0.2 +-- abbrev@1.1.0 +-- bluebird@3.5.0 +-- chalk@1.1.3 | +-- ansi-styles@2.2.1 | +-- escape-string-regexp@1.0.5 | +-- has-ansi@2.0.0 | | `-- ansi-regex@2.1.1 | +-- strip-ansi@3.0.1 | `-- supports-color@2.0.0 +-- hexo-fs@0.1.6 | +-- chokidar@1.7.0 | | +-- anymatch@1.3.0 | | | +-- arrify@1.0.1 | | | `-- micromatch@2.3.11 | | | +-- arr-diff@2.0.0 | | | | `-- arr-flatten@1.0.3 | | | +-- array-unique@0.2.1 | | | +-- braces@1.8.5 | | | | +-- expand-range@1.8.2 | | | | | `-- fill-range@2.2.3 | | | | | +-- is-number@2.1.0 | | | | | +-- isobject@2.1.0 | | | | | +-- randomatic@1.1.6 | | | | | `-- repeat-string@1.6.1 | | | | +-- preserve@0.2.0 | | | | `-- repeat-element@1.1.2 | | | +-- expand-brackets@0.1.5 | | | | `-- is-posix-bracket@0.1.1 | | | +-- extglob@0.3.2 | | | +-- filename-regex@2.0.1 | | | +-- kind-of@3.2.0 | | | | `-- is-buffer@1.1.5 | | | +-- normalize-path@2.1.1 | | | | `-- remove-trailing-separator@1.0.1 | | | +-- object.omit@2.0.1 | | | | +-- for-own@0.1.5 | | | | | `-- for-in@1.0.2 | | | | `-- is-extendable@0.1.1 | | | +-- parse-glob@3.0.4 | | | | +-- glob-base@0.3.0 | | | | `-- is-dotfile@1.0.2 | | | `-- regex-cache@0.4.3 | | | +-- is-equal-shallow@0.1.3 | | | `-- is-primitive@2.0.0 | | +-- async-each@1.0.1 | | +-- glob-parent@2.0.0 | | +-- inherits@2.0.3 | | +-- is-binary-path@1.0.1 | | | `-- binary-extensions@1.8.0 | | +-- is-glob@2.0.1 | | | `-- is-extglob@1.0.0 | | +-- path-is-absolute@1.0.1 | | `-- readdirp@2.1.0 | | +-- minimatch@3.0.4 | | | `-- brace-expansion@1.1.7 | | | +-- balanced-match@0.4.2 | | | `-- concat-map@0.0.1 | | +-- readable-stream@2.2.9 | | | +-- buffer-shims@1.0.0 | | | +-- core-util-is@1.0.2 | | | +-- isarray@1.0.0 | | | +-- process-nextick-args@1.0.7 | | | +-- string_decoder@1.0.0 | | | `-- util-deprecate@1.0.2 | | `-- set-immediate-shim@1.0.1 | `-- graceful-fs@4.1.11 +-- hexo-log@0.1.2 | `-- bunyan@1.8.10 | +-- dtrace-provider@0.8.1 | | `-- nan@2.6.2 | +-- moment@2.18.1 | +-- mv@2.1.1 | | +-- mkdirp@0.5.1 | | | `-- minimist@0.0.8 | | +-- ncp@2.0.0 | | `-- rimraf@2.4.5 | | `-- glob@6.0.4 | | +-- inflight@1.0.6 | | | `-- wrappy@1.0.2 | | `-- once@1.4.0 | `-- safe-json-stringify@1.0.4 +-- hexo-util@0.6.0 | +-- camel-case@3.0.0 | | +-- no-case@2.3.1 | | | `-- lower-case@1.1.4 | | `-- upper-case@1.1.3 | +-- cross-spawn@4.0.2 | | +-- lru-cache@4.0.2 | | | +-- pseudomap@1.0.2 | | | `-- yallist@2.1.2 | | `-- which@1.2.14 | | `-- isexe@2.0.0 | +-- highlight.js@9.11.0 | +-- html-entities@1.2.1 | `-- striptags@2.2.1 +-- minimist@1.2.0 +-- object-assign@4.1.1 `-- tildify@1.2.0 `-- os-homedir@1.0.2npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\hexo-cli\node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) cd g:hexohexo initit appears1234567xxxxxxnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)INFO Start blogging with Hexo! 2017-05-10 add local searchdetails in this websiteopen git bash and type cd g:hexo npm install hexo-generator-searchdb --save 123456789hexo-site@0.0.0 G:\hexo`-- hexo-generator-searchdb@1.0.7 `-- striptags@3.0.1npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) 2017-05-10 fix tag bug from h1 to h2find tag.swig in G:\hexo_old\hexo\themes\next\layout replace following code123456&lt;div class=&quot;collection-title&quot;&gt; &lt;&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt;&#123;# #&#125;&#123;&#123; page.tag &#125;&#125;&#123;# #&#125;&lt;small&gt;&#123;&#123; __(&apos;title.tag&apos;) &#125;&#125;&lt;/small&gt; &lt;/&#123;% if theme.seo %&#125;h2&#123;% else %&#125;h1&#123;% endif %&#125;&gt;&lt;/div&gt; with123456&lt;div class=&quot;collection-title&quot;&gt; &lt;h2 &gt; &#123;&#123; page.tag &#125;&#125; &lt;small&gt;&#123;&#123; __(&apos;title.tag&apos;) &#125;&#125;&lt;/small&gt; &lt;/h2&gt;&lt;/div&gt; 2017-05-12 note tagFollow this websiteContent (md partial supported) e.g. default primary success info warning danger 2017-05-22 input tag bugsIn the following code1&lt;input type=&quot;radio&quot; disabled&gt;&lt;label&gt;2&lt;/label&gt; &lt;label&gt; must follow the close tag &gt; of input closely, or it will appears bugs in content. 2017-05-25 space bug2017-06-09 Templateradio CONTENT chackbox CONTENT template table1---|--- Restart Hexo Blog open git bash npm install -g hexo-cli run 123hexo init g:hexocd g:hexonpm install run 12git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot; do in _config.yml 1234567# Sitetitle: SSQsubtitle:description:author: SSQlanguage: entimezone: 1234deploy: type: git repo: https://github.com/SSQ/SSQ.github.io.git branch: master run npm install hexo-deployer-git --save run hexo d check my blog in https://ssq.github.io/ in _config.yml change to theme: next copy file to g:hexo/source run npm install hexo-generator-searchdb --saveadd in _config.yml(hexo)12345search: path: search.xml field: post format: html limit: 10000 add in _config.yml(next)123# Local searchlocal_search: enable: true 2017-06-22 bug occurs in this circumstances use following code1&#123;% keyword %&#125; 2017-06-23 Cannot find second page of posts in my websiteproblem solved in this commit 2017-07-13 Export html file with markdownpreivewSublime Text 3 package MarkdownPreview can generate html file when typing ctrl + b.It can also generate content when typing [TOC] at the beginning of your md file. 2017-07-17 Course and Certificate TemplateCourse can be found hereMy certificate can be found hereLecture slides can be found here 2017-07-17 Change Blog Anchor Text Hover Color to BlueFind base.styl in G:\hexo\themes\next\source\css\_variablesChange variables $link-color to the following code1$link-color = $blue where blue is #0684bd 2017-07-17 Change read-more to BlueFind Mist.styl in G:\hexo\themes\next\source\css\_variablesChange variable $read-more-color to #0077b3(blue)123// $read-more-color = $link-color// red-&gt;#DE5233 blue-&gt;#0077b3$read-more-color = #0077b3 2017-07-17 Change code-foreground to redFind base.styl in G:\hexo\themes\next\source\css\_variableschange variable $code-foreground to #DE5233(red) in line 1511$code-foreground = #DE5233 2017-07-23 Add exam module primary True False Yes No 2017-07-25 comment donate modulecomment following code in hexo/themes/next1234# Donate 文章末尾显示打赏按钮# reward_comment: 打赏随意，感谢支持# wechatpay: /images/wechatpay.jpg# alipay: /images/alipay.jpg 2017-08-14 Set fonts in a larger sizecustom.styl123$font-size-base = 18px; // 默认是 16px 大小$font-family-headings = Georgia, sans // 标题，修改成你期望的字体族$font-family-base = &quot;Microsoft YaHei&quot;, Verdana, sans-serif // 修改成你期望的字体族 2017-08-20 Add PA readme model12345678910# GoalImplement Heap application: Median Maintenence# File Description- `.txt` files is data file. - `Median.txt`: undirected weighted graph with 10,000 integers with unsorted order- `.py` file is the solution of Week 2 program assignment - `Median.py` # Algorithm- Median Maintenence Algorithm [Reference](https://github.com/SSQ/Coursera-Stanford-Graph-Search-Shortest-Paths-and-Data-Structures/blob/master/Lecture%20Slides/12.2-slides_algo-ds-heaps-basics_typed.pdf)]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Test blog]]></title>
    <url>%2F2017%2F02%2F02%2Ftest%20blog%2F</url>
    <content type="text"><![CDATA[This is a test blog $\omega_i$]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F02%2F01%2Fhello%20world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>